{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini_api_key=os.getenv(\"google_api_key\")\n",
    "gemini_api_key=os.environ['google_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (0.10.37)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.35 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.10.37)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.9)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.19)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.22)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index) (0.1.4)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.30.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.6.1)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2.31.0)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.7.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: ply in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.11)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.7.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.4.28)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=SimpleDirectoryReader(\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='974f27a3-0363-4c6b-ab57-4075a70ac9d6', embedding=None, metadata={'page_label': '1', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Deep L ea r ni n g\\nI a n G o o d f e l l o w\\nY o s h u a B e n g i o\\nA a r o n C o u r v i l l e', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a42f4473-e5eb-43d0-97c7-a7ab9c150432', embedding=None, metadata={'page_label': '2', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Con ten ts\\nWebsite vii\\nAcknowledgments viii\\nNotation xi\\n1 Introduction 1\\n1.1 WhoShouldReadThisBook? .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0. 8\\n1.2 HistoricalTrendsinDeepLearning .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 11\\nI AppliedMathandMachineLearningBasics 29\\n2 LinearAlgebra 31\\n2.1 Scalars,Vectors,MatricesandTensors .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 31\\n2.2 MultiplyingMatricesandVectors .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. . 34\\n2.3 IdentityandInverseMatrices .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0. 36\\n2.4 LinearDependenceandSpan .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0. 37\\n2.5 Norms .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 39\\n2.6 SpecialKindsofMatricesandVectors .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 40\\n2.7 Eigendecomposition .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 42\\n2.8 SingularValueDecomposition .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0. 44\\n2.9 TheMoore-PenrosePseudoinverse .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. . 45\\n2.10 TheTraceOperator .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 46\\n2.11 TheDeterminant .\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 47\\n2.12 Example: PrincipalComponentsAnalysis .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0. 48\\n3 ProbabilityandInformationTheory 53\\n3.1 WhyProbability? .\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0. 54\\ni', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10c0a155-57bf-4bed-b3ca-2c863552d82e', embedding=None, metadata={'page_label': '3', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\n3.2RandomVariables.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.56\\n3.3ProbabilityDistributions.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.56\\n3.4MarginalProbability.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 58\\n3.5ConditionalProbability.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.59\\n3.6TheChainRuleofConditionalProbabilities.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.59\\n3.7IndependenceandConditionalIndependence.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.60\\n3.8Expectation,VarianceandCovariance.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.60\\n3.9CommonProbabilityDistributions.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.62\\n3.10UsefulPropertiesofCommonFunctions.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.67\\n3.11Bayes’Rule.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.70\\n3.12TechnicalDetailsofContinuousVariables.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 71\\n3.13InformationTheory.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 73\\n3.14StructuredProbabilisticModels.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 75\\n4NumericalComputation 80\\n4.1OverﬂowandUnderﬂow.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.80\\n4.2PoorConditioning .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 82\\n4.3Gradient-BasedOptimization .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.82\\n4.4ConstrainedOptimization .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .93\\n4.5Example:LinearLeastSquares.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.96\\n5MachineLearningBasics 98\\n5.1LearningAlgorithms.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.99\\n5.2Capacity,OverﬁttingandUnderﬁtting.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.110\\n5.3HyperparametersandValidationSets..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.120\\n5.4Estimators,BiasandVariance.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.122\\n5.5MaximumLikelihoodEstimation.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.131\\n5.6BayesianStatistics.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.135\\n5.7SupervisedLearningAlgorithms.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 140\\n5.8UnsupervisedLearningAlgorithms.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.146\\n5.9StochasticGradientDescent.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 151\\n5.10BuildingaMachineLearningAlgorithm.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .153\\n5.11ChallengesMotivatingDeepLearning.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.155\\nIIDeepNetworks:ModernPractices 166\\n6DeepFeedforwardNetworks 168\\n6.1Example:LearningXOR..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.171\\n6.2Gradient-BasedLearning..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.177\\ni i', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6ce8802-d5e2-4d7b-97c0-5de21b891a0d', embedding=None, metadata={'page_label': '4', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\n6.3HiddenUnits.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.191\\n6.4ArchitectureDesign.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .197\\n6.5Back-PropagationandOtherDiﬀerentiationAlgorithms.\\xa0.\\xa0.\\xa0.\\xa0.204\\n6.6HistoricalNotes.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.224\\n7RegularizationforDeepLearning 228\\n7.1ParameterNormPenalties.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 230\\n7.2NormPenaltiesasConstrainedOptimization .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.237\\n7.3RegularizationandUnder-ConstrainedProblems.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.239\\n7.4DatasetAugmentation.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.240\\n7.5NoiseRobustness.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.242\\n7.6Semi-SupervisedLearning.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.243\\n7.7Multi-TaskLearning.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.244\\n7.8EarlyStopping.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.246\\n7.9ParameterTyingandParameterSharing\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 253\\n7.10SparseRepresentations.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.254\\n7.11BaggingandOtherEnsembleMethods..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.256\\n7.12Dropout.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.258\\n7.13AdversarialTraining.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 268\\n7.14TangentDistance,TangentProp,andManifoldTangentClassiﬁer270\\n8OptimizationforTrainingDeepModels 274\\n8.1HowLearningDiﬀersfromPureOptimization .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 275\\n8.2ChallengesinNeuralNetworkOptimization .\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.282\\n8.3BasicAlgorithms.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.294\\n8.4ParameterInitialization Strategies..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.301\\n8.5AlgorithmswithAdaptiveLearningRates.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.306\\n8.6ApproximateSecond-Order Methods.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.310\\n8.7Optimization StrategiesandMeta-Algorithms.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.317\\n9ConvolutionalNetworks 330\\n9.1TheConvolutionOperation.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.331\\n9.2Motivation.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.335\\n9.3Pooling.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.339\\n9.4ConvolutionandPoolingasanInﬁnitelyStrongPrior.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.345\\n9.5VariantsoftheBasicConvolutionFunction.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 347\\n9.6StructuredOutputs..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 358\\n9.7DataTypes.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 360\\n9.8EﬃcientConvolutionAlgorithms.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.362\\n9.9RandomorUnsupervisedFeatures.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .363\\ni i i', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fca174b2-f9aa-4593-815b-dc4c398b7686', embedding=None, metadata={'page_label': '5', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\n9.10TheNeuroscientiﬁcBasisforConvolutionalNetworks.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..364\\n9.11ConvolutionalNetworksandtheHistoryofDeepLearning.\\xa0.\\xa0.\\xa0.371\\n10\\xa0SequenceModeling:RecurrentandRecursiveNets373\\n10.1UnfoldingComputational Graphs.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.375\\n10.2RecurrentNeuralNetworks.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .378\\n10.3BidirectionalRNNs\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.394\\n10.4Encoder-DecoderSequence-to-SequenceArchitectures.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..396\\n10.5DeepRecurrentNetworks.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.398\\n10.6RecursiveNeuralNetworks.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 400\\n10.7TheChallengeofLong-TermDependencies.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.401\\n10.8EchoStateNetworks.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.404\\n10.9LeakyUnitsandOtherStrategiesforMultipleTimeScales.\\xa0.\\xa0..406\\n10.10\\xa0TheLongShort-TermMemoryandOtherGatedRNNs.\\xa0..\\xa0.\\xa0.\\xa0.408\\n10.11\\xa0Optimization forLong-TermDependencies.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.413\\n10.12\\xa0Explicit Memory.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 416\\n11\\xa0PracticalMethodology 421\\n11.1PerformanceMetrics.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.422\\n11.2DefaultBaselineModels.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.425\\n11.3DeterminingWhethertoGatherMoreData.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 426\\n11.4SelectingHyperparameters.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.427\\n11.5DebuggingStrategies.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.436\\n11.6Example:Multi-DigitNumberRecognition.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 440\\n12\\xa0Applications 443\\n12.1Large-ScaleDeepLearning..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.443\\n12.2ComputerVision.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.452\\n12.3SpeechRecognition\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.458\\n12.4NaturalLanguageProcessing.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .461\\n12.5OtherApplications.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .478\\nIIIDeepLearningResearch 486\\n13\\xa0LinearFactorModels 489\\n13.1ProbabilisticPCAandFactorAnalysis.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 490\\n13.2IndependentComponentAnalysis(ICA).\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.491\\n13.3SlowFeatureAnalysis.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .493\\n13.4SparseCoding.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.496\\ni v', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cb5ee616-3647-4a6f-805a-7e242c2abe5e', embedding=None, metadata={'page_label': '6', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\n13.5ManifoldInterpretation ofPCA.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.499\\n14\\xa0Autoencoders 502\\n14.1Undercomplete Autoencoders.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.503\\n14.2RegularizedAutoencoders.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.504\\n14.3RepresentationalPower,LayerSizeandDepth.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.508\\n14.4StochasticEncodersandDecoders.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.509\\n14.5DenoisingAutoencoders.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.510\\n14.6LearningManifoldswithAutoencoders.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 515\\n14.7ContractiveAutoencoders..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.521\\n14.8PredictiveSparseDecomposition.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .523\\n14.9ApplicationsofAutoencoders.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.524\\n15\\xa0RepresentationLearning 526\\n15.1GreedyLayer-WiseUnsupervisedPretraining.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.528\\n15.2TransferLearningandDomainAdaptation.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .536\\n15.3Semi-SupervisedDisentanglingofCausalFactors.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.541\\n15.4DistributedRepresentation.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .546\\n15.5ExponentialGainsfromDepth.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .553\\n15.6ProvidingCluestoDiscoverUnderlyingCauses.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.554\\n16\\xa0StructuredProbabilisticModelsforDeepLearning558\\n16.1TheChallengeofUnstructuredModeling..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.559\\n16.2UsingGraphstoDescribeModelStructure.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.563\\n16.3SamplingfromGraphicalModels.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.580\\n16.4AdvantagesofStructuredModeling\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.582\\n16.5LearningaboutDependencies.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 582\\n16.6InferenceandApproximateInference.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.584\\n16.7TheDeepLearningApproachtoStructuredProbabilisticModels585\\n17\\xa0MonteCarloMethods 590\\n17.1SamplingandMonteCarloMethods.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 590\\n17.2ImportanceSampling.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.592\\n17.3MarkovChainMonteCarloMethods.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.595\\n17.4GibbsSampling\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.599\\n17.5TheChallengeofMixingbetweenSeparatedModes.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..599\\n18\\xa0ConfrontingthePartitionFunction 605\\n18.1TheLog-LikelihoodGradient..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.606\\n18.2StochasticMaximumLikelihoodandContrastiveDivergence.\\xa0.\\xa0.607\\nv', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4aa56479-55c7-4446-a203-2e3bffee5c18', embedding=None, metadata={'page_label': '7', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\n18.3Pseudolikelihood.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.615\\n18.4ScoreMatchingandRatioMatching.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 617\\n18.5DenoisingScoreMatching.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.619\\n18.6Noise-ContrastiveEstimation.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.620\\n18.7EstimatingthePartitionFunction.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.623\\n19\\xa0ApproximateInference 631\\n19.1InferenceasOptimization ..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.633\\n19.2ExpectationMaximization .\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.634\\n19.3MAPInferenceandSparseCoding..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.635\\n19.4VariationalInferenceandLearning.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.638\\n19.5LearnedApproximateInference.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.651\\n20\\xa0DeepGenerativeModels 654\\n20.1BoltzmannMachines.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.654\\n20.2RestrictedBoltzmannMachines.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.656\\n20.3DeepBeliefNetworks..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 660\\n20.4DeepBoltzmannMachines.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.663\\n20.5BoltzmannMachinesforReal-ValuedData.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.676\\n20.6ConvolutionalBoltzmannMachines\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .683\\n20.7BoltzmannMachinesforStructuredorSequentialOutputs.\\xa0.\\xa0.\\xa0.685\\n20.8OtherBoltzmannMachines.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 686\\n20.9Back-PropagationthroughRandomOperations.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.687\\n20.10\\xa0DirectedGenerativeNets.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.692\\n20.11\\xa0DrawingSamplesfromAutoencoders.\\xa0.\\xa0.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.711\\n20.12\\xa0Generativ eStochasticNetworks.\\xa0.\\xa0..\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 714\\n20.13\\xa0OtherGenerationSchemes.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. 716\\n20.14\\xa0EvaluatingGenerativeModels\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.717\\n20.15\\xa0Conclus ion.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.\\xa0. .\\xa0.\\xa0.\\xa0.\\xa0.\\xa0.720\\nBibliography 721\\nIndex 777\\nv i', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db87cbdc-61bf-459b-97f2-c52dd83ec88a', embedding=None, metadata={'page_label': '8', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='W e b s i t e\\nwww.deeplearningb ook.org\\nThisbookisaccompanied bytheabovewebsite.Thewebsiteprovidesa\\nvarietyofsupplementarymaterial,includingexercises,lectureslides,correctionsof\\nmistakes,andotherresourcesthatshouldbeusefultobothreadersandinstructors.\\nvii', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd9f9d0a-fe70-49c0-9a5b-c3433b46c7b2', embedding=None, metadata={'page_label': '9', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Acknowledgments\\nThisbookwouldnothavebeenpossiblewithoutthecontributionsofmanypeople.\\nWewouldliketothankthosewhocommentedonourproposalforthebook\\nandhelpedplanitscontentsandorganization: GuillaumeAlain,KyunghyunCho,\\nÇağlarGülçehre,DavidKrueger,HugoLarochelle,RazvanPascanuandThomas\\nRohée.\\nWewouldliketothankthepeoplewhooﬀeredfeedbackonthecontentofthe\\nbookitself.Someoﬀeredfeedbackonmanychapters:MartínAbadi,Guillaume\\nAlain,IonAndroutsopoulos ,FredBertsch,OlexaBilaniuk,UfukCanBiçici,Matko\\nBošnjak,JohnBoersma,GregBrockman,AlexandredeBrébisson,PierreLuc\\nCarrier,SarathChandar,PawelChilinski,MarkDaoust,OlegDashevskii,Laurent\\nDinh,StephanDreseitl,JimFan,MiaoFan,MeireFortunato,FrédéricFrancis,\\nNando\\xa0deFreitas,Çağlar\\xa0Gülçehre,\\xa0Jurgen\\xa0V anGael,JavierAlonso\\xa0García,\\nJonathanHunt,GopiJeyaram,ChingizKabytayev,LukaszKaiser,VarunKanade,\\nAsifullahKhan,AkielKhan,JohnKing,DiederikP.Kingma,YannLeCun,Rudolf\\nMathey,MatíasMattamala,AbhinavMaurya,KevinMurphy,OlegMürk,Roman\\nNovak,AugustusQ.Odena,SimonPavlik,KarlPichotta,EddiePierce,KariPulli,\\nRousselRahman,TapaniRaiko,AnuragRanjan,JohannesRoith,MihaelaRosca,\\nHalisSak,\\xa0CésarSalgado,GrigorySapunov,YoshinoriSasaki,\\xa0MikeSchuster,\\nJulianSerban,NirShabat,KenShirriﬀ,AndreSimpelo,ScottStanley,David\\nSussillo,IlyaSutskever,CarlesGeladaSáez,GrahamTaylor,ValentinTolmer,\\nMassimilianoTomassoli,AnTran,ShubhenduTrivedi,AlexeyUmnov,Vincent\\nVanhoucke,MarcoVisentini-Scarzanella,MartinVita,DavidWarde-Farley,Dustin\\nWebb,KelvinXu,WeiXue,KeYang,LiYao,ZygmuntZającandOzanÇağlayan.\\nWewouldalsoliketothankthosewhoprovideduswithusefulfeedbackon\\nindividualchapters:\\n•Notation:ZhangYuanhang.\\n•Chapter, :YusufAkgul,SebastienBratieres,SamiraEbrahimi, 1Introduction\\nviii', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b655bb6e-1b47-4c1c-bdd5-402af0e99fa2', embedding=None, metadata={'page_label': '10', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\nCharlieGorichanaz,BrendanLoudermilk,EricMorris,CosminPârvulescu\\nandAlfredoSolano.\\n•Chapter, :AmjadAlmahairi,NikolaBanić,KevinBennett, 2LinearAlgebra\\nPhilippeCastonguay,OscarChang,EricFosler-Lussier,AndreyKhalyavin,\\nSergeyOreshkov,IstvánPetrás,DennisPrangle,ThomasRohée,Gitanjali\\nGulveSehgal,ColbyToland,AlessandroVitaleandBobWelland.\\n•Chapter, :JohnPhilipAnderson,Kai 3ProbabilityandInformationTheory\\nArulkumaran,VincentDumoulin,RuiFa,StephanGouws,ArtemOboturov,\\nAnttiRasmus,AlexeySurkovandVolkerTresp.\\n•Chapter\\xa0,\\xa0 :Tran\\xa0LamAnIan\\xa0Fischer\\xa0andHu 4NumericalComputation\\nYuhuang.\\n•Chapter, :DzmitryBahdanau,JustinDomingue, 5MachineLearningBasics\\nNikhilGarg,MakotoOtsuka,BobPepin,PhilipPopien,EmmanuelRayner,\\nPeterShepard,Kee-BongSong,ZhengSunandAndyWu.\\n•Chapter,6DeepFeedforwardNetworks:UrielBerdugo,FabrizioBottarel,\\nElizabethBurl,IshanDurugkar,JeﬀHlywa,JongWookKim,DavidKrueger\\nandAdityaKumarPraharaj.\\n•Chapter, :MortenKolbæk,KshitijLauria, 7RegularizationforDeepLearning\\nInkyuLee,SunilMohan,HaiPhongPhanandJoshuaSalisbury.\\n•Chapter,8Optimization forTrainingDeepModels:MarcelAckermann,Peter\\nArmitage,RowelAtienza,AndrewBrock,TeganMaharaj,JamesMartens,\\nKashifRasul,KlausStroblandNicholasTurner.\\n•Chapter,9ConvolutionalNetworks:MartínArjovsky,EugeneBrevdo,Kon-\\nstantinDivilov,EricJensen,MehdiMirza,AlexPaino,MarjorieSayer,Ryan\\nStoutandWentaoWu.\\n•Chapter,10SequenceModeling:RecurrentandRecursiveNets:Gökçen\\nEraslan,StevenHickson,RazvanPascanu,LorenzovonRitter,RuiRodrigues,\\nDmitriySerdyuk,DongyuShiandKaiyuYang.\\n•Chapter, :DanielBeckstein. 11PracticalMethodology\\n•Chapter, :GeorgeDahl,VladimirNekrasovandRibana 12Applications\\nRoscher.\\n•Chapter,13LinearFactorModels:JayanthKoushik.\\ni x', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aed51872-6eea-4f2b-863b-1173b3649775', embedding=None, metadata={'page_label': '11', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\n•Chapter, :KunalGhosh. 15RepresentationLearning\\n•Chapter, :\\xa0MinhLê 16StructuredProbabilisticModelsforDeepLearning\\nandAntonVarfolom.\\n•Chapter,18ConfrontingthePartitionFunction:SamBowman.\\n•Chapter, :YujiaBao. 19ApproximateInference\\n•Chapter,20DeepGenerativeModels:NicolasChapados,DanielGalvez,\\nWenmingMa,FadyMedhat,ShakirMohamedandGrégoireMontavon.\\n•Bibliography:LukasMichelbacherandLeslieN.Smith.\\nWealsowanttothankthosewhoallowedustoreproduceimages,ﬁguresor\\ndatafromtheirpublications.Weindicatetheircontributionsintheﬁgurecaptions\\nthroughoutthetext.\\nWewouldliketothankLuWangforwritingpdf2htmlEX,whichweusedto\\nmakethewebversionofthebook,andforoﬀeringsupporttoimprovethequality\\noftheresultingHTML.\\nWe\\xa0would\\xa0liketothank\\xa0Ian’swifeDaniela\\xa0FloriGoodfellowforpatiently\\nsupportingIanduringthewritingofthebookaswellasforhelpwithproofreading.\\nWewouldliketothanktheGoogleBrainteamforprovidinganintellectual\\nenvironmentwhereIancoulddevoteatremendousamountoftimetowritingthis\\nbookandreceivefeedbackandguidancefromcolleagues.Wewouldespeciallylike\\ntothankIan’sformermanager,GregCorrado,andhiscurrentmanager,Samy\\nBengio,fortheirsupportofthisproject.Finally,wewouldliketothankGeoﬀrey\\nHintonforencouragement whenwritingwasdiﬃcult.\\nx', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f683e8c1-109f-469f-8da2-250d0f85c843', embedding=None, metadata={'page_label': '12', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='N ot at i o n\\nThissectionprovidesaconcisereferencedescribingthenotationusedthroughout\\nthisbook.Ifyouareunfamiliarwithanyofthecorrespondingmathematical\\nconcepts,wedescribemostoftheseideasinchapters2–4.\\nNum b e r s and Ar r a y s\\naAscalar(integerorreal)\\naAvector\\nAAmatrix\\nAAtensor\\nI nIdentitymatrixwithrowsandcolumns n n\\nIIdentitymatrixwithdimensionalityimpliedby\\ncontext\\ne( ) iStandardbasisvector[0 , . . . ,0 ,1 ,0 , . . . ,0]witha\\n1atposition i\\ndiag()aAsquare,diagonalmatrixwithdiagonalentries\\ngivenbya\\naAscalarrandomvariable\\naAvector-valuedrandomvariable\\nAAmatrix-valuedrandomvariable\\nxi', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15332a6f-24e7-46c5-8b24-a12027096d2b', embedding=None, metadata={'page_label': '13', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\nSet s and G r aphs\\nAAset\\nRThesetofrealnumbers\\n{}01 ,Thesetcontaining0and1\\n{ } 01 , , . . . , nThesetofallintegersbetweenand0 n\\n[] a , bTherealintervalincludingand a b\\n(] a , bTherealintervalexcludingbutincluding a b\\nA B\\\\Setsubtraction,i.e.,\\xa0thesetcontainingtheele-\\nmentsofthatarenotin A B\\nGAgraph\\nP a G(x i)Theparentsofx iinG\\nI ndexing\\na iElement iofvectora,withindexingstartingat1\\na − iAllelementsofvectorexceptforelementa i\\nA i , jElementofmatrix i , jA\\nA i , :Rowofmatrix iA\\nA : , iColumnofmatrix iA\\nA i , j , kElementofa3-Dtensor ( ) i , j , k A\\nA : : , , i2-Dsliceofa3-Dtensor\\na iElementoftherandomvector i a\\nL i near Al g e br a O p e r at i o ns\\nA\\ue03eTransposeofmatrixA\\nA+Moore-PenrosepseudoinverseofA\\nAB\\ue00cElement-wise(Hadamard)productofandAB\\ndet()ADeterminantofA\\nx i i', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='222949a6-4a1c-43f0-b1a4-689f27939004', embedding=None, metadata={'page_label': '14', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\nCal c ul usd y\\nd xDerivativeofwithrespectto y x\\n∂ y\\n∂ xPartialderivativeofwithrespectto y x\\n∇ x yGradientofwithrespectto y x\\n∇ X yMatrixderivativesofwithrespectto y X\\n∇ X yTensorcontainingderivativesof ywithrespectto\\nX\\n∂ f\\n∂xJacobianmatrixJ∈ Rm n ×of f: Rn→ Rm\\n∇2\\nx f f f () (xorH)()xTheHessianmatrixofatinputpointx\\ue05a\\nf d()xxDeﬁniteintegralovertheentiredomainofx\\n\\ue05a\\nSf d()xx x Deﬁniteintegralwithrespecttoovertheset S\\nP r o babil i t y and I nf o r m at i o n T heor y\\nabTherandomvariablesaandbareindependent ⊥\\nabcTheyareconditionallyindependentgivenc ⊥|\\nP()aAprobabilitydistributionoveradiscretevariable\\np()aAprobabilitydistributionoveracontinuousvari-\\nable,oroveravariablewhosetypehasnotbeen\\nspeciﬁed\\na Randomvariableahasdistribution ∼ P P\\nE x ∼ P[()] () () () f xor E f xExpectationof f xwithrespectto Px\\nVar(()) f xVarianceofunderx f x() P()\\nCov(()()) f x , g xCovarianceofandunderx f x() g x() P()\\nH()xShannonentropyoftherandomvariablex\\nD K L( ) P Q\\ue06bKullback-LeiblerdivergenceofPandQ\\nN(; )xµ ,ΣGaussiandistributionoverxwithmeanµand\\ncovarianceΣ\\nx i i i', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b81408cb-ece3-4bf9-b445-ca40788a8d16', embedding=None, metadata={'page_label': '15', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CO NTE NT S\\nF unc t i o ns\\nf f : A B→Thefunctionwithdomainandrange A B\\nf g f g ◦Compositionofthefunctionsand\\nf(;)xθAfunctionofxparametrized byθ.\\xa0(Sometimes\\nwewrite f(x)andomittheargumentθtolighten\\nnotation)\\nlog x x Naturallogarithmof\\nσ x()Logisticsigmoid,1\\n1+exp()− x\\nζ x x () log(1+exp( Softplus, ))\\n||||x p Lpnormofx\\n||||x L2normofx\\nx+Positivepartof,i.e., x max(0) , x\\n1 c o ndi t i o nis1iftheconditionistrue,0otherwise\\nSometimesweuseafunction fwhoseargumentisascalarbutapplyittoa\\nvector,matrix,ortensor: f(x), f(X),or f( X).Thisdenotestheapplicationof f\\ntothearrayelement-wise. Forexample,if C= σ( X),then C i , j , k= σ( X i , j , k)forall\\nvalidvaluesof,and. i j k\\nD at aset s and D i st r i but i o n s\\np da t aThedatageneratingdistribution\\nˆ p da t aTheempiricaldistributiondeﬁnedbythetraining\\nset\\nXAsetoftrainingexamples\\nx( ) iThe-thexample(input)fromadataset i\\ny( ) iory( ) iThetargetassociatedwithx( ) iforsupervisedlearn-\\ning\\nXThe m n×matrixwithinputexamplex( ) iinrow\\nX i , :\\nx i v', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6221cb11-c2a0-4e82-a419-3ee8c3fb7514', embedding=None, metadata={'page_label': '16', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1\\nI n t ro d u ct i on\\nInventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates\\nbacktoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,\\nDaedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and\\nGalatea,Talos,andPandoramayallberegardedasartiﬁciallife( , OvidandMartin\\n2004Sparkes1996Tandy1997 ;,;,).\\nWhenprogrammable computerswereﬁrstconceived,peoplewonderedwhether\\nsuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas\\nbuilt(Lovelace1842,).Today, ar t i ﬁc i al i n t e l l i g e nc e(AI)isathrivingﬁeldwith\\nmanypracticalapplicationsandactiveresearchtopics.Welooktointelligent\\nsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnoses\\ninmedicineandsupportbasicscientiﬁcresearch.\\nIntheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolved\\nproblemsthatareintellectually diﬃcultforhumanbeingsbutrelativelystraight-\\nforwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-\\nematicalrules.\\xa0Thetruechallengetoartiﬁcialintelligenceprovedtobesolving\\nthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe\\nformally—probl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing\\nspokenwordsorfacesinimages.\\nThisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis\\ntoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa\\nhierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimpler\\nconcepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed\\nforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer\\nneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts\\nbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese\\n1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e739c1fb-3b49-4143-a0d6-0b2bc51800e4', embedding=None, metadata={'page_label': '17', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nconceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For\\nthisreason,wecallthisapproachtoAI . deep l e ar ni ng\\nManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal\\nenvironmentsanddidnotrequirecomputerstohavemuchknowledgeabout\\ntheworld.Forexample,IBM’sDeepBluechess-playingsystemdefeatedworld\\nchampionGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002\\nworld,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove\\ninonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis\\xa0a\\ntremendousaccomplishment,\\xa0butthechallengeisnotduetothediﬃcultyof\\ndescribingthesetofchesspiecesandallowablemovestothecomputer.Chess\\ncanbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily\\nprovidedaheadoftimebytheprogrammer.\\nIronically,abstractandformaltasksthatareamongthemostdiﬃcultmental\\nundertakings forahumanbeingareamongtheeasiestforacomputer.Computers\\nhavelongbeenabletodefeateventhebesthumanchessplayer,butareonly\\nrecentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects\\norspeech.Aperson’severydayliferequiresanimmenseamountofknowledge\\nabouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore\\ndiﬃculttoarticulateinaformalway.Computersneedtocapturethissame\\nknowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin\\nartiﬁcialintelligenceishowtogetthisinformalknowledgeintoacomputer.\\nSeveralartiﬁcialintelligenceprojectshavesoughttohard-codeknowledgeabout\\ntheworldinformallanguages.Acomputercanreasonaboutstatementsinthese\\nformallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe\\nk no wl e dge baseapproachtoartiﬁcialintelligence.Noneoftheseprojectshasled\\ntoamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha\\n1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage\\ncalledCycL.Thesestatementsareenteredbyastaﬀofhumansupervisors.Itisan\\nunwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity\\ntoaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory\\naboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992\\nenginedetectedaninconsistencyinthestory:\\xa0itknewthatpeopledonothave\\nelectricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe\\nentity“FredWhileShaving”containedelectricalparts.Itthereforeaskedwhether\\nFredwasstillapersonwhilehewasshaving.\\nThediﬃcultiesfacedbysystemsrelyingonhard-codedknowledgesuggest\\nthatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting\\npatternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ace16fb-a357-4a47-ab1f-9ee87a219a66', embedding=None, metadata={'page_label': '18', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nintroductionofmachinelearningallowedcomputerstotackleproblemsinvolving\\nknowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple\\nmachinelearningalgorithmcalled l o g i st i c r e g r e ssi o ncandeterminewhetherto\\nrecommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning\\nalgorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail.\\nTheperformanceofthesesimplemachinelearningalgorithmsdependsheavily\\nonthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic\\nregressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine\\nthepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant\\ninformation, suchasthepresenceorabsenceofauterinescar.Eachpieceof\\ninformationincludedintherepresentationofthepatientisknownasa f e at ur e.\\nLogisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith\\nvariousoutcomes.However,itcannotinﬂuencethewaythatthefeaturesare\\ndeﬁnedinanyway.\\xa0IflogisticregressionwasgivenanMRIscanofthepatient,\\nratherthanthedoctor’sformalizedreport,itwouldnotbeabletomakeuseful\\npredictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany\\ncomplications thatmightoccurduringdelivery.\\nThisdependenceonrepresentationsisageneralphenomenon thatappears\\nthroughoutcomputerscienceandevendailylife.Incomputerscience,opera-\\ntionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif\\nthecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform\\narithmeticonArabicnumerals,butﬁndarithmeticonRomannumeralsmuch\\nmoretime-consuming. Itisnotsurprisingthatthechoiceofrepresentationhasan\\nenormouseﬀectontheperformanceofmachinelearningalgorithms.Forasimple\\nvisualexample,seeﬁgure.1.1\\nManyartiﬁcialintelligencetaskscanbesolvedbydesigningtherightsetof\\nfeaturestoextractforthattask,thenprovidingthesefeaturestoasimplemachine\\nlearningalgorithm.Forexample,ausefulfeatureforspeakeridentiﬁcationfrom\\nsoundisanestimateofthesizeofspeaker’svocaltract.Itthereforegivesastrong\\nclueastowhetherthespeakerisaman,woman,orchild.\\nHowever,formanytasks,itisdiﬃculttoknowwhatfeaturesshouldbeextracted.\\nForexample,supposethatwewouldliketowriteaprogramtodetectcarsin\\nphotographs. Weknowthatcarshavewheels,sowemightliketousethepresence\\nofawheelasafeature.Unfortunately,itisdiﬃculttodescribeexactlywhata\\nwheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut\\nitsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoﬀ\\nthemetalpartsofthewheel,thefenderofthecaroranobjectintheforeground\\nobscuringpartofthewheel,andsoon.\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ce01295-24d1-4613-b596-a232dc1bafcc', embedding=None, metadata={'page_label': '19', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\n\\ue078\\ue079\\ue043 \\ue061 \\ue072 \\ue074 \\ue065 \\ue073\\ue069\\ue061\\ue06e\\ue020 \\ue063 \\ue06f \\ue06f \\ue072 \\ue064 \\ue069 \\ue06e \\ue061 \\ue074 \\ue065 \\ue073\\n\\ue072\\ue0b5\\ue050 \\ue06f \\ue06c \\ue061 \\ue072 \\ue020 \\ue063 \\ue06f \\ue06f \\ue072 \\ue064 \\ue069 \\ue06e \\ue061 \\ue074 \\ue065 \\ue073\\nFigure1.1:Exampleofdiﬀerentrepresentations:supposewewanttoseparatetwo\\ncategoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft,\\nwerepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot\\nontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto\\nsolvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley.\\nOnesolutiontothisproblemistousemachinelearningtodiscovernotonly\\nthemappingfromrepresentationtooutputbutalsotherepresentationitself.\\nThisapproachisknownas r e pr e se n t at i o n l e ar ni ng.\\xa0Learnedrepresentations\\noftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed\\nrepresentations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with\\nminimalhumanintervention.Arepresentationlearningalgorithmcandiscovera\\ngoodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto\\nmonths.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof\\nhumantimeandeﬀort;itcantakedecadesforanentirecommunityofresearchers.\\nThequintessentialexampleofarepresentationlearningalgorithmisthe au-\\nt o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat\\nconvertstheinputdataintoadiﬀerentrepresentation,anda dec o derfunction\\nthatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders\\naretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun\\nthroughtheencoderandthenthedecoder,butarealsotrainedtomakethenew\\nrepresentationhavevariousniceproperties.Diﬀerentkindsofautoencodersaimto\\nachievediﬀerentkindsofproperties.\\nWhendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually\\ntoseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis\\ncontext,weusetheword“factors”simplytorefertoseparatesourcesofinﬂuence;\\nthefactorsareusuallynotcombinedbymultiplication. Suchfactorsareoftennot\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61e93b7e-d3b8-4e7d-9239-c299f02853f3', embedding=None, metadata={'page_label': '20', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nquantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved\\nobjectsorunobservedforcesinthephysicalworldthataﬀectobservablequantities.\\nTheymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying\\nexplanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas\\nconceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata.\\nWhenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker’s\\nage,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing\\nanimageofacar,thefactorsofvariationincludethepositionofthecar,itscolor,\\nandtheangleandbrightnessofthesun.\\nAmajorsourceofdiﬃcultyinmanyreal-worldartiﬁcialintelligenceapplications\\nisthatmanyofthefactorsofvariationinﬂuenceeverysinglepieceofdataweare\\nabletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose\\ntoblackatnight.Theshapeofthecar’ssilhouettedependsontheviewingangle.\\nMostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e\\nonesthatwedonotcareabout.\\nOfcourse,itcanbeverydiﬃculttoextractsuchhigh-level,abstractfeatures\\nfromrawdata.Manyofthesefactorsofvariation,suchasaspeaker’saccent,\\ncanbeidentiﬁedonlyusingsophisticated,nearlyhuman-levelunderstandingof\\nthedata.Whenitisnearlyasdiﬃculttoobtainarepresentationastosolvethe\\noriginalproblem,representationlearningdoesnot,atﬁrstglance,seemtohelpus.\\nD e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro-\\nducingrepresentationsthatareexpressedintermsofother,simplerrepresentations.\\nDeeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon-\\ncepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2\\nanimageofapersonbycombiningsimplerconcepts,suchascornersandcontours,\\nwhichareinturndeﬁnedintermsofedges.\\nThequintessentialexampleofadeeplearningmodelisthefeedforwarddeep\\nnetworkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta\\nmathematical functionmappingsomesetofinputvaluestooutputvalues.The\\nfunctionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach\\napplicationofadiﬀerentmathematical functionasprovidinganewrepresentation\\noftheinput.\\nTheideaoflearningtherightrepresentationforthedataprovidesoneperspec-\\ntiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe\\ncomputertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation\\ncanbethoughtofasthestateofthecomputer’smemoryafterexecutinganother\\nsetofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore\\ninstructionsinsequence.Sequentialinstructionsoﬀergreatpowerbecauselater\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d2578a8f-c089-4466-ac0d-28821409c53f', embedding=None, metadata={'page_label': '21', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nVisible\\xa0layer\\n(input\\xa0pixels)1st\\xa0hidden\\xa0layer\\n(edges)2nd\\xa0hidden\\xa0layer\\n(corners\\xa0and\\ncontours)3rd\\xa0hidden\\xa0layer\\n(object\\xa0parts)CARPERSONANIMALOutput\\n(object\\xa0identity)\\nFigure1.2:Illustrationofadeeplearningmodel.Itisdiﬃcultforacomputertounderstand\\nthemeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection\\nofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery\\ncomplicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly.\\nDeeplearningresolvesthisdiﬃcultybybreakingthedesiredcomplicatedmappingintoa\\nseriesofnestedsimplemappings,eachdescribedbyadiﬀerentlayerofthemodel.The\\ninputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat\\nweareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract\\nfeaturesfromtheimage.Theselayersarecalled“hidden”becausetheirvaluesarenotgiven\\ninthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining\\ntherelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind\\noffeaturerepresentedbyeachhiddenunit.Giventhepixels,theﬁrstlayercaneasily\\nidentifyedges,bycomparingthebrightnessofneighboringpixels.Giventheﬁrsthidden\\nlayer’sdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand\\nextendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden\\nlayer’sdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer\\ncandetectentirepartsofspeciﬁcobjects,byﬁndingspeciﬁccollectionsofcontoursand\\ncorners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan\\nbeusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission\\nfromZeilerandFergus2014().\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5e2c7716-d55c-482d-80bb-2294c3a539ff', embedding=None, metadata={'page_label': '22', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nx 1 x 1σ\\nw 1 w 1×\\nx 2 x 2 w 2 w 2×+El e me n t\\nS e t\\n+\\n×\\nσ\\nxx wwEl e me n t\\nS e t\\nL ogi s t i c\\nR e gr e s s i onL ogi s t i c\\nR e gr e s s i on\\nFigure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere\\neachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto\\noutputbutdependsonthedeﬁnitionofwhatconstitutesapossiblecomputationalstep.\\nThecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel,\\nσ ( wTx ),whereσisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand\\nlogisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth\\nthree.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone.\\ninstructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis\\nviewofdeeplearning,notalloftheinformationinalayer’sactivationsnecessarily\\nencodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores\\nstateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput.\\nThisstateinformationcouldbeanalogoustoacounterorpointerinatraditional\\ncomputerprogram.Ithasnothingtodowiththecontentoftheinputspeciﬁcally,\\nbutithelpsthemodeltoorganizeitsprocessing.\\nTherearetwomainwaysofmeasuringthedepthofamodel.Theﬁrstviewis\\nbasedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate\\nthearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough\\naﬂowchartthatdescribeshowtocomputeeachofthemodel’soutputsgiven\\nitsinputs.Justastwoequivalentcomputerprogramswillhavediﬀerentlengths\\ndependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay\\nbedrawnasaﬂowchartwithdiﬀerentdepthsdependingonwhichfunctionswe\\nallowtobeusedasindividualstepsintheﬂowchart.Figureillustrateshowthis 1.3\\nchoiceoflanguagecangivetwodiﬀerentmeasurementsforthesamearchitecture.\\nAnotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa\\nmodelasbeingnotthedepthofthecomputational graphbutthedepthofthe\\ngraphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10130209-4cf2-4e15-891e-7b87bff700d8', embedding=None, metadata={'page_label': '23', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\noftheﬂowchartofthecomputations neededtocomputetherepresentationof\\neachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves.\\nThisisbecausethesystem’sunderstandingofthesimplerconceptscanbereﬁned\\ngiveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem\\nobservinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye.\\nAfterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably\\npresentaswell.\\xa0Inthiscase,thegraphofconceptsonlyincludestwolayers—a\\nlayerforeyesandalayerforfaces—butthegraphofcomputations includes 2n\\nlayersifwereﬁneourestimateofeachconceptgiventheothertimes. n\\nBecauseitisnotalwaysclearwhichofthesetwoviews—thedepthofthe\\ncomputational graph,orthedepthoftheprobabilisticmodelinggraph—ismost\\nrelevant,andbecausediﬀerentpeoplechoosediﬀerentsetsofsmallestelements\\nfromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe\\ndepthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof\\nacomputerprogram.\\xa0Nor isthereaconsensusabouthowmuchdepthamodel\\nrequirestoqualifyas“deep.”However,deeplearningcansafelyberegardedasthe\\nstudyofmodelsthateitherinvolveagreateramountofcompositionoflearned\\nfunctionsorlearnedconceptsthantraditionalmachinelearningdoes.\\nTosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI.\\nSpeciﬁcally,itisatypeofmachinelearning,atechniquethatallowscomputer\\nsystemstoimprovewithexperienceanddata.\\xa0Accordingtotheauthorsofthis\\nbook,machinelearningistheonlyviableapproachtobuildingAIsystemsthat\\ncanoperateincomplicated,real-worldenvironments.Deeplearningisaparticular\\nkindofmachinelearningthatachievesgreatpowerandﬂexibilitybylearningto\\nrepresenttheworldasanestedhierarchyofconcepts,witheachconceptdeﬁnedin\\nrelationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms\\noflessabstractones.Figureillustratestherelationshipbetweenthesediﬀerent 1.4\\nAIdisciplines.Figuregivesahigh-levelschematicofhoweachworks. 1.5\\n1. 1 Wh o S h ou l d R ead T h i s Bo ok ?\\nThisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain\\ntargetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents\\n(undergraduate orgraduate)learningaboutmachinelearning,includingthosewho\\narebeginningacareerindeeplearningandartiﬁcialintelligenceresearch.The\\nothertargetaudienceissoftwareengineerswhodonothaveamachinelearning\\norstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep\\nlearningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da584805-ef74-49c3-92dd-9028331ed04e', embedding=None, metadata={'page_label': '24', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nAIMachine\\xa0learningRepresentation\\xa0learningDeep\\xa0learning\\nExample:\\nKnowledge\\nbasesExample:\\nLogistic\\nregressionExample:\\nShallow\\nautoencoders Example:\\nMLPs\\nFigure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning,\\nwhichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches\\ntoAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology.\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8800587f-fe2d-45b3-aca4-a903d647d7ad', embedding=None, metadata={'page_label': '25', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nInputHand-\\ndesigned\\xa0\\nprogramOutput\\nInputHand-\\ndesigned\\xa0\\nfeaturesMapping\\xa0from\\xa0\\nfeaturesOutput\\nInputFeaturesMapping\\xa0from\\xa0\\nfeaturesOutput\\nInputSimple\\xa0\\nfeaturesMapping\\xa0from\\xa0\\nfeaturesOutput\\nAdditional\\xa0\\nlayers\\xa0of\\xa0more\\xa0\\nabstract\\xa0\\nfeatures\\nRule-based\\nsystemsClassic\\nmachine\\nlearning Representation\\nlearningDeep\\nlearning\\nFigure1.5:\\xa0FlowchartsshowinghowthediﬀerentpartsofanAIsystemrelatetoeach\\notherwithindiﬀerentAIdisciplines.Shadedboxesindicatecomponentsthatareableto\\nlearnfromdata.\\n1 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c4180da2-eb5c-418d-b761-f3b60ef7c523', embedding=None, metadata={'page_label': '26', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nmanysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing,\\nnaturallanguageprocessing,robotics,bioinformatics andchemistry,videogames,\\nsearchengines,onlineadvertisingandﬁnance.\\nThisbookhasbeenorganizedintothreepartsinordertobestaccommodatea\\nvarietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I\\nconcepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II\\nessentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III\\nwidelybelievedtobeimportantforfutureresearchindeeplearning.\\nReadersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests\\norbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental\\nmachinelearningconceptscanskippart,forexample,whilereaderswhojustwant I\\ntoimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II\\nchapterstoread,ﬁgureprovidesaﬂowchartshowingthehigh-levelorganization 1.6\\nofthebook.\\nWedoassumethatallreaderscomefromacomputersciencebackground. We\\nassumefamiliaritywithprogramming, abasicunderstandingofcomputational\\nperformanceissues,complexitytheory,introductory levelcalculusandsomeofthe\\nterminologyofgraphtheory.\\n1. 2 Hi s t or i c a l T ren d s i n D eep L earni n g\\nItiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan\\nprovidingadetailedhistoryofdeeplearning,weidentifyafewkeytrends:\\n•Deeplearninghashadalongandrichhistory,buthasgonebymanynames\\nreﬂectingdiﬀerentphilosophicalviewpoints,andhaswaxedandwanedin\\npopularity.\\n•Deeplearninghasbecomemoreusefulastheamountofavailabletraining\\ndatahasincreased.\\n•Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure\\n(bothhardwareandsoftware)fordeeplearninghasimproved.\\n•Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing\\naccuracyovertime.\\n1 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='59902c45-06e0-4dd0-ad90-6bdabd07455e', embedding=None, metadata={'page_label': '27', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1. INTRODUCTION\\n1. Introduction\\nPart\\xa0I:\\xa0Applied\\xa0Math\\xa0and\\xa0Machine\\xa0Learning\\xa0Basics\\n2. Linear\\xa0Algebra3. Probability\\xa0and\\nInformation\\xa0Theory\\n4. Numerical\\nComputation5. Machine\\xa0Learning\\nBasics\\nPart\\xa0II:\\xa0Deep\\xa0Networks:\\xa0Modern\\xa0Practices\\n6. Deep\\xa0Feedforwar d\\nNetworks\\n7. Regularization 8. Optimization 9. CNNs 10. RNNs\\n11. Practical\\nMethodology12. Applications\\nPart\\xa0III:\\xa0Deep\\xa0Learning\\xa0Research\\n13. Linear\\xa0Factor\\nModels14. Autoencoders15. Representation\\nLearning\\n16. Structured\\nProbabilistic\\xa0Models17. Monte\\xa0Carlo\\nMethods\\n18. Partition\\nFunction19. Inference\\n20. Deep\\xa0Generative\\nModels\\nFigure1.6: Thehigh-levelorganizationofthebook. Anarrowfromonechaptertoanother\\nindicates that the former chapteris prerequisite material for understanding the latter.\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a44ad0b-b34f-4d5c-a9ca-b87578d28c71', embedding=None, metadata={'page_label': '28', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\n1 . 2 . 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net -\\nw o rks\\nWeexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan\\nexcitingnewtechnology,andaresurprisedtoseeamentionof“history”inabook\\naboutanemergingﬁeld.Infact,deeplearningdatesbacktothe1940s.Deep\\nlearningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral\\nyearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany\\ndiﬀerentnames,andhasonlyrecentlybecomecalled“deeplearning.”Theﬁeld\\nhasbeenrebrandedmanytimes,reﬂectingtheinﬂuenceofdiﬀerentresearchers\\nanddiﬀerentperspectives.\\nAcomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook.\\nHowever,somebasiccontextisusefulforunderstandingdeeplearning.Broadly\\nspeaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep\\nlearning\\xa0known\\xa0as c y b e r net i c sin\\xa0the\\xa01940s–1960s,\\xa0deep\\xa0learning\\xa0knownas\\nc o nnec t i o n i s minthe1980s–1990s,andthecurrentresurgenceunderthename\\ndeeplearningbeginningin2006.Thisisquantitativelyillustratedinﬁgure.1.7\\nSomeoftheearliestlearningalgorithmswerecognizetodaywereintended\\ntobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning\\nhappensorcouldhappeninthebrain.\\xa0Asaresult,oneofthenamesthatdeep\\nlearninghasgonebyis ar t i ﬁc i al neur al net w o r k s(ANNs).Thecorresponding\\nperspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired\\nbythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal).\\nWhilethekindsofneuralnetworksusedformachinelearninghavesometimes\\nbeenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991\\ngenerallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural\\nperspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat\\nthebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda\\nconceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe\\ncomputational principlesbehindthebrainandduplicateitsfunctionality.Another\\nperspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe\\nprinciplesthatunderliehumanintelligence,somachinelearningmodelsthatshed\\nlightonthesebasicscientiﬁcquestionsareusefulapartfromtheirabilitytosolve\\nengineeringapplications.\\nThemodernterm“deeplearning”goesbeyondtheneuroscientiﬁcperspective\\nonthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral\\nprincipleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine\\nlearningframeworksthatarenotnecessarilyneurallyinspired.\\n1 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='984c6bb7-42a8-4c8a-b7ee-89f2bd5c6205', embedding=None, metadata={'page_label': '29', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\n1940 1950 1960 1970 1980 1990 2000\\nYear0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase\\nc y b e r n e t i c s\\n( c o n n e c t i o n i s m + n e u r a l n e t w o r k s )\\nFigure1.7:Theﬁgureshowstwoofthethreehistoricalwavesofartiﬁcialneuralnets\\nresearch,asmeasuredbythefrequencyofthephrases“cybernetics”and“connectionism”or\\n“neuralnetworks”accordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The\\nﬁrstwavestartedwithcyberneticsinthe1940s–1960s, withthedevelopmentoftheories\\nofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949\\ntheﬁrstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle\\nneuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980–1995period,\\nwithback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a\\nhiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton\\ne t a l . e t a l . e t a l . ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a\\nformasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan\\nthecorrespondingscientiﬁcactivityoccurred.\\n1 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='436bcaa6-ecbd-43ea-8d2d-cdcc77f0b052', embedding=None, metadata={'page_label': '30', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nTheearliestpredecessorsofmoderndeeplearningweresimplelinearmodels\\nmotivatedfromaneuroscientiﬁcperspective.Thesemodelsweredesignedto\\ntakeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y.\\nThesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput\\nf ( x w, ) =x 1w 1 + · · · +x nw n.Thisﬁrstwaveofneuralnetworksresearchwas\\nknownascybernetics,asillustratedinﬁgure.1.7\\nTheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943\\nofbrainfunction.Thislinearmodelcouldrecognizetwodiﬀerentcategoriesof\\ninputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel\\ntocorrespondtothedesireddeﬁnitionofthecategories,theweightsneededtobe\\nsetcorrectly.Theseweightscouldbesetbythehumanoperator.\\xa0Inthe1950s,\\ntheperceptron(Rosenblatt19581962,,)becametheﬁrstmodelthatcouldlearn\\ntheweightsdeﬁningthecategoriesgivenexamplesofinputsfromeachcategory.\\nThe adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame\\ntime,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow\\nandHoﬀ1960,),andcouldalsolearntopredictthesenumbersfromdata.\\nThesesimplelearningalgorithmsgreatlyaﬀectedthemodernlandscapeofma-\\nchinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE\\nwasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly\\nmodiﬁedversionsofthestochasticgradientdescentalgorithmremainthedominant\\ntrainingalgorithmsfordeeplearningmodelstoday.\\nModelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled\\nl i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine\\nlearningmodels,thoughinmanycasestheyare t r a i ne dindiﬀerentwaysthanthe\\noriginalmodelsweretrained.\\nLinearmodelshavemanylimitations.Mostfamously,theycannotlearnthe\\nXORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0\\nandf ( [ 0, 0], w ) = 0.Criticswhoobservedtheseﬂawsinlinearmodelscaused\\nabacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert,\\n1969).Thiswastheﬁrstmajordipinthepopularityofneuralnetworks.\\nToday,neuroscienceisregardedasanimportantsourceofinspirationfordeep\\nlearningresearchers,butitisnolongerthepredominant guidefortheﬁeld.\\nThemainreasonforthediminishedrole\\xa0ofneuroscienceindeeplearning\\nresearchtodayisthatwesimplydonothaveenoughinformationaboutthebrain\\ntouseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused\\nbythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery\\nleast)thousandsofinterconnectedneuronssimultaneously.Becausewearenot\\nabletodothis,wearefarfromunderstandingevensomeofthemostsimpleand\\n1 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9ed11368-e624-478e-8f58-9fded96661df', embedding=None, metadata={'page_label': '31', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nwell-studiedpartsofthebrain( ,). OlshausenandField2005\\nNeurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm\\ncansolvemanydiﬀerenttasks.Neuroscientistshavefoundthatferretscanlearnto\\n“see”withtheauditoryprocessingregionoftheirbrainiftheirbrainsarerewired\\ntosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat\\nmuchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe\\ndiﬀerenttasksthatthebrainsolves.Beforethishypothesis,machinelearning\\nresearchwasmorefragmented,withdiﬀerentcommunitiesofresearchersstudying\\nnaturallanguageprocessing,vision,motionplanningandspeechrecognition.Today,\\ntheseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning\\nresearchgroupstostudymanyorevenalloftheseapplicationareassimultaneously.\\nWeareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof\\nhavingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions\\nwitheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,)\\nintroducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired\\nbythestructureofthemammalianvisualsystemandlaterbecamethebasis\\nforthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b\\nsection.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10\\nthe r e c t i ﬁed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced\\namorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain\\nfunction.Thesimpliﬁedmodernversionwasdevelopedincorporatingideasfrom\\nmanyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a\\nneuroscienceasaninﬂuence,and ()citingmoreengineering- Jarrett e t a l .2009\\norientedinﬂuences.Whileneuroscienceisanimportantsourceofinspiration,it\\nneednotbetakenasarigidguide.Weknowthatactualneuronscomputevery\\ndiﬀerentfunctionsthanmodernrectiﬁedlinearunits,butgreaterneuralrealism\\nhasnotyetledtoanimprovementinmachinelearningperformance.Also,while\\nneurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we\\ndonotyetknowenoughaboutbiologicallearningforneurosciencetooﬀermuch\\nguidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures.\\nMediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain.\\nWhileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan\\ninﬂuencethanresearchersworkinginothermachinelearningﬁeldssuchaskernel\\nmachinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt\\ntosimulatethebrain.Moderndeeplearningdrawsinspirationfrommanyﬁelds,\\nespeciallyappliedmathfundamentalslikelinearalgebra,probability,information\\ntheory,andnumericaloptimization. Whilesomedeeplearningresearcherscite\\nneuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith\\n1 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4d98f84b-753e-4c6d-942d-593f345c01b0', embedding=None, metadata={'page_label': '32', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nneuroscienceatall.\\nItis\\xa0worth\\xa0notingthat\\xa0theeﬀorttounderstandhowthe\\xa0brainworkson\\nan\\xa0algorithmic\\xa0lev el\\xa0is\\xa0alive\\xa0andwell.This\\xa0endeavor\\xa0is\\xa0primarily\\xa0knownas\\n“computational neuroscience”andisaseparateﬁeldofstudyfromdeeplearning.\\nItiscommonforresearcherstomovebackandforthbetweenbothﬁelds.The\\nﬁeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems\\nthatareabletosuccessfullysolvetasksrequiringintelligence,whiletheﬁeldof\\ncomputational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate\\nmodelsofhowthebrainactuallyworks.\\nInthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat\\npartviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss-\\ni ng( ,; ,).\\xa0Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995\\nthecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach\\ntounderstandingthemind,combiningmultiplediﬀerentlevelsofanalysis.During\\ntheearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning.\\nDespitetheirpopularity,symbolicmodelswerediﬃculttoexplainintermsof\\nhowthebraincouldactuallyimplementthemusingneurons.Theconnectionists\\nbegantostudymodelsofcognitionthatcouldactuallybegroundedinneural\\nimplementations(TouretzkyandMinton1985,),revivingmanyideasdatingback\\ntotheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949\\nThecentralideainconnectionism isthatalargenumberofsimplecomputational\\nunitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight\\nappliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin\\ncomputational models.\\nSeveralkeyconceptsaroseduringtheconnectionism movementofthe1980s\\nthatremaincentraltotoday’sdeeplearning.\\nOneoftheseconceptsisthatof di st r i but e d r e pr e se n t at i o n(Hinton e t a l .,\\n1986).Thisistheideathateachinputtoasystemshouldberepresentedby\\nmanyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany\\npossibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize\\ncars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway\\nofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit\\nthatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red\\nbird,greentruck,andsoon.Thisrequiresninediﬀerentneurons,andeachneuron\\nmustindependentlylearntheconceptofcolorandobjectidentity.Onewayto\\nimproveonthissituationistouseadistributedrepresentation,withthreeneurons\\ndescribingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires\\nonlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto\\n1 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a3bf1424-e771-4756-ad84-75b10e0b046d', embedding=None, metadata={'page_label': '33', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nlearnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages\\nofonespeciﬁccategoryofobjects.\\xa0Theconceptofdistributedrepresentationis\\ncentraltothisbook,andwillbedescribedingreaterdetailinchapter.15\\nAnothermajoraccomplishmentoftheconnectionistmovementwasthesuc-\\ncessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre-\\nsentationsandthepopularization oftheback-propagation algorithm(Rumelhart\\ne t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987\\nbutasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels.\\nDuringthe1990s,researchersmadeimportantadvancesinmodelingsequences\\nwithneuralnetworks.()and ()identiﬁedsomeof Hochreiter1991Bengio e t a l .1994\\nthefundamentalmathematical diﬃcultiesinmodelinglongsequences,describedin\\nsection.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term\\nmemoryorLSTMnetworktoresolvesomeofthesediﬃculties.Today,theLSTM\\niswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage\\nprocessingtasksatGoogle.\\nThesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven-\\nturesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti-\\ncallyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulﬁll\\ntheseunreasonableexpectations,investorsweredisappointed.Simultaneously,\\notherﬁeldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l .\\n1992CortesandVapnik1995Schölkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels(\\ndan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors\\nledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007.\\nDuringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance\\nonsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001\\nforAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive\\nviaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative.\\nThisprogramunitedmachinelearningresearchgroupsledbyGeoﬀreyHinton\\natUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann\\nLeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada\\nmulti-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman\\nandcomputervision.\\nAtthispointintime,deepnetworksweregenerallybelievedtobeverydiﬃcult\\ntotrain.\\xa0Wenowknowthatalgorithmsthathaveexistedsincethe1980swork\\nquitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat\\nthesealgorithmsweretoocomputationally costlytoallowmuchexperimentation\\nwiththehardwareavailableatthetime.\\nThethirdwaveofneuralnetworksresearchbeganwithabreakthrough in\\n1 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9ec485e-2f47-40ba-a847-831f3dbfc69f', embedding=None, metadata={'page_label': '34', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\n2006.GeoﬀreyHintonshowedthatakindofneuralnetworkcalledadeepbelief\\nnetworkcouldbeeﬃcientlytrainedusingastrategycalledgreedylayer-wisepre-\\ntraining( ,),whichwillbedescribedinmoredetailinsection. Hinton e t a l .2006 15.1\\nTheotherCIFAR-aﬃliatedresearchgroupsquicklyshowedthatthesamestrategy\\ncouldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007\\nRanzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on\\ntestexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe\\nterm“deeplearning”toemphasizethatresearcherswerenowabletotraindeeper\\nneuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe\\ntheoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio\\n2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural\\nnetworksoutperformedcompetingAIsystemsbasedonothermachinelearning\\ntechnologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity\\nofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep\\nlearningresearchhaschangeddramatically withinthetimeofthiswave.The\\nthirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe\\nabilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis\\nmoreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep\\nmodelstoleveragelargelabeleddatasets.\\n1 . 2 . 2 In creasin g D a t a s et S i zes\\nOnemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa\\ncrucialtechnologythoughtheﬁrstexperimentswithartiﬁcialneuralnetworkswere\\nconductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial\\napplicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan\\natechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue\\nthatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm.\\nFortunately,theamountofskillrequiredreducesastheamountoftrainingdata\\nincreases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks\\ntodayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy\\nproblemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave\\nundergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost\\nimportantnewdevelopmentisthattodaywecanprovidethesealgorithmswith\\ntheresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8\\ndatasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing\\ndigitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers,\\nmoreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly\\nnetworkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem\\n1 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c56b194-59f0-40ca-b50e-f4176371cef6', embedding=None, metadata={'page_label': '35', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nintoadatasetappropriateformachinelearningapplications.Theageof“Big\\nData”hasmademachinelearningmucheasierbecausethekeyburdenofstatistical\\nestimation—generalizingwelltonewdataafterobservingonlyasmallamount\\nofdata—hasbeenconsiderablylightened.Asof2016,aroughruleofthumb\\nisthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable\\nperformancewitharound5,000labeledexamplespercategory,andwillmatchor\\nexceedhumanperformancewhentrainedwithadatasetcontainingatleast10\\nmillionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis\\nanimportantresearcharea,focusinginparticularonhowwecantakeadvantage\\noflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised\\nlearning.\\n1 . 2 . 3 In creasin g Mo d el S i zes\\nAnotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying\\ncomparativelylittlesuccesssincethe1980sisthatwehavethecomputational\\nresourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection-\\nismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether.\\nAnindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful.\\nBiologicalneuronsarenotespeciallydenselyconnected.Asseeninﬁgure,1.10\\nourmachinelearningmodelshavehadanumberofconnectionsperneuronthat\\nwaswithinanorderofmagnitudeofevenmammalianbrainsfordecades.\\nIntermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly\\nsmalluntilquiterecently,asshowninﬁgure.Sincetheintroductionofhidden 1.11\\nunits,artiﬁcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This\\ngrowthisdrivenbyfastercomputerswithlargermemoryandbytheavailability\\noflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore\\ncomplextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies\\nallowfasterscaling,artiﬁcialneuralnetworkswillnothavethesamenumberof\\nneuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay\\nrepresentmorecomplicatedfunctionsthancurrentartiﬁcialneurons,sobiological\\nneuralnetworksmaybeevenlargerthanthisplotportrays.\\nInretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer\\nneuronsthanaleechwereunabletosolvesophisticatedartiﬁcialintelligenceprob-\\nlems.Eventoday’snetworks,whichweconsiderquitelargefromacomputational\\nsystemspointofview,aresmallerthanthenervoussystemofevenrelatively\\nprimitivevertebrateanimalslikefrogs.\\nTheincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs,\\n2 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aae634e3-dedf-4996-be4e-91a0a5f0efca', embedding=None, metadata={'page_label': '36', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\n1900 1950 198520002015\\nYear100101102103104105106107108109Datasetsize(numberexamples)\\nIrisMNISTPublicSVHN\\nImageNet\\nCIFAR-10ImageNet10k\\nILSVRC  2014Sports-1M\\nRotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard\\nWMT\\nFigure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians\\nstudieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson\\n1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers\\nofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such\\naslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand\\ndemonstratethatneuralnetworkswereabletolearnspeciﬁckindsoffunctions(Widrow\\nandHoﬀ1960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning\\nbecamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens\\nofthousandsofexamplessuchastheMNISTdataset(showninﬁgure)ofscans 1.9\\nofhandwrittennumbers( ,).Intheﬁrstdecadeofthe2000s,more LeCun e t a l .1998b\\nsophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand\\nHinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout\\ntheﬁrsthalfofthe2010s,signiﬁcantlylargerdatasets,containinghundredsofthousands\\ntotensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning.\\nThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l .\\n2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky\\ne t a l . e t a l . ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014\\ngraph,weseethatdatasetsoftranslatedsentences,suchasIBM’sdatasetconstructed\\nfromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990\\ndataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes.\\n2 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3acd9287-6f16-4997-8a19-7493166e47e4', embedding=None, metadata={'page_label': '37', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nFigure1.9:ExampleinputsfromtheMNISTdataset.The“NIST”standsforNational\\nInstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata.\\nThe“M”standsfor“modiﬁed,”sincethedatahasbeenpreprocessedforeasierusewith\\nmachinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits\\nandassociatedlabelsdescribingwhichdigit0–9iscontainedineachimage.Thissimple\\nclassiﬁcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning\\nresearch.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve.\\nGeoﬀreyHintonhasdescribeditas“the d r o s o p h i l aofmachinelearning,”meaningthat\\nitallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory\\nconditions,muchasbiologistsoftenstudyfruitﬂies.\\n2 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce262294-4e5a-4251-8091-d041b79ab7c1', embedding=None, metadata={'page_label': '38', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\ntheadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2\\nconnectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof\\nthemostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally\\nexpectedtocontinuewellintothefuture.\\n1 . 2 . 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct\\nSincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide\\naccuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen\\nappliedwithsuccesstobroaderandbroadersetsofapplications.\\nTheearliestdeepmodelswereusedtorecognizeindividualobjectsintightly\\ncropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a\\nbeenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern\\nobjectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot\\nhavearequirementthatthephotobecroppedneartheobjecttoberecognized\\n( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012\\ntwokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof\\nobject),whilethesemodernnetworkstypicallyrecognizeatleast1,000diﬀerent\\ncategoriesofobjects.\\xa0ThelargestcontestinobjectrecognitionistheImageNet\\nLargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic\\nmomentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork\\nwonthischallengefortheﬁrsttimeandbyawidemargin,bringingdownthe\\nstate-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012\\nmeaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories\\nforeachimageandthecorrectcategoryappearedintheﬁrstﬁveentriesofthis\\nlistforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare\\nconsistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin\\ndeeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%,\\nasshowninﬁgure.1.12\\nDeeplearninghasalsohadadramaticimpactonspeechrecognition.After\\nimprovingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated\\nstartinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng\\ne t a l . e t a l . e t a l . ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a\\ninasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore\\nthishistoryinmoredetailinsection.12.3\\nDeepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand\\nimagesegmentation( ,; Sermanet e t a l .2013Farabet2013Couprie e t a l .,; e t a l .,\\n2013)andyieldedsuperhumanperformanceintraﬃcsignclassiﬁcation(Ciresan\\n2 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='83d5d106-6d6f-4d74-a368-9344dd350777', embedding=None, metadata={'page_label': '39', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\n1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5\\nY e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n\\n12\\n34\\n567\\n89\\n1 0\\nF r ui t ﬂyMo useC a tH um a n\\nFigure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiﬁcialneural\\nnetworkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween\\nneuronsismostlyadesignconsideration.Someartiﬁcialneuralnetworkshavenearlyas\\nmanyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks\\ntohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman\\nbraindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural\\nnetworksizesfrom (). Wikipedia2015\\n1.Adaptivelinearelement( ,) WidrowandHoﬀ1960\\n2.Neocognitron(Fukushima1980,)\\n3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\\n4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\\n5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\\n6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\\n7.Distributedautoencoder(,) Le e t al.2012\\n8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\\n9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\\n10.GoogLeNet( ,) Szegedy e t al.2014a\\n2 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c172902-4b82-455d-b317-4f666a1d8d02', embedding=None, metadata={'page_label': '40', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\ne t a l .,).2012\\nAtthesametimethatthescaleandaccuracyofdeepnetworkshasincreased,\\nsohasthecomplexityofthetasksthattheycansolve. () Goodfellow e t a l .2014d\\nshowedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters\\ntranscribedfromanimage,ratherthanjustidentifyingasingleobject.Previously,\\nitwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual\\nelementsofthesequence( ,).Recurrentneuralnetworks, GülçehreandBengio2013\\nsuchastheLSTMsequencemodelmentionedabove,arenowusedtomodel\\nrelationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjustﬁxedinputs.\\nThissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing\\nanotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l .,\\n2015).\\nThistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion\\nwiththeintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn\\ntoreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such\\nneuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For\\nexample,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand\\nsortedsequences.Thisself-programming technologyisinitsinfancy,butinthe\\nfuturecouldinprinciplebeappliedtonearlyanytask.\\nAnothercrowningachievementofdeeplearningisitsextensiontothedomainof\\nr e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous\\nagentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom\\nthehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem\\nbasedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching\\nhuman-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015\\nalsosigniﬁcantlyimprovedtheperformanceofreinforcementlearningforrobotics\\n(,). Finn e t a l .2015\\nManyoftheseapplicationsofdeeplearningarehighlyproﬁtable.Deeplearning\\nisnowused\\xa0bymanytoptechnologycompanies\\xa0includi ngGoogle,\\xa0Microsoft,\\nFacebook,IBM,Baidu,Apple,Adobe,Netﬂix,NVIDIAandNEC.\\nAdvancesindeeplearninghavealsodependedheavilyonadvancesinsoftware\\ninfrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien\\ne t a l . e t a l . ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b\\nDistBelief(,),Caﬀe(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015\\nTensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015\\ncommercialproducts.\\nDeeplearninghasalsomadecontributionsbacktoothersciences.Modern\\nconvolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing\\n2 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='511685ec-0afd-49f4-927d-f726838291ea', embedding=None, metadata={'page_label': '41', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\nthatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013\\ntoolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin\\nscientiﬁcﬁelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract\\ninordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014\\ntosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014\\nmicroscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles-\\nBarley2014 e t a l .,).Weexpectdeeplearningtoappearinmoreandmorescientiﬁc\\nﬁeldsinthefuture.\\nInsummary,deeplearningisanapproachtomachinelearningthathasdrawn\\nheavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit\\ndevelopedoverthepastseveraldecades.Inrecentyears,ithasseentremendous\\ngrowthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom-\\nputers,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead\\narefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand\\nbringittonewfrontiers.\\n2 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e454ba8a-0074-482c-a6c6-cfb1cb130fed', embedding=None, metadata={'page_label': '42', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\n1950 198520002015 2056\\nYear10− 210− 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale)\\n123\\n456\\n78\\n91011\\n121314\\n151617\\n181920\\nSpongeRoundwormLeechAntBeeFrogOctopusHuman\\nFigure1.11:Sincetheintroductionofhiddenunits,artiﬁcialneuralnetworkshavedoubled\\ninsizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom (). Wikipedia2015\\n1.Perceptron(,,) Rosenblatt19581962\\n2.Adaptivelinearelement( ,) WidrowandHoﬀ1960\\n3.Neocognitron(Fukushima1980,)\\n4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b\\n5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,)\\n6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991\\n7.Meanﬁeldsigmoidbeliefnetwork(,) Saul e t al.1996\\n8.LeNet-5( ,) LeCun e t al.1998b\\n9.Echostatenetwork( ,) JaegerandHaas2004\\n10.Deepbeliefnetwork( ,) Hinton e t al.2006\\n11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006\\n12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,)\\n13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009\\n14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009\\n15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010\\n16.OMP-1network( ,) CoatesandNg2011\\n17.Distributedautoencoder(,) Le e t al.2012\\n18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012\\n19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013\\n20.GoogLeNet( ,) Szegedy e t al.2014a\\n2 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70bdb94d-5ead-4f17-807f-124a01fea85f', embedding=None, metadata={'page_label': '43', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER1.INTRODUCTION\\n2010 2011 2012 2013 2014 2015\\nYear000 .005 .010 .015 .020 .025 .030 .ILSVRC  classiﬁcationerrorrate\\nFigure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet\\nLargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition\\neveryyear,andyieldedlowerandlowererrorrateseachtime.\\xa0DatafromRussakovsky\\ne t a l . e t a l . ()and2014b He().2015\\n2 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dbfb7ee3-3313-4374-9eac-68bf39bbb085', embedding=None, metadata={'page_label': '44', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='P a rt I\\nAppliedMathandMachine\\nLearningBasics\\n29', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='217b7a63-a77c-4d61-a677-2e0688dad781', embedding=None, metadata={'page_label': '45', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This part of t he b o ok in t r o duces t he bas ic mathematical c oncepts needed t o\\nunders t an d deep learning. W e b e gin with general ideas f r om applied math t hat\\nallo w us t o deﬁne f unctions of many v ariables , ﬁ nd t he highes t and low e s t p oints\\non t hes e f unctions and q uantify degrees of b e lief.\\nN e x t , w e des c r ib e t he f undamen t al goals of machine learning. W e des c r ibe how\\nt o accomplis h t hes e goals b y s p e c ifying a mo del t hat r e pres e n t s c e r t ain b e liefs ,\\ndes igning a c os t f unction t hat meas ures how well t hos e beliefs c orres p ond with\\nr e alit y and us ing a t r aining algorithm t o minimize t hat c os t f unction.\\nThis e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning\\nalgorithms , including approac hes t o machine learning t hat are not deep.\\xa0In t he\\ns ubs e q uen t parts of t he bo ok, we develop deep learning algorithms within t his\\nf r amew ork.\\n3 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='de14ed12-6188-47ad-8edf-ca589a141581', embedding=None, metadata={'page_label': '46', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 2\\nL i n e ar A l ge b ra\\nLinearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience\\nandengineering.However,becauselinearalgebraisaformofcontinuousrather\\nthandiscretemathematics,manycomputerscientistshavelittleexperiencewithit.\\nAgoodunderstandingoflinearalgebraisessentialforunderstandingandworking\\nwithmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We\\nthereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof\\nthekeylinearalgebraprerequisites.\\nIfyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If\\nyouhavepreviousexperiencewiththeseconceptsbutneedadetailedreference\\nsheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand\\nPedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter\\nwillteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso\\nconsultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas\\nShilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra\\ntopicsthatarenotessentialforunderstandingdeeplearning.\\n2.1Scalars,Vectors,MatricesandTensors\\nThestudyoflinearalgebrainvolvesseveraltypesofmathematical objects:\\n•Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheother\\nobjectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers.\\nWewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames.\\nWhenweintroducethem,wespecifywhatkindofnumbertheyare.For\\n31', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='781066a9-461b-4ed8-a862-a12a845c8129', embedding=None, metadata={'page_label': '47', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nexample,wemightsay“Let s∈ Rbetheslopeoftheline,”whiledeﬁninga\\nreal-valuedscalar,or“Let n∈ Nbethenumberofunits,”whiledeﬁninga\\nnaturalnumberscalar.\\n•Vectors:\\xa0Avectorisanarrayofnumbers.Thenumbersarearrangedin\\norder.Wecanidentifyeachindividualnumberbyitsindexinthatordering.\\nTypicallywegivevectorslowercasenameswritteninboldtypeface,such\\nasx.Theelementsofthevectorareidentiﬁedbywritingitsnameinitalic\\ntypeface,withasubscript.Theﬁrstelementofxis x 1,thesecondelement\\nis x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin\\nthevector.Ifeachelementisin R,andthevectorhas nelements,thenthe\\nvectorliesinthesetformedbytakingtheCartesianproductof R ntimes,\\ndenotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector,\\nwewritethemasacolumnenclosedinsquarebrackets:\\nx=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8f0x 1\\nx 2\\n...\\nx n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fb. (2.1)\\nWecanthinkofvectorsasidentifyingpointsinspace,witheachelement\\ngivingthecoordinatealongadiﬀerentaxis.\\nSometimesweneedtoindexasetofelementsofavector.Inthiscase,we\\ndeﬁneasetcontainingtheindicesandwritethesetasasubscript.For\\nexample,toaccess x 1, x 3and x 6,wedeﬁnetheset S={1 ,3 ,6}andwrite\\nx S.Weusethe−signtoindexthecomplementofaset.Forexamplex − 1is\\nthevectorcontainingallelementsofxexceptfor x 1,andx − Sisthevector\\ncontainingalloftheelementsofexceptforx x 1, x 3and x 6.\\n•Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiﬁed\\nbytwoindicesinsteadofjustone.Weusuallygivematricesupper-case\\nvariablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas\\naheightof mandawidthof n,thenwesaythatA∈ Rm n ×.\\xa0Weusually\\nidentifytheelementsofamatrixusingitsnameinitalicbutnotboldfont,\\nandtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe\\nupperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall\\nofthenumberswithverticalcoordinate ibywritinga“”forthehorizontal :\\ncoordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith\\nverticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis\\n3 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='44f60a4c-8745-42f8-8f77-1a5ee739b5a7', embedding=None, metadata={'page_label': '48', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nA =\\uf8ee\\n\\uf8f0A 1 1 , A 1 2 ,\\nA 2 1 , A 2 2 ,\\nA 3 1 , A 3 2 ,\\uf8f9\\n\\uf8fb ⇒ A\\ue021=\\ue025A 1 1 , A 2 1 , A 3 1 ,\\nA 1 2 , A 2 2 , A 3 2 ,\\ue026\\nFigure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe\\nmaindiagonal.\\nthe-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA\\namatrix,wewritethemasanarrayenclosedinsquarebrackets:\\n\\ue014A 1 1 , A 1 2 ,\\nA 2 1 , A 2 2 ,\\ue015\\n. (2.2)\\nSometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust\\nasingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo\\nnotconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j)\\nofthematrixcomputedbyapplyingthefunctionto. fA\\n•Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes.\\nInthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha\\nvariablenumberofaxesisknownasatensor.Wedenoteatensornamed“A”\\nwiththistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k)\\nbywriting A i , j , k.\\nOneimportantoperationonmatricesisthetranspose.\\xa0Thetransposeofa\\nmatrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain\\ndiagonal,runningdownandtotheright,startingfromitsupperleftcorner.See\\nﬁgureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1\\nmatrixasAA\\ue03e,anditisdeﬁnedsuchthat\\n(A\\ue03e) i , j= A j , i . (2.3)\\nVectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The\\ntransposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe\\n3 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='940f69a7-cf2a-4fe0-8a6f-c0fd94c221b6', embedding=None, metadata={'page_label': '49', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\ndeﬁneavectorbywritingoutitselementsinthetextinlineasarowmatrix,\\nthenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g.,\\nx= [ x 1 , x 2 , x 3]\\ue03e.\\nAscalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we\\ncanseethatascalarisitsowntranspose: a a= \\ue03e.\\nWecanaddmatricestoeachother,aslongastheyhavethesameshape,just\\nbyaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j .\\nWecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just\\nbyperformingthatoperationoneachelementofamatrix:D= a·B+ cwhere\\nD i , j= a B· i , j+ c.\\nInthecontextofdeeplearning,wealsousesomelessconventionalnotation.\\nWeallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b,\\nwhere C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe\\nmatrix.Thisshorthandeliminatestheneedtodeﬁneamatrixwithbcopiedinto\\neachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations\\niscalled .broadcasting\\n2.2MultiplyingMatricesandVectors\\nOneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo\\nmatrices.ThematrixproductofmatricesAandBisathirdmatrixC.In\\norderforthisproducttobedeﬁned,Amusthavethesamenumberofcolumnsas\\nBhasrows.IfAisofshape m n×andBisofshape n p×,thenCisofshape\\nm p×.Wecanwritethematrixproductjustbyplacingtwoormorematrices\\ntogether,e.g.\\nCAB= . (2.4)\\nTheproductoperationisdeﬁnedby\\nC i , j=\\ue058\\nkA i , k B k, j . (2.5)\\nNotethatthestandardproductoftwomatricesisjustamatrixcontaining not\\ntheproductoftheindividualelements.Suchanoperationexistsandiscalledthe\\nelement-wiseproductHadamardproduct or ,andisdenotedas.AB\\ue00c\\nThedotproductbetweentwovectorsxandyofthesamedimensionality\\nisthematrixproductx\\ue03ey.WecanthinkofthematrixproductC=ABas\\ncomputing C i , jasthedotproductbetweenrowofandcolumnof. iA jB\\n3 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cb7374cc-abb8-4ee1-8003-9510ac020f8c', embedding=None, metadata={'page_label': '50', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nMatrixproductoperationshavemanyusefulpropertiesthatmakemathematical\\nanalysis\\xa0ofmatrices\\xa0moreconvenient.For\\xa0example,\\xa0matrix\\xa0m ultiplication\\xa0is\\ndistributive:\\nABCABAC (+) = + . (2.6)\\nItisalsoassociative:\\nABCABC ( ) = ( ) . (2.7)\\nMatrixmultiplication iscommutative(thecondition not AB=BAdoesnot\\nalwayshold),unlikescalarmultiplication. However,thedotproductbetweentwo\\nvectorsiscommutative:\\nx\\ue03eyy= \\ue03ex . (2.8)\\nThetransposeofamatrixproducthasasimpleform:\\n( )AB\\ue03e= B\\ue03eA\\ue03e. (2.9)\\nThisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8\\nofsuchaproductisascalarandthereforeequaltoitsowntranspose:\\nx\\ue03ey=\\ue010\\nx\\ue03ey\\ue011\\ue03e\\n= y\\ue03ex . (2.10)\\nSincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto\\ndevelopacomprehensivelistofusefulpropertiesofthematrixproducthere,but\\nthereadershouldbeawarethatmanymoreexist.\\nWenowknowenoughlinearalgebranotationtowritedownasystemoflinear\\nequations:\\nAxb= (2.11)\\nwhereA∈ Rm n ×isaknownmatrix,b∈ Rmisaknownvector,andx∈ Rnisa\\nvectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone\\noftheseunknownvariables.EachrowofAandeachelementofbprovideanother\\nconstraint.Wecanrewriteequationas:2.11\\nA 1 : ,x= b 1 (2.12)\\nA 2 : ,x= b 2 (2.13)\\n. . . (2.14)\\nA m , :x= b m (2.15)\\nor,evenmoreexplicitly,as:\\nA 1 1 , x 1+A 1 2 , x 2+ +···A 1 , n x n= b 1 (2.16)\\n3 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='526fd0c9-899b-466a-b7ec-86c50a506604', embedding=None, metadata={'page_label': '51', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\n\\uf8ee\\n\\uf8f0100\\n010\\n001\\uf8f9\\n\\uf8fb\\nFigure2.2:Exampleidentitymatrix:ThisisI 3.\\nA 2 1 , x 1+A 2 2 , x 2+ +···A 2 , n x n= b 2 (2.17)\\n. . . (2.18)\\nA m , 1 x 1+A m , 2 x 2+ +···A m , n x n= b m . (2.19)\\nMatrix-vectorproductnotationprovidesamorecompactrepresentationfor\\nequationsofthisform.\\n2.3IdentityandInverseMatrices\\nLinearalgebraoﬀersapowerfultoolcalledmatrixinversionthatallowsusto\\nanalyticallysolveequationformanyvaluesof. 2.11 A\\nTodescribematrixinversion,weﬁrstneedtodeﬁnetheconceptofanidentity\\nmatrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe\\nmultiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves\\nn-dimensionalvectorsasI n.Formally,I n∈ Rn n ×,and\\n∀∈x Rn,I nxx= . (2.20)\\nThestructureoftheidentitymatrixissimple:alloftheentriesalongthemain\\ndiagonalare1,whilealloftheotherentriesarezero.Seeﬁgureforanexample.2.2\\nThematrixinverseofAisdenotedasA− 1,anditisdeﬁnedasthematrix\\nsuchthat\\nA− 1AI= n . (2.21)\\nWecannowsolveequationbythefollowingsteps: 2.11\\nAxb= (2.22)\\nA− 1AxA= − 1b (2.23)\\nI nxA= − 1b (2.24)\\n3 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0405ab53-81a9-40ff-8be8-ddaa15e8b0cf', embedding=None, metadata={'page_label': '52', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nxA= − 1b . (2.25)\\nOfcourse,thisprocessdependsonitbeingpossibletoﬁndA− 1.Wediscuss\\ntheconditionsfortheexistenceofA− 1inthefollowingsection.\\nWhenA− 1exists,severaldiﬀerentalgorithmsexistforﬁndingitinclosedform.\\nIntheory,thesameinversematrixcanthenbeusedtosolvetheequationmany\\ntimesfordiﬀerentvaluesofb.However,A− 1isprimarilyusefulasatheoretical\\ntool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications.\\nBecauseA− 1canberepresentedwithonlylimitedprecisiononadigitalcomputer,\\nalgorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate\\nestimatesof.x\\n2.4LinearDependenceandSpan\\nInorderforA− 1toexist,equationmusthaveexactlyonesolutionforevery 2.11\\nvalueofb.However,itisalsopossibleforthesystemofequationstohaveno\\nsolutionsorinﬁnitelymanysolutionsforsomevaluesofb.\\xa0Itisnotpossibleto\\nhavemorethanonebutlessthaninﬁnitelymanysolutionsforaparticularb;if\\nbothandaresolutionsthen xy\\nzxy = α+(1 )− α (2.26)\\nisalsoasolutionforanyreal. α\\nToanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns\\nofAasspecifyingdiﬀerentdirectionswecantravelfromtheorigin(thepoint\\nspeciﬁedbythevectorofallzeros),anddeterminehowmanywaysthereareof\\nreachingb.Inthisview,eachelementofxspeciﬁeshowfarweshouldtravelin\\neachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof\\ncolumn: i\\nAx=\\ue058\\nix iA : , i . (2.27)\\nIngeneral,thiskindofoperationiscalledalinearcombination.Formally,a\\nlinearcombinationofsomesetofvectors{v( 1 ), . . . ,v( ) n}isgivenbymultiplying\\neachvectorv( ) ibyacorrespondingscalarcoeﬃcientandaddingtheresults:\\n\\ue058\\nic iv( ) i. (2.28)\\nThespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination\\noftheoriginalvectors.\\n3 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1da34a3c-2a91-42cb-bad2-cf6c7116f306', embedding=None, metadata={'page_label': '53', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nDeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb\\nisinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn\\nspacerangeortheof.A\\nInorderforthesystemAx=btohaveasolutionforallvaluesofb∈ Rm,\\nwethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm\\nisexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas\\nnosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies\\nimmediately thatAmusthaveatleast mcolumns,i.e., n m≥.\\xa0Otherwise, the\\ndimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera\\n3×2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx\\natbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution\\nifandonlyifliesonthatplane.b\\nHaving n m≥isonlyanecessaryconditionforeverypointtohaveasolution.\\nItisnotasuﬃcientcondition,becauseitispossibleforsomeofthecolumnsto\\nberedundant.Considera2×2matrixwherebothofthecolumnsareidentical.\\nThishasthesamecolumnspaceasa2×1matrixcontainingonlyonecopyofthe\\nreplicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto\\nencompassallof R2,eventhoughtherearetwocolumns.\\nFormally,thiskindofredundancyisknownaslineardependence.Asetof\\nvectorsislinearlyindependentifnovectorinthesetisalinearcombination\\noftheothervectors.\\xa0Ifweaddavectortoasetthatisalinearcombinationof\\ntheothervectorsintheset,thenewvectordoesnotaddanypointstotheset’s\\nspan.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm,\\nthematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This\\nconditionisbothnecessaryandsuﬃcientforequationtohaveasolutionfor 2.11\\neveryvalueofb.Notethattherequirementisforasettohaveexactly mlinear\\nindependentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave\\nmorethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan\\nmcolumnsmayhavemorethanonesuchset.\\nInorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat\\nequationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto\\nensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone\\nwayofparametrizing eachsolution.\\nTogether,thismeansthatthematrixmustbesquare,thatis,werequirethat\\nm= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix\\nwithlinearlydependentcolumnsisknownas.singular\\nIfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe\\nequation.However,wecannotusethemethodofmatrixinversiontoﬁndthe\\n3 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea33d0b9-969b-4e67-9356-0162819c7ed4', embedding=None, metadata={'page_label': '54', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nsolution.\\nSofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis\\nalsopossibletodeﬁneaninversethatismultipliedontheright:\\nAA− 1= I . (2.29)\\nForsquarematrices,theleftinverseandrightinverseareequal.\\n2.5Norms\\nSometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually\\nmeasurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm\\nisgivenby\\n||||x p=\\ue020\\ue058\\ni| x i|p\\ue021 1\\np\\n(2.30)\\nfor p , p . ∈ R≥1\\nNorms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative\\nvalues.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom\\ntheorigintothepointx.Morerigorously,anormisanyfunction fthatsatisﬁes\\nthefollowingproperties:\\n• ⇒ f() = 0 xx= 0\\n• ≤ f(+) xy f f ()+x ()y(thetriangleinequality)\\n•∀∈ || α R , f α(x) = α f()x\\nThe L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe\\nEuclideandistancefromtheorigintothepointidentiﬁedbyx.The L2normis\\nusedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with\\nthesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2\\nthesquared L2norm,whichcanbecalculatedsimplyasx\\ue03ex.\\nThesquared L2normismoreconvenienttoworkwithmathematically and\\ncomputationally thanthe L2normitself.Forexample,thederivativesofthe\\nsquared L2normwithrespecttoeachelementofxeachdependonlyonthe\\ncorrespondingelementofx,whileallofthederivativesofthe L2normdepend\\nontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable\\nbecauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning\\n3 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4133e84-a2dd-4fcf-b534-317c2a66f6e1', embedding=None, metadata={'page_label': '55', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\napplications,itisimportanttodiscriminatebetweenelementsthatareexactly\\nzeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction\\nthatgrowsatthesamerateinalllocations,butretainsmathematical simplicity:\\nthe L1norm.The L1normmaybesimpliﬁedto\\n||||x 1=\\ue058\\ni| x i| . (2.31)\\nThe L1normiscommonlyusedinmachinelearningwhenthediﬀerencebetween\\nzeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves\\nawayfrom0by,the \\ue00f L1normincreasesby. \\ue00f\\nWesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero\\nelements.Someauthorsrefertothisfunctionasthe“ L0norm,”butthisisincorrect\\nterminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because\\nscalingthevectorby αdoesnotchangethenumberofnonzeroentries.\\xa0The L1\\nnormisoftenusedasasubstituteforthenumberofnonzeroentries.\\nOneothernormthatcommonlyarisesinmachinelearningisthe L∞norm,\\nalsoknownasthemaxnorm.Thisnormsimpliﬁestotheabsolutevalueofthe\\nelementwiththelargestmagnitudeinthevector,\\n||||x ∞= max\\ni| x i| . (2.32)\\nSometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext\\nofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure\\nFrobeniusnorm:\\n|||| A F=\\ue073\\ue058\\ni , jA2\\ni , j , (2.33)\\nwhichisanalogoustothe L2normofavector.\\nThedotproductoftwovectorscanberewrittenintermsofnorms.Speciﬁcally,\\nx\\ue03eyx= |||| 2||||y 2cos θ (2.34)\\nwhereistheanglebetweenand. θ xy\\n2.6SpecialKindsofMatricesandVectors\\nSomespecialkindsofmatricesandvectorsareparticularlyuseful.\\nDiagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong\\nthemaindiagonal.\\xa0Formally,amatrixDisdiagonalifandonlyif D i , j=0for\\n4 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='266954de-9e32-4900-960d-cf543c731081', embedding=None, metadata={'page_label': '56', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nall i\\ue036= j.\\xa0Wehavealreadyseenoneexampleofadiagonalmatrix:\\xa0theidentity\\nmatrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare\\ndiagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv.\\nDiagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix\\nisverycomputationally eﬃcient.Tocomputediag(v)x,weonlyneedtoscaleeach\\nelement x iby v i.Inotherwords,diag(v)x=vx\\ue00c.Invertingasquarediagonal\\nmatrixisalsoeﬃcient.Theinverseexistsonlyifeverydiagonalentryisnonzero,\\nandinthatcase,diag(v)− 1=diag([1 /v 1 , . . . ,1 /v n]\\ue03e).Inmanycases,wemay\\nderivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices,\\nbutobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome\\nmatricestobediagonal.\\nNotalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular\\ndiagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill\\npossibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the\\nproductDxwillinvolvescalingeachelementofx,andeitherconcatenating some\\nzerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast\\nelementsofthevectorifiswiderthanitistall. D\\nA matrixisanymatrixthatisequaltoitsowntranspose: symmetric\\nAA= \\ue03e. (2.35)\\nSymmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof\\ntwoargumentsthatdoesnotdependontheorderofthearguments.Forexample,\\nifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint\\nitopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric.\\nA isavectorwith : unitvectorunitnorm\\n||||x 2= 1 . (2.36)\\nAvectorxandavectoryareorthogonaltoeachotherifx\\ue03ey= 0.Ifboth\\nvectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach\\nother.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm.\\nIfthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem\\northonormal.\\nAnorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor-\\nmalandwhosecolumnsaremutuallyorthonormal:\\nA\\ue03eAAA= \\ue03e= I . (2.37)\\n4 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='03c3a243-724e-47b1-bdba-fb0b24bc91ab', embedding=None, metadata={'page_label': '57', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nThisimpliesthat\\nA− 1= A\\ue03e, (2.38)\\nsoorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute.\\nPaycarefulattentiontothedeﬁnitionoforthogonalmatrices.Counterintuitively,\\ntheirrowsarenotmerelyorthogonalbutfullyorthonormal. Thereisnospecial\\ntermforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal.\\n2.7Eigendecomposition\\nManymathematical objectscanbeunderstoodbetterbybreakingtheminto\\nconstituentparts,orﬁndingsomepropertiesofthemthatareuniversal,notcaused\\nbythewaywechoosetorepresentthem.\\nForexample,integerscanbedecomposedintoprimefactors.Thewaywe\\nrepresentthenumberwillchangedependingonwhetherwewriteitinbaseten 12\\norinbinary,butitwillalwaysbetruethat12 = 2×2×3.Fromthisrepresentation\\nwecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5\\nintegermultipleofwillbedivisibleby. 12 3\\nMuchaswecandiscoversomethingaboutthetruenatureofanintegerby\\ndecomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat\\nshowusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe\\nrepresentationofthematrixasanarrayofelements.\\nOneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen-\\ndecomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand\\neigenvalues.\\nAneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti-\\nplicationbyaltersonlythescaleof: A v\\nAvv= λ . (2.39)\\nThescalar λisknownastheeigenvaluecorrespondingtothiseigenvector.(One\\ncanalsoﬁndalefteigenvectorsuchthatv\\ue03eA= λv\\ue03e,\\xa0butweareusually\\nconcernedwithrighteigenvectors).\\nIfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s ∈ R\\ue036= 0.\\nMoreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook\\nforuniteigenvectors.\\nSupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . ,\\nv( ) n},withcorrespondingeigenvalues { λ 1 , . . . , λ n}.Wemayconcatenateallofthe\\n4 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a9dcfabd-35c0-49fc-a151-7dc3d1838304', embedding=None, metadata={'page_label': '58', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\n\\x00\\ue033 \\x00\\ue032 \\x00\\ue031 \\ue030 \\ue031 \\ue032 \\ue033\\n\\ue078\\ue030\\x00\\ue033\\x00\\ue032\\x00\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue031\\ue076\\ue028\\ue031 \\ue029\\n\\ue076\\ue028\\ue032 \\ue029\\ue042 \\ue065 \\ue066 \\ue06f \\ue072 \\ue065 \\ue020 \\ue06d \\ue075 \\ue06c \\ue074 \\ue069 \\ue070 \\ue06c \\ue069 \\ue063 \\ue061 \\ue074 \\ue069 \\ue06f \\ue06e\\n\\x00\\ue033 \\x00\\ue032 \\x00\\ue031 \\ue030 \\ue031 \\ue032 \\ue033\\n\\ue078\\ue030\\n\\ue030\\x00\\ue033\\x00\\ue032\\x00\\ue031\\ue030\\ue031\\ue032\\ue033\\ue078\\ue030\\n\\ue031\\ue076\\ue028\\ue031 \\ue029\\ue0b8\\ue031 \\ue076\\ue028\\ue031 \\ue029\\n\\ue076\\ue028\\ue032 \\ue029\\ue0b8\\ue032\\ue076\\ue028\\ue032 \\ue029\\ue041 \\ue066 \\ue074 \\ue065 \\ue072 \\ue020 \\ue06d \\ue075 \\ue06c \\ue074 \\ue069 \\ue070 \\ue06c \\ue069 \\ue063 \\ue061 \\ue074 \\ue069 \\ue06f \\ue06e\\ue045 \\ue066 \\ue066 \\ue065 \\ue063 \\ue074 \\ue020 \\ue06f\\ue066 \\ue020 \\ue065 \\ue069 \\ue067 \\ue065 \\ue06e \\ue076 \\ue065 \\ue063 \\ue074 \\ue06f\\ue072 \\ue073 \\ue020 \\ue061\\ue06e \\ue064 \\ue020 \\ue065 \\ue069 \\ue067\\ue065 \\ue06e \\ue076 \\ue061\\ue06c \\ue075 \\ue065 \\ue073\\nFigure2.3:Anexampleoftheeﬀectofeigenvectorsandeigenvalues.Here,wehave\\namatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue λ 1andv( 2 )with\\neigenvalue λ 2. ( L e f t )Weplotthesetofallunitvectorsu∈ R2asaunitcircle. ( R i g h t )We\\nplotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we\\ncanseethatitscalesspaceindirectionv( ) iby λ i.\\neigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . ,\\nv( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorλ= [ λ 1 , . . . ,\\nλ n]\\ue03e.The ofisthengivenby eigendecompositionA\\nAVλV = diag()− 1. (2.40)\\nWehaveseenthatconstructingmatriceswithspeciﬁceigenvaluesandeigenvec-\\ntorsallowsustostretchspaceindesireddirections.\\xa0Ho wever,weoftenwantto\\ndecomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp\\nustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger\\nintoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger.\\nNoteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome\\n4 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5fa7eeb4-1167-4f54-af30-f385aadf25ae', embedding=None, metadata={'page_label': '59', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\ncases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers.\\nFortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciﬁcclassof\\nmatricesthathaveasimpledecomposition.Speciﬁcally,everyrealsymmetric\\nmatrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors\\nandeigenvalues:\\nAQQ = Λ\\ue03e, (2.41)\\nwhereQisanorthogonalmatrixcomposedofeigenvectorsofA,and Λisa\\ndiagonalmatrix.TheeigenvalueΛ i , iisassociatedwiththeeigenvectorincolumn i\\nofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas\\nscalingspaceby λ iindirectionv( ) i.Seeﬁgureforanexample.2.3\\nWhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi-\\ntion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors\\nsharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan\\narealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ\\nusingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof Λ\\nindescendingorder.Underthisconvention,theeigendecompositionisuniqueonly\\nifalloftheeigenvaluesareunique.\\nTheeigendecompositionof\\xa0amatrix\\xa0tellsus\\xa0many\\xa0usefulfactsabout\\xa0the\\nmatrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero.\\nTheeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize\\nquadraticexpressionsoftheform f(x) =x\\ue03eAxsubjectto||||x 2= 1.Wheneverx\\nisequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue.\\nThemaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue\\nanditsminimumvaluewithintheconstraintregionistheminimumeigenvalue.\\nAmatrixwhoseeigenvaluesareallpositiveiscalledpositivedeﬁnite.A\\nmatrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideﬁ-\\nnite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeﬁnite,and\\nifalleigenvaluesarenegativeorzero-valued,itisnegativesemideﬁnite.Positive\\nsemideﬁnitematricesareinterestingbecausetheyguaranteethat∀xx ,\\ue03eAx≥0.\\nPositivedeﬁnitematricesadditionallyguaranteethatx\\ue03eAxx = 0 ⇒ = 0.\\n2.8SingularValueDecomposition\\nInsection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7\\nThesingularvaluedecomposition(SVD)providesanotherwaytofactorize\\namatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto\\ndiscoversomeofthesamekindofinformationastheeigendecomposition.However,\\n4 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f71363f-98f7-4867-bc4d-0b15d81ebc48', embedding=None, metadata={'page_label': '60', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\ntheSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue\\ndecomposition,butthesameisnottrueoftheeigenvaluedecomposition.For\\nexample,ifamatrixisnotsquare,theeigendecompositionisnotdeﬁned,andwe\\nmustuseasingularvaluedecompositioninstead.\\nRecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover\\namatrixVofeigenvectorsandavectorofeigenvaluesλsuchthatwecanrewrite\\nAas\\nAVλV = diag()− 1. (2.42)\\nThesingularvaluedecompositionissimilar,exceptthistimewewillwriteA\\nasaproductofthreematrices:\\nAUDV = \\ue03e. (2.43)\\nSupposethatAisan m n×matrix.ThenUisdeﬁnedtobean m m×matrix,\\nD V tobeanmatrix,and m n× tobeanmatrix. n n×\\nEachofthesematricesisdeﬁnedtohaveaspecialstructure.ThematricesU\\nandVarebothdeﬁnedtobeorthogonalmatrices.ThematrixDisdeﬁnedtobe\\nadiagonalmatrix.Notethatisnotnecessarilysquare. D\\nTheelementsalongthediagonalofDareknownasthesingularvaluesof\\nthematrixA.ThecolumnsofUareknownastheleft-singularvectors.The\\ncolumnsofareknownasasthe V right-singularvectors.\\nWecanactuallyinterpretthesingularvaluedecompositionofAintermsof\\ntheeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe\\neigenvectorsofAA\\ue03e.Theright-singularvectorsofAaretheeigenvectorsofA\\ue03eA.\\nThenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofA\\ue03eA.\\nThesameistrueforAA\\ue03e.\\nPerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially\\ngeneralizematrixinversiontonon-squarematrices,aswewillseeinthenext\\nsection.\\n2.9TheMoore-PenrosePseudoinverse\\nMatrixinversionisnotdeﬁnedformatricesthatarenotsquare.Supposewewant\\ntomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA\\nAxy= (2.44)\\n4 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d47abfa0-fefd-41d9-871c-de4b1bdcd40f', embedding=None, metadata={'page_label': '61', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nbyleft-multiplyingeachsidetoobtain\\nxBy= . (2.45)\\nDependingonthestructureoftheproblem,itmaynotbepossibletodesigna\\nuniquemappingfromto.AB\\nIfAistallerthanitiswide,\\xa0thenitispossibleforthisequationtohave\\nnosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible\\nsolutions.\\nTheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin\\nthesecases.Thepseudoinverseofisdeﬁnedasamatrix A\\nA+=lim\\nα \\ue026 0(A\\ue03eAI+ α)− 1A\\ue03e. (2.46)\\nPracticalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeﬁni-\\ntion,butrathertheformula\\nA+= VD+U\\ue03e, (2.47)\\nwhereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse\\nD+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero\\nelementsthentakingthetransposeoftheresultingmatrix.\\nWhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe\\npseudoinverseprovidesoneofthemanypossiblesolutions.Speciﬁcally,itprovides\\nthesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible\\nsolutions.\\nWhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution.\\nInthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas\\npossibletointermsofEuclideannorm y ||−||Axy 2.\\n2.10TheTraceOperator\\nThetraceoperatorgivesthesumofallofthediagonalentriesofamatrix:\\nTr() =A\\ue058\\niA i , i . (2.48)\\nThetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare\\ndiﬃculttospecifywithoutresortingtosummationnotationcanbespeciﬁedusing\\n4 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6fcf1119-46d9-4dbd-a81c-2eb8e0e9672d', embedding=None, metadata={'page_label': '62', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nmatrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides\\nanalternativewayofwritingtheFrobeniusnormofamatrix:\\n|||| A F=\\ue071\\nTr(AA\\ue03e) . (2.49)\\nWritinganexpressionintermsofthetraceoperatoropensupopportunitiesto\\nmanipulatetheexpressionusingmanyusefulidentities.\\xa0Forexample,thetrace\\noperatorisinvarianttothetransposeoperator:\\nTr() = Tr(AA\\ue03e) . (2.50)\\nThetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto\\nmovingthelastfactorintotheﬁrstposition,iftheshapesofthecorresponding\\nmatricesallowtheresultingproducttobedeﬁned:\\nTr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51)\\normoregenerally,\\nTr(n\\ue059\\ni = 1F( ) i) = Tr(F( ) nn − 1\\ue059\\ni = 1F( ) i) . (2.52)\\nThisinvariancetocyclicpermutationholdseveniftheresultingproducthasa\\ndiﬀerentshape.Forexample,forA∈ Rm n ×andB∈ Rn m ×,wehave\\nTr( ) = Tr( )ABBA (2.53)\\neventhoughAB∈ Rm m ×andBA∈ Rn n ×.\\nAnotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a).\\n2.11TheDeterminant\\nThedeterminant ofa\\xa0squarematrix,\\xa0denoted det(A),\\xa0isa\\xa0functionmapping\\nmatricesto\\xa0realscalars.Thedeterminant isequal\\xa0totheproductof\\xa0allthe\\neigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought\\nofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts\\nspace.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast\\nonedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then\\nthetransformationpreservesvolume.\\n4 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='068f2ebd-7eff-4b28-a39c-eb46e55a4ae2', embedding=None, metadata={'page_label': '63', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\n2.12Example:PrincipalComponentsAnalysis\\nOnesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA\\ncanbederivedusingonlyknowledgeofbasiclinearalgebra.\\nSupposewehaveacollectionof mpoints{x( 1 ), . . . ,x( ) m}in Rn.Supposewe\\nwouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans\\nstoringthepointsinawaythatrequireslessmemorybutmaylosesomeprecision.\\nWewouldliketoloseaslittleprecisionaspossible.\\nOnewaywecanencodethesepointsistorepresentalower-dimensionalversion\\nofthem.Foreachpointx( ) i∈ Rnwewillﬁndacorrespondingcodevectorc( ) i∈ Rl.\\nIf lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe\\noriginaldata.Wewillwanttoﬁndsomeencodingfunctionthatproducesthecode\\nforaninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed\\ninputgivenitscode, .xx ≈ g f(())\\nPCAisdeﬁnedbyourchoiceofthedecodingfunction.Speciﬁcally,tomakethe\\ndecoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback\\ninto Rn.Let,where g() = cDcD∈ Rn l ×isthematrixdeﬁningthedecoding.\\nComputingtheoptimalcodeforthisdecodercouldbeadiﬃcultproblem.To\\nkeeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal\\ntoeachother.(NotethatDisstillnottechnically“anorthogonalmatrix”unless\\nl n= )\\nWiththeproblemasdescribedsofar,manysolutionsarepossible,becausewe\\ncanincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive\\ntheproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD\\nnorm.\\nInordertoturnthisbasicideaintoanalgorithmwecanimplement,theﬁrst\\nthingweneedtodoisﬁgureouthowtogeneratetheoptimalcodepointc∗for\\neachinputpointx.Onewaytodothisistominimizethedistancebetweenthe\\ninputpointxanditsreconstruction, g(c∗).Wecanmeasurethisdistanceusinga\\nnorm.Intheprincipalcomponentsalgorithm,weusethe L2norm:\\nc∗= argmin\\nc||− ||x g()c 2 . (2.54)\\nWecanswitchtothesquared L2norminsteadofthe L2normitself,because\\nbothareminimizedbythesamevalueofc.Bothareminimizedbythesame\\nvalueofcbecausethe L2normisnon-negative andthesquaringoperationis\\n4 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='591456fc-3201-475a-8949-6861adf6439b', embedding=None, metadata={'page_label': '64', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nmonotonically increasingfornon-negative arguments.\\nc∗= argmin\\nc||− ||x g()c2\\n2 . (2.55)\\nThefunctionbeingminimizedsimpliﬁesto\\n( ())x− gc\\ue03e( ())x− gc (2.56)\\n(bythedeﬁnitionofthe L2norm,equation)2.30\\n= x\\ue03exx−\\ue03eg g ()c−()c\\ue03exc+( g)\\ue03eg()c (2.57)\\n(bythedistributiveproperty)\\n= x\\ue03exx−2\\ue03eg g ()+c ()c\\ue03eg()c (2.58)\\n(becausethescalar g()c\\ue03exisequaltothetransposeofitself).\\nWecannowchangethefunctionbeingminimizedagain,toomittheﬁrstterm,\\nsincethistermdoesnotdependon:c\\nc∗= argmin\\nc−2x\\ue03eg g ()+c ()c\\ue03eg .()c (2.59)\\nTomakefurtherprogress,wemustsubstituteinthedeﬁnitionof: g()c\\nc∗= argmin\\nc−2x\\ue03eDcc+\\ue03eD\\ue03eDc (2.60)\\n= argmin\\nc−2x\\ue03eDcc+\\ue03eI lc (2.61)\\n(bytheorthogonalityandunitnormconstraintson)D\\n= argmin\\nc−2x\\ue03eDcc+\\ue03ec (2.62)\\nWecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3\\nyoudonotknowhowtodothis):\\n∇ c(2−x\\ue03eDcc+\\ue03ec) = 0 (2.63)\\n−2D\\ue03exc+2= 0 (2.64)\\ncD= \\ue03ex . (2.65)\\n4 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fece025a-ecf9-43e9-a568-52326a236843', embedding=None, metadata={'page_label': '65', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nThismakesthealgorithmeﬃcient:\\xa0wecanoptimallyencodexjustusinga\\nmatrix-vectoroperation.Toencodeavector,weapplytheencoderfunction\\nf() = xD\\ue03ex . (2.66)\\nUsingafurthermatrixmultiplication, wecanalsodeﬁnethePCAreconstruction\\noperation:\\nr g f () = x (()) = xDD\\ue03ex . (2.67)\\nNext,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea\\nofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill\\nusethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe\\npointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix\\noferrorscomputedoveralldimensionsandallpoints:\\nD∗= argmin\\nD\\ue073\\ue058\\ni , j\\ue010\\nx( ) i\\nj− r(x( ) i) j\\ue0112\\nsubjecttoD\\ue03eDI= l(2.68)\\nToderivethealgorithmforﬁndingD∗,wewillstartbyconsideringthecase\\nwhere l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67\\nintoequationandsimplifyinginto,theproblemreducesto 2.68 Dd\\nd∗= argmin\\nd\\ue058\\ni||x( ) i−dd\\ue03ex( ) i||2\\n2subjectto||||d 2= 1 .(2.69)\\nTheaboveformulationisthemostdirectwayofperformingthesubstitution,\\nbutisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe\\nscalarvalued\\ue03ex( ) iontherightofthevectord.Itismoreconventionaltowrite\\nscalarcoeﬃcientsontheleftofvectortheyoperateon.Wethereforeusuallywrite\\nsuchaformulaas\\nd∗= argmin\\nd\\ue058\\ni||x( ) i−d\\ue03ex( ) id||2\\n2subjectto||||d 2= 1 ,(2.70)\\nor,exploitingthefactthatascalarisitsowntranspose,as\\nd∗= argmin\\nd\\ue058\\ni||x( ) i−x( ) i \\ue03edd||2\\n2subjectto||||d 2= 1 .(2.71)\\nThereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements .\\n5 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b99e631-cd6e-4349-9653-9642babcb181', embedding=None, metadata={'page_label': '66', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\nAtthispoint,itcanbehelpfultorewritetheproblemintermsofasingle\\ndesignmatrixofexamples,ratherthanasasumoverseparateexamplevectors.\\nThiswillallowustousemorecompactnotation.LetX∈ Rm n ×bethematrix\\ndeﬁnedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) i\\ue03e.\\nWecannowrewritetheproblemas\\nd∗= argmin\\nd||−XXdd\\ue03e||2\\nFsubjecttod\\ue03ed= 1 .(2.72)\\nDisregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm\\nportionasfollows:\\nargmin\\nd||−XXdd\\ue03e||2\\nF (2.73)\\n= argmin\\ndTr\\ue012\\ue010\\nXXdd −\\ue03e\\ue011\\ue03e\\ue010\\nXXdd −\\ue03e\\ue011\\ue013\\n(2.74)\\n(byequation)2.49\\n= argmin\\ndTr(X\\ue03eXX−\\ue03eXdd\\ue03e−dd\\ue03eX\\ue03eXdd+\\ue03eX\\ue03eXdd\\ue03e)(2.75)\\n= argmin\\ndTr(X\\ue03eX)Tr(−X\\ue03eXdd\\ue03e)Tr(−dd\\ue03eX\\ue03eX)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)\\n(2.76)\\n= argmin\\nd−Tr(X\\ue03eXdd\\ue03e)Tr(−dd\\ue03eX\\ue03eX)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.77)\\n(becausetermsnotinvolvingdonotaﬀectthe) d argmin\\n= argmin\\nd−2Tr(X\\ue03eXdd\\ue03e)+Tr(dd\\ue03eX\\ue03eXdd\\ue03e)(2.78)\\n(becausewecancycletheorderofthematricesinsideatrace,equation)2.52\\n= argmin\\nd−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03edd\\ue03e)(2.79)\\n(usingthesamepropertyagain)\\nAtthispoint,were-introducetheconstraint:\\nargmin\\nd−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03edd\\ue03e)subjecttod\\ue03ed= 1(2.80)\\n= argmin\\nd−2Tr(X\\ue03eXdd\\ue03e)+Tr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.81)\\n(duetotheconstraint)\\n= argmin\\nd−Tr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.82)\\n5 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ae887b54-b446-4fbe-9264-8cce2375d375', embedding=None, metadata={'page_label': '67', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER2.LINEARALGEBRA\\n= argmax\\ndTr(X\\ue03eXdd\\ue03e)subjecttod\\ue03ed= 1(2.83)\\n= argmax\\ndTr(d\\ue03eX\\ue03eXdd )subjectto\\ue03ed= 1(2.84)\\nThisoptimizationproblemmaybesolvedusingeigendecomposition.Speciﬁcally,\\ntheoptimaldisgivenbytheeigenvectorofX\\ue03eXcorrespondingtothelargest\\neigenvalue.\\nThisderivationisspeciﬁctothecaseof l=1andrecoversonlytheﬁrst\\nprincipalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal\\ncomponents,thematrixDisgivenbythe leigenvectorscorrespondingtothe\\nlargesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend\\nwritingthisproofasanexercise.\\nLinearalgebraisoneofthefundamentalmathematical disciplinesthatis\\nnecessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis\\nubiquitousinmachinelearningisprobabilitytheory,presentednext.\\n5 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0ba5b3a5-0822-4ff5-8306-cc772beb05e4', embedding=None, metadata={'page_label': '68', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 3\\nProbabilityandInformation\\nTheory\\nInthischapter,wedescribeprobabilitytheoryandinformationtheory.\\nProbabilitytheoryisamathematical frameworkforrepresentinguncertain\\nstatements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving\\nnewuncertainstatements.Inartiﬁcialintelligenceapplications,weuseprobability\\ntheoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems\\nshouldreason,sowedesignouralgorithmstocomputeorapproximate various\\nexpressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand\\nstatisticstotheoreticallyanalyzethebehaviorofproposedAIsystems.\\nProbabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand\\nengineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis\\nprimarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan\\nunderstandthematerialinthisbook.\\nWhileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin\\nthepresenceofuncertainty,informationtheoryallowsustoquantifytheamount\\nofuncertaintyinaprobabilitydistribution.\\nIfyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you\\nmaywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14\\ngraphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If\\nyouhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould\\nbesuﬃcienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo\\nsuggestthatyouconsultanadditionalresource,suchasJaynes2003().\\n53', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='142e8cb4-15fb-4937-910f-2f57954e9190', embedding=None, metadata={'page_label': '69', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n3.1WhyProbability?\\nManybranchesofcomputersciencedealmostlywithentitiesthatareentirely\\ndeterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill\\nexecuteeachmachineinstructionﬂawlessly.Errorsinhardwaredooccur,butare\\nrareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount\\nforthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina\\nrelativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning\\nmakesheavyuseofprobabilitytheory.\\nThisisbecausemachinelearningmustalwaysdealwithuncertainquantities,\\nandsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities.\\nUncertaintyandstochasticitycanarisefrommanysources.Researchershavemade\\ncompellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast\\nthe1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired\\nbyPearl1988().\\nNearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty.\\nInfact,beyondmathematical statementsthataretruebydeﬁnition,itisdiﬃcult\\ntothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely\\nguaranteedtooccur.\\nTherearethreepossiblesourcesofuncertainty:\\n1.Inherentstochasticityinthesystembeingmodeled.Forexample,most\\ninterpretationsofquantummechanicsdescribethedynamicsofsubatomic\\nparticlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat\\nwepostulatetohaverandomdynamics,suchasahypothetical cardgame\\nwhereweassumethatthecardsaretrulyshuﬄedintoarandomorder.\\n2.Incompleteobservability.Evendeterministicsystemscanappearstochastic\\nwhenwecannotobserveallofthevariablesthatdrivethebehaviorofthe\\nsystem.Forexample,intheMontyHallproblem,agameshowcontestantis\\naskedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen\\ndoor.Twodoorsleadtoagoatwhileathirdleadstoacar.\\xa0Theoutcome\\ngiventhecontestant’schoiceisdeterministic,butfromthecontestant’spoint\\nofview,theoutcomeisuncertain.\\n3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof\\nthe\\xa0information wehave\\xa0observed,\\xa0the\\xa0discarded\\xa0i nformationresults\\xa0in\\nuncertaintyinthemodel’spredictions. Forexample,supposewebuilda\\nrobotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe\\n54', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dfdcce0a-7988-4024-abd7-b299fffd6263', embedding=None, metadata={'page_label': '70', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nrobotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects,\\nthenthediscretizationmakestherobotimmediatelybecomeuncertainabout\\ntheprecisepositionofobjects:\\xa0eachobjectcouldbeanywherewithinthe\\ndiscretecellthatitwasobservedtooccupy.\\nInmanycases,itismorepracticaltouseasimplebutuncertainrulerather\\nthanacomplexbutcertainone,evenifthetrueruleisdeterministicandour\\nmodelingsystemhastheﬁdelitytoaccommodateacomplexrule.Forexample,the\\nsimplerule“Mostbirdsﬂy”ischeaptodevelopandisbroadlyuseful,whilearule\\noftheform,“Birdsﬂy,exceptforveryyoungbirdsthathavenotyetlearnedto\\nﬂy,sickorinjuredbirdsthathavelosttheabilitytoﬂy,ﬂightlessspeciesofbirds\\nincludingthecassowary,ostrichandkiwi...”\\xa0isexpensivetodevelop,maintainand\\ncommunicate,andafterallofthiseﬀortisstillverybrittleandpronetofailure.\\nWhileitshouldbeclearthatweneedameansofrepresentingandreasoning\\naboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide\\nallofthetoolswewantforartiﬁcialintelligenceapplications.Probabilitytheory\\nwasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee\\nhowprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof\\ncardsinagameofpoker.Thesekindsofeventsareoftenrepeatable.\\xa0Whenwe\\nsaythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated\\ntheexperiment(e.g.,drawahandofcards)inﬁnitelymanytimes,thenproportion\\npoftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot\\nseemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor\\nanalyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheﬂu,\\nthismeanssomethingverydiﬀerent—wecannotmakeinﬁnitelymanyreplicasof\\nthepatient,noristhereanyreasontobelievethatdiﬀerentreplicasofthepatient\\nwouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In\\nthecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta\\ndegr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthastheﬂu\\nand0indicatingabsolutecertaintythatthepatientdoesnothavetheﬂu.\\xa0The\\nformerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is\\nknownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels\\nofcertainty,isknownas B ay e si an pr o babili t y.\\nIfwelistseveralpropertiesthatweexpectcommonsensereasoningabout\\nuncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat\\nBayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities.\\nForexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker\\ngamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas\\naswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe\\n55', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='26abf432-f321-4f2d-867b-07b087588262', embedding=None, metadata={'page_label': '71', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nhascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense\\nassumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability,\\nsee(). Ramsey1926\\nProbabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic\\nprovidesasetofformalrulesfordeterminingwhatpropositionsareimpliedto\\nbetrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue\\norfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe\\nlikelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions.\\n3.2RandomVariables\\nA r andom v ar i abl eisavariablethatcantakeondiﬀerentvaluesrandomly.We\\ntypicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface,\\nandthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2\\narebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued\\nvariables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On\\nitsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it\\nmustbecoupledwithaprobabilitydistributionthatspeciﬁeshowlikelyeachof\\nthesestatesare.\\nRandomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable\\nisonethathasaﬁniteorcountablyinﬁnitenumberofstates.Notethatthese\\nstatesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat\\narenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis\\nassociatedwitharealvalue.\\n3.3ProbabilityDistributions\\nA pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor\\nsetofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe\\ndescribeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor\\ncontinuous.\\n3.3.1DiscreteVariablesandProbabilityMassFunctions\\nAprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba-\\nbi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith\\nacapitalP.Oftenweassociateeachrandomvariablewithadiﬀerentprobability\\n56', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='733313f6-3f96-4774-b8e5-65c86ee5713e', embedding=None, metadata={'page_label': '72', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nmassfunctionandthereadermustinferwhichprobabilitymassfunctiontouse\\nbasedontheidentityoftherandomvariable,ratherthanthenameofthefunction;\\nP P ()xisusuallynotthesameas()y.\\nTheprobabilitymassfunctionmapsfromastateofarandomvariableto\\ntheprobabilityofthatrandomvariabletakingonthatstate.Theprobability\\nthatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis\\ncertainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes\\ntodisambiguatewhichPMFtouse,wewritethenameoftherandomvariable\\nexplicitly:P(x=x).Sometimeswedeﬁneavariableﬁrst,thenuse∼notationto\\nspecifywhichdistributionitfollowslater:xx. ∼P()\\nProbabilitymassfunctionscanactonmanyvariablesatthesametime.Such\\naprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y\\ndi st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y\\nsimultaneously.Wemayalsowrite forbrevity. Px,y()\\nTobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust\\nsatisfythefollowingproperties:\\n•Thedomainofmustbethesetofallpossiblestatesofx. P\\n•∀∈xx,0≤P(x)≤1.Animpossibleeventhasprobabilityandnostatecan 0 \\nbelessprobablethanthat.Likewise,aneventthatisguaranteedtohappen\\nhasprobability,andnostatecanhaveagreaterchanceofoccurring. 1\\n•\\ue050\\nx ∈ xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without\\nthisproperty,wecouldobtainprobabilities greaterthanonebycomputing\\ntheprobabilityofoneofmanyeventsoccurring.\\nForexample,considerasinglediscreterandomvariablexwithkdiﬀerent\\nstates.Wecanplacea uni f o r m di st r i but i o nonx—thatis,makeeachofits\\nstatesequallylikely—bysettingitsprobabilitymassfunctionto\\nPx (= x i) =1\\nk(3.1)\\nforalli.Wecanseethatthisﬁtstherequirementsforaprobabilitymassfunction.\\nThevalue1\\nkispositivebecauseisapositiveinteger.Wealsoseethat k\\n\\ue058\\niPx (= x i) =\\ue058\\ni1\\nk=k\\nk= 1, (3.2)\\nsothedistributionisproperlynormalized.\\n57', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9457ad1-f7a7-4fe2-a48a-a76ace18c3d5', embedding=None, metadata={'page_label': '73', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n3.3.2ContinuousVariablesandProbabilityDensityFunctions\\nWhenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri-\\nbutionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability\\nmassfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe\\nfollowingproperties:\\n•Thedomainofmustbethesetofallpossiblestatesofx. p\\n•∀∈ ≥ ≤ xx,px() 0 () . p Notethatwedonotrequirex 1.\\n•\\ue052\\npxdx()= 1.\\nAprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciﬁc\\nstatedirectly,insteadtheprobabilityoflandinginsideaninﬁnitesimalregionwith\\nvolumeisgivenby. δx pxδx()\\nWecanintegratethedensityfunctiontoﬁndtheactualprobabilitymassofa\\nsetofpoints.Speciﬁcally,theprobabilitythatxliesinsomeset Sisgivenbythe\\nintegralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx\\nliesintheintervalisgivenby []a,b\\ue052\\n[ ] a , bpxdx().\\nForanexampleofaprobabilitydensityfunctioncorrespondingtoaspeciﬁc\\nprobabilitydensityoveracontinuousrandomvariable,considerauniformdistribu-\\ntiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b),\\nwhereaandbaretheendpointsoftheinterval,withb>a.The“;”notationmeans\\n“parametrized by”;weconsiderxtobetheargumentofthefunction,whileaand\\nbareparametersthatdeﬁnethefunction.Toensurethatthereisnoprobability\\nmassoutsidetheinterval,wesayu(x;a,b)=0forallx\\ue036∈[a,b] [.Withina,b],\\nuxa,b (;) =1\\nb a −.Wecanseethatthisisnonnegativeeverywhere.Additionally,it\\nintegratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b]\\nbywritingx. ∼Ua,b()\\n3.4MarginalProbability\\nSometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant\\ntoknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability\\ndistributionoverthesubsetisknownasthe distribution. m ar g i nal pr o babili t y\\nForexample,supposewehavediscreterandomvariablesxandy,andweknow\\nP,(xy.Wecanﬁndxwiththe : ) P() sum r ul e\\n∀∈xxx,P(= ) =x\\ue058\\nyPx,y. (= xy= ) (3.3)\\n58', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='043d16f9-d162-4d74-8871-7d06643353f4', embedding=None, metadata={'page_label': '74', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nThename“marginalprobability”comesfromtheprocessofcomputingmarginal\\nprobabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith\\ndiﬀerentvaluesofxinrowsanddiﬀerentvaluesofyincolumns,itisnaturalto\\nsumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto\\ntherightoftherow.\\nForcontinuousvariables,weneedtouseintegrationinsteadofsummation:\\npx() =\\ue05a\\npx,ydy. () (3.4)\\n3.5ConditionalProbability\\nInmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome\\nothereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote\\ntheconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This\\nconditionalprobabilitycanbecomputedwiththeformula\\nPyx (= y |x= ) =Py,x (= yx= )\\nPx (= x ). (3.5)\\nTheconditionalprobabilityisonlydeﬁnedwhenP(x=x)>0.Wecannotcompute\\ntheconditionalprobabilityconditionedonaneventthatneverhappens.\\nItisimportantnottoconfuseconditionalprobabilitywithcomputingwhat\\nwouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat\\napersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif\\narandomlyselectedpersonistaughttospeakGerman,theircountryoforigin\\ndoesnotchange.Computingtheconsequencesofanactioniscalledmakingan\\ni n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng,\\nwhichwedonotexploreinthisbook.\\n3.6TheChainRuleofConditionalProbabilities\\nAnyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed\\nintoconditionaldistributionsoveronlyonevariable:\\nP(x( 1 ),...,x( ) n) = (Px( 1 ))Πn\\ni = 2P(x( ) i|x( 1 ),...,x( 1 ) i −).(3.6)\\nThisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability.\\nItfollowsimmediatelyfromthedeﬁnitionofconditionalprobabilityinequation.3.5\\n59', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76e05b40-1e5a-45d4-8a83-104a7735dfb7', embedding=None, metadata={'page_label': '75', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nForexample,applyingthedeﬁnitiontwice,weget\\nP,,P,P, (abc)= (ab|c)(bc)\\nP,PP (bc)= ( )bc| ()c\\nP,,P,PP. (abc)= (ab|c)( )bc| ()c\\n3.7IndependenceandConditionalIndependence\\nTworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution\\ncanbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving\\nonlyy:\\n∀∈ ∈xx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y.\\nTworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom\\nvariableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis\\nwayforeveryvalueofz:\\n∀∈ ∈ ∈ | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z.\\n(3.8)\\nWe\\xa0candenoteindependence\\xa0andconditionalindependence\\xa0with compact\\nnotation:xy⊥meansthatxandyareindependent,whilexyz ⊥|meansthatx\\nandyareconditionallyindependentgivenz.\\n3.8Expectation,VarianceandCovariance\\nThe e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa\\nprobabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx\\nisdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P\\nE x ∼ P[()] =fx\\ue058\\nxPxfx, ()() (3.9)\\nwhileforcontinuousvariables,itiscomputedwithanintegral:\\nE x ∼ p[()] =fx\\ue05a\\npxfxdx. ()() (3.10)\\n60', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9cb4782c-33ab-4b24-9cda-c00423795e1e', embedding=None, metadata={'page_label': '76', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nWhentheidentityofthedistributionisclearfromthecontext,wemaysimply\\nwritethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)].\\nIfitisclearwhichrandomvariabletheexpectationisover,wemayomitthe\\nsubscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[·]averagesover\\nthevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis\\nnoambiguity,wemayomitthesquarebrackets.\\nExpectationsarelinear,forexample,\\nE x[()+ ()] = αfxβgxα E x[()]+fxβ E x[()]gx, (3.11)\\nwhenandarenotdependenton. αβ x\\nThe v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom\\nvariablexvaryaswesamplediﬀerentvaluesofxfromitsprobabilitydistribution:\\nVar(()) = fx E\\ue068\\n(() [()]) fx− Efx2\\ue069\\n. (3.12)\\nWhenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The\\nsquarerootofthevarianceisknownasthe . st andar d dev i at i o n\\nThe c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated\\ntoeachother,aswellasthescaleofthesevariables:\\nCov(()()) = [(() [()])(() [()])] fx,gy Efx− Efxgy− Egy.(3.13)\\nHighabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch\\nandarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe\\ncovarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues\\nsimultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto\\ntakeonarelativelyhighvalueatthetimesthattheothertakesonarelatively\\nlowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe\\ncontributionofeachvariableinordertomeasureonlyhowmuchthevariablesare\\nrelated,ratherthanalsobeingaﬀectedbythescaleoftheseparatevariables.\\nThenotionsofcovarianceanddependencearerelated,butareinfactdistinct\\nconcepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero\\ncovariance,andtwovariablesthathavenon-zerocovariancearedependent.How-\\never,independence isadistinctpropertyfromcovariance.Fortwovariablestohave\\nzerocovariance,theremustbenolineardependencebetweenthem.Independence\\nisastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes\\nnonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave\\nzerocovariance.Forexample,supposeweﬁrstsamplearealnumberxfroma\\nuniformdistributionovertheinterval[−1,1].Wenextsamplearandomvariable\\n61', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db82fc46-371a-43ac-96dd-b7d4b910f6ff', embedding=None, metadata={'page_label': '77', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\ns.Withprobability1\\n2,wechoosethevalueofstobe.Otherwise,wechoose 1\\nthevalueofstobe−1.Wecanthengeneratearandomvariableybyassigning\\ny=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines\\nthemagnitudeof.However,y Cov() = 0x,y.\\nThe c o v ar i anc e m at r i xofarandomvector x∈ Rnisannn×matrix,such\\nthat\\nCov() x i , j= Cov(x i,x j). (3.14)\\nThediagonalelementsofthecovariancegivethevariance:\\nCov(x i,x i) = Var(x i). (3.15)\\n3.9CommonProbabilityDistributions\\nSeveralsimpleprobabilitydistributionsareusefulinmanycontextsinmachine\\nlearning.\\n3.9.1BernoulliDistribution\\nThe B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable.\\nItiscontrolledbyasingleparameterφ∈[0,1],whichgivestheprobabilityofthe\\nrandomvariablebeingequalto1.Ithasthefollowingproperties:\\nP φ (= 1) = x (3.16)\\nP φ (= 0) = 1x − (3.17)\\nPxφ (= x ) = x(1 )−φ1 − x(3.18)\\nE x[] = xφ (3.19)\\nVar x() = (1 )xφ−φ (3.20)\\n3.9.2MultinoulliDistribution\\nThe m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete\\nvariablewithkdiﬀerentstates,wherekisﬁnite.1Themultinoullidistributionis\\n1“Multinoulli”isatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby\\nMurphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution.\\nAmultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany\\ntimeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution.\\nManytextsusetheterm“multinomial”torefertomultinoullidistributionswithoutclarifying\\nthattheyreferonlytothecase. n= 1\\n62', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4df4fcdf-f38c-4abc-abcb-c4126e497494', embedding=None, metadata={'page_label': '78', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nparametrized byavector p∈[0,1]k − 1,wherep igivestheprobabilityofthei-th\\nstate.Theﬁnal,k-thstate’sprobabilityisgivenby1− 1\\ue03ep.Notethatwemust\\nconstrain 1\\ue03ep≤1.Multinoullidistributionsareoftenusedtorefertodistributions\\novercategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical\\nvalue1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation\\norvarianceofmultinoulli-dis tributedrandomvariables.\\nTheBernoulliandmultinoullidistributionsaresuﬃcienttodescribeanydistri-\\nbutionovertheirdomain.\\xa0They areabletodescribeanydistributionovertheir\\ndomainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause\\ntheirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto\\nenumerateallofthestates.Whendealingwithcontinuousvariables,thereare\\nuncountablymanystates,soanydistributiondescribedbyasmallnumberof\\nparametersmustimposestrictlimitsonthedistribution.\\n3.9.3GaussianDistribution\\nThemostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu-\\nt i o n,alsoknownasthe : G aussian di st r i but i o n\\nN(;xµ,σ2) =\\ue072\\n1\\n2πσ2exp\\ue012\\n−1\\n2σ2( )xµ−2\\ue013\\n.(3.21)\\nSeeﬁgureforaplotofthedensityfunction. 3.1\\nThetwoparameters µ∈ Randσ∈(0,∞)controlthenormaldistribution.\\nTheparameterµgivesthecoordinateofthecentralpeak.Thisisalsothemeanof\\nthedistribution: E[x] =µ.Thestandarddeviationofthedistributionisgivenby\\nσ,andthevariancebyσ2.\\nWhenweevaluatethePDF,weneedtosquareandinvertσ.Whenweneedto\\nfrequentlyevaluatethePDFwithdiﬀerentparametervalues,amoreeﬃcientway\\nofparametrizing thedistributionistouseaparameterβ∈(0,∞)tocontrolthe\\npr e c i si o norinversevarianceofthedistribution:\\nN(;xµ,β− 1) =\\ue072\\nβ\\n2πexp\\ue012\\n−1\\n2βxµ (−)2\\ue013\\n. (3.22)\\nNormaldistributionsareasensiblechoiceformanyapplications.Intheabsence\\nofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould\\ntake,thenormaldistributionisagooddefaultchoicefortwomajorreasons.\\n63', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='01deff93-fe53-449f-bb55-c85df3919436', embedding=None, metadata={'page_label': '79', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n− − − − 20 . 15 . 10 . 05 00 05 10 15 20 . . . . . .\\nx000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x µ\\nInﬂectionpointsat\\nx µ σ = ±\\nFigure3.1:Thenormaldistribution:ThenormaldistributionN(x;µ,σ2)exhibits\\naclassic“bellcurve”shape,withthexcoordinateofitscentralpeakgivenbyµ,and\\nthewidthofitspeakcontrolledbyσ.Inthisexample,wedepictthestandardnormal\\ndistribution,withand. µ= 0σ= 1\\nFirst,manydistributionswewishtomodelaretrulyclosetobeingnormal\\ndistributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen-\\ndentrandomvariablesisapproximatelynormallydistributed.Thismeansthat\\ninpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally\\ndistributednoise,evenifthesystemcanbedecomposedintopartswithmore\\nstructuredbehavior.\\nSecond,outofallpossibleprobabilitydistributionswiththesamevariance,\\nthenormaldistributionencodesthemaximumamountofuncertaintyoverthe\\nrealnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone\\nthatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping\\nandjustifyingthisidearequiresmoremathematical tools,andispostponedto\\nsection.19.4.2\\nThenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe\\nm ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive\\ndeﬁnitesymmetricmatrix: Σ\\nN(; ) = x µ, Σ\\ue073\\n1\\n(2)πndet() Σexp\\ue012\\n−1\\n2( ) x µ−\\ue03eΣ− 1( ) x µ−\\ue013\\n.(3.23)\\n64', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9be17992-c420-4365-8111-a001b679e97a', embedding=None, metadata={'page_label': '80', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nTheparameter µstillgivesthemeanofthedistribution,thoughnowitis\\nvector-valued.Theparameter Σgivesthecovariancematrixofthedistribution.\\nAsintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor\\nmanydiﬀerentvaluesoftheparameters,thecovarianceisnotacomputationally\\neﬃcientwaytoparametrizethedistribution,sinceweneedtoinvert Σtoevaluate\\nthePDF.Wecaninsteadusea : pr e c i si o n m at r i x β\\nN(; x µ β,− 1) =\\ue073\\ndet() β\\n(2)πnexp\\ue012\\n−1\\n2( ) x µ−\\ue03eβ x µ (−)\\ue013\\n.(3.24)\\nWeoftenﬁxthecovariancematrixtobeadiagonalmatrix.Anevensimpler\\nversionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar\\ntimestheidentitymatrix.\\n3.9.4ExponentialandLaplaceDistributions\\nInthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution\\nwithasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al\\ndi st r i but i o n:\\npxλλ (;) = 1 x ≥ 0exp( )−λx. (3.25)\\nTheexponentialdistributionusestheindicatorfunction 1 x ≥ 0toassignprobability\\nzerotoallnegativevaluesof.x\\nAcloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak\\nofprobabilitymassatanarbitrarypointistheµ L apl ac e di st r i but i o n\\nLaplace(;) =xµ,γ1\\n2γexp\\ue012\\n−|−|xµ\\nγ\\ue013\\n. (3.26)\\n3.9.5TheDiracDistributionandEmpiricalDistribution\\nInsomecases,wewishtospecifythatallofthemassinaprobabilitydistribution\\nclustersaroundasinglepoint.ThiscanbeaccomplishedbydeﬁningaPDFusing\\ntheDiracdeltafunction,:δx()\\npxδxµ. () = (−) (3.27)\\nTheDiracdeltafunctionisdeﬁnedsuchthatitiszero-valuedeverywhereexcept\\n0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat\\nassociateseachvaluexwithareal-valuedoutput,insteaditisadiﬀerentkindof\\n65', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e1a11b9-8030-4646-a325-afdd8a3d5828', embedding=None, metadata={'page_label': '81', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nmathematical objectcalleda g e ner al i z e d f unc t i o nthatisdeﬁnedintermsofits\\npropertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe\\nlimitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother\\nthanzero.\\nBydeﬁningp(x)tobeδshiftedby−µweobtainaninﬁnitelynarrowand\\ninﬁnitelyhighpeakofprobabilitymasswhere.xµ= \\nAcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l\\ndi st r i but i o n,\\nˆp() = x1\\nmm\\ue058\\ni = 1δ( x x−( ) i) (3.28)\\nwhichputsprobabilitymass1\\nmoneachofthempoints x( 1 ),..., x( ) mforminga\\ngivendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary\\ntodeﬁnetheempiricaldistributionovercontinuousvariables.Fordiscretevariables,\\nthesituationissimpler:anempiricaldistributioncanbeconceptualized asa\\nmultinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue\\nthatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset.\\nWecanviewtheempiricaldistributionformedfromadatasetoftraining\\nexamplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel\\nonthisdataset.\\xa0Anotherimportantperspectiveontheempiricaldistributionis\\nthatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata\\n(seesection).5.5\\n3.9.6MixturesofDistributions\\nItisalsocommontodeﬁneprobabilitydistributionsbycombiningothersimpler\\nprobabilitydistributions.Onecommon\\xa0wayof\\xa0combining\\xa0distributionsis\\xa0to\\nconstructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral\\ncomponentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution\\ngeneratesthesampleisdeterminedbysamplingacomponentidentityfroma\\nmultinoullidistribution:\\nP() =x\\ue058\\niPiPi (= c )( = xc| ) (3.29)\\nwherecisthemultinoullidistributionovercomponentidentities. P()\\nWehavealreadyseenoneexampleofamixturedistribution:theempirical\\ndistributionoverreal-valuedvariablesisamixturedistributionwithoneDirac\\ncomponentforeachtrainingexample.\\n66', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='33bb42fd-3e3d-4635-a119-eb130dd3aa8d', embedding=None, metadata={'page_label': '82', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nThemixturemodelisonesimplestrategyforcombiningprobabilitydistributions\\ntocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16\\nprobabilitydistributionsfromsimpleonesinmoredetail.\\nThemixturemodelallowsustobrieﬂyglimpseaconceptthatwillbeof\\nparamountimportancelater—the l at e n t v ar i abl e.Alatentvariableisarandom\\nvariablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe\\nmixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough\\nthejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c)\\noverthelatentvariableandthedistributionP(xc|)relatingthelatentvariables\\ntothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough\\nitispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent\\nvariablesarediscussedfurtherinsection.16.5\\nAverypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e\\nmodel,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas\\naseparatelyparametrized mean µ( ) iandcovariance Σ( ) i.Somemixturescanhave\\nmoreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents\\nviatheconstraint Σ( ) i= Σ,i∀.AswithasingleGaussiandistribution,themixture\\nofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe\\ndiagonalorisotropic.\\nInadditiontothemeansandcovariances,theparametersofaGaussianmixture\\nspecifythe pr i o r pr o babili t yα i=P(c=i) giventoeachcomponenti.Theword\\n“prior”indicatesthatitexpressesthemodel’sbeliefsaboutc b e f o r eithasobserved\\nx.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed\\na f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r\\nofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany\\nspeciﬁc,non-zeroamountoferrorbyaGaussianmixturemodelwithenough\\ncomponents.\\nFigureshowssamplesfromaGaussianmixturemodel. 3.2\\n3.10UsefulPropertiesofCommonFunctions\\nCertainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially\\ntheprobabilitydistributionsusedindeeplearningmodels.\\nOneofthesefunctionsisthe : l o g i st i c si g m o i d\\nσx() =1\\n1+exp()−x. (3.30)\\nThelogisticsigmoidiscommonlyusedtoproducetheφparameterofaBernoulli\\n67', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='71ba5dd0-e7e4-4951-b78e-31de8adf3ef0', embedding=None, metadata={'page_label': '83', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nx 1x 2\\nFigure3.2:\\xa0SamplesfromaGaussianmixturemodel.Inthisexample,therearethree\\ncomponents.Fromlefttoright,theﬁrstcomponenthasanisotropiccovariancematrix,\\nmeaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal\\ncovariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned\\ndirection.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The\\nthirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance\\nseparatelyalonganarbitrarybasisofdirections.\\ndistributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues\\nfortheφparameter.Seeﬁgureforagraphofthesigmoidfunction.The 3.3\\nsigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative,\\nmeaningthatthefunctionbecomesveryﬂatandinsensitivetosmallchangesinits\\ninput.\\nAnothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l .\\n2001):\\nζx x. () = log(1+exp()) (3.31)\\nThesoftplusfunctioncanbeusefulforproducingtheβorσparameterofanormal\\ndistributionbecauseitsrangeis(0,∞).Italsoarisescommonlywhenmanipulating\\nexpressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe\\nfactthatitisasmoothedor“softened”versionof\\nx+= max(0),x. (3.32)\\nSeeﬁgureforagraphofthesoftplusfunction. 3.4\\nThefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize\\nthem:\\n68', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1a5a88c-cb9d-402f-a186-76c300298448', embedding=None, metadata={'page_label': '84', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n− − 1 0 5 0 5 1 0\\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .σ x ( )\\nFigure3.3:Thelogisticsigmoidfunction.\\n− − 1 0 5 0 5 1 0\\nx024681 0ζ x ( )\\nFigure3.4:Thesoftplusfunction.\\n69', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='40fe68d4-93e8-41a3-8d32-efba50d98f27', embedding=None, metadata={'page_label': '85', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nσx() =exp()x\\nexp()+exp(0)x(3.33)\\nd\\ndxσxσxσx () = ()(1−()) (3.34)\\n1 () = () −σxσ−x (3.35)\\nlog() = () σx −ζ−x (3.36)\\nd\\ndxζxσx () = () (3.37)\\n∀∈x(01),,σ− 1() = logx\\ue012x\\n1−x\\ue013\\n(3.38)\\n∀x>,ζ0− 1() = log(exp()1) x x− (3.39)\\nζx() =\\ue05ax\\n− ∞σydy() (3.40)\\nζxζxx ()−(−) = (3.41)\\nThefunctionσ− 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely\\nusedinmachinelearning.\\nEquationprovidesextrajustiﬁcationforthename“softplus.”Thesoftplus 3.41\\nfunctionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+=\\nmax{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t\\nfunction,x−=max{0,x−}.Toobtainasmoothfunctionthatisanalogoustothe\\nnegativepart,onecanuseζ(−x).Justasxcanberecoveredfromitspositivepart\\nandnegativepartviatheidentityx+−x−=x,itisalsopossibletorecoverx\\nusingthesamerelationshipbetweenand,asshowninequation. ζx()ζx(−) 3.41\\n3.11Bayes’Rule\\nWeoftenﬁndourselvesinasituationwhereweknowP(yx|)andneedtoknow\\nP(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity\\nusing B a y e s’ r ul e:\\nP( ) =xy|PP()x( )yx|\\nP()y. (3.42)\\nNotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute\\nP() =y\\ue050\\nxPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y.\\n70', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9811f363-da7f-4b51-9f85-49e091401be1', embedding=None, metadata={'page_label': '86', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nBayes’ruleis\\xa0straightforwardto\\xa0derivefrom\\xa0thedeﬁnitionofconditional\\nprobability,butitisusefultoknowthenameofthisformulasincemanytexts\\nrefertoitbyname.ItisnamedaftertheReverendThomasBayes,whoﬁrst\\ndiscoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas\\nindependentlydiscoveredbyPierre-SimonLaplace.\\n3.12TechnicalDetailsofContinuousVariables\\nAproperformalunderstandingofcontinuousrandomvariablesandprobability\\ndensityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof\\nmathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof\\nthistextbook,butwecanbrieﬂysketchsomeoftheissuesthatmeasuretheoryis\\nemployedtoresolve.\\nInsection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x\\nlyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices\\nofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets\\nS 1and S 2suchthatp( x∈ S 1) +p( x∈ S 2)>1but S 1∩ S 2=∅.Thesesets\\naregenerallyconstructedmakingveryheavyuseoftheinﬁniteprecisionofreal\\nnumbers,forexamplebymakingfractal-shapedsetsorsetsthataredeﬁnedby\\ntransformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure\\ntheoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe\\nprobabilityofwithoutencounteringparadoxes.\\xa0Inthisbook,weonlyintegrate\\noversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever\\nbecomesarelevantconcern.\\nForourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat\\napplytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory\\nprovidesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such\\nasetissaidtohave m e asur e z e r o.Wedonotformallydeﬁnethisconceptinthis\\ntextbook.Forourpurposes,itissuﬃcienttounderstandtheintuitionthataset\\nofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample,\\nwithin R2,alinehasmeasurezero,whileaﬁlledpolygonhaspositivemeasure.\\nLikewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets\\nthateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational\\nnumbershasmeasurezero,forinstance).\\nAnotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty\\nthatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof\\n2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets.\\n71', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9c05cb2-975c-4b73-8481-cc48691acdc5', embedding=None, metadata={'page_label': '87', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nmeasurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they\\ncanbesafelyignoredformanyapplications.Someimportantresultsinprobability\\ntheoryholdforalldiscretevaluesbutonlyhold“almosteverywhere”forcontinuous\\nvalues.\\nAnothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous\\nrandomvariablesthataredeterministicfunctionsofoneanother.Supposewehave\\ntworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con-\\ntinuous,diﬀerentiabletransformation.Onemightexpectthatp y( y) =p x(g− 1( y)).\\nThisisactuallynotthecase.\\nAsasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose\\ny=x\\n2andx∼U(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0\\neverywhereexcepttheinterval[0,1\\n2] 1 ,anditwillbeonthisinterval.Thismeans\\n\\ue05a\\np y()=ydy1\\n2, (3.43)\\nwhichviolatesthedeﬁnitionofaprobabilitydistribution.Thisisacommonmistake.\\nTheproblemwiththisapproachisthatitfailstoaccountforthedistortionof\\nspaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan\\ninﬁnitesimallysmallregionwithvolumeδ xisgivenbyp( x)δ x.Sincegcanexpand\\norcontractspace,theinﬁnitesimalvolumesurrounding xin xspacemayhave\\ndiﬀerentvolumeinspace. y\\nToseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto\\npreservetheproperty\\n|p y(())= gxdy||p x()xdx.| (3.44)\\nSolvingfromthis,weobtain\\np y() = yp x(g− 1())y\\ue00c\\ue00c\\ue00c\\ue00c∂x\\n∂y\\ue00c\\ue00c\\ue00c\\ue00c(3.45)\\norequivalently\\np x() = xp y(())gx\\ue00c\\ue00c\\ue00c\\ue00c∂gx()\\n∂x\\ue00c\\ue00c\\ue00c\\ue00c. (3.46)\\nInhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an\\nm at r i x—thematrixwithJ i , j=∂ x i\\n∂ y j.Thus,forreal-valuedvectorsand, x y\\np x() = xp y(())g x\\ue00c\\ue00c\\ue00c\\ue00cdet\\ue012∂g() x\\n∂ x\\ue013 \\ue00c\\ue00c\\ue00c\\ue00c. (3.47)\\n72', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8cebd30d-ce70-4c36-b4b8-071cc1aa1517', embedding=None, metadata={'page_label': '88', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n3.13InformationTheory\\nInformationtheory\\xa0isa\\xa0branchof\\xa0appliedmathematics\\xa0thatrevolvesaround\\nquantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented\\ntostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas\\ncommunicationviaradiotransmission.Inthiscontext,informationtheorytellshow\\ntodesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom\\nspeciﬁcprobabilitydistributionsusingvariousencodingschemes.Inthecontextof\\nmachinelearning,wecanalsoapplyinformationtheorytocontinuousvariables\\nwheresomeofthesemessagelengthinterpretations donotapply.Thisﬁeldis\\nfundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis\\ntextbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize\\nprobabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions.\\nFormoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or\\n().2003\\nThebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely\\neventhas\\xa0occurredismoreinformativethanlearningthata\\xa0likely\\xa0eventhas\\noccurred.Amessagesaying“thesunrosethismorning”issouninformative as\\ntobeunnecessarytosend,butamessagesaying“therewasasolareclipsethis\\nmorning”isveryinformative.\\nWewouldliketoquantifyinformationinawaythatformalizesthisintuition.\\nSpeciﬁcally,\\n•Likelyeventsshouldhavelowinformationcontent,andintheextremecase,\\neventsthatareguaranteedtohappenshouldhavenoinformationcontent\\nwhatsoever.\\n•Lesslikelyeventsshouldhavehigherinformationcontent.\\n•Independenteventsshouldhaveadditiveinformation. Forexample,ﬁnding\\noutthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas\\nmuchinformationasﬁndingoutthatatossedcoinhascomeupasheads\\nonce.\\nInordertosatisfyallthreeoftheseproperties,wedeﬁnethe se l f - i nf o r m a t i o n\\nofaneventxtobe = x\\nIxPx. () = log− () (3.48)\\nInthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our\\ndeﬁnitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof\\n73', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fff643c2-a5f4-42bc-8dd6-ca58f9705f64', embedding=None, metadata={'page_label': '89', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\ninformationgainedbyobservinganeventofprobability1\\ne.Othertextsusebase-2\\nlogarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis\\njustarescalingofinformationmeasuredinnats.\\nWhenxiscontinuous,weusethesamedeﬁnitionofinformationbyanalogy,\\nbutsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent\\nwithunitdensitystillhaszeroinformation, despitenotbeinganeventthatis\\nguaranteedtooccur.\\nSelf-information dealsonlywithasingleoutcome.Wecanquantifytheamount\\nofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y:\\nH() = x E x ∼ P[()] = Ix − E x ∼ P[log()]Px. (3.49)\\nalsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe\\nexpectedamountofinformationinaneventdrawnfromthatdistribution.Itgives\\nalowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits\\narediﬀerent)neededonaveragetoencodesymbolsdrawnfromadistributionP.\\nDistributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain)\\nhavelowentropy;distributionsthatareclosertouniformhavehighentropy.See\\nﬁgureforademonstration.When 3.5 xiscontinuous,theShannonentropyis\\nknownasthe di ﬀ e r e n t i al e nt r o p y.\\nIfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame\\nrandomvariablex,wecanmeasurehowdiﬀerentthesetwodistributionsareusing\\nthe K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e:\\nD K L( ) = PQ\\ue06b E x ∼ P\\ue014\\nlogPx()\\nQx()\\ue015\\n= E x ∼ P[log()log()] Px−Qx.(3.50)\\nInthecaseofdiscretevariables,itistheextraamountofinformation(measured\\ninbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2\\nandthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn\\nfromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize\\nthelengthofmessagesdrawnfromprobabilitydistribution.Q\\nTheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon-\\nnegative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin\\nthecaseofdiscretevariables,orequal“almosteverywhere”inthecaseofcontinuous\\nvariables.BecausetheKLdivergenceisnon-negativeandmeasuresthediﬀerence\\nbetweentwodistributions,itisoftenconceptualized asmeasuringsomesortof\\ndistancebetweenthesedistributions.However,itisnotatruedistancemeasure\\nbecauseitisnotsymmetric:D K L(PQ\\ue06b)\\ue036=D K L(QP\\ue06b)forsomePandQ.\\xa0This\\n74', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0af86c9b-fb80-4cda-87ec-836fb123fc4e', embedding=None, metadata={'page_label': '90', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\\np0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s\\nFigure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow\\nShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy.\\nOnthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal\\nto.Theentropyisgivenby 1 (p−1)log(1−p)−pplog.Whenpisnear0,thedistribution\\nisnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1,\\nthedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1.\\nWhenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo\\noutcomes.\\nasymmetrymeansthatthereareimportantconsequencestothechoiceofwhether\\ntouseD K L( )PQ\\ue06borD K L( )QP\\ue06b.Seeﬁgureformoredetail.3.6\\nAquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y\\nH(P,Q) =H(P)+D K L(PQ\\ue06b),whichissimilartotheKLdivergencebutlacking\\nthetermontheleft:\\nHP,Q( ) = − E x ∼ Plog()Qx. (3.51)\\nMinimizingthecross-entropywithrespecttoQisequivalenttominimizingthe\\nKLdivergence,becausedoesnotparticipateintheomittedterm. Q\\nWhencomputingmanyofthesequantities,itiscommontoencounterexpres-\\nsionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we\\ntreattheseexpressionsaslim x → 0xxlog= 0.\\n3.14StructuredProbabilisticModels\\nMachinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery\\nlargenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve\\ndirectinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto\\n75', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='923f5e35-1629-41a6-b1c2-9c1b5ce924c1', embedding=None, metadata={'page_label': '91', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\nxProbability Densityq∗= argminq D K L() p q \\ue06b\\np x()\\nq∗() x\\nxProbability Densityq∗= argminq D K L() q p \\ue06b\\np() x\\nq∗() x\\nFigure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and\\nwishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing\\neitherD KL(pq\\ue06b)orD KL(qp\\ue06b).Weillustratetheeﬀectofthischoiceusingamixtureof\\ntwoGaussiansforp,andasingleGaussianforq.\\xa0Thechoiceofwhichdirectionofthe\\nKLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation\\nthatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh\\nprobability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh\\nprobabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe\\ndirectionoftheKLdivergencereﬂectswhichoftheseconsiderationstakespriorityforeach\\napplication. ( L e f t )TheeﬀectofminimizingD KL(pq\\ue06b).Inthiscase,weselectaqthathas\\nhighprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto\\nblurthemodestogether,inordertoputhighprobabilitymassonallofthem. ( R i g h t )The\\neﬀectofminimizingD KL(qp\\ue06b).Inthiscase,weselectaqthathaslowprobabilitywhere\\nphaslowprobability.Whenphasmultiplemodesthataresuﬃcientlywidelyseparated,\\nasinthisﬁgure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto\\navoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we\\nillustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave\\nachievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes\\narenotseparatedbyasuﬃcientlystronglowprobabilityregion,thenthisdirectionofthe\\nKLdivergencecanstillchoosetoblurthemodes.\\n76', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6b77ecee-0e5d-4ab2-866d-c21d72f279a0', embedding=None, metadata={'page_label': '92', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\ndescribetheentirejointprobabilitydistributioncanbeveryineﬃcient(both\\ncomputationally andstatistically).\\nInsteadofusingasinglefunctiontorepresentaprobabilitydistribution,we\\ncansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether.\\nForexample,supposewehavethreerandomvariables:a,bandc.Supposethat\\nainﬂuencesthevalueofbandbinﬂuencesthevalueofc,butthataandcare\\nindependentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree\\nvariablesasaproductofprobabilitydistributionsovertwovariables:\\np,,ppp. (abc) = ()a( )ba|( )cb| (3.52)\\nThesefactorizationscangreatlyreducethenumberofparametersneeded\\ntodescribethedistribution.Eachfactorusesanumberofparametersthatis\\nexponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly\\nreducethecostofrepresentingadistributionifweareabletoﬁndafactorization\\nintodistributionsoverfewervariables.\\nWecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword\\n“graph”inthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach\\notherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution\\nwithagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del.\\nTherearetwomainkindsofstructuredprobabilisticmodels:directedand\\nundirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode\\ninthegraphcorrespondstoarandomvariable,\\xa0and anedgeconnectingtwo\\nrandomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect\\ninteractionsbetweenthosetworandomvariables.\\nD i r e c t e dmodelsuse\\xa0graphswithdirectededges,\\xa0andtheyrepresentfac-\\ntorizationsintoconditionalprobabilitydistributions,asintheexampleabove.\\nSpeciﬁcally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin\\nthedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i\\ngiventheparentsofx i,denotedPa G(x i):\\np() = x\\ue059\\nip(x i|Pa G(x i)). (3.53)\\nSeeﬁgureforanexampleofadirectedgraphandthefactorizationofprobability 3.7\\ndistributionsitrepresents.\\nU ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent\\nfactorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions\\n77', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad119fb3-eadf-43af-974b-695466f18f7e', embedding=None, metadata={'page_label': '93', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\naa\\nccbb\\needd\\nFigure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph\\ncorrespondstoprobabilitydistributionsthatcanbefactoredas\\np,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54)\\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\\nareusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall\\nconnectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected\\nmodelisassociatedwithafactorφ( ) i(C( ) i).Thesefactorsarejustfunctions,not\\nprobabilitydistributions.Theoutputofeachfactormustbenon-negative, but\\nthereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability\\ndistribution.\\nTheprobabilityofaconﬁgurationofrandomvariablesis pr o p o r t i o naltothe\\nproductofallofthesefactors—assignmentsthatresultinlargerfactorvaluesare\\nmorelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We\\nthereforedividebyanormalizingconstantZ,deﬁnedtobethesumorintegral\\noverallstatesoftheproductoftheφfunctions,inordertoobtainanormalized\\nprobabilitydistribution:\\np() = x1\\nZ\\ue059\\niφ( ) i\\ue010\\nC( ) i\\ue011\\n. (3.55)\\nSeeﬁgureforanexampleofanundirectedgraphandthefactorizationof 3.8\\nprobabilitydistributionsitrepresents.\\nKeep\\xa0inmind\\xa0thatthese\\xa0graphicalrepresentationsof\\xa0factorizations are\\xa0a\\nlanguagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive\\nfamiliesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty\\nofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa\\n78', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='752cd5cf-429e-4367-975d-9b900676608f', embedding=None, metadata={'page_label': '94', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER3.PROBABILITYANDINFORMATIONTHEORY\\naa\\nccbb\\needd\\nFigure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This\\ngraphcorrespondstoprobabilitydistributionsthatcanbefactoredas\\np,,,, (abcde) =1\\nZφ( 1 )( )abc,,φ( 2 )()bd,φ( 3 )()ce,. (3.56)\\nThisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a\\nandcinteractdirectly,butaandeinteractonlyindirectlyviac.\\nprobabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth\\nways.\\nThroughoutpartsandofthisbook,wewillusestructuredprobabilistic III\\nmodelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships\\ndiﬀerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding\\nofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics,\\ninpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III\\ndetail.\\nThischapterhasreviewedthebasicconceptsofprobabilitytheorythatare\\nmostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools\\nremains:numericalmethods.\\n79', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='742ec071-935d-46bc-8dca-79b7b5da2e60', embedding=None, metadata={'page_label': '95', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 4\\nNumericalComputation\\nMachinelearningalgorithmsusuallyrequireahighamountofnumericalcompu-\\ntation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby\\nmethodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan\\nanalyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso-\\nlution.Commonoperationsincludeoptimization (ﬁndingthevalueofanargument\\nthatminimizesormaximizesafunction)andsolvingsystemsoflinearequations.\\nEvenjustevaluatingamathematical functiononadigitalcomputercanbediﬃcult\\nwhenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely\\nusingaﬁniteamountofmemory.\\n4. 1 O v erﬂ o w an d Un d erﬂ o w\\nThefundamentaldiﬃcultyinperformingcontinuousmathonadigitalcomputer\\nisthatweneedtorepresentinﬁnitelymanyrealnumberswithaﬁnitenumber\\nofbitpatterns.Thismeansthatforalmostallrealnumbers,\\xa0weincursome\\napproximationerrorwhenwerepresentthenumberinthecomputer.Inmany\\ncases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen\\nitcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin\\ntheorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof\\nroundingerror.\\nOneformofroundingerrorthatisparticularlydevastatingis under ﬂo w.\\nUnderﬂowoccurswhennumbersnearzeroareroundedtozero.Manyfunctions\\nbehavequalitativelydiﬀerentlywhentheirargumentiszeroratherthanasmall\\npositivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some\\n80', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee56e8e6-f7a5-46ee-bde2-5cb3a272f428', embedding=None, metadata={'page_label': '96', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nsoftwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna\\nresultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this\\nisusuallytreatedas−∞,whichthenbecomesnot-a-numberifitisusedformany\\nfurtherarithmeticoperations).\\nAnotherhighlydamagingformofnumericalerroris o v e r ﬂo w.Overﬂowoccurs\\nwhennumberswithlargemagnitudeareapproximatedas∞or−∞.Further\\narithmeticwillusuallychangetheseinﬁnitevaluesintonot-a-numbervalues.\\nOneexampleofafunctionthatmustbestabilizedagainstunderﬂowand\\noverﬂowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe\\nprobabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis\\ndeﬁnedtobe\\nsoftmax() x i=exp( x i)\\ue050n\\nj = 1exp( x j). (4.1)\\nConsiderwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically,\\nwecanseethatalloftheoutputsshouldbeequalto1\\nn.Numerically,thismay\\nnotoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will\\nunderﬂow.Thismeansthedenominator ofthesoftmaxwillbecome0,sotheﬁnal\\nresultisundeﬁned.When cisverylargeandpositive,exp( c)willoverﬂow,again\\nresultingintheexpressionasawholebeingundeﬁned.Bothofthesediﬃculties\\ncanberesolvedbyinsteadevaluating softmax( z)where z= x−max i x i.Simple\\nalgebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby\\naddingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults\\ninthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverﬂow.\\nLikewise,atleastoneterminthedenominator hasavalueof1,whichrulesout\\nthepossibilityofunderﬂowinthedenominator leadingtoadivisionbyzero.\\nThereisstillonesmallproblem.Underﬂowinthenumeratorcanstillcause\\ntheexpressionasawholetoevaluatetozero.Thismeansthatifweimplement\\nlogsoftmax( x)byﬁrstrunningthesoftmaxsubroutinethenpassingtheresultto\\nthelogfunction,wecoulderroneouslyobtain −∞.Instead,wemustimplement\\naseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The\\nlogsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize\\nthefunction. softmax\\nForthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations\\ninvolvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers\\noflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing\\ndeeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow-\\nlevellibrariesthatprovidestableimplementations .Insomecases,itispossible\\ntoimplementanewalgorithmandhavethenewimplementation automatically\\n8 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f00e1fb4-6912-46a6-b977-0185c3583bb3', embedding=None, metadata={'page_label': '97', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4. NUMERICALCOMPUTATION\\nstabilized. Theano ( , ; , ) is an example Bergstra et al.2010 Bastien et al.2012\\nof asoftwarepackage that automatically detects and stabilizes many common\\nnumericallyunstableexpressionsthatariseinthecontextofdeeplearning.\\n4.2 P o or Conditioning\\nConditioningreferstohowrapidlyafunctionchangeswithrespecttosmallchanges\\ninitsinputs. Functionsthatchangerapidlywhentheirinputsareperturbedslightly\\ncanbeproblematicforscientiﬁccomputationbecauseroundingerrorsintheinputs\\ncanresultinlargechangesintheoutput.\\nConsider the function f( x) = A−1x. When A∈ Rn n ×has an eigenvalue\\ndecomposition,its condition num ber is\\nmax\\ni,j\\ue00c\\ue00c\\ue00c\\ue00cλ i\\nλ j\\ue00c\\ue00c\\ue00c\\ue00c. (4.2)\\nThisistheratioofthemagnitudeofthelargestandsmallesteigenvalue. When\\nthisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput.\\nThis sensitivity is an intrinsic property of the matrix itself, not the result\\nofroundingerrorduringmatrixinversion. Poorlyconditionedmatricesamplify\\npre-existingerrorswhenwemultiplybythetruematrixinverse. Inpractice,the\\nerrorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself.\\n4.3 Gradien t-Based Optimization\\nMostdeeplearningalgorithmsinvolveoptimizationofsomesort.\\xa0Optimization\\nreferstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering\\nx. Weusuallyphrasemostoptimizationproblemsintermsofminimizing f( x).\\nMaximizationmaybeaccomplishedviaaminimizationalgorithmbyminimizing\\n−f( ) x.\\nThefunctionwewanttominimizeormaximizeiscalledthe ob jectiv e func-\\ntionor criterion. When we are minimizing it,\\xa0we may also call it the cost\\nfunction, loss function ,or error function .\\xa0Inthisbook,weusetheseterms\\ninterchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning\\ntosomeoftheseterms.\\nWe often denote the value that minimizes or maximizes a function with a\\nsuperscript . Forexample,wemightsay ∗ x∗= arg min ( ) f x.\\n82', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bbaed94a-6b30-4e43-9790-4f83d91baa14', embedding=None, metadata={'page_label': '98', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\n− − − − 20. 15. 10. 05 00 05 10 15 20 ......\\nx−20.−15.−10.−05.00.05.10.15.20.\\nGlobalminimumat= 0.x\\nSincef\\ue030() = 0,gradient x\\ndescent haltshere.\\nFor 0,wehave x< f\\ue030() 0,x<\\nsowecandecreasebyf\\nmoving rightward.For 0,wehave x> f\\ue030() 0,x>\\nsowecandecreasebyf\\nmoving leftward.\\nf x() =1\\n2x2\\nf\\ue030() = x x\\nFigure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa\\nfunctioncanbeusedtofollowthefunctiondownhilltoaminimum.\\nWeassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief\\nreviewofhowcalculusconceptsrelatetooptimization here.\\nSupposewehaveafunction y= f( x),whereboth xand yarerealnumbers.\\nThe der i v at i v eofthisfunctionisdenotedas f\\ue030( x)orasd y\\nd x.Thederivative f\\ue030( x)\\ngivestheslopeof f( x)atthepoint x.Inotherwords,itspeciﬁeshowtoscale\\nasmallchangeintheinputinordertoobtainthecorrespondingchangeinthe\\noutput: f x \\ue00f f x \\ue00f f (+) ≈()+\\ue030() x.\\nThederivativeisthereforeusefulforminimizingafunctionbecauseittells\\nushowtochange xinordertomakeasmallimprovementin y.Forexample,\\nweknowthat f( x \\ue00f−sign( f\\ue030( x)))islessthan f( x)forsmallenough \\ue00f.Wecan\\nthusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative.\\nThistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).Seeﬁgureforan4.1\\nexampleofthistechnique.\\nWhen f\\ue030( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection\\ntomove.Pointswhere f\\ue030( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y\\np o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring\\npoints,soitisnolongerpossibletodecrease f( x)bymakinginﬁnitesimalsteps.\\nA l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints,\\n8 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c3088323-2cc4-4d22-bb45-96fb623d9f3e', embedding=None, metadata={'page_label': '99', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nMinimum Maximum Saddlepoint\\nFigure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis\\napointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan\\ntheneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or\\nasaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself.\\nsoitisnotpossibletoincrease f( x)bymakinginﬁnitesimalsteps.Somecritical\\npointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See\\nﬁgureforexamplesofeachtypeofcriticalpoint. 4.2\\nApointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um.\\nItispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof\\nthefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally\\noptimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave\\nmanylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby\\nveryﬂatregions.Allofthismakesoptimization verydiﬃcult,especiallywhenthe\\ninputtothefunctionismultidimensional.Wethereforeusuallysettleforﬁndinga\\nvalueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See\\nﬁgureforanexample.4.3\\nWeoftenminimizefunctionsthathavemultipleinputs: f: Rn→ R.Forthe\\nconceptof“minimization”\\xa0to makesense,theremuststillbeonlyone(scalar)\\noutput.\\nForfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al\\nder i v at i v e s.Thepartialderivative∂\\n∂ x if( x)measureshow fchangesasonlythe\\nvariable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative\\ntothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe\\nvectorcontainingallofthepartialderivatives,denoted ∇ x f( x).Element iofthe\\ngradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions,\\n8 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ee7f8df-86a1-4424-857b-84b54945fb7a', embedding=None, metadata={'page_label': '100', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nxf x()\\nIdeally,wewouldlike\\ntoarriveattheglobal\\nminimum, butthis\\nmight notbepossible.Thislocalminimum\\nperformsnearlyaswellas\\ntheglobalone,\\nsoitisanacceptable\\nhaltingpoint.\\nThislocalminimumperforms\\npoorlyandshouldbeavoided.\\nFigure4.3:Optimizationalgorithmsmayfailtoﬁndaglobalminimumwhenthereare\\nmultiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally\\nacceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond\\ntosigniﬁcantlylowvaluesofthecostfunction.\\ncriticalpointsarepointswhereeveryelementofthegradientisequaltozero.\\nThe di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u\\nfunction findirection u.Inotherwords,thedirectionalderivativeisthederivative\\nofthefunction f( x+ α u)withrespectto α,evaluatedat α= 0.Usingthechain\\nrule,wecanseethat∂\\n∂ αf α (+ x u)evaluatesto u\\ue03e∇ x f α () xwhen = 0.\\nTominimize f,wewouldliketoﬁndthedirectioninwhich fdecreasesthe\\nfastest.Wecandothisusingthedirectionalderivative:\\nmin\\nu u ,\\ue03e u = 1u\\ue03e∇ x f() x (4.3)\\n=min\\nu u ,\\ue03e u = 1|||| u 2||∇ x f() x|| 2cos θ (4.4)\\nwhere θistheanglebetween uandthegradient.Substitutingin|||| u 2= 1and\\nignoringfactorsthatdonotdependon u,thissimpliﬁestomin ucos θ.Thisis\\nminimizedwhen upointsintheoppositedirectionasthegradient.Inother\\nwords,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly\\ndownhill.Wecandecrease fbymovinginthedirectionofthenegativegradient.\\nThisisknownasthe or . m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt\\nSteepestdescentproposesanewpoint\\nx\\ue030= x−∇ \\ue00f x f() x (4.5)\\n8 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58934dff-5118-4fc8-a332-fed929bbb47f', embedding=None, metadata={'page_label': '101', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nwhere \\ue00fisthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep.\\nWecanchoose \\ue00finseveraldiﬀerentways.Apopularapproachistoset \\ue00ftoasmall\\nconstant.Sometimes,wecansolveforthestepsizethatmakesthedirectional\\nderivativevanish.Anotherapproachistoevaluate f \\ue00f ( x−∇ x f()) xforseveral\\nvaluesof \\ue00fandchoosetheonethatresultsinthesmallestobjectivefunctionvalue.\\nThislaststrategyiscalleda l i ne se ar c h.\\nSteepestdescentconvergeswheneveryelementofthegradientiszero(or,in\\npractice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis\\niterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe\\nequation ∇ x f() = 0 xfor. x\\nAlthoughgradientdescentislimitedtooptimization incontinuousspaces,the\\ngeneralconceptofrepeatedlymakingasmallmove(thatisapproximately thebest\\nsmallmove)towardsbetterconﬁgurations canbegeneralizedtodiscretespaces.\\nAscendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng\\n( ,). RusselandNorvig2003\\n4 . 3 . 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces\\nSometimesweneedtoﬁndallofthepartialderivativesofafunctionwhoseinput\\nandoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis\\nknownasa J ac o bi an m at r i x.Speciﬁcally,ifwehaveafunction f: Rm→ Rn,\\nthentheJacobianmatrix J∈ Rn m ×ofisdeﬁnedsuchthat f J i , j=∂\\n∂ x jf() x i.\\nWearealsosometimesinterestedinaderivativeofaderivative.Thisisknown\\nasa se c o nd der i v at i v e.Forexample,forafunction f: Rn→ R,thederivative\\nwithrespectto x iofthederivativeof fwithrespectto x jisdenotedas∂2\\n∂ x i ∂ x jf.\\nInasingledimension,wecandenoted2\\nd x2 fby f\\ue030 \\ue030( x).Thesecondderivativetells\\nushowtheﬁrstderivativewillchangeaswevarytheinput.Thisisimportant\\nbecauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement\\naswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond\\nderivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many\\nfunctionsthatariseinpracticearenotquadraticbutcanbeapproximated well\\nasquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero,\\nthenthereisnocurvature.Itisaperfectlyﬂatline,anditsvaluecanbepredicted\\nusingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 \\ue00f\\nalongthenegativegradient,andthecostfunctionwilldecreaseby \\ue00f.Ifthesecond\\nderivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill\\nactuallydecreasebymorethan \\ue00f.Finally,ifthesecondderivativeispositive,the\\nfunctioncurvesupward,sothecostfunctioncandecreasebylessthan \\ue00f.See\\n8 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad22a6c5-1e96-48be-a0a5-548d2b927868', embedding=None, metadata={'page_label': '102', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nxf x()N e g a t i v e c u r v a t u r e\\nxf x()N o c u r v a t u r e\\nxf x()P o s i t i v e c u r v a t u r e\\nFigure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow\\nquadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost\\nfunctionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient\\nstepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster\\nthanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease\\ncorrectly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected\\nandeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe\\nfunctioninadvertently.\\nﬁguretoseehowdiﬀerentformsofcurvatureaﬀecttherelationshipbetween 4.4\\nthevalueofthecostfunctionpredictedbythegradientandthetruevalue.\\nWhenourfunctionhasmultipleinputdimensions,therearemanysecond\\nderivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe\\nHessian m at r i x.TheHessianmatrix isdeﬁnedsuchthat H x()( f)\\nH x()( f) i , j=∂2\\n∂ x i ∂ x jf .() x (4.6)\\nEquivalently,theHessianistheJacobianofthegradient.\\nAnywherethatthesecondpartialderivativesarecontinuous,thediﬀerential\\noperatorsarecommutative,i.e.theirordercanbeswapped:\\n∂2\\n∂ x i ∂ x jf() = x∂2\\n∂ x j ∂ x if .() x (4.7)\\nThisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints.\\nMostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric\\nHessianalmosteverywhere.\\xa0Because theHessianmatrixisrealandsymmetric,\\nwecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof\\n8 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b16a7e50-ba3d-494b-b087-c68e6662115a', embedding=None, metadata={'page_label': '103', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\neigenvectors.Thesecondderivativeinaspeciﬁcdirectionrepresentedbyaunit\\nvector disgivenby d\\ue03eH d.When disaneigenvectorof H,thesecondderivative\\ninthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof\\nd,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues,\\nwithweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d\\nreceivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond\\nderivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative.\\nThe(directional)secondderivativetellsushowwellwecanexpectagradient\\ndescentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation\\ntothefunction aroundthecurrentpoint f() x x( 0 ):\\nf f () x≈( x( 0 ))+( x x−( 0 ))\\ue03eg+1\\n2( x x−( 0 ))\\ue03eH x x (−( 0 )) .(4.8)\\nwhere gisthegradientand HistheHessianat x( 0 ).\\xa0Ifweusealearningrate\\nof \\ue00f,thenthenewpoint xwillbegivenby x( 0 )− \\ue00f g.Substitutingthisintoour\\napproximation,weobtain\\nf( x( 0 )− ≈ \\ue00f g) f( x( 0 ))− \\ue00f g\\ue03eg+1\\n2\\ue00f2g\\ue03eH g . (4.9)\\nTherearethree\\xa0termshere:theoriginalvalue\\xa0ofthefunction,\\xa0the expected\\nimprovementduetotheslopeofthefunction,andthecorrectionwemustapply\\ntoaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the\\ngradientdescentstepcanactuallymoveuphill.When g\\ue03eH giszeroornegative,\\ntheTaylorseriesapproximationpredictsthatincreasing \\ue00fforeverwilldecrease f\\nforever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge \\ue00f,so\\nonemustresorttomoreheuristicchoicesof \\ue00finthiscase.When g\\ue03eH gispositive,\\nsolvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of\\nthefunctionthemostyields\\n\\ue00f∗=g\\ue03eg\\ng\\ue03eH g. (4.10)\\nIntheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe\\nmaximaleigenvalue λ m a x,thenthisoptimalstepsizeisgivenby1\\nλmax.Tothe\\nextentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic\\nfunction,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning\\nrate.\\nThesecondderivativecanbeusedtodeterminewhetheracriticalpointis\\nalocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical\\npoint, f\\ue030( x) = 0.Whenthesecondderivative f\\ue030 \\ue030( x) >0,theﬁrstderivative f\\ue030( x)\\nincreasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans\\n8 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2f1d4a8e-fa4e-4590-9eba-7b68597a681a', embedding=None, metadata={'page_label': '104', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nf\\ue030( x \\ue00f−) <0and f\\ue030( x+ \\ue00f) >0forsmallenough \\ue00f.Inotherwords,aswemove\\nright,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope\\nbeginstopointuphilltotheleft.\\xa0Thus,when f\\ue030( x)=0and f\\ue030 \\ue030( x) >0,wecan\\nconcludethat xisalocalminimum.Similarly,when f\\ue030( x) = 0and f\\ue030 \\ue030( x) <0,we\\ncanconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e\\nt e st.Unfortunately,when f\\ue030 \\ue030( x) = 0,thetestisinconclusive.Inthiscase xmay\\nbeasaddlepoint,orapartofaﬂatregion.\\nInmultipledimensions,weneedtoexamineallofthesecondderivativesofthe\\nfunction.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize\\nthesecondderivativetesttomultipledimensions.Atacriticalpoint,where\\n∇ x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether\\nthecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe\\nHessianispositivedeﬁnite(allitseigenvaluesarepositive),thepointisalocal\\nminimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative\\ninanydirectionmustbepositive,andmakingreferencetotheunivariatesecond\\nderivativetest.Likewise,whentheHessianisnegativedeﬁnite(allitseigenvalues\\narenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually\\npossibletoﬁndpositiveevidenceofsaddlepointsinsomecases.\\xa0Whenatleast\\noneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat\\nxisalocalmaximumononecrosssectionof fbutalocalminimumonanother\\ncrosssection.Seeﬁgureforanexample.Finally,themultidimensionalsecond 4.5\\nderivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis\\ninconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat\\nleastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis\\ninconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue.\\nInmultipledimensions,thereisadiﬀerentsecondderivativeforeachdirection\\natasinglepoint.TheconditionnumberoftheHessianatthispointmeasures\\nhowmuchthesecondderivativesdiﬀerfromeachother.WhentheHessianhasa\\npoorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone\\ndirection,thederivativeincreasesrapidly,whileinanotherdirection,itincreases\\nslowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot\\nknowthatitneedstoexplorepreferentially inthedirectionwherethederivative\\nremainsnegativeforlonger.Italsomakesitdiﬃculttochooseagoodstepsize.\\nThestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing\\nuphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe\\nstepsizeistoosmalltomakesigniﬁcantprogressinotherdirectionswithless\\ncurvature.Seeﬁgureforanexample.4.6\\nThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide\\n8 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8a4a955b-e1d6-4222-88f2-00db981a4bf1', embedding=None, metadata={'page_label': '105', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\n\\ue078\\ue031\\x00\\ue031 \\ue035\\ue030\\ue031 \\ue035\\ue078 \\ue032\\x00\\ue031 \\ue035\\ue030\\ue031 \\ue035\\ue066\\ue078\\ue028\\ue031\\ue03b \\ue078\\ue032\\ue029\\n\\x00\\ue035 \\ue030 \\ue030\\ue030\\ue035 \\ue030 \\ue030\\nFigure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction\\ninthisexampleis f( x)= x2\\n1− x2\\n2.Alongtheaxiscorrespondingto x 1,thefunction\\ncurvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue.\\nAlongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan\\neigenvectoroftheHessianwithnegativeeigenvalue.Thename“saddlepoint”derivesfrom\\nthesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction\\nwithasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue\\nof0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative\\neigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal\\nmaximumwithinonecrosssectionandalocalminimumwithinanothercrosssection.\\n9 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b6e74c1-76ab-4205-b30b-07df60a9b3cc', embedding=None, metadata={'page_label': '106', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\n− − − 3 0 2 0 1 0 0 1 0 2 0\\nx 1− 3 0− 2 0− 1 001 02 0x 2\\nFigure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe\\nHessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose\\nHessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature\\nhasﬁvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost\\ncurvatureisinthedirection[1 ,1]\\ue03eandtheleastcurvatureisinthedirection[1 ,−1]\\ue03e.The\\nredlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic\\nfunctionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending\\ncanyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat\\ntoolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto\\ndescendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue\\noftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat\\nthisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon\\ntheHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch\\ndirectioninthiscontext.\\n9 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b0d280c6-ba48-45c3-aa49-3a42d971d181', embedding=None, metadata={'page_label': '107', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nthesearch.Thesimplestmethodfordoingsoisknownas Newt o n’ s m e t ho d.\\nNewton’smethodisbasedonusingasecond-orderTaylorseriesexpansionto\\napproximatenearsomepoint f() x x( 0 ):\\nf f () x≈( x( 0 ))+( x x−( 0 ))\\ue03e∇ x f( x( 0 ))+1\\n2( x x−( 0 ))\\ue03eH x()( f( 0 ))( x x−( 0 )) .(4.11)\\nIfwethensolveforthecriticalpointofthisfunction,weobtain:\\nx∗= x( 0 )− H x()( f( 0 ))− 1∇ x f( x( 0 )) . (4.12)\\nWhen fisapositivedeﬁnitequadraticfunction,Newton’smethodconsistsof\\napplyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12\\nWhen fisnottrulyquadraticbutcanbelocallyapproximatedasapositive\\ndeﬁnitequadratic,Newton’smethodconsistsofapplyingequationmultiple4.12\\ntimes.\\xa0Iterativelyupdatingtheapproximation andjumpingtotheminimumof\\ntheapproximation canreachthecriticalpointmuchfasterthangradientdescent\\nwould.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful\\npropertynearasaddlepoint.Asdiscussedinsection,Newton’smethodis 8.2.3\\nonlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues\\noftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle\\npointsunlessthegradientpointstowardthem.\\nOptimization algorithmsthatuseonlythegradient,suchasgradientdescent,\\narecalled ﬁr st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat\\nalsousetheHessianmatrix,suchasNewton’smethod,arecalled se c o nd-or d e r\\no pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,).\\nThe\\xa0optimization algorithms\\xa0employedin\\xa0mostcontextsin\\xa0this\\xa0book\\xa0are\\napplicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees.\\nDeeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions\\nusedindeeplearningisquitecomplicated.Inmanyotherﬁelds,thedominant\\napproachtooptimization istodesignoptimization algorithmsforalimitedfamily\\noffunctions.\\nInthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict-\\ningourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz\\ncontinuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate\\nofchangeisboundedbya L i psc hi t z c o nst antL:\\n∀∀| − |≤L||−|| x , y , f() x f() y x y 2 . (4.13)\\nThispropertyisusefulbecauseitallowsustoquantifyourassumptionthata\\nsmallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave\\n9 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df2d3661-37f5-43ae-ad9e-4426bc85d747', embedding=None, metadata={'page_label': '108', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nasmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint,\\nandmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous\\nwithrelativelyminormodiﬁcations.\\nPerhapsthemostsuccessfulﬁeldofspecializedoptimization is c o n v e x o p-\\nt i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore\\nguaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare\\napplicableonlytoconvexfunctions—functionsforwhichtheHessianispositive\\nsemideﬁniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle\\npointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most\\nproblemsindeeplearningarediﬃculttoexpressintermsofconvexoptimization.\\nConvexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms.\\nIdeasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving\\ntheconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance\\nofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For\\nmoreinformationaboutconvexoptimization, seeBoydandVandenberghe2004()\\norRockafellar1997().\\n4. 4 C on s t ra i n ed O p t i m i z a t i o n\\nSometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall\\npossible\\xa0values\\xa0of x.Insteadwemay\\xa0wishto\\xa0ﬁnd\\xa0themaximal\\xa0or\\xa0minimal\\nvalue\\xa0of f( x)for\\xa0valuesof xinsome\\xa0set S.Thisis\\xa0known\\xa0as c o nst r ai n e d\\no pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin\\nconstrainedoptimization terminology.\\nWeoftenwishtoﬁndasolutionthatissmallinsomesense.Acommon\\napproachinsuchsituationsistoimposeanormconstraint,suchas. ||||≤ x 1\\nOnesimpleapproachtoconstrainedoptimization issimplytomodifygradient\\ndescenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize \\ue00f,\\nwecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse\\nalinesearch,wecansearchonlyoverstepsizes \\ue00fthatyieldnew xpointsthatare\\nfeasible,orwecanprojecteachpointonthelinebackintotheconstraintregion.\\nWhenpossible,thismethodcanbemademoreeﬃcientbyprojectingthegradient\\nintothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning\\nthelinesearch(,).Rosen1960\\nAmoresophisticatedapproachistodesignadiﬀerent,unconstrainedopti-\\nmizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal,\\nconstrainedoptimization problem.Forexample,ifwewanttominimize f( x)for\\n9 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fda9e576-17c1-4076-a108-c20f045126c8', embedding=None, metadata={'page_label': '109', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nx∈ R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize\\ng( θ) = f([cossin θ , θ]\\ue03e)withrespectto θ,thenreturn[cossin θ , θ]asthesolution\\ntotheoriginalproblem.Thisapproachrequirescreativity;thetransformation\\nbetweenoptimization problemsmustbedesignedspeciﬁcallyforeachcasewe\\nencounter.\\nThe K ar ush– K u h n – T uc k e r(KKT)approach1providesaverygeneralso-\\nlutiontoconstrainedoptimization. WiththeKKTapproach,weintroducea\\nnewfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange\\nf unc t i o n.\\nTodeﬁnetheLagrangian,weﬁrstneedtodescribe Sintermsofequations\\nandinequalities.\\xa0W ewantadescriptionof Sintermsof mfunctions g( ) iand n\\nfunctions h( ) jsothat S={|∀ x i , g( ) i( x) = 0and∀ j , h( ) j( x)≤0}.Theequations\\ninvolving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving\\nh( ) jarecalled . i neq ual i t y c o nst r ai n t s\\nWeintroducenewvariables λ iand α jforeachconstraint,thesearecalledthe\\nKKTmultipliers.ThegeneralizedLagrangianisthendeﬁnedas\\nL , , f ( x λ α) = ()+ x\\ue058\\niλ i g( ) i()+ x\\ue058\\njα j h( ) j() x .(4.14)\\nWecannowsolveaconstrainedminimization problemusingunconstrained\\noptimization ofthegeneralizedLagrangian.Observethat,solongasatleastone\\nfeasiblepointexistsandisnotpermittedtohavevalue,then f() x ∞\\nmin\\nxmax\\nλmax\\nα α , ≥ 0L , , . ( x λ α) (4.15)\\nhasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x\\nmin\\nx ∈ Sf .() x (4.16)\\nThisfollowsbecauseanytimetheconstraintsaresatisﬁed,\\nmax\\nλmax\\nα α , ≥ 0L , , f , ( x λ α) = () x (4.17)\\nwhileanytimeaconstraintisviolated,\\nmax\\nλmax\\nα α , ≥ 0L , , . ( x λ α) = ∞ (4.18)\\n1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y\\nc o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s .\\n9 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9797c1cf-8326-444d-bf17-8e8bb32e9513', embedding=None, metadata={'page_label': '110', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nThesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe\\noptimumwithinthefeasiblepointsisunchanged.\\nToperformconstrainedmaximization, wecanconstructthegeneralizedLa-\\ngrangefunctionof,whichleadstothisoptimization problem: − f() x\\nmin\\nxmax\\nλmax\\nα α , ≥ 0− f()+ x\\ue058\\niλ i g( ) i()+ x\\ue058\\njα j h( ) j() x .(4.19)\\nWemayalsoconvertthistoaproblemwithmaximization intheouterloop:\\nmax\\nxmin\\nλmin\\nα α , ≥ 0f()+ x\\ue058\\niλ i g( ) i() x−\\ue058\\njα j h( ) j() x .(4.20)\\nThesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeﬁneit\\nwithadditionorsubtractionaswewish,becausetheoptimization isfreetochoose\\nanysignforeach λ i.\\nTheinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint\\nh( ) i( x)is ac t i v eif h( ) i( x∗) = 0.Ifaconstraintisnotactive,thenthesolutionto\\ntheproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif\\nthatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes\\nothersolutions.Forexample,aconvexproblemwithanentireregionofglobally\\noptimalpoints(awide,ﬂat,regionofequalcost)couldhaveasubsetofthis\\nregioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal\\nstationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However,\\nthepointfoundatconvergenceremainsastationarypointwhetherornotthe\\ninactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then\\nthesolutiontomin xmax λmax α α , ≥ 0 L( x λ α , ,)willhave α i=0.Wecanthus\\nobservethatatthesolution, α h\\ue00c( x)= 0.Inotherwords,forall i,weknow\\nthatatleastoneoftheconstraints α i≥0and h( ) i( x)≤0mustbeactiveatthe\\nsolution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution\\nisontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier\\ntoinﬂuencethesolutionto x,ortheinequalityhasnoinﬂuenceonthesolution\\nandwerepresentthisbyzeroingoutitsKKTmultiplier.\\nAsimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti-\\nmizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT)\\nconditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions,\\nbutnotalwayssuﬃcientconditions,forapointtobeoptimal.Theconditionsare:\\n•ThegradientofthegeneralizedLagrangianiszero.\\n•AllconstraintsonbothandtheKKTmultipliersaresatisﬁed. x\\n9 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='524dfc5f-c483-46b9-88e7-a4db736c8f56', embedding=None, metadata={'page_label': '111', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\n•Theinequalityconstraintsexhibit“complementary slackness”: α h\\ue00c( x) = 0.\\nFormoreinformationabouttheKKTapproach,seeNocedalandWright2006().\\n4. 5 E x am p l e: L i n ear L eas t S q u are s\\nSupposewewanttoﬁndthevalueofthatminimizes x\\nf() = x1\\n2||−|| A x b2\\n2 . (4.21)\\nTherearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeﬃciently.\\nHowever,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as\\nasimpleexampleofhowthesetechniqueswork.\\nFirst,weneedtoobtainthegradient:\\n∇ x f() = x A\\ue03e( ) = A x b− A\\ue03eA x A−\\ue03eb . (4.22)\\nWecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1\\nfordetails.\\nAl g o r i t hm 4 . 1Analgorithmtominimize f( x) =1\\n2||−|| A x b2\\n2withrespectto x\\nusinggradientdescent,startingfromanarbitraryvalueof. x\\nSetthestepsize()andtolerance()tosmall,positivenumbers. \\ue00f δ\\nwhi l e|| A\\ue03eA x A−\\ue03eb|| 2 > δ do\\nx x← − \\ue00f\\ue000\\nA\\ue03eA x A−\\ue03eb\\ue001\\ne nd whi l e\\nOnecanalsosolvethisproblemusingNewton’smethod.Inthiscase,because\\nthetruefunctionisquadratic,thequadraticapproximation employedbyNewton’s\\nmethodisexact,andthealgorithmconvergestotheglobalminimuminasingle\\nstep.\\nNowsuppose\\xa0we\\xa0wishto\\xa0minimizethesame\\xa0function,butsubjectto\\xa0the\\nconstraint x\\ue03ex≤1.Todoso,weintroducetheLagrangian\\nL , λ f λ ( x) = ()+ x\\ue010\\nx\\ue03ex−1\\ue011\\n. (4.23)\\nWecannowsolvetheproblem\\nmin\\nxmax\\nλ , λ ≥ 0L , λ . ( x) (4.24)\\n9 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fb42b57e-4d0e-4a48-bd27-1714245e6f38', embedding=None, metadata={'page_label': '112', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER4.NUMERICALCOMPUTATION\\nThesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe\\nfoundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible,\\nthenitisthesolutiontotheconstrainedproblem.Otherwise,wemustﬁnda\\nsolutionwheretheconstraintisactive.Bydiﬀerentiating theLagrangianwith\\nrespectto,weobtaintheequation x\\nA\\ue03eA x A−\\ue03eb x+2 λ= 0 . (4.25)\\nThistellsusthatthesolutionwilltaketheform\\nx A= (\\ue03eA I+2 λ)− 1A\\ue03eb . (4.26)\\nThemagnitudeof λmustbechosensuchthattheresultobeystheconstraint.We\\ncanﬁndthisvaluebyperforminggradientascenton.Todoso,observe λ\\n∂\\n∂ λL , λ( x) = x\\ue03ex−1 . (4.27)\\nWhenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative\\nuphillandincreasetheLagrangianwithrespectto λ,weincrease λ.Becausethe\\ncoeﬃcientonthe x\\ue03expenaltyhasincreased,solvingthelinearequationfor xwill\\nnowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation\\nandadjusting λcontinuesuntil xhasthecorrectnormandthederivativeon λis\\n0.\\nThisconcludesthemathematical preliminaries thatweusetodevelopmachine\\nlearningalgorithms.Wearenowreadytobuildandanalyzesomefull-ﬂedged\\nlearningsystems.\\n9 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d8a7ccf-3b05-47cb-9b66-43c140532c15', embedding=None, metadata={'page_label': '113', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 5\\nMac h i n e L e ar n i n g B asics\\nDeeplearningisaspeciﬁckindofmachinelearning.Inordertounderstand\\ndeeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof\\nmachinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral\\nprinciplesthatwillbeappliedthroughouttherestofthebook.Novicereadersor\\nthosewhowantawiderperspectiveareencouragedtoconsidermachinelearning\\ntextbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy\\n()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006\\nfeelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11\\non\\xa0traditional machinelearning\\xa0techniques\\xa0thathavestrongly\\xa0inﬂuenced the\\ndevelopmentofdeeplearningalgorithms.\\nWebeginwithadeﬁnitionofwhatalearningalgorithmis,andpresentan\\nexample:thelinearregressionalgorithm.\\xa0W ethenproceedtodescribehowthe\\nchallengeofﬁttingthetrainingdatadiﬀersfromthechallengeofﬁndingpatterns\\nthatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings\\ncalledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm\\nitself;wediscusshowtosettheseusingadditionaldata.Machinelearningis\\nessentiallyaformofappliedstatisticswithincreasedemphasisontheuseof\\ncomputerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis\\nonprovingconﬁdenceintervalsaroundthesefunctions;wethereforepresentthe\\ntwocentralapproachestostatistics:frequentistestimatorsandBayesianinference.\\nMostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised\\nlearningandunsupervisedlearning;wedescribethesecategoriesandgivesome\\nexamplesofsimplelearningalgorithmsfromeachcategory.\\xa0Mostdeeplearning\\nalgorithmsare\\xa0basedonan\\xa0optimization algorithmcalled\\xa0stochasticgradient\\ndescent.Wedescribehowtocombinevariousalgorithmcomponentssuchas\\n98', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7652a59d-2902-4436-8735-3ece35f57368', embedding=None, metadata={'page_label': '114', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nanoptimization algorithm,acostfunction,amodel,andadatasettobuilda\\nmachinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11\\nfactorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize.\\nThesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat\\novercometheseobstacles.\\n5.1LearningAlgorithms\\nAmachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But\\nwhatdowemeanbylearning?Mitchell1997()providesthedeﬁnition“Acomputer\\nprogramissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT\\nandperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP,\\nimproveswithexperienceE.”Onecanimagineaverywidevarietyofexperiences\\nE,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis\\nbooktoprovideaformaldeﬁnitionofwhatmaybeusedforeachoftheseentities.\\nInstead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe\\ndiﬀerentkindsoftasks,performance measuresandexperiencesthatcanbeused\\ntoconstructmachinelearningalgorithms.\\n5.1.1TheTask, T\\nMachinelearningallowsustotackletasksthataretoodiﬃculttosolvewith\\nﬁxedprogramswrittenanddesignedbyhumanbeings.Fromascientiﬁcand\\nphilosophicalpointofview,machinelearningisinterestingbecausedevelopingour\\nunderstandingofmachinelearningentailsdevelopingourunderstandingofthe\\nprinciplesthatunderlieintelligence.\\nInthisrelativelyformaldeﬁnitionoftheword“task,”theprocessoflearning\\nitselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe\\ntask.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask.\\nWecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite\\naprogramthatspeciﬁeshowtowalkmanually.\\nMachinelearningtasksareusuallydescribedintermsofhowthemachine\\nlearningsystemshouldprocessanexample.Anexampleisacollectionoffeatures\\nthathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant\\nthemachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa\\nvectorx∈ Rnwhereeachentryx iofthevectorisanotherfeature.Forexample,\\nthefeaturesofanimageareusuallythevaluesofthepixelsintheimage.\\n9 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad0aea8f-de54-4e44-a7ec-19d875877739', embedding=None, metadata={'page_label': '115', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nManykindsoftaskscanbesolvedwithmachinelearning.Someofthemost\\ncommonmachinelearningtasksincludethefollowing:\\n•Classiﬁcation:Inthistypeoftask,thecomputerprogramisaskedtospecify\\nwhichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning\\nalgorithmisusuallyaskedtoproduceafunctionf: Rn→{1,...,k}.When\\ny=f(x),themodelassignsaninputdescribedbyvectorxtoacategory\\nidentiﬁedbynumericcodey.Thereareothervariantsoftheclassiﬁcation\\ntask,forexample,wherefoutputsaprobabilitydistributionoverclasses.\\nAnexampleofaclassiﬁcationtaskisobjectrecognition,wheretheinput\\nisanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe\\noutputisanumericcodeidentifyingtheobjectintheimage.Forexample,\\ntheWillowGaragePR2robotisabletoactasawaiterthatcanrecognize\\ndiﬀerentkindsofdrinksanddeliverthemtopeopleoncommand(Good-\\nfellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith\\ndeeplearning( ,; ,).Object Krizhevskyetal.2012IoﬀeandSzegedy2015\\nrecognitionisthesamebasictechnologythatallowscomputerstorecognize\\nfaces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople\\ninphotocollectionsandallowcomputerstointeractmorenaturallywith\\ntheirusers.\\n•Classiﬁcationwithmissinginputs:Classiﬁcationbecomesmorechal-\\nlengingifthecomputerprogramisnotguaranteedthateverymeasurement\\ninitsinputvectorwillalwaysbeprovided.Inordertosolvetheclassiﬁcation\\ntask,thelearningalgorithmonlyhastodeﬁneafunctionmapping single\\nfromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay\\nbemissing,ratherthanprovidingasingleclassiﬁcationfunction,thelearning\\nalgorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set\\nfyingxwithadiﬀerentsubsetofitsinputsmissing.Thiskindofsituation\\narisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests\\nareexpensiveorinvasive.Onewaytoeﬃcientlydeﬁnesuchalargeset\\noffunctionsistolearnaprobabilitydistributionoveralloftherelevant\\nvariables,thensolvetheclassiﬁcationtaskbymarginalizing outthemissing\\nvariables.Withninputvariables,wecannowobtainall2ndiﬀerentclassiﬁ-\\ncationfunctionsneededforeachpossiblesetofmissinginputs,butweonly\\nneedtolearnasinglefunctiondescribingthejointprobabilitydistribution.\\nSeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel\\nappliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis\\nsectioncanalsobegeneralizedtoworkwithmissinginputs;classiﬁcation\\nwithmissinginputsisjustoneexampleofwhatmachinelearningcando.\\n1 0 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6cfd81d6-660f-494d-8557-62781a3fdc7e', embedding=None, metadata={'page_label': '116', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n•Regression:Inthistypeoftask,thecomputerprogramisaskedtopredicta\\nnumericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm\\nisaskedtooutputafunctionf: Rn→ R.Thistypeoftaskissimilarto\\nclassiﬁcation,exceptthattheformatofoutputisdiﬀerent.Anexampleof\\naregressiontaskisthepredictionoftheexpectedclaimamountthatan\\ninsuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction\\noffuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor\\nalgorithmictrading.\\n•Transcription:Inthistypeoftask,themachinelearningsystemisasked\\ntoobservearelativelyunstructuredrepresentationofsomekindofdataand\\ntranscribeitintodiscrete,textualform.Forexample,inopticalcharacter\\nrecognition,thecomputerprogramisshownaphotographcontainingan\\nimageoftextandisaskedtoreturnthistextintheformofasequence\\nofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses\\ndeeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal.\\n2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram\\nisprovidedanaudiowaveformandemitsasequenceofcharactersorword\\nIDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep\\nlearningisacrucialcomponentofmodernspeechrecognitionsystemsused\\natmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal.\\n2012b).\\n•Machinetranslation:Inamachinetranslationtask,theinputalready\\nconsistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram\\nmustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis\\ncommonlyappliedtonaturallanguages,suchastranslatingfromEnglishto\\nFrench.Deeplearninghasrecentlybeguntohaveanimportantimpacton\\nthiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,).\\n•Structuredoutput:Structuredoutputtasksinvolveanytaskwherethe\\noutputisavector(orotherdatastructurecontainingmultiplevalues)with\\nimportantrelationshipsbetweenthediﬀerentelements.Thisisabroad\\ncategory,andsubsumesthetranscriptionandtranslationtasksdescribed\\nabove,butalsomanyothertasks.Oneexampleisparsing—mappinga\\nnaturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure\\nandtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon.\\nSee ()foranexampleofdeeplearningappliedtoaparsing Collobert2011\\ntask.Anotherexampleispixel-wisesegmentationofimages,\\xa0wherethe\\ncomputerprogramassignseverypixelinanimagetoaspeciﬁccategory.For\\n1 0 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d65409e-87f8-4c5a-ae84-16f83b82c254', embedding=None, metadata={'page_label': '117', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nexample,deeplearningcanbeusedtoannotatethelocationsofroadsin\\naerialphotographs(MnihandHinton2010,).Theoutputneednothaveits\\nformmirrorthestructureoftheinputascloselyasintheseannotation-style\\ntasks.Forexample,inimagecaptioning,thecomputerprogramobservesan\\nimageandoutputsanaturallanguagesentencedescribingtheimage(Kiros\\netal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,;\\nKarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare\\ncalledstructuredoutputtasksbecausetheprogrammustoutputseveral\\nvaluesthatarealltightlyinter-related.Forexample,thewordsproducedby\\nanimagecaptioningprogrammustformavalidsentence.\\n•Anomalydetection:Inthistypeoftask,thecomputerprogramsifts\\nthroughasetofeventsorobjects,andﬂagssomeofthemasbeingunusual\\noratypical.Anexampleofananomalydetectiontaskiscreditcardfraud\\ndetection.Bymodelingyourpurchasinghabits,acreditcardcompanycan\\ndetectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard\\ninformation,thethief’spurchaseswilloftencomefromadiﬀerentprobability\\ndistributionoverpurchasetypesthanyourown.Thecreditcardcompany\\ncanpreventfraudbyplacingaholdonanaccountassoonasthatcardhas\\nbeenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009\\nsurveyofanomalydetectionmethods.\\n•Synthesisandsampling:Inthistypeoftask,themachinelearningal-\\ngorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe\\ntrainingdata.\\xa0Synthesisandsamplingviamachinelearningcanbeuseful\\nformediaapplicationswhereitcanbeexpensiveorboringforanartistto\\ngeneratelargevolumesofcontentbyhand.Forexample,videogamescan\\nautomatically generatetexturesforlargeobjectsorlandscapes,ratherthan\\nrequiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013\\ncases,wewantthesamplingorsynthesisproceduretogeneratesomespeciﬁc\\nkindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we\\nprovideawrittensentenceandasktheprogramtoemitanaudiowaveform\\ncontainingaspokenversionofthatsentence.\\xa0Thisisakindofstructured\\noutputtask,butwiththeaddedqualiﬁcationthatthereisnosinglecorrect\\noutputforeachinput,andweexplicitlydesirealargeamountofvariationin\\ntheoutput,inorderfortheoutputtoseemmorenaturalandrealistic.\\n•Imputationofmissingvalues:Inthistypeoftask,themachinelearning\\nalgorithmisgivenanewexamplex∈ Rn,butwithsomeentriesx iofx\\nmissing.Thealgorithmmustprovideapredictionofthevaluesofthemissing\\nentries.\\n1 0 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2c528111-ae8d-4879-af96-7e2047e6890d', embedding=None, metadata={'page_label': '118', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n•Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenin\\ninputacorruptedexample˜x∈ Rnobtainedbyanunknowncorruptionprocess\\nfromacleanexamplex∈ Rn.Thelearnermustpredictthecleanexample\\nxfromitscorruptedversion˜x,ormoregenerallypredicttheconditional\\nprobabilitydistributionp(x|˜x).\\n•Densityestimationorprobabilitymassfunctionestimation:In\\nthedensityestimationproblem,themachinelearningalgorithmisasked\\ntolearnafunctionpmodel: Rn→ R,wherepmodel(x)canbeinterpreted\\nasaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass\\nfunction(if xisdiscrete)onthespacethattheexamplesweredrawnfrom.\\nTodosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe\\ndiscussperformancemeasuresP),thealgorithmneedstolearnthestructure\\nofthedataithasseen.Itmustknowwhereexamplesclustertightlyand\\nwheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire\\nthelearningalgorithmtoatleastimplicitlycapturethestructureofthe\\nprobabilitydistribution.Densityestimationallowsustoexplicitlycapture\\nthatdistribution.Inprinciple,wecanthenperformcomputations onthat\\ndistributioninordertosolvetheothertasksaswell.Forexample,ifwe\\nhaveperformeddensityestimationtoobtainaprobabilitydistributionp(x),\\nwecanusethatdistributiontosolvethemissingvalueimputationtask.If\\navaluex iismissingandalloftheothervalues,denotedx − i,aregiven,\\nthenweknowthedistributionoveritisgivenbyp(x i|x − i).Inpractice,\\ndensityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks,\\nbecauseinmanycasestherequiredoperationsonp(x)arecomputationally\\nintractable.\\nOfcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks\\nwelisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan\\ndo,nottodeﬁnearigidtaxonomyoftasks.\\n5.1.2ThePerformanceMeasure, P\\nInordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign\\naquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis\\nspeciﬁctothetaskbeingcarriedoutbythesystem. T\\nFortaskssuchasclassiﬁcation,classiﬁcationwithmissinginputs,andtran-\\nscription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe\\nproportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan\\n1 0 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b90505fd-108f-4f19-b8be-3aec4dde9677', embedding=None, metadata={'page_label': '119', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nalsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion\\nofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto\\ntheerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0\\nifitiscorrectlyclassiﬁedand1ifitisnot.Fortaskssuchasdensityestimation,\\nitdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1\\nloss.Instead,wemustuseadiﬀerentperformancemetricthatgivesthemodel\\nacontinuous-valuedscoreforeachexample.Themostcommonapproachisto\\nreporttheaveragelog-probabilit ythemodelassignstosomeexamples.\\nUsuallyweareinterestedinhowwellthemachinelearningalgorithmperforms\\nondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen\\ndeployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing\\natestsetofdatathatisseparatefromthedatausedfortrainingthemachine\\nlearningsystem.\\nThechoiceofperformancemeasuremayseemstraightforwardandobjective,\\nbutitisoftendiﬃculttochooseaperformancemeasurethatcorrespondswellto\\nthedesiredbehaviorofthesystem.\\nInsomecases,thisisbecauseitisdiﬃculttodecidewhatshouldbemeasured.\\nForexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy\\nofthesystemattranscribingentiresequences,orshouldweuseamoreﬁne-grained\\nperformancemeasurethatgivespartialcreditforgettingsomeelementsofthe\\nsequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe\\nsystemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes\\nverylargemistakes?Thesekindsofdesignchoicesdependontheapplication.\\nInothercases,weknowwhatquantitywewouldideallyliketomeasure,but\\nmeasuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof\\ndensityestimation.Manyofthebestprobabilisticmodelsrepresentprobability\\ndistributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto\\naspeciﬁcpointinspaceinmanysuchmodelsisintractable.Inthesecases,one\\nmustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives,\\nordesignagoodapproximationtothedesiredcriterion.\\n5.1.3TheExperience, E\\nMachinelearningalgorithmscanbebroadlycategorizedasunsupervisedor\\nsupervisedbywhatkindofexperiencetheyareallowedtohaveduringthe\\nlearningprocess.\\nMostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed\\ntoexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as\\n1 0 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a353ec03-2647-4367-8f4f-2c4b224d006b', embedding=None, metadata={'page_label': '120', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\ndeﬁnedinsection.Sometimeswewillalsocallexamples . 5.1.1 datapoints\\nOneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre-\\nsearchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936\\ndiﬀerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample.\\nThefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe\\nplant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset\\nalsorecordswhichspecieseachplantbelongedto.Threediﬀerentspeciesare\\nrepresentedinthedataset.\\nUnsupervisedlearningalgorithmsexperienceadatasetcontainingmany\\nfeatures,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext\\nofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat\\ngeneratedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor\\ntaskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms\\nperformotherroles,likeclustering,whichconsistsofdividingthedatasetinto\\nclustersofsimilarexamples.\\nSupervisedlearningalgorithmsexperienceadatasetcontainingfeatures,\\nbuteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris\\ndatasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning\\nalgorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree\\ndiﬀerentspeciesbasedontheirmeasurements.\\nRoughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples\\nofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba-\\nbilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while\\nsupervisedlearninginvolvesobservingseveralexamplesofarandomvector xand\\nanassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby\\nestimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof\\nthetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine\\nlearningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror\\nteacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide.\\nUnsupervisedlearningandsupervisedlearningarenotformallydeﬁnedterms.\\nThelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan\\nbeusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates\\nthatforavector x∈ Rn,thejointdistributioncanbedecomposedas\\np() = xn\\ue059\\ni=1p(x i|x1,...,x i −1). (5.1)\\nThisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof\\nmodelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we\\n1 0 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f09864cc-0ec7-46d5-88c6-d0d561551944', embedding=None, metadata={'page_label': '121', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\ncansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional\\nunsupervised\\xa0learningtechnologiesto\\xa0learn\\xa0thejointdistributionp( x,y)and\\ninferring\\npy(| x) =p,y( x)\\ue050\\ny\\ue030p,y( x\\ue030). (5.2)\\nThoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor\\ndistinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith\\nmachinelearningalgorithms.Traditionally,peoplerefertoregression,classiﬁcation\\nandstructuredoutputproblemsassupervisedlearning.Densityestimationin\\nsupportofothertasksisusuallyconsideredunsupervisedlearning.\\nOthervariantsofthelearningparadigmarepossible.Forexample,insemi-\\nsupervisedlearning,someexamplesincludeasupervisiontargetbutothersdo\\nnot.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas\\ncontainingornotcontaininganexampleofaclass,buttheindividualmembers\\nofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning\\nwithdeepmodels,seeKotzias 2015etal.().\\nSomemachinelearningalgorithmsdonotjustexperienceaﬁxeddataset.For\\nexample,reinforcementlearningalgorithmsinteractwithanenvironment,so\\nthereisafeedbackloopbetweenthelearningsystemanditsexperiences.\\xa0Such\\nalgorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998\\norBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning,\\nand ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013\\nMostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan\\nbedescribedinmanyways.Inallcases,adatasetisacollectionofexamples,\\nwhichareinturncollectionsoffeatures.\\nOnecommonwayofdescribingadatasetiswitha .Adesign designmatrix\\nmatrixisamatrixcontainingadiﬀerentexampleineachrow.Eachcolumnofthe\\nmatrixcorrespondstoadiﬀerentfeature.Forinstance,theIrisdatasetcontains\\n150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent\\nthedatasetwithadesignmatrixX∈ R1504 ×,whereX i ,1isthesepallengthof\\nplanti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning\\nalgorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets.\\nOfcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto\\ndescribeeachexampleasavector,andeachofthesevectorsmustbethesamesize.\\nThisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs\\nwithdiﬀerentwidthsandheights,thendiﬀerentphotographswillcontaindiﬀerent\\nnumbersofpixels,sonotallofthephotographs maybedescribedwiththesame\\nlengthofvector.Sectionandchapterdescribehowtohandlediﬀerent 9.7 10\\n1 0 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b2b54bc-2827-42bb-9158-b4c4f11b8bb3', embedding=None, metadata={'page_label': '122', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\ntypesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe\\ndatasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements:\\n{x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors\\nx() iandx() jhavethesamesize.\\nInthecaseofsupervisedlearning,theexamplecontainsalabelortargetas\\nwellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm\\ntoperformobjectrecognitionfromphotographs, weneedtospecifywhichobject\\nappearsineachofthephotos.Wemightdothiswithanumericcode,with0\\nsignifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking\\nwithadatasetcontainingadesignmatrixoffeatureobservationsX,wealso\\nprovideavectoroflabels,withyy iprovidingthelabelforexample.i\\nOfcourse,sometimesthelabelmaybemorethanjustasinglenumber.For\\nexample,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire\\nsentences,thenthelabelforeachexamplesentenceisasequenceofwords.\\nJustasthereisnoformaldeﬁnitionofsupervisedandunsupervisedlearning,\\nthereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere\\ncovermostcases,butitisalwayspossibletodesignnewonesfornewapplications.\\n5.1.4Example:LinearRegression\\nOurdeﬁnitionofamachinelearningalgorithmasanalgorithmthatiscapable\\nofimprovingacomputerprogram’sperformanceatsometaskviaexperienceis\\nsomewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa\\nsimplemachinelearningalgorithm:linearregression.Wewillreturntothis\\nexamplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto\\nunderstanditsbehavior.\\nAsthenameimplies,linearregressionsolvesaregressionproblem.\\xa0Inother\\nwords,thegoalistobuildasystemthatcantakeavectorx∈ Rnasinputand\\npredictthevalueofascalary∈ Rasitsoutput.Inthecaseoflinearregression,\\ntheoutputisalinearfunctionoftheinput.Letˆybethevaluethatourmodel\\npredictsshouldtakeon.Wedeﬁnetheoutputtobe y\\nˆy= w\\ue03ex (5.3)\\nwherew∈ Rnisavectorof .parameters\\nParametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis\\nthecoeﬃcientthatwemultiplybyfeaturex ibeforesummingupthecontributions\\nfromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow\\neachfeatureaﬀectstheprediction.\\xa0If afeaturex ireceivesapositiveweightw i,\\n1 0 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='17d65e25-ee09-4256-8348-7c85b84a24ce', embedding=None, metadata={'page_label': '123', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nthenincreasingthevalueofthatfeatureincreasesthevalueofourprediction ˆy.\\nIfafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature\\ndecreasesthevalueofourprediction.Ifafeature’sweightislargeinmagnitude,\\nthenithasalargeeﬀectontheprediction.Ifafeature’sweightiszero,ithasno\\neﬀectontheprediction.\\nWethushaveadeﬁnitionofourtaskT:\\xa0topredictyfromxbyoutputting\\nˆy= w\\ue03ex.Nextweneedadeﬁnitionofourperformancemeasure,.P\\nSupposethatwehaveadesignmatrixofmexampleinputsthatwewillnot\\nusefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave\\navectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese\\nexamples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest\\nset.WerefertothedesignmatrixofinputsasX()testandthevectorofregression\\ntargetsasy()test.\\nOnewayofmeasuringtheperformanceofthemodelistocomputethemean\\nsquarederrorofthemodelonthetestset.Ifˆy()testgivesthepredictionsofthe\\nmodelonthetestset,thenthemeansquarederrorisgivenby\\nMSEtest=1\\nm\\ue058\\ni(ˆy()test−y()test)2\\ni. (5.4)\\nIntuitively,onecanseethatthiserrormeasuredecreasesto0when ˆy()test=y()test.\\nWecanalsoseethat\\nMSEtest=1\\nm||ˆy()test−y()test||2\\n2, (5.5)\\nsotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions\\nandthetargetsincreases.\\nTomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat\\nwillimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm\\nisallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One\\nintuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1\\nminimizethemeansquarederroronthetrainingset,MSEtrain.\\nTominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0\\n∇ wMSEtrain= 0 (5.6)\\n⇒∇ w1\\nm||ˆy()train−y()train||2\\n2= 0 (5.7)\\n⇒1\\nm∇ w||X()trainwy−()train||2\\n2= 0 (5.8)\\n1 0 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99eb39ce-9375-4c55-9ac4-648ef5a65fec', embedding=None, metadata={'page_label': '124', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n− − 1 0 . 0 5 0 0 0 5 1 0 . . . .\\nx1− 3− 2− 10123yL i n ea r r eg r es s i o n ex a m p l e\\n0 5 1 0 1 5 . . .\\nw10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w\\nFigure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints,\\neachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw\\ncontainsonlyasingleparametertolearn,w 1. ( L e f t )Observethatlinearregressionlearns\\ntosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe\\ntrainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal\\nequations,whichwecanseeminimizesthemeansquarederroronthetrainingset.\\n⇒∇ w\\ue010\\nX()trainwy−()train\\ue011\\ue03e\\ue010\\nX()trainwy−()train\\ue011\\n= 0(5.9)\\n⇒∇ w\\ue010\\nw\\ue03eX()train \\ue03eX()trainww−2\\ue03eX()train \\ue03ey()train+y()train \\ue03ey()train\\ue011\\n= 0\\n(5.10)\\n⇒2X()train \\ue03eX()trainwX−2()train \\ue03ey()train= 0(5.11)\\n⇒w=\\ue010\\nX()train \\ue03eX()train\\ue011−1\\nX()train \\ue03ey()train(5.12)\\nThesystemofequationswhosesolutionisgivenbyequationisknownas 5.12\\nthenormalequations.Evaluatingequationconstitutesasimplelearning 5.12\\nalgorithm.Foranexampleofthelinearregressionlearningalgorithminaction,\\nseeﬁgure.5.1\\nItisworthnotingthatthetermlinearregressionisoftenusedtoreferto\\naslightlymoresophisticatedmodelwithoneadditionalparameter—an intercept\\nterm.Inthismodelb\\nˆy= w\\ue03ex+b (5.13)\\nsothemappingfromparameterstopredictionsisstillalinearfunctionbutthe\\nmappingfromfeaturestopredictionsisnowanaﬃnefunction.Thisextensionto\\naﬃnefunctionsmeansthattheplotofthemodel’spredictionsstilllookslikea\\nline,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter\\n1 0 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62de6d98-080e-49de-a90c-8540c5b3e771', embedding=None, metadata={'page_label': '125', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nb,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan\\nextraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1\\nplaystheroleofthebiasparameter.Wewillfrequentlyusetheterm“linear”when\\nreferringtoaﬃnefunctionsthroughoutthisbook.\\nTheintercepttermbisoftencalledthebiasparameteroftheaﬃnetransfor-\\nmation.Thisterminologyderivesfromthepointofviewthattheoutputofthe\\ntransformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm\\nisdiﬀerentfromtheideaofastatisticalbias,inwhichastatisticalestimation\\nalgorithm’sexpectedestimateofaquantityisnotequaltothetruequantity.\\nLinearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm,\\nbutitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent\\nsectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm\\ndesignanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated\\nlearningalgorithms.\\n5.2Capacity,OverﬁttingandUnderﬁtting\\nThecentralchallengeinmachinelearningisthatwemustperformwellonnew,\\npreviouslyunseeninputs—notjustthoseonwhichourmodelwastrained.\\xa0The\\nabilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization.\\nTypically,whentrainingamachinelearningmodel,wehaveaccesstoatraining\\nset,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining\\nerror,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply\\nanoptimization problem.Whatseparatesmachinelearningfromoptimization is\\nthatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas\\nwell.\\xa0Thegeneralization errorisdeﬁnedastheexpectedvalueoftheerrorona\\nnewinput.Heretheexpectationistakenacrossdiﬀerentpossibleinputs,drawn\\nfromthedistributionofinputsweexpectthesystemtoencounterinpractice.\\nWetypicallyestimatethegeneralization errorofamachinelearningmodelby\\nmeasuringitsperformanceonatestsetofexamplesthatwerecollectedseparately\\nfromthetrainingset.\\nInourlinearregressionexample,wetrainedthemodelbyminimizingthe\\ntrainingerror,\\n1\\nm()train||X()trainwy−()train||2\\n2, (5.14)\\nbutweactuallycareaboutthetesterror,1\\nm()test||X()testwy−()test||2\\n2.\\nHowcanweaﬀectperformanceonthetestsetwhenwegettoobserveonlythe\\n1 1 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='020deb65-6e59-4393-b337-f28092d7495d', embedding=None, metadata={'page_label': '126', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5. MACHINELEARNINGBASICS\\ntrainingset? Theﬁeldof statisticallearningtheory providessomeanswers. If\\nthetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan\\ndo. Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest\\nsetarecollected,thenwecanmakesomeprogress.\\nThetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets\\ncalledthedatageneratingprocess . Wetypicallymakeasetofassumptions\\nknowncollectivelyasthe i.i.d.\\xa0assumptions .\\xa0Theseassumptionsarethatthe\\nexamplesineachdatasetare independent fromeachother,andthatthetrain\\nsetandtestsetare identicallydistributed ,drawnfromthesameprobability\\ndistributionaseachother.\\xa0Thisassumptionallowsustodescribethedatagen-\\neratingprocesswithaprobabilitydistributionoverasingleexample. Thesame\\ndistributionisthenusedtogenerateeverytrainexampleandeverytestexample.\\nWecallthatsharedunderlyingdistributionthe datageneratingdistribution ,\\ndenotedpdata. Thisprobabilisticframeworkandthei.i.d. assumptionsallowusto\\nmathematicallystudytherelationshipbetweentrainingerrorandtesterror.\\nOneimmediateconnectionwecanobservebetweenthetrainingandtesterror\\nisthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe\\nexpected test error of that model. Suppose wehavea probabilitydistributionp(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest\\nset. Forsomeﬁxedvalue w,theexpectedtrainingseterrorisexactlythesameas\\ntheexpectedtestseterror,becausebothexpectationsareformedusingthesame\\ndatasetsamplingprocess. Theonlydiﬀerencebetweenthetwoconditionsisthe\\nnameweassigntothedatasetwesample.\\nOf course,\\xa0when\\xa0we use a machine learning\\xa0algorithm,\\xa0we do not ﬁx the\\nparametersaheadoftime,thensamplebothdatasets. Wesamplethetrainingset,\\nthenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe\\ntestset. Underthisprocess,theexpectedtesterrorisgreaterthanorequalto\\ntheexpectedvalueoftrainingerror. Thefactorsdetermininghowwellamachine\\nlearningalgorithmwillperformareitsabilityto:\\n1. Makethetrainingerrorsmall.\\n2. Makethegapbetweentrainingandtesterrorsmall.\\nThesetwofactorscorrespondtothetwocentralchallengesinmachinelearning:\\nunderﬁtting andoverﬁtting. Underﬁttingoccurswhenthemodelisnotableto\\nobtainasuﬃcientlylowerrorvalueonthetrainingset. Overﬁttingoccurswhen\\nthegapbetweenthetrainingerrorandtesterroristoolarge.\\nWecancontrolwhetheramodelismorelikelytooverﬁtorunderﬁtbyaltering\\nitscapacity. Informally,amodel’scapacityisitsabilitytoﬁtawidevarietyof\\n111', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6dd8ff2e-d0bb-4af9-8e04-e489d49cd22b', embedding=None, metadata={'page_label': '127', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nfunctions.Modelswithlowcapacitymaystruggletoﬁtthetrainingset.Models\\nwithhighcapacitycanoverﬁtbymemorizingpropertiesofthetrainingsetthatdo\\nnotservethemwellonthetestset.\\nOnewaytocontrolthecapacityofalearningalgorithmisbychoosingits\\nhypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto\\nselectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe\\nsetofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize\\nlinearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits\\nhypothesisspace.Doingsoincreasesthemodel’scapacity.\\nApolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe\\narealreadyfamiliar,withprediction\\nˆybwx. = + (5.15)\\nByintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we\\ncanlearnamodelthatisquadraticasafunctionof:x\\nˆybw = +1xw+2x2. (5.16)\\nThoughthismodelimplementsaquadraticfunctionofits,theoutputis input\\nstillalinearfunctionoftheparameters,sowecanstillusethenormalequations\\ntotrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas\\nadditionalfeatures,forexampletoobtainapolynomialofdegree9:\\nˆyb= +9\\ue058\\ni=1w ixi. (5.17)\\nMachinelearningalgorithmswillgenerallyperformbestwhentheircapacity\\nisappropriateforthetruecomplexityofthetasktheyneedtoperformandthe\\namountoftrainingdatatheyareprovidedwith.Modelswithinsuﬃcientcapacity\\nareunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex\\ntasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey\\nmayoverﬁt.\\nFigureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2\\nanddegree-9predictorattemptingtoﬁtaproblemwherethetrueunderlying\\nfunctionisquadratic.\\xa0Thelinearfunctionisunabletocapturethecurvaturein\\nthetrueunderlyingproblem,soitunderﬁts.Thedegree-9predictoriscapableof\\nrepresentingthecorrectfunction,butitisalsocapableofrepresentinginﬁnitely\\nmanyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe\\n1 1 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aeb2e78a-aea9-4758-8e46-fe2f7b45277a', embedding=None, metadata={'page_label': '128', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nhavemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing\\nasolutionthatgeneralizeswellwhensomanywildlydiﬀerentsolutionsexist.In\\nthisexample,thequadraticmodelisperfectlymatchedtothetruestructureof\\nthetasksoitgeneralizeswelltonewdata.\\n\\ue078\\ue030\\ue079\\ue055\\ue06e \\ue064 \\ue065 \\ue072 \\ue066 \\ue069 \\ue074 \\ue074 \\ue069 \\ue06e \\ue067\\n\\ue078\\ue030\\ue079\\ue041\\ue070 \\ue070 \\ue072 \\ue06f \\ue070 \\ue072 \\ue069 \\ue061 \\ue074 \\ue065 \\ue020 \\ue063 \\ue061 \\ue070 \\ue061 \\ue063 \\ue069 \\ue074 \\ue079\\n\\ue078\\ue030\\ue079\\ue04f \\ue076 \\ue065 \\ue072 \\ue066 \\ue069 \\ue074 \\ue074 \\ue069 \\ue06e \\ue067\\nFigure5.2:Weﬁtthreemodelstothisexampletrainingset.Thetrainingdatawas\\ngeneratedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically\\nbyevaluatingaquadraticfunction.\\xa0 ( L e f t )Alinearfunctionﬁttothedatasuﬀersfrom\\nunderﬁtting—itcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r )\\nquadraticfunctionﬁttothedatageneralizeswelltounseenpoints.Itdoesnotsuﬀerfrom\\nasigniﬁcantamountofoverﬁttingorunderﬁtting.Apolynomialofdegree9ﬁtto ( R i g h t )\\nthedatasuﬀersfromoverﬁtting.HereweusedtheMoore-Penrosepseudoinversetosolve\\ntheunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining\\npointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure.\\nItnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue\\nunderlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue\\nfunctiondecreasesinthisarea.\\nSofarwehavedescribedonlyonewayofchangingamodel’scapacity:by\\nchangingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew\\nparametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging\\namodel’scapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The\\nmodelspeciﬁeswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom\\nwhenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled\\ntherepresentationalcapacityofthemodel.Inmanycases,ﬁndingthebest\\nfunctionwithinthisfamilyisaverydiﬃcultoptimization problem.Inpractice,\\nthelearningalgorithmdoesnotactuallyﬁndthebestfunction,butmerelyone\\nthatsigniﬁcantlyreducesthetrainingerror.Theseadditionallimitations,suchas\\n1 1 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f24b72bc-ceb5-4fd8-ac26-d8e262e35c51', embedding=None, metadata={'page_label': '129', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\ntheimperfectionoftheoptimization algorithm,meanthatthelearningalgorithm’s\\neﬀectivecapacitymaybelessthantherepresentationalcapacityofthemodel\\nfamily.\\nOurmodernideasaboutimprovingthegeneralization ofmachinelearning\\nmodelsarereﬁnementsofthoughtdatingbacktophilosophersatleastasearly\\nasPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow\\nmostwidelyknownasOccam’srazor(c.1287-1347).Thisprinciplestatesthat\\namongcompetinghypothesesthatexplainknownobservationsequallywell,one\\nshouldchoosethe“simplest”one.Thisideawasformalizedandmademoreprecise\\ninthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand\\nChervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,).\\nStatisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity.\\nAmongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or\\nVCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiﬁer.The\\nVCdimensionisdeﬁnedasbeingthelargestpossiblevalueofmforwhichthere\\nexistsatrainingsetofmdiﬀerentxpointsthattheclassiﬁercanlabelarbitrarily.\\nQuantifyingthecapacityofthemodelallowsstatisticallearningtheoryto\\nmakequantitativepredictions.Themostimportantresultsinstatisticallearning\\ntheoryshowthatthediscrepancybetweentrainingerrorandgeneralization error\\nisboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut\\nshrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis,\\n1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide\\nintellectualjustiﬁcationthatmachinelearningalgorithmscanwork,buttheyare\\nrarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin\\npartbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite\\ndiﬃculttodeterminethecapacityofdeeplearningalgorithms.\\xa0Theproblemof\\ndeterminingthecapacityofadeeplearningmodelisespeciallydiﬃcultbecausethe\\neﬀectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and\\nwehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization\\nproblemsinvolvedindeeplearning.\\nWemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize\\n(tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea\\nsuﬃcientlycomplexhypothesistoachievelowtrainingerror.Typically,training\\nerrordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel\\ncapacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically,\\ngeneralization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis\\nillustratedinﬁgure.5.3\\nToreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce\\n1 1 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df2701d4-c18c-4a55-831f-5554fac53eb0', embedding=None, metadata={'page_label': '130', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n0 O pti m a l C a pa c i t y\\nC a pa c i t yE r r o rU nde r ﬁtti ng z o ne O v e r ﬁtti ng z o ne\\nG e ne r a l i z a t i o n g a pT r a i n i n g e r r o r\\nG e n e r a l i z a t i o n e r r o r\\nFigure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror\\nbehavediﬀerently.Attheleftendofthegraph,trainingerrorandgeneralizationerror\\narebothhigh.Thisistheunderﬁttingregime.Asweincreasecapacity,trainingerror\\ndecreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually,\\nthesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverﬁtting\\nregime,wherecapacityistoolarge,abovetheoptimalcapacity.\\ntheconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric\\nmodels,suchaslinearregression.Parametricmodelslearnafunctiondescribed\\nbyaparametervectorwhosesizeisﬁniteandﬁxedbeforeanydataisobserved.\\nNon-parametric modelshavenosuchlimitation.\\nSometimes,non-parametric modelsarejusttheoreticalabstractions(suchas\\nanalgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot\\nbeimplemented inpractice.However,wecanalsodesignpracticalnon-parametric\\nmodelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample\\nofsuchanalgorithmisnearestneighborregression.Unlikelinearregression,\\nwhichhasaﬁxed-lengthvectorofweights,thenearestneighborregressionmodel\\nsimplystorestheXandyfromthetrainingset.\\xa0Whenaskedtoclassifyatest\\npointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe\\nassociatedregressiontarget.Inotherwords,ˆy=y iwherei=argmin||X i ,:−||x2\\n2.\\nThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm,\\nsuchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005\\nallowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest,\\nthenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which\\nmightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiﬀerent\\noutputs)onanyregressiondataset.\\nFinally,wecanalsocreateanon-parametric learningalgorithmbywrappinga\\n1 1 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b7ff1f48-e1d3-477f-bf71-1b1c425c93fe', embedding=None, metadata={'page_label': '131', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nparametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber\\nofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning\\nthatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa\\npolynomialexpansionoftheinput.\\nTheidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution\\nthatgeneratesthedata.\\xa0Evensuchamodelwillstillincursomeerroronmany\\nproblems,becausetheremaystillbesomenoiseinthedistribution.Inthecase\\nofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic,\\norymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose\\nincludedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue\\ndistributioniscalledthe p,y(x)Bayeserror.\\nTrainingandgeneralization errorvaryasthesizeofthetrainingsetvaries.\\nExpectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples\\nincreases.Fornon-parametric models,moredatayieldsbettergeneralization until\\nthebestpossibleerrorisachieved.Anyﬁxedparametricmodelwithlessthan\\noptimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See\\nﬁgureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4\\ncapacityandyetstillhavealargegapbetweentrainingandgeneralization error.\\nInthissituation,wemaybeabletoreducethisgapbygatheringmoretraining\\nexamples.\\n5.2.1TheNoFreeLunchTheorem\\nLearningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom\\naﬁnitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof\\nlogic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples,\\nisnotlogicallyvalid.\\xa0Tologicallyinferaruledescribingeverymemberofaset,\\nonemusthaveinformationabouteverymemberofthatset.\\nInpart,machinelearningavoidsthisproblembyoﬀeringonlyprobabilisticrules,\\nratherthantheentirelycertainrulesusedinpurelylogicalreasoning.\\xa0Machine\\nlearningpromisestoﬁndrulesthatareprobably most correctaboutmembersof\\nthesettheyconcern.\\nUnfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree\\nlunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover\\nallpossibledatageneratingdistributions,everyclassiﬁcationalgorithmhasthe\\nsameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords,\\ninsomesense,nomachinelearningalgorithmisuniversallyanybetterthanany\\nother.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage\\n1 1 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e4b50a6-7dc3-4f54-8592-eae628f2fa1f', embedding=None, metadata={'page_label': '132', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n\\ue031 \\ue030\\ue030\\ue031 \\ue030\\ue031\\ue031 \\ue030\\ue032\\ue031 \\ue030\\ue033\\ue031 \\ue030\\ue034\\ue031 \\ue030\\ue035\\n\\ue04e\\ue075\\ue06d \\ue062 \\ue065\\ue072\\ue020 \\ue06f \\ue066 \\ue020 \\ue074 \\ue072\\ue061 \\ue069 \\ue06e \\ue069 \\ue06e \\ue067 \\ue020 \\ue065\\ue078 \\ue061 \\ue06d \\ue070 \\ue06c \\ue065 \\ue073\\ue030 \\ue02e \\ue030\\ue030 \\ue02e \\ue035\\ue031 \\ue02e \\ue030\\ue031 \\ue02e \\ue035\\ue032 \\ue02e \\ue030\\ue032 \\ue02e \\ue035\\ue033 \\ue02e \\ue030\\ue033 \\ue02e \\ue035\\ue045 \\ue072\\ue072\\ue06f \\ue072\\ue020 \\ue028 \\ue04d \\ue053 \\ue045 \\ue029\\ue042 \\ue061 \\ue079 \\ue065 \\ue073\\ue020 \\ue065 \\ue072 \\ue072 \\ue06f \\ue072\\n\\ue054 \\ue072 \\ue061 \\ue069 \\ue06e \\ue020 \\ue028 \\ue071 \\ue075 \\ue061 \\ue064 \\ue072 \\ue061 \\ue074 \\ue069 \\ue063 \\ue029\\n\\ue054 \\ue065 \\ue073\\ue074 \\ue020 \\ue028 \\ue071 \\ue075 \\ue061 \\ue064 \\ue072 \\ue061 \\ue074 \\ue069 \\ue063 \\ue029\\n\\ue054 \\ue065 \\ue073\\ue074 \\ue020 \\ue028 \\ue06f \\ue070 \\ue074 \\ue069 \\ue06d \\ue061 \\ue06c \\ue020 \\ue063 \\ue061 \\ue070 \\ue061 \\ue063 \\ue069 \\ue074 \\ue079 \\ue029\\n\\ue054 \\ue072 \\ue061 \\ue069 \\ue06e \\ue020 \\ue028 \\ue06f \\ue070 \\ue074 \\ue069 \\ue06d \\ue061 \\ue06c \\ue020 \\ue063 \\ue061 \\ue070 \\ue061 \\ue063 \\ue069 \\ue074 \\ue079 \\ue029\\n\\ue031 \\ue030\\ue030\\ue031 \\ue030\\ue031\\ue031 \\ue030\\ue032\\ue031 \\ue030\\ue033\\ue031 \\ue030\\ue034\\ue031 \\ue030\\ue035\\n\\ue04e\\ue075\\ue06d \\ue062 \\ue065\\ue072\\ue020 \\ue06f \\ue066 \\ue020 \\ue074 \\ue072\\ue061 \\ue069 \\ue06e \\ue069 \\ue06e \\ue067 \\ue020 \\ue065\\ue078 \\ue061 \\ue06d \\ue070 \\ue06c \\ue065 \\ue073\\ue030\\ue035\\ue031 \\ue030\\ue031 \\ue035\\ue032 \\ue030\\ue04f \\ue070 \\ue074 \\ue069 \\ue06d \\ue061 \\ue06c \\ue020 \\ue063\\ue061 \\ue070 \\ue061 \\ue063\\ue069\\ue074\\ue079 \\ue020 \\ue028 \\ue070 \\ue06f \\ue06c \\ue079 \\ue06e \\ue06f \\ue06d \\ue069 \\ue061 \\ue06c \\ue020 \\ue064 \\ue065 \\ue067 \\ue072\\ue065 \\ue065 \\ue029\\nFigure5.4:Theeﬀectofthetrainingdatasetsizeonthetrainandtesterror,aswellas\\nontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon\\naddingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset,\\nandthengeneratedseveraldiﬀerentsizesoftrainingset.Foreachsize,wegenerated40\\ndiﬀerenttrainingsetsinordertoploterrorbarsshowing95percentconﬁdenceintervals.\\n( T o p )TheMSEonthetrainingandtestsetfortwodiﬀerentmodels:aquadraticmodel,\\nandamodelwithdegreechosentominimizethetesterror.Bothareﬁtinclosedform.For\\nthequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases.\\nThisisbecauselargerdatasetsarehardertoﬁt.Simultaneously,thetesterrordecreases,\\nbecausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic\\nmodeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto\\nahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The\\ntrainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm\\ntomemorizespeciﬁcinstancesofthetrainingset.Asthetrainingsizeincreasestoinﬁnity,\\nthetrainingerrorofanyﬁxed-capacitymodel(here,thequadraticmodel)mustrisetoat\\nleasttheBayeserror.\\xa0Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom )\\n(shownhereasthedegreeoftheoptimalpolynomialregressor)increases.\\xa0Theoptimal\\ncapacityplateausafterreachingsuﬃcientcomplexitytosolvethetask.\\n1 1 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88046b28-5f42-4c64-8a5d-e43cd59bfd39', embedding=None, metadata={'page_label': '133', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nperformance(overallpossibletasks)asmerelypredictingthateverypointbelongs\\ntothesameclass.\\nFortunately,theseresultsholdonlywhenweaverageoverpossibledata all\\ngeneratingdistributions.Ifwemakeassumptionsaboutthekindsofprobability\\ndistributionsweencounterinreal-worldapplications,thenwecandesignlearning\\nalgorithmsthatperformwellonthesedistributions.\\nThismeansthatthegoalofmachinelearningresearchisnottoseekauniversal\\nlearningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto\\nunderstandwhatkindsofdistributionsarerelevanttothe“realworld”thatanAI\\nagentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon\\ndatadrawnfromthekindsofdatageneratingdistributionswecareabout.\\n5.2.2Regularization\\nThenofreelunchtheoremimpliesthatwemustdesignourmachinelearning\\nalgorithmstoperformwellonaspeciﬁctask.Wedosobybuildingasetof\\npreferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith\\nthelearningproblemsweaskthealgorithmtosolve,itperformsbetter.\\nSofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed\\nconcretelyistoincreaseordecreasethemodel’srepresentationalcapacitybyadding\\norremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm\\nisabletochoose.Wegavethespeciﬁcexampleofincreasingordecreasingthe\\ndegreeofapolynomialforaregressionproblem.Theviewwehavedescribedso\\nfarisoversimpliﬁed.\\nThebehaviorofouralgorithmisstronglyaﬀectednotjustbyhowlargewe\\nmakethesetoffunctionsallowedinitshypothesisspace,butbythespeciﬁcidentity\\nofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression,\\nhasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These\\nlinearfunctionscanbeveryusefulforproblemswheretherelationshipbetween\\ninputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems\\nthatbehaveinaverynonlinearfashion.Forexample,linearregressionwould\\nnotperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus\\ncontroltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe\\nallowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese\\nfunctions.\\nWecanalsogivealearningalgorithmapreferenceforonesolutioninits\\nhypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone\\nispreferred.Theunpreferredsolutionwillbechosenonlyifitﬁtsthetraining\\n1 1 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c9251ca8-5e37-4148-b3ee-2e063784b3bb', embedding=None, metadata={'page_label': '134', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\ndatasigniﬁcantlybetterthanthepreferredsolution.\\nForexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude\\nweightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum\\ncomprisingboththemeansquarederroronthetrainingandacriterionJ(w)that\\nexpressesapreferencefortheweightstohavesmallersquaredL2norm.Speciﬁcally,\\nJ() = wMSEtrain+λw\\ue03ew, (5.18)\\nwhereλisavaluechosenaheadoftimethatcontrolsthestrengthofourpreference\\nforsmallerweights.Whenλ= 0,weimposenopreference,andlargerλforcesthe\\nweightstobecomesmaller.\\xa0MinimizingJ(w)resultsinachoiceofweightsthat\\nmakeatradeoﬀbetweenﬁttingthetrainingdataandbeingsmall.Thisgivesus\\nsolutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan\\nexampleofhowwecancontrolamodel’stendencytooverﬁtorunderﬁtviaweight\\ndecay,wecantrainahigh-degreepolynomialregressionmodelwithdiﬀerentvalues\\nof.Seeﬁgurefortheresults. λ 5.5\\n\\ue078\\ue030\\ue079\\ue055 \\ue06e \\ue064 \\ue065 \\ue072 \\ue066 \\ue069 \\ue074 \\ue074 \\ue069 \\ue06e \\ue067\\n\\ue028 \\ue045 \\ue078 \\ue063 \\ue065 \\ue073\\ue073\\ue069\\ue076 \\ue065 \\ue020 \\ue0b8 \\ue029\\n\\ue078\\ue030\\ue079\\ue041 \\ue070 \\ue070 \\ue072 \\ue06f \\ue070 \\ue072 \\ue069 \\ue061 \\ue074 \\ue065 \\ue020 \\ue077 \\ue065 \\ue069 \\ue067 \\ue068 \\ue074 \\ue020 \\ue064 \\ue065 \\ue063 \\ue061 \\ue079\\n\\ue028 \\ue04d \\ue065 \\ue064 \\ue069 \\ue075 \\ue06d \\ue020 \\ue0b8 \\ue029\\n\\ue078\\ue030\\ue079\\ue04f \\ue076 \\ue065 \\ue072 \\ue066 \\ue069 \\ue074 \\ue074 \\ue069 \\ue06e \\ue067\\n\\ue028 \\ue030 \\ue029 \\ue0b8 \\ue021\\nFigure5.5:Weﬁtahigh-degreepolynomialregressionmodeltoourexampletrainingset\\nfromﬁgure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2\\nWevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverﬁtting.\\n( L e f t )Withverylargeλ,wecanforcethemodeltolearnafunctionwithnoslopeat\\nall.Thisunderﬁtsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r )\\nmediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape. λ\\nEventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated\\nshape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller\\ncoeﬃcients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t )\\npseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the\\ndegree-9polynomialoverﬁtssigniﬁcantly,aswesawinﬁgure.5.2\\n1 1 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5345ab1d-9276-4f17-bc2b-738c80c59863', embedding=None, metadata={'page_label': '135', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nMoregenerally,wecanregularizeamodelthatlearnsafunctionf(x;θ)by\\naddingapenaltycalledaregularizertothecostfunction.Inthecaseofweight\\ndecay,theregularizerisΩ(w) =w\\ue03ew.Inchapter,wewillseethatmanyother 7\\nregularizersarepossible.\\nExpressingpreferencesforonefunctionoveranotherisamoregeneralway\\nofcontrollingamodel’scapacitythanincludingorexcludingmembersfromthe\\nhypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas\\nexpressinganinﬁnitelystrongpreferenceagainstthatfunction.\\nInourweightdecayexample,weexpressedourpreferenceforlinearfunctions\\ndeﬁnedwithsmallerweightsexplicitly,\\xa0viaanextraterminthecriterionwe\\nminimize.Thereare\\xa0many\\xa0otherwaysof\\xa0expressing preferencesfor\\xa0diﬀerent\\nsolutions,bothimplicitlyandexplicitly.Together,thesediﬀerentapproaches\\nareknownasregularization.\\xa0Regularizationisanymodiﬁcationwemaketoa\\nlearningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits\\ntrainingerror.Regularizationisoneofthecentralconcernsoftheﬁeldofmachine\\nlearning,rivaledinitsimportanceonlybyoptimization.\\nThenofreelunchtheoremhasmadeitclearthatthereisnobestmachine\\nlearningalgorithm,and,inparticular,nobestformofregularization. Instead\\nwemustchooseaformofregularizationthatiswell-suitedtotheparticulartask\\nwewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin\\nparticularisthataverywiderangeoftasks(suchasalloftheintellectualtasks\\nthatpeoplecando)mayallbesolvedeﬀectivelyusingverygeneral-purposeforms\\nofregularization.\\n5.3HyperparametersandValidationSets\\nMostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol\\nthebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame-\\nters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm\\nitself(thoughwecan\\xa0designa\\xa0nestedlearning\\xa0procedure\\xa0where one\\xa0learning\\nalgorithmlearnsthebesthyperparametersforanotherlearningalgorithm).\\nInthepolynomialregressionexamplewesawinﬁgure,thereisasingle 5.2\\nhyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper-\\nparameter.Theλvalueusedtocontrolthestrengthofweightdecayisanother\\nexampleofahyperparameter.\\nSometimesasettingischosentobeahyperparameter thatthelearningal-\\ngorithmdoesnotlearnbecauseitisdiﬃculttooptimize.Morefrequently,the\\n1 2 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ebffcb43-6307-495f-b2d9-1f3a36ff3aa5', embedding=None, metadata={'page_label': '136', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nsettingmustbeahyperparameter becauseitisnotappropriatetolearnthat\\nhyperparameteronthetrainingset.Thisappliestoallhyperparameters that\\ncontrolmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would\\nalwayschoosethemaximumpossiblemodelcapacity,resultinginoverﬁtting(refer\\ntoﬁgure).Forexample,wecanalwaysﬁtthetrainingsetbetterwithahigher 5.3\\ndegreepolynomialandaweightdecaysettingofλ= 0thanwecouldwithalower\\ndegreepolynomialandapositiveweightdecaysetting.\\nTosolvethisproblem,weneedavalidationsetofexamplesthatthetraining\\nalgorithmdoesnotobserve.\\nEarlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom\\nthesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization\\nerrorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe\\ntestexamplesarenotusedinanywaytomakechoicesaboutthemodel,including\\nitshyperparameters .\\xa0Forthisreason,noexamplefromthetestsetcanbeused\\ninthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe\\ntrainingdata.Speciﬁcally,wesplitthetrainingdataintotwodisjointsubsets.One\\nofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation\\nset,usedtoestimatethegeneralization errorduringoraftertraining,allowing\\nforthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto\\nlearntheparametersisstilltypicallycalledthetrainingset,eventhoughthis\\nmaybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess.\\nThesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe\\nvalidationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand\\n20%forvalidation.Sincethevalidationsetisusedto“train”thehyperparameters ,\\nthevalidationseterrorwillunderestimatethegeneralization error,thoughtypically\\nbyasmalleramountthanthetrainingerror.Afterallhyperparameter optimization\\niscomplete,thegeneralization errormaybeestimatedusingthetestset.\\nInpractice,\\xa0when thesametestsethasbeenusedrepeatedlytoevaluate\\nperformanceofdiﬀerentalgorithmsovermanyyears,andespeciallyifweconsider\\nalltheattemptsfromthescientiﬁccommunityatbeatingthereportedstate-of-\\nthe-artperformanceonthattestset,weenduphavingoptimisticevaluationswith\\nthetestsetaswell.Benchmarkscanthusbecomestaleandthendonotreﬂectthe\\ntrueﬁeldperformance ofatrainedsystem.Thankfully,thecommunitytendsto\\nmoveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets.\\n1 2 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0397625-f91e-4679-8d51-c3611807f65e', embedding=None, metadata={'page_label': '137', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n5.3.1Cross-Validation\\nDividingthedatasetintoaﬁxedtrainingsetandaﬁxedtestsetcanbeproblematic\\nifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty\\naroundtheestimatedaveragetesterror,makingitdiﬃculttoclaimthatalgorithm\\nAworksbetterthanalgorithmonthegiventask. B\\nWhenthedatasethashundredsofthousandsofexamplesormore,thisisnota\\nseriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone\\ntousealloftheexamplesintheestimationofthemeantesterror,atthepriceof\\nincreasedcomputational cost.Theseproceduresarebasedontheideaofrepeating\\nthetrainingandtestingcomputationondiﬀerentrandomlychosensubsetsorsplits\\noftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation\\nprocedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1\\nsplittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated\\nbytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe\\ndataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One\\nproblemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage\\nerrorestimators(BengioandGrandvalet2004,),butapproximationsaretypically\\nused.\\n5.4Estimators,BiasandVariance\\nTheﬁeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine\\nlearninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize.\\nFoundationalconceptssuchasparameterestimation,biasandvarianceareuseful\\ntoformallycharacterizenotionsofgeneralization, underﬁttingandoverﬁtting.\\n5.4.1PointEstimation\\nPointestimationistheattempttoprovidethesingle“best”predictionofsome\\nquantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter\\noravectorofparametersinsomeparametricmodel,suchastheweightsinour\\nlinearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4\\nInordertodistinguishestimatesofparametersfromtheirtruevalue,our\\nconventionwillbetodenoteapointestimateofaparameterbyθ ˆθ.\\nLet{x(1),...,x() m}beasetofmindependentandidenticallydistributed\\n1 2 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b04d5dcf-b36d-418e-a1e0-600e5fe9d3bc', embedding=None, metadata={'page_label': '138', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nAlgorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate\\ngeneralization errorofalearningalgorithmAwhenthegivendataset Distoo\\nsmallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof\\ngeneralization error,becausethemeanofalossLonasmalltestsetmayhavetoo\\nhighvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for\\nthei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i)\\ninthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase\\nofunsupervisedlearning.\\xa0The algorithmreturnsthevectoroferrorseforeach\\nexamplein D,whosemeanistheestimatedgeneralization error.\\xa0Theerrorson\\nindividualexamplescanbeusedtocomputeaconﬁdenceintervalaroundthemean\\n(equation).\\xa0Whiletheseconﬁdenceintervalsarenotwell-justiﬁedafterthe 5.47\\nuseofcross-validation,itisstillcommonpracticetousethemtodeclarethat\\nalgorithmAisbetterthanalgorithmBonlyiftheconﬁdenceintervaloftheerror\\nofalgorithmAliesbelowanddoesnotintersecttheconﬁdenceintervalofalgorithm\\nB.\\nDeﬁneKFoldXV(): D,A,L,k\\nRequire: D,thegivendataset,withelementsz() i\\nRequire:A,thelearningalgorithm,seenasafunctionthattakesadatasetas\\ninputandoutputsalearnedfunction\\nRequire:L,thelossfunction,seenasafunctionfromalearnedfunctionfand\\nanexamplez() i∈ ∈ Dtoascalar R\\nRequire:k,thenumberoffolds\\nSplitintomutuallyexclusivesubsets Dk D i,whoseunionis. D\\nfordoikfromto1\\nf i= (A D D\\\\ i)\\nforz() jin D ido\\ne j= (Lf i,z() j)\\nendfor\\nendfor\\nReturne\\n1 2 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='be12a598-0ebc-4ba0-a62a-88ebf781a14c', embedding=None, metadata={'page_label': '139', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n(i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic\\nˆθ m= (gx(1),...,x() m). (5.19)\\nThedeﬁnitiondoesnotrequirethatgreturnavaluethatisclosetothetrue\\nθoreventhattherangeofgisthesameasthesetofallowablevaluesofθ.\\nThisdeﬁnitionofapointestimatorisverygeneralandallowsthedesignerofan\\nestimatorgreatﬂexibility.Whilealmostanyfunctionthusqualiﬁesasanestimator,\\nagoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingθthat\\ngeneratedthetrainingdata.\\nFornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume\\nthatthetrueparametervalueθisﬁxedbutunknown,whilethepointestimate\\nˆθisafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any\\nfunctionofthedataisrandom.Therefore ˆθisarandomvariable.\\nPointestimationcanalsorefertotheestimationoftherelationshipbetween\\ninputandtargetvariables.Werefertothesetypesofpointestimatesasfunction\\nestimators.\\nFunctionEstimationAswementionedabove,sometimesweareinterestedin\\nperformingfunctionestimation(orfunctionapproximation).Herewearetryingto\\npredictavariableygivenaninputvectorx.Weassumethatthereisafunction\\nf(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample,\\nwemayassumethaty=f(x)+\\ue00f,where\\ue00fstandsforthepartofythatisnot\\npredictablefromx.\\xa0Infunctionestimation,weareinterestedinapproximating\\nfwithamodelorestimate ˆf.Functionestimationisreallyjustthesameas\\nestimatingaparameterθ;thefunctionestimator ˆfissimplyapointestimatorin\\nfunctionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4\\nthepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2\\nscenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating\\nafunction ˆf y mappingfromtox.\\nWenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand\\ndiscusswhattheytellusabouttheseestimators.\\n5.4.2Bias\\nThebiasofanestimatorisdeﬁnedas:\\nbias(ˆθ m) = ( Eˆθ m)−θ (5.20)\\n1 2 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6e66d1a-b629-429d-abdd-c1d45f70264d', embedding=None, metadata={'page_label': '140', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nwheretheexpectationisoverthedata(seenassamplesfromarandomvariable)\\nandθisthetrueunderlyingvalueofθusedtodeﬁnethedatageneratingdistri-\\nbution.Anestimator ˆθ missaidtobeunbiasedifbias(ˆθ m) = 0,whichimplies\\nthat E(ˆθ m)=θ.Anestimator ˆθ missaidtobeasymptoticallyunbiasedif\\nlim m → ∞bias(ˆθ m) = 0,whichimpliesthatlim m → ∞ E(ˆθ m) = θ.\\nExample:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m}\\nthatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri-\\nbutionwithmean:θ\\nPx(() i;) = θθx() i(1 )−θ(1 − x() i). (5.21)\\nAcommonestimatorfortheθparameterofthisdistributionisthemeanofthe\\ntrainingsamples:\\nˆθ m=1\\nmm\\ue058\\ni=1x() i. (5.22)\\nTodeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22\\nintoequation:5.20\\nbias(ˆθ m) = [ Eˆθ m]−θ (5.23)\\n= E\\ue022\\n1\\nmm\\ue058\\ni=1x() i\\ue023\\n−θ (5.24)\\n=1\\nmm\\ue058\\ni=1E\\ue068\\nx() i\\ue069\\n−θ (5.25)\\n=1\\nmm\\ue058\\ni=11\\ue058\\nx() i=0\\ue010\\nx() iθx() i(1 )−θ(1 − x() i)\\ue011\\n−θ(5.26)\\n=1\\nmm\\ue058\\ni=1()θ−θ (5.27)\\n= = 0θθ− (5.28)\\nSince bias(ˆθ) = 0,wesaythatourestimator ˆθisunbiased.\\nExample:GaussianDistributionEstimatoroftheMeanNow,consider\\nasetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed\\naccordingtoaGaussiandistributionp(x() i) =N(x() i;µ,σ2),wherei∈{1,...,m}.\\n1 2 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='94fbb7f3-d1af-4dc8-ba1a-60ea02ae33d9', embedding=None, metadata={'page_label': '141', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nRecallthattheGaussianprobabilitydensityfunctionisgivenby\\npx(() i;µ,σ2) =1√\\n2πσ2exp\\ue020\\n−1\\n2(x() i−µ)2\\nσ2\\ue021\\n.(5.29)\\nAcommonestimatoroftheGaussianmeanparameterisknownasthesample\\nmean:\\nˆµ m=1\\nmm\\ue058\\ni=1x() i(5.30)\\nTodeterminethebiasofthesamplemean,weareagaininterestedincalculating\\nitsexpectation:\\nbias(ˆµ m) = [ˆ Eµ m]−µ (5.31)\\n= E\\ue022\\n1\\nmm\\ue058\\ni=1x() i\\ue023\\n−µ (5.32)\\n=\\ue020\\n1\\nmm\\ue058\\ni=1E\\ue068\\nx() i\\ue069\\ue021\\n−µ (5.33)\\n=\\ue020\\n1\\nmm\\ue058\\ni=1µ\\ue021\\n−µ (5.34)\\n= = 0µµ− (5.35)\\nThusweﬁndthatthesamplemeanisanunbiasedestimatorofGaussianmean\\nparameter.\\nExample:EstimatorsoftheVarianceofaGaussianDistributionAsan\\nexample,wecomparetwodiﬀerentestimatorsofthevarianceparameterσ2ofa\\nGaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased.\\nTheﬁrstestimatorofσ2weconsiderisknownasthesamplevariance:\\nˆσ2\\nm=1\\nmm\\ue058\\ni=1\\ue010\\nx() i−ˆµ m\\ue0112\\n, (5.36)\\nwhere ˆµ misthesamplemean,deﬁnedabove.Moreformally,weareinterestedin\\ncomputing\\nbias(ˆσ2\\nm) = [ˆ Eσ2\\nm]−σ2(5.37)\\n1 2 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89d33add-5022-4f93-8eb0-45b5a33ca6b1', embedding=None, metadata={'page_label': '142', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nWebeginbyevaluatingtheterm E[ˆσ2\\nm]:\\nE[ˆσ2\\nm] = E\\ue022\\n1\\nmm\\ue058\\ni=1\\ue010\\nx() i−ˆµ m\\ue0112\\ue023\\n(5.38)\\n=m−1\\nmσ2(5.39)\\nReturningtoequation,weconcludethatthebiasof 5.37 ˆσ2\\nmis−σ2/m.Therefore,\\nthesamplevarianceisabiasedestimator.\\nTheunbiasedsamplevarianceestimator\\n˜σ2\\nm=1\\nm−1m\\ue058\\ni=1\\ue010\\nx() i−ˆµ m\\ue0112\\n(5.40)\\nprovidesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased.\\nThatis,weﬁndthat E[˜σ2\\nm] = σ2:\\nE[˜σ2\\nm] = E\\ue022\\n1\\nm−1m\\ue058\\ni=1\\ue010\\nx() i−ˆµ m\\ue0112\\ue023\\n(5.41)\\n=m\\nm−1E[ˆσ2\\nm] (5.42)\\n=m\\nm−1\\ue012m−1\\nmσ2\\ue013\\n(5.43)\\n= σ2. (5.44)\\nWehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased\\nestimatorsareclearlydesirable,theyarenotalwaysthe“best”estimators.Aswe\\nwillseeweoftenusebiasedestimatorsthatpossessotherimportantproperties.\\n5.4.3VarianceandStandardError\\nAnotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch\\nweexpectittovaryasafunctionofthedatasample.Justaswecomputedthe\\nexpectationoftheestimatortodetermineitsbias,wecancomputeitsvariance.\\nThevarianceofanestimatorissimplythevariance\\nVar(ˆθ) (5.45)\\nwheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe\\nvarianceiscalledthe ,denotedstandarderror SE(ˆθ).\\n1 2 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c014e7d5-2cd7-48bc-ac66-66839c61f165', embedding=None, metadata={'page_label': '143', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nThevarianceorthestandarderrorofanestimatorprovidesameasureofhow\\nwewouldexpecttheestimatewecomputefromdatatovaryasweindependently\\nresamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe\\nmightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively\\nlowvariance.\\nWhenwecomputeanystatisticusingaﬁnitenumberofsamples,ourestimate\\nofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave\\nobtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave\\nbeendiﬀerent.Theexpecteddegreeofvariationinanyestimatorisasourceof\\nerrorthatwewanttoquantify.\\nThestandarderrorofthemeanisgivenby\\nSE(ˆµ m) =\\ue076\\ue075\\ue075\\ue074Var\\ue022\\n1\\nmm\\ue058\\ni=1x() i\\ue023\\n=σ√m, (5.46)\\nwhereσ2isthetruevarianceofthesamplesxi.Thestandarderrorisoften\\nestimatedbyusinganestimateofσ.Unfortunately,neitherthesquarerootof\\nthesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance\\nprovideanunbiasedestimateofthestandarddeviation.Bothapproachestend\\ntounderestimatethetruestandarddeviation,butarestillusedinpractice.The\\nsquarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate.\\nForlarge,theapproximation isquitereasonable. m\\nThestandarderrorofthemeanisveryusefulinmachinelearningexperiments.\\nWeoftenestimatethegeneralization errorbycomputingthesamplemeanofthe\\nerroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe\\naccuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which\\ntellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution,\\nwecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation\\nfallsinanychoseninterval.Forexample,the95%conﬁdenceintervalcenteredon\\nthemean ˆµ mis\\n(ˆµ m−196SE( ˆ.µ m)ˆ,µ m+196SE( ˆ.µ m)), (5.47)\\nunderthenormaldistributionwithmean ˆµ mandvariance SE(ˆµ m)2.Inmachine\\nlearningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm\\nBiftheupperboundofthe95%conﬁdenceintervalfortheerrorofalgorithmAis\\nlessthanthelowerboundofthe95%conﬁdenceintervalfortheerrorofalgorithm\\nB.\\n1 2 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2d57717-6a95-43df-a9df-6a8ae92a63e5', embedding=None, metadata={'page_label': '144', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nExample:\\xa0BernoulliDistributionWeonceagainconsiderasetofsamples\\n{x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution\\n(recallP(x() i;θ) =θx() i(1−θ)(1 − x() i)).Thistimeweareinterestedincomputing\\nthevarianceoftheestimator ˆθ m=1\\nm\\ue050m\\ni=1x() i.\\nVar\\ue010\\nˆθ m\\ue011\\n= Var\\ue020\\n1\\nmm\\ue058\\ni=1x() i\\ue021\\n(5.48)\\n=1\\nm2m\\ue058\\ni=1Var\\ue010\\nx() i\\ue011\\n(5.49)\\n=1\\nm2m\\ue058\\ni=1θθ (1−) (5.50)\\n=1\\nm2mθθ (1−) (5.51)\\n=1\\nmθθ (1−) (5.52)\\nThevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples\\ninthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill\\nreturntowhenwediscussconsistency(seesection).5.4.5\\n5.4.4TradingoﬀBiasandVariancetoMinimizeMeanSquared\\nError\\nBiasandvariancemeasuretwodiﬀerentsourcesoferrorinanestimator.Bias\\nmeasurestheexpecteddeviationfromthetruevalueofthefunctionorparameter.\\nVarianceontheotherhand,providesameasureofthedeviationfromtheexpected\\nestimatorvaluethatanyparticularsamplingofthedataislikelytocause.\\nWhathappenswhenwearegivenachoicebetweentwoestimators,onewith\\nmorebiasandonewithmorevariance?Howdowechoosebetweenthem?For\\nexample,imaginethatweareinterestedinapproximating thefunctionshownin\\nﬁgureandweareonlyoﬀeredthechoicebetweenamodelwithlargebiasand 5.2\\nonethatsuﬀersfromlargevariance.Howdowechoosebetweenthem?\\nThemostcommonwaytonegotiatethistrade-oﬀistousecross-validation.\\nEmpirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter-\\nnatively,wecanalsocomparethemeansquarederror(MSE)oftheestimates:\\nMSE = [( Eˆθ m−θ)2] (5.53)\\n= Bias(ˆθ m)2+Var(ˆθ m) (5.54)\\n1 2 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='02a99376-718c-48c1-8466-148d29894a7b', embedding=None, metadata={'page_label': '145', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nTheMSEmeasurestheoverallexpecteddeviation—in asquarederrorsense—\\nbetweentheestimatorandthetruevalueoftheparameterθ.Asisclearfrom\\nequation,evaluatingtheMSEincorporatesboththebiasandthevariance. 5.54\\nDesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat\\nmanagetokeepboththeirbiasandvariancesomewhatincheck.\\nC apac i t yB i as Ge ne r al i z at i on\\ne r r orV ar i anc e\\nO pt i m al\\nc apac i t yO v e r ﬁt t i ng\\xa0z o n e U nde r ﬁt t i ng\\xa0z o n e\\nFigure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance\\n(dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold\\ncurve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderﬁtting\\nwhenthecapacityisbelowthisoptimumandoverﬁttingwhenitisabove.Thisrelationship\\nissimilartotherelationshipbetweencapacity,underﬁtting,andoverﬁtting,discussedin\\nsectionandﬁgure. 5.2 5.3\\nTherelationshipbetweenbiasandvarianceistightlylinkedtothemachine\\nlearningconceptsofcapacity,underﬁttingandoverﬁtting.Inthecasewheregen-\\neralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful\\ncomponentsofgeneralization error),increasingcapacitytendstoincreasevariance\\nanddecreasebias.Thisisillustratedinﬁgure,whereweseeagaintheU-shaped 5.6\\ncurveofgeneralization errorasafunctionofcapacity.\\n5.4.5Consistency\\nSofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof\\nﬁxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe\\namountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber\\nofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue\\n1 3 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fe6752e8-d2eb-47f9-8858-3eaf0ade5059', embedding=None, metadata={'page_label': '146', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nvalueofthecorrespondingparameters.Moreformally,wewouldlikethat\\nplimm → ∞ˆθ m= θ. (5.55)\\nThesymbolplimindicatesconvergenceinprobability,meaningthatforany\\ue00f>0,\\nP(|ˆθ m−|θ>\\ue00f)→0asm→∞.Theconditiondescribedbyequationis5.55\\nknownasconsistency.Itissometimesreferredtoasweakconsistency,with\\nstrongconsistencyreferringtothealmostsureconvergenceofˆθtoθ.Almost\\nsureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex\\noccurswhenp(lim m → ∞ x() m= ) = 1x.\\nConsistencyensuresthatthebiasinducedbytheestimatordiminishesasthe\\nnumberofdataexamplesgrows.However,thereverseisnottrue—asymptotic\\nunbiasednessdoesnotimplyconsistency.\\xa0Forexample,considerestimatingthe\\nmeanparameterµofanormaldistributionN(x;µ,σ2),withadatasetconsisting\\nofmsamples:{x(1),...,x() m}.Wecouldusetheﬁrstsamplex(1)ofthedataset\\nasanunbiasedestimator:ˆθ=x(1).Inthatcase, E(ˆθ m)=θsotheestimator\\nisunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies\\nthattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent\\nestimatorasitisthecasethat not ˆθ m→ →∞θmas.\\n5.5MaximumLikelihoodEstimation\\nPreviously,wehaveseensomedeﬁnitionsofcommonestimatorsandanalyzed\\ntheirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing\\nthatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand\\nvariance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciﬁc\\nfunctionsthataregoodestimatorsfordiﬀerentmodels.\\nThemostcommonsuchprincipleisthemaximumlikelihoodprinciple.\\nConsiderasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom\\nthetruebutunknowndatageneratingdistributionpdata() x.\\nLetpmodel( x;θ)beaparametricfamilyofprobabilitydistributionsoverthe\\nsamespaceindexedbyθ.Inotherwords,pmodel(x;θ)mapsanyconﬁgurationx\\ntoarealnumberestimatingthetrueprobabilitypdata()x.\\nThemaximumlikelihoodestimatorforisthendeﬁnedas θ\\nθML= argmax\\nθpmodel(;) Xθ (5.56)\\n= argmax\\nθm\\ue059\\ni=1pmodel(x() i;)θ (5.57)\\n1 3 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7d43de27-9495-46bd-ab09-bb97a0c07a2d', embedding=None, metadata={'page_label': '147', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nThisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons.\\nForexample,itispronetonumericalunderﬂow.Toobtainamoreconvenient\\nbutequivalentoptimization problem,weobservethattakingthelogarithmofthe\\nlikelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct\\nintoasum:\\nθML= argmax\\nθm\\ue058\\ni=1logpmodel(x() i;)θ. (5.58)\\nBecausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan\\ndividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation\\nwithrespecttotheempiricaldistributionˆpdatadeﬁnedbythetrainingdata:\\nθML= argmax\\nθE x ∼ˆ pdatalogpmodel(;)xθ. (5.59)\\nOnewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing\\nthedissimilaritybetweentheempiricaldistributionˆpdatadeﬁnedbythetraining\\nsetandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo\\nmeasuredbytheKLdivergence.TheKLdivergenceisgivenby\\nDKL(ˆpdata\\ue06bpmodel) = E x ∼ˆ pdata[log ˆpdata()logx−pmodel()]x.(5.60)\\nThetermontheleftisafunctiononlyofthedatageneratingprocess,notthe\\nmodel.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we\\nneedonlyminimize\\n− E x ∼ˆ pdata[logpmodel()]x (5.61)\\nwhichisofcoursethesameasthemaximization inequation.5.59\\nMinimizingthisKLdivergencecorrespondsexactlytominimizingthecross-\\nentropybetweenthedistributions.Manyauthorsusetheterm“cross-entropy”to\\nidentifyspeciﬁcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution,\\nbutthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross-\\nentropybetweentheempiricaldistributiondeﬁnedbythetrainingsetandthe\\nprobabilitydistributiondeﬁnedbymodel.Forexample,meansquarederroristhe\\ncross-entropybetweentheempiricaldistributionandaGaussianmodel.\\nWecanthusseemaximumlikelihoodasanattempttomakethemodeldis-\\ntributionmatchtheempiricaldistributionˆpdata.Ideally,wewouldliketomatch\\nthetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis\\ndistribution.\\nWhiletheoptimalθisthesameregardlessofwhetherwearemaximizingthe\\nlikelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions\\n1 3 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70b35b70-0685-482e-a785-d1ae0f0e379f', embedding=None, metadata={'page_label': '148', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\narediﬀerent.Insoftware,weoftenphrasebothasminimizingacostfunction.\\nMaximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood\\n(NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof\\nmaximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase\\nbecausetheKLdivergencehasaknownminimumvalueofzero.Thenegative\\nlog-likelihoodcanactuallybecomenegativewhenisreal-valued.x\\n5.5.1ConditionalLog-LikelihoodandMeanSquaredError\\nThemaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere\\nourgoalistoestimateaconditionalprobabilityP( y x|;θ)inordertopredict y\\ngiven x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor\\nmostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved\\ntargets,thentheconditionalmaximumlikelihoodestimatoris\\nθML= argmax\\nθP. ( ;)YX|θ (5.62)\\nIftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto\\nθML= argmax\\nθm\\ue058\\ni=1log(Py() i|x() i;)θ. (5.63)\\nExample:LinearRegressionasMaximumLikelihoodLinearregression,\\nintroducedearlierinsection,maybejustiﬁedasamaximumlikelihood 5.1.4\\nprocedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns\\ntotakeaninputxandproduceanoutputvalue ˆy.Themappingfromxtoˆyis\\nchosentominimizemeansquarederror,acriterionthatweintroducedmoreorless\\narbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum\\nlikelihoodestimation.Insteadofproducingasingleprediction ˆy,wenowthink\\nofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine\\nthatwithaninﬁnitelylargetrainingset,wemightseeseveraltrainingexamples\\nwiththesameinputvaluexbutdiﬀerentvaluesofy.\\xa0Thegoalofthelearning\\nalgorithmisnowtoﬁtthedistributionp(y|x)toallofthosediﬀerentyvalues\\nthatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm\\nweobtainedbefore,wedeﬁnep(y|x) =N(y;ˆy(x;w),σ2).Thefunction ˆy(x;w)\\ngivesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat\\nthevarianceisﬁxedtosomeconstantσ2chosenbytheuser.Wewillseethatthis\\nchoiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation\\nproceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe\\n1 3 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e6fec77-08e4-4cc8-8470-6c3be085ad02', embedding=None, metadata={'page_label': '149', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nexamplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63\\ngivenby\\nm\\ue058\\ni=1log(py() i|x() i;)θ (5.64)\\n= log −mσ−m\\n2log(2)π−m\\ue058\\ni=1\\ue00d\\ue00dˆy() i−y() i\\ue00d\\ue00d2\\n2σ2,(5.65)\\nwhere ˆy() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe\\nnumberofthetrainingexamples.Comparingthelog-likelihoodwiththemean\\nsquarederror,\\nMSEtrain=1\\nmm\\ue058\\ni=1||ˆy() i−y() i||2, (5.66)\\nweimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields\\nthesameestimateoftheparameterswasdoesminimizingthemeansquarederror.\\nThetwocriteriahavediﬀerentvaluesbutthesamelocationoftheoptimum.This\\njustiﬁestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe\\nwillsee,themaximumlikelihoodestimatorhasseveraldesirableproperties.\\n5.5.2PropertiesofMaximumLikelihood\\nThemainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto\\nbethebestestimatorasymptotically,asthenumberofexamplesm→∞,interms\\nofitsrateofconvergenceasincreases.m\\nUnderappropriate\\xa0conditions,\\xa0the maximumlikelihood\\xa0estimatorhas\\xa0the\\npropertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5\\noftrainingexamplesapproachesinﬁnity,themaximumlikelihoodestimateofa\\nparameterconvergestothetruevalueoftheparameter.Theseconditionsare:\\n•Thetruedistributionpdatamustliewithinthemodelfamilypmodel(·;θ).\\nOtherwise,noestimatorcanrecoverpdata.\\n•Thetruedistributionpdatamustcorrespondtoexactlyonevalueofθ.Other-\\nwise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable\\ntodeterminewhichvalueofwasusedbythedatageneratingprocessing. θ\\nThereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima-\\ntor,manyofwhichsharethepropertyofbeingconsistentestimators.\\xa0However,\\n1 3 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6b4aa33f-edbd-4851-911a-6e818cc1dee0', embedding=None, metadata={'page_label': '150', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nconsistentestimatorscandiﬀerintheirstatisticeﬃciency,meaningthatone\\nconsistentestimatormayobtainlowergeneralization errorforaﬁxednumberof\\nsamplesm,orequivalently,mayrequirefewerexamplestoobtainaﬁxedlevelof\\ngeneralization error.\\nStatisticaleﬃciencyistypicallystudiedintheparametriccase(likeinlinear\\nregression)whereourgoalistoestimatethevalueofaparameter(andassuming\\nitispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto\\nmeasurehowclosewearetothetrueparameterisbytheexpectedmeansquared\\nerror,computingthesquareddiﬀerencebetweentheestimatedandtrueparameter\\nvalues,wheretheexpectationisovermtrainingsamplesfromthedatagenerating\\ndistribution.Thatparametricmeansquarederrordecreasesasmincreases,and\\nformlarge,theCramér-Raolowerbound(,;,)showsthatno Rao1945Cramér1946\\nconsistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood\\nestimator.\\nForthesereasons(consistencyandeﬃciency),maximumlikelihoodisoften\\nconsideredthepreferredestimatortouseformachinelearning.Whenthenumber\\nofexamplesissmallenoughtoyieldoverﬁttingbehavior,regularizationstrategies\\nsuchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood\\nthathaslessvariancewhentrainingdataislimited.\\n5.6BayesianStatistics\\nSofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat-\\ningasinglevalueofθ,thenmakingallpredictionsthereafterbasedonthatone\\nestimate.Anotherapproachistoconsiderallpossiblevaluesofθwhenmakinga\\nprediction.ThelatteristhedomainofBayesianstatistics.\\nAsdiscussed\\xa0insection\\xa0,\\xa0the\\xa0frequen tist\\xa0perspective\\xa0isthat\\xa0thetrue 5.4.1\\nparametervalueθisﬁxedbutunknown,whilethepointestimate ˆθisarandom\\nvariableonaccountofitbeingafunctionofthedataset(whichisseenasrandom).\\nTheBayesianperspectiveonstatisticsisquitediﬀerent.\\xa0The Bayesianuses\\nprobabilitytoreﬂectdegreesofcertaintyofstatesofknowledge.Thedatasetis\\ndirectlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterθ\\nisunknownoruncertainandthusisrepresentedasarandomvariable.\\nBeforeobservingthedata,werepresentourknowledgeofθusingtheprior\\nprobabilitydistribution,p(θ)(sometimesreferredtoassimply“theprior”).\\nGenerally,themachinelearningpractitionerselectsapriordistributionthatis\\nquitebroad(i.e.withhighentropy)toreﬂectahighdegreeofuncertaintyinthe\\n1 3 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa0ae520-ef53-45fe-a685-ef559f55159d', embedding=None, metadata={'page_label': '151', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nvalueofθbeforeobservinganydata.Forexample,onemightassume that apriori\\nθliesinsomeﬁniterangeorvolume,withauniformdistribution.\\xa0Manypriors\\ninsteadreﬂectapreferencefor“simpler”\\xa0solutions(suchassmallermagnitude\\ncoeﬃcients,orafunctionthatisclosertobeingconstant).\\nNowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan\\nrecovertheeﬀectofdataonourbeliefaboutθbycombiningthedatalikelihood\\npx((1),...,x() m|θ)withthepriorviaBayes’rule:\\npx(θ|(1),...,x() m) =px((1),...,x() m|θθ)(p)\\npx((1),...,x() m)(5.67)\\nInthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa\\nrelativelyuniformorGaussiandistributionwithhighentropy,andtheobservation\\nofthedatausuallycausestheposteriortoloseentropyandconcentratearounda\\nfewhighlylikelyvaluesoftheparameters.\\nRelativetomaximumlikelihoodestimation,Bayesianestimationoﬀerstwo\\nimportantdiﬀerences.First,unlikethemaximumlikelihoodapproachthatmakes\\npredictionsusingapointestimateofθ,theBayesianapproachistomakepredictions\\nusingafulldistributionoverθ.Forexample,afterobservingmexamples,the\\npredicteddistributionoverthenextdatasample,x(+1) m,isgivenby\\npx((+1) m|x(1),...,x() m) =\\ue05a\\npx((+1) m| |θθ)(px(1),...,x() m)d.θ(5.68)\\nHereeachvalueofθwithpositiveprobabilitydensitycontributestotheprediction\\nofthenextexample,withthecontributionweightedbytheposteriordensityitself.\\nAfterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe\\nvalueofθ,thenthisuncertaintyisincorporated directlyintoanypredictionswe\\nmightmake.\\nInsection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4\\ntaintyinagivenpointestimateofθbyevaluatingitsvariance.Thevarianceof\\ntheestimatorisanassessmentofhowtheestimatemightchangewithalternative\\nsamplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal\\nwiththeuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto\\nprotectwellagainstoverﬁtting.\\xa0Thisintegralisofcoursejustanapplicationof\\nthelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe\\nfrequentistmachineryforconstructinganestimatorisbasedontheratheradhoc\\ndecisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint\\nestimate.\\nThesecondimportantdiﬀerencebetweentheBayesianapproachtoestimation\\nandthemaximumlikelihoodapproachisduetothecontributionoftheBayesian\\n1 3 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a9791137-8bb8-48a8-b26a-527c2aacd966', embedding=None, metadata={'page_label': '152', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\npriordistribution.Thepriorhasaninﬂuencebyshiftingprobabilitymassdensity\\ntowardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori\\ntheprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth.\\nCriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman\\njudgmentimpactingthepredictions.\\nBayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata\\nisavailable,buttypicallysuﬀerfromhighcomputational costwhenthenumberof\\ntrainingexamplesislarge.\\nExample:BayesianLinearRegressionHereweconsidertheBayesianesti-\\nmationapproachtolearningthelinearregressionparameters.Inlinearregression,\\nwelearnalinearmappingfromaninputvectorx∈ Rntopredictthevalueofa\\nscalar.Thepredictionisparametrized bythevector y∈ R w∈ Rn:\\nˆy= w\\ue03ex. (5.69)\\nGivenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction\\nofovertheentiretrainingsetas: y\\nˆy()train= X()trainw. (5.70)\\nExpressedasaGaussianconditionaldistributionony()train,wehave\\np(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71)\\n∝exp\\ue012\\n−1\\n2(y()train−X()trainw)\\ue03e(y()train−X()trainw)\\ue013\\n,\\n(5.72)\\nwherewefollowthestandardMSEformulationinassumingthattheGaussian\\nvarianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto\\n(X()train,y()train) ( ) assimplyXy,.\\nTodeterminetheposteriordistributionoverthemodelparametervectorw,we\\nﬁrstneedtospecifyapriordistribution.Thepriorshouldreﬂectournaivebelief\\naboutthevalueoftheseparameters.Whileitissometimesdiﬃcultorunnatural\\ntoexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe\\ntypicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty\\naboutθ.\\xa0Forreal-valuedparametersitiscommontouseaGaussianasaprior\\ndistribution:\\np() = (;w Nwµ0, Λ0) exp∝\\ue012\\n−1\\n2(wµ−0)\\ue03eΛ−1\\n0(wµ−0)\\ue013\\n,(5.73)\\n1 3 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='51ec87c6-05c6-4543-9348-51cfd4c23686', embedding=None, metadata={'page_label': '153', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nwhereµ0and Λ0arethepriordistributionmeanvectorandcovariancematrix\\nrespectively.1\\nWiththepriorthusspeciﬁed,wecannowproceedindeterminingtheposterior\\ndistributionoverthemodelparameters.\\np,p,p (wX|y) ∝(yX|w)()w (5.74)\\n∝exp\\ue012\\n−1\\n2( )yXw−\\ue03e( )yXw−\\ue013\\nexp\\ue012\\n−1\\n2(wµ−0)\\ue03eΛ−1\\n0(wµ−0)\\ue013\\n(5.75)\\n∝exp\\ue012\\n−1\\n2\\ue010\\n−2y\\ue03eXww+\\ue03eX\\ue03eXww+\\ue03eΛ−1\\n0wµ−2\\ue03e\\n0 Λ−1\\n0w\\ue011\\ue013\\n.\\n(5.76)\\nWenowdeﬁne Λ m=\\ue000\\nX\\ue03eX+ Λ−1\\n0\\ue001 −1andµ m= Λ m\\ue000\\nX\\ue03ey+ Λ−1\\n0µ0\\ue001\\n.Using\\nthesenewvariables,weﬁndthattheposteriormayberewrittenasaGaussian\\ndistribution:\\np, (wX|y) exp∝\\ue012\\n−1\\n2(wµ− m)\\ue03eΛ−1\\nm(wµ− m)+1\\n2µ\\ue03e\\nm Λ−1\\nmµ m\\ue013\\n(5.77)\\n∝exp\\ue012\\n−1\\n2(wµ− m)\\ue03eΛ−1\\nm(wµ− m)\\ue013\\n. (5.78)\\nAlltermsthatdonotincludetheparametervectorwhavebeenomitted;they\\nareimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1\\nEquationshowshowtonormalizeamultivariateGaussiandistribution. 3.23\\nExaminingthisposteriordistributionallowsustogainsomeintuitionforthe\\neﬀectofBayesianinference.Inmostsituations,wesetµ0to 0.Ifweset Λ0=1\\nαI,\\nthenµ mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha\\nweightdecaypenaltyofαw\\ue03ew.OnediﬀerenceisthattheBayesianestimateis\\nundeﬁnedifαissettozero—-wearenotallowedtobegintheBayesianlearning\\nprocesswithaninﬁnitelywideprioronw.Themoreimportantdiﬀerenceisthat\\ntheBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe\\ndiﬀerentvaluesofare,ratherthanprovidingonlytheestimate w µ m.\\n5.6.1Maximum (MAP)Estimation A P o s t e ri o ri\\nWhilethemostprincipledapproachistomakepredictionsusingthefullBayesian\\nposteriordistributionovertheparameterθ,itisstilloftendesirabletohavea\\n1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a\\nd i a g o n a l c o v a ria n c e m a t rix Λ0= diag( λ0) .\\n1 3 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='323e2898-aae5-418d-a113-01d7005b2249', embedding=None, metadata={'page_label': '154', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nsinglepointestimate.\\xa0Onecommonreasonfordesiringapointestimateisthat\\nmostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare\\nintractable,andapointestimateoﬀersatractableapproximation.Ratherthan\\nsimplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof\\nthebeneﬁtoftheBayesianapproachbyallowingthepriortoinﬂuencethechoice\\nofthepointestimate.Onerationalwaytodothisistochoosethemaximum\\naposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof\\nmaximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon\\ncaseofcontinuous):θ\\nθMAP= argmax\\nθp( ) = argmaxθx|\\nθlog( )+log() pxθ|pθ.(5.79)\\nWerecognize,aboveontherighthandside,logp(xθ|),i.e.thestandardlog-\\nlikelihoodterm,and,correspondingtothepriordistribution. log()pθ\\nAsanexample,consideralinearregressionmodelwithaGaussianprioron\\ntheweightsw.IfthispriorisgivenbyN(w; 0,1\\nλI2),thenthelog-priortermin\\nequationisproportional tothefamiliar 5.79 λw\\ue03ewweightdecaypenalty,plusa\\ntermthatdoesnotdependonwanddoesnotaﬀectthelearningprocess.MAP\\nBayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight\\ndecay.\\nAswithfullBayesianinference,MAPBayesianinferencehastheadvantageof\\nleveraginginformationthatisbroughtbythepriorandcannotbefoundinthe\\ntrainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe\\nMAPpointestimate(incomparisontotheMLestimate).However,itdoessoat\\nthepriceofincreasedbias.\\nManyregularizedestimationstrategies,suchasmaximumlikelihoodlearning\\nregularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima-\\ntiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof\\naddinganextratermtotheobjectivefunctionthatcorrespondstologp(θ).Not\\nallregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample,\\nsomeregularizertermsmaynotbethelogarithmofaprobabilitydistribution.\\nOtherregularizationtermsdependonthedata,whichofcourseapriorprobability\\ndistributionisnotallowedtodo.\\nMAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated\\nyetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty\\ntermcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian\\ndistribution,astheprior(NowlanandHinton1992,).\\n1 3 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b78b875e-a771-4c2a-971c-f45714589214', embedding=None, metadata={'page_label': '155', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n5.7SupervisedLearningAlgorithms\\nRecallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3\\nlearningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena\\ntrainingsetofexamplesofinputsxandoutputsy.\\xa0Inmanycasestheoutputs\\nymaybediﬃculttocollectautomatically andmustbeprovidedbyahuman\\n“supervisor,”butthetermstillappliesevenwhenthetrainingsettargetswere\\ncollectedautomatically .\\n5.7.1ProbabilisticSupervisedLearning\\nMost\\xa0supervised\\xa0learning\\xa0algorithms\\xa0inthis\\xa0book\\xa0are\\xa0based\\xa0on estimating\\xa0a\\nprobabilitydistributionp(y|x).Wecandothissimplybyusingmaximum\\nlikelihoodestimationtoﬁndthebestparametervectorθforaparametricfamily\\nofdistributions .py(|xθ;)\\nWehavealreadyseenthatlinearregressioncorrespondstothefamily\\npyy (| Nxθ;) = (;θ\\ue03exI,). (5.80)\\nWecangeneralizelinearregressiontotheclassiﬁcationscenariobydeﬁninga\\ndiﬀerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and\\nclass1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The\\nprobabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues\\nmustaddupto1.\\nThenormaldistributionoverreal-valuednumbersthatweusedforlinear\\nregressionisparametrized intermsofamean.Anyvaluewesupplyforthismean\\nisvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because\\nitsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse\\nthelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe\\ninterval(0,1)andinterpretthatvalueasaprobability:\\npy σ (= 1 ;) = |xθ (θ\\ue03ex). (5.81)\\nThisapproachisknownaslogisticregression(asomewhatstrangenamesince\\nweusethemodelforclassiﬁcationratherthanregression).\\nInthecaseoflinearregression,wewereabletoﬁndtheoptimalweightsby\\nsolvingthenormalequations.Logisticregressionissomewhatmorediﬃcult.There\\nisnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor\\nthembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative\\nlog-likelihood(NLL)usinggradientdescent.\\n1 4 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5668781-2496-4df4-8c37-a5b216e80fee', embedding=None, metadata={'page_label': '156', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nThissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem,\\nbywritingdownaparametricfamilyofconditionalprobabilitydistributionsover\\ntherightkindofinputandoutputvariables.\\n5.7.2SupportVectorMachines\\nOneofthemostinﬂuentialapproachestosupervisedlearningisthesupportvector\\nmachine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto\\nlogisticregressioninthatitisdrivenbyalinearfunctionw\\ue03ex+b.Unlikelogistic\\nregression,thesupportvectormachinedoesnotprovideprobabilities, butonly\\noutputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen\\nw\\ue03ex+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen\\nw\\ue03ex+bisnegative.\\nOnekeyinnovationassociatedwithsupportvectormachinesisthekernel\\ntrick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms\\ncanbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample,\\nitcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan\\nbere-writtenas\\nw\\ue03ex+= +bbm\\ue058\\ni=1α ix\\ue03ex() i(5.82)\\nwherex() iisatrainingexampleandαisavectorofcoeﬃcients.Rewritingthe\\nlearningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature\\nfunctionφ(x) andthedotproductwithafunctionk(xx,() i) =φ(x)·φ(x() i) called\\nakernel.The ·operatorrepresentsaninnerproductanalogoustoφ(x)\\ue03eφ(x() i).\\nForsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In\\nsomeinﬁnitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for\\nexample,innerproductsbasedonintegrationratherthansummation.Acomplete\\ndevelopmentofthesekindsofinnerproductsisbeyondthescopeofthisbook.\\nAfterreplacingdotproductswithkernelevaluations,wecanmakepredictions\\nusingthefunction\\nfb () = x +\\ue058\\niα ik,(xx() i). (5.83)\\nThisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenφ(x)\\nandf(x)islinear.Also,therelationshipbetweenαandf(x)islinear.The\\nkernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying\\nφ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace.\\nThekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels\\nthatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare\\n1 4 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7a7579a-c508-4056-90c2-90647f289c33', embedding=None, metadata={'page_label': '157', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nguaranteedtoconvergeeﬃciently.Thisispossiblebecauseweconsiderφﬁxedand\\noptimizeonlyα,i.e.,theoptimization algorithmcanviewthedecisionfunction\\nasbeinglinearinadiﬀerentspace.Second,thekernelfunctionkoftenadmits\\nanimplementationthatissigniﬁcantlymorecomputational eﬃcientthannaively\\nconstructingtwovectorsandexplicitlytakingtheirdotproduct. φ()x\\nInsomecases,φ(x)canevenbeinﬁnitedimensional,whichwouldresultin\\naninﬁnitecomputational costforthenaive,explicitapproach.Inmanycases,\\nk(xx,\\ue030)isanonlinear,tractablefunctionofxevenwhenφ(x)isintractable.As\\nanexampleofaninﬁnite-dimens ionalfeaturespacewithatractablekernel,we\\nconstructafeaturemappingφ(x)overthenon-negativeintegersx.Supposethat\\nthismappingreturnsavectorcontainingxonesfollowedbyinﬁnitelymanyzeros.\\nWecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent\\ntothecorrespondinginﬁnite-dimens ionaldotproduct.\\nThemostcommonlyusedkernelistheGaussiankernel\\nk, ,σ (uvuv ) = (N −;02I) (5.84)\\nwhere N(x;µ, Σ)isthestandardnormaldensity.Thiskernelisalsoknownas\\ntheradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines\\ninvspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot\\nproductinaninﬁnite-dimens ionalspace,butthederivationofthisspaceisless\\nstraightforwardthaninourexampleofthekernelovertheintegers. min\\nWecanthinkoftheGaussiankernelasperformingakindoftemplatematch-\\ning.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate\\nforclassy.Whenatestpointx\\ue030isnearxaccordingtoEuclideandistance,the\\nGaussiankernelhasalargeresponse,indicatingthatx\\ue030isverysimilartothex\\ntemplate.Themodelthenputsalargeweightontheassociatedtraininglabely.\\nOverall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe\\nsimilarityofthecorrespondingtrainingexamples.\\nSupportvectormachinesarenottheonlyalgorithmthatcanbeenhanced\\nusingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The\\ncategoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines\\norkernelmethods( ,; WilliamsandRasmussen1996Schölkopf1999etal.,).\\nAmajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision\\nfunctionislinearinthenumberoftrainingexamples,becausethei-thexample\\ncontributesatermα ik(xx,() i)tothedecisionfunction.Supportvectormachines\\nareabletomitigatethisbylearninganαvectorthatcontainsmostlyzeros.\\nClassifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor\\nthetrainingexamplesthathavenon-zeroα i.Thesetrainingexamplesareknown\\n1 4 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6ffcd31a-9762-41b8-bc06-e4f57d452525', embedding=None, metadata={'page_label': '158', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nassupportvectors.\\nKernelmachinesalsosuﬀerfromahighcomputational costoftrainingwhen\\nthedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9\\ngenerickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11\\nmodernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof\\nkernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal.\\n()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006\\nontheMNISTbenchmark.\\n5.7.3OtherSimpleSupervisedLearningAlgorithms\\nWehavealreadybrieﬂyencounteredanothernon-probabilis ticsupervisedlearning\\nalgorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis\\nafamilyoftechniquesthatcanbeusedforclassiﬁcationorregression.Asa\\nnon-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoaﬁxed\\nnumberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm\\nasnothavinganyparameters,butratherimplementingasimplefunctionofthe\\ntrainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess.\\nInstead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx,\\nweﬁndthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe\\naverageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially\\nanykindofsupervisedlearningwherewecandeﬁneanaverageoveryvalues.In\\nthecaseofclassiﬁcation,wecanaverageoverone-hotcodevectorscwithc y= 1\\nandc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese\\none-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric\\nlearningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample,\\nsupposewehaveamulticlassclassiﬁcationtaskandmeasureperformancewith0-1\\nloss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1\\nnumberoftrainingexamplesapproachesinﬁnity.TheerrorinexcessoftheBayes\\nerrorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally\\ndistantneighborsrandomly.Whenthereisinﬁnitetrainingdata,alltestpointsx\\nwillhaveinﬁnitelymanytrainingsetneighborsatdistancezero.Ifweallowthe\\nalgorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone\\nofthem,theprocedureconvergestotheBayeserrorrate.\\xa0Thehighcapacityof\\nk-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset.\\nHowever,itdoessoathighcomputational cost,anditmaygeneralizeverybadly\\ngivenasmall,ﬁnitetrainingset.Oneweaknessofk-nearestneighborsisthatit\\ncannotlearnthatonefeatureismorediscriminativethananother.Forexample,\\nimaginewehavearegressiontaskwithx∈ R100drawnfromanisotropicGaussian\\n1 4 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e2d7e79f-ba6c-488d-888d-ee02d6b3b138', embedding=None, metadata={'page_label': '159', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\ndistribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose\\nfurtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall\\ncases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern.\\nThenearestneighborofmostpointsxwillbedeterminedbythelargenumberof\\nfeaturesx2throughx100,notbythelonefeaturex1.\\xa0Thustheoutputonsmall\\ntrainingsetswillessentiallyberandom.\\n1 4 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6ec0b4d-8db6-40c1-a1d3-6a8a5437c568', embedding=None, metadata={'page_label': '160', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n0\\n101\\n1110 1\\n011\\n1111 1110110\\n10010\\n001110 11111101001 00\\n010 01111\\n111\\n11\\nFigure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree\\nchoosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon\\ntheright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis\\ndisplayedwithabinarystringidentiﬁercorrespondingtoitspositioninthetree,obtained\\nbyappendingabittoitsparentidentiﬁer(0=chooseleftortop,1=chooserightorbottom).\\n( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree\\nmightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode\\ndrawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe\\ncenteroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction,\\nwithonepieceperleaf.Eachleafrequiresatleastonetrainingexampletodeﬁne,soitis\\nnotpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe\\nnumberoftrainingexamples.\\n1 4 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4524faeb-5800-4e24-9014-206a7bd35427', embedding=None, metadata={'page_label': '161', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nAnothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions\\nandhasseparateparametersforeachregionisthedecisiontree( , Breimanetal.\\n1984)anditsmanyvariants.Asshowninﬁgure,eachnodeofthedecision 5.7\\ntreeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat\\nregionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned\\ncut).\\xa0Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one\\ncorrespondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps\\neverypointinitsinputregiontothesameoutput.Decisiontreesareusually\\ntrainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The\\nlearningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree\\nofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints\\nthatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare\\ntypicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode,\\nstruggletosolvesomeproblemsthatareeasyevenforlogisticregression.For\\nexample,ifwehaveatwo-classproblemandthepositiveclassoccurswherever\\nx2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus\\nneedtoapproximatethedecisionboundarywithmanynodes,implementingastep\\nfunctionthatconstantlywalksbackandforthacrossthetruedecisionfunction\\nwithaxis-alignedsteps.\\nAswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany\\nlimitations.Nonetheless,theyareusefullearningalgorithmswhencomputational\\nresourcesareconstrained.Wecanalsobuildintuitionformoresophisticated\\nlearningalgorithmsbythinkingaboutthesimilaritiesanddiﬀerencesbetween\\nsophisticatedalgorithmsand-NNordecisiontreebaselines. k\\nSee (),\\xa0(),\\xa0 ()orothermachine Murphy2012Bishop2006Hastieetal.2001\\nlearningtextbooksformorematerialontraditionalsupervisedlearningalgorithms.\\n5.8UnsupervisedLearningAlgorithms\\nRecallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3\\nonly“features”butnotasupervisionsignal.Thedistinctionbetweensupervised\\nandunsupervisedalgorithmsisnotformallyandrigidlydeﬁnedbecausethereisno\\nobjectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby\\nasupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract\\ninformationfromadistributionthatdonotrequirehumanlabortoannotate\\nexamples.Thetermisusuallyassociatedwithdensityestimation,learningto\\ndrawsamplesfromadistribution,learningtodenoisedatafromsomedistribution,\\nﬁndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof\\n1 4 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e8413b49-9b5c-46a0-825f-1b98f1b30c5e', embedding=None, metadata={'page_label': '162', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nrelatedexamples.\\nAclassicunsupervisedlearningtaskistoﬁndthe“best”representationofthe\\ndata.By‘best’wecanmeandiﬀerentthings,butgenerallyspeakingwearelooking\\nforarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile\\nobeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler\\nmoreaccessiblethanitself.x\\nTherearemultiplewaysofdeﬁningarepresentation.Threeofthe simpler\\xa0\\nmostcommonincludelowerdimensionalrepresentations,sparserepresentations\\nandindependentrepresentations.Low-dimensionalrepresentationsattemptto\\ncompressasmuchinformationaboutxaspossibleinasmallerrepresentation.\\nSparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand\\nGhahramani1997,)embedthedatasetintoarepresentationwhoseentriesare\\nmostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires\\nincreasingthedimensionalityoftherepresentation,sothattherepresentation\\nbecomingmostlyzeroesdoesnotdiscardtoomuchinformation. Thisresultsinan\\noverallstructureoftherepresentationthattendstodistributedataalongtheaxes\\noftherepresentationspace.Independentrepresentationsattempttodisentangle\\nthesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions\\noftherepresentationarestatisticallyindependent.\\nOf\\xa0coursethese\\xa0three\\xa0criteriaare\\xa0certainly\\xa0notmutuallyexclusive.Low-\\ndimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde-\\npendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto\\nreducethesizeofarepresentationistoﬁndandremoveredundancies.Identifying\\nandremovingmoreredundancyallowsthedimensionalityreductionalgorithmto\\nachievemorecompressionwhilediscardinglessinformation.\\nThenotionofrepresentationisoneofthecentralthemesofdeeplearningand\\nthereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome\\nsimpleexamplesofrepresentationlearningalgorithms.Together,theseexample\\nalgorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe\\nremainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat\\ndevelopthesecriteriaindiﬀerentwaysorintroduceothercriteria.\\n5.8.1PrincipalComponentsAnalysis\\nInsection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12\\nameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning\\nalgorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon\\ntwoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa\\n1 4 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d63bb9b-6c7f-47bb-b67e-b64259d09f5d', embedding=None, metadata={'page_label': '163', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n− − 2 0 1 0 0 1 0 2 0\\nx 1− 2 0− 1 001 02 0x 2\\n− − 2 0 1 0 0 1 0 2 0\\nz 1− 2 0− 1 001 02 0z 2\\nFigure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance\\nwiththeaxesofthenewspace. ( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis\\nspace,thevariancemightoccuralongdirectionsthatarenotaxis-aligned.\\xa0 ( R i g h t )The\\ntransformeddataz=x\\ue03eWnowvariesmostalongtheaxisz 1.Thedirectionofsecond\\nmostvarianceisnowalongz 2.\\nrepresentationthathaslowerdimensionalitythantheoriginalinput.Italsolearns\\narepresentationwhoseelementshavenolinearcorrelationwitheachother.This\\nisaﬁrststeptowardthecriterionoflearningrepresentationswhoseelementsare\\nstatisticallyindependent.Toachievefullindependence,arepresentationlearning\\nalgorithmmustalsoremovethenonlinearrelationshipsbetweenvariables.\\nPCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan\\ninputxtoarepresentationzasshowninﬁgure.Insection,wesawthat 5.8 2.12\\nwecouldlearnaone-dimensional representationthatbestreconstructstheoriginal\\ndata(inthesenseofmeansquarederror)andthatthisrepresentationactually\\ncorrespondstotheﬁrstprincipalcomponentofthedata.ThuswecanusePCA\\nasasimpleandeﬀectivedimensionalityreductionmethodthatpreservesasmuch\\noftheinformationinthedataaspossible(again,asmeasuredbyleast-squares\\nreconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation\\ndecorrelatestheoriginaldatarepresentation.X\\nLetusconsiderthemn×-dimensionaldesignmatrixX.Wewillassumethat\\nthedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily\\nbecenteredbysubtractingthemeanfromallexamplesinapreprocessingstep.\\nTheunbiasedsamplecovariancematrixassociatedwithisgivenby:X\\nVar[] =x1\\nm−1X\\ue03eX. (5.85)\\n1 4 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7af62dbc-04cd-41d9-b49a-0a918194ee33', embedding=None, metadata={'page_label': '164', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nPCAﬁndsarepresentation(throughlineartransformation)z=x\\ue03eWwhere\\nVar[]zisdiagonal.\\nInsection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X\\naregivenbytheeigenvectorsofX\\ue03eX.Fromthisview,\\nX\\ue03eXWW = Λ\\ue03e. (5.86)\\nInthissection,weexploitanalternativederivationoftheprincipalcomponents.The\\nprincipalcomponentsmayalsobeobtainedviathesingularvaluedecomposition.\\nSpeciﬁcally,theyaretherightsingularvectorsofX.Toseethis,letWbethe\\nrightsingularvectorsinthedecompositionX=UW Σ\\ue03e.\\xa0Wethenrecoverthe\\noriginaleigenvectorequationwithastheeigenvectorbasis: W\\nX\\ue03eX=\\ue010\\nUW Σ\\ue03e\\ue011\\ue03e\\nUW Σ\\ue03e= W Σ2W\\ue03e.(5.87)\\nTheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe\\nSVDof,wecanexpressthevarianceofas: X X\\nVar[] =x1\\nm−1X\\ue03eX (5.88)\\n=1\\nm−1(UW Σ\\ue03e)\\ue03eUW Σ\\ue03e(5.89)\\n=1\\nm−1W Σ\\ue03eU\\ue03eUW Σ\\ue03e(5.90)\\n=1\\nm−1W Σ2W\\ue03e, (5.91)\\nwhereweusethefactthatU\\ue03eU=IbecausetheUmatrixofthesingularvalue\\ndecompositionisdeﬁnedtobeorthogonal.Thisshowsthatifwetakez=x\\ue03eW,\\nwecanensurethatthecovarianceofisdiagonalasrequired: z\\nVar[] =z1\\nm−1Z\\ue03eZ (5.92)\\n=1\\nm−1W\\ue03eX\\ue03eXW (5.93)\\n=1\\nm−1W\\ue03eW Σ2W\\ue03eW (5.94)\\n=1\\nm−1Σ2, (5.95)\\nwherethistimeweusethefactthatW\\ue03eW=I,againfromthedeﬁnitionofthe\\nSVD.\\n1 4 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3585f038-8c7a-4004-b3d0-98d3cf3eedf1', embedding=None, metadata={'page_label': '165', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nTheaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear\\ntransformationW,theresultingrepresentationhasadiagonalcovariancematrix\\n(asgivenby Σ2)whichimmediatelyimpliesthattheindividualelementsofzare\\nmutuallyuncorrelated.\\nThisabilityofPCAtotransformdataintoarepresentationwheretheelements\\naremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple\\nexampleofarepresentationthatattemptstodisentangletheunknownfactorsof\\nvariationunderlyingthedata.\\xa0InthecaseofPCA,thisdisentanglingtakesthe\\nformofﬁndingarotationoftheinputspace(describedbyW)thatalignsthe\\nprincipalaxesofvariancewiththebasisofthenewrepresentationspaceassociated\\nwith.z\\nWhilecorrelationisanimportantcategoryofdependencybetweenelementsof\\nthedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore\\ncomplicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat\\ncanbedonewithasimplelineartransformation.\\n5.8.2-meansClustering k\\nAnotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering.\\nThek-meansclusteringalgorithmdividesthetrainingsetintokdiﬀerentclusters\\nofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas\\nprovidingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx\\nbelongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare\\nzero.\\nTheone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse\\nrepresentation,becausethemajorityofitsentriesarezeroforeveryinput.Later,\\nwewilldevelopotheralgorithmsthatlearnmoreﬂexiblesparserepresentations,\\nwheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes\\nareanextremeexampleofsparserepresentationsthatlosemanyofthebeneﬁts\\nofadistributedrepresentation.Theone-hotcodestillconferssomestatistical\\nadvantages(itnaturallyconveystheideathatallexamplesinthesameclusterare\\nsimilartoeachother)anditconfersthecomputational advantagethattheentire\\nrepresentationmaybecapturedbyasingleinteger.\\nThek-meansalgorithmworksbyinitializingkdiﬀerentcentroids{µ(1),...,µ() k}\\ntodiﬀerentvalues,thenalternatingbetweentwodiﬀerentstepsuntilconvergence.\\nInonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof\\nthenearestcentroidµ() i.Intheotherstep,eachcentroidµ() iisupdatedtothe\\nmeanofalltrainingexamplesx() jassignedtocluster.i\\n1 5 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dc5bafe0-3643-4fa1-8dfe-33100fea4375', embedding=None, metadata={'page_label': '166', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nOnediﬃcultypertainingtoclusteringisthattheclusteringproblemisinherently\\nill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella\\nclusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof\\ntheclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe\\nmembersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct\\nthetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe\\nclusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there\\nmaybemanydiﬀerentclusteringsthatallcorrespondwelltosomepropertyof\\ntherealworld.Wemayhopetoﬁndaclusteringthatrelatestoonefeaturebut\\nobtainadiﬀerent,equallyvalidclusteringthatisnotrelevanttoourtask.For\\nexample,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof\\nimagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray\\ncars.Ifweaskeachclusteringalgorithmtoﬁndtwoclusters,onealgorithmmay\\nﬁndaclusterofcarsandaclusteroftrucks,whileanothermayﬁndaclusterof\\nredvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering\\nalgorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign\\ntheexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This\\nnewclusteringnowatleastcapturesinformationaboutbothattributes,butithas\\nlostinformationaboutsimilarity.Redcarsareinadiﬀerentclusterfromgray\\ncars,justastheyareinadiﬀerentclusterfromgraytrucks.\\xa0Theoutputofthe\\nclusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars\\nthantheyaretograytrucks.Theyarediﬀerentfromboththings,andthatisall\\nweknow.\\nTheseissuesillustratesomeofthereasonsthatwemaypreferadistributed\\nrepresentationtoaone-hotrepresentation.Adistributedrepresentationcouldhave\\ntwoattributesforeachvehicle—onerepresentingitscolorandonerepresenting\\nwhetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal\\ndistributedrepresentationis(howcanthelearningalgorithmknowwhetherthe\\ntwoattributesweareinterestedinarecolorandcar-versus-truckratherthan\\nmanufacturerandage?)buthavingmanyattributesreducestheburdenonthe\\nalgorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure\\nsimilaritybetweenobjectsinaﬁne-grainedwaybycomparingmanyattributes\\ninsteadofjusttestingwhetheroneattributematches.\\n5.9StochasticGradientDescent\\nNearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic\\ngradientdescentorSGD.Stochasticgradientdescentisanextensionofthe\\n1 5 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0cb9f8b3-46a1-496a-a4a7-1c6695aa6645', embedding=None, metadata={'page_label': '167', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\ngradientdescentalgorithmintroducedinsection.4.3\\nArecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary\\nforgoodgeneralization, butlargetrainingsetsarealsomorecomputationally\\nexpensive.\\nThecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa\\nsumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the\\nnegativeconditionallog-likelihoodofthetrainingdatacanbewrittenas\\nJ() = θ E x ,y ∼ˆ pdataL,y,(xθ) =1\\nmm\\ue058\\ni=1L(x() i,y() i,θ)(5.96)\\nwhereistheper-exampleloss L L,y,py. (xθ) = log− (|xθ;)\\nFortheseadditivecostfunctions,gradientdescentrequirescomputing\\n∇ θJ() =θ1\\nmm\\ue058\\ni=1∇ θL(x() i,y() i,.θ) (5.97)\\nThecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto\\nbillionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively\\nlong.\\nTheinsightofstochasticgradientdescentisthatthegradientisanexpectation.\\nTheexpectationmaybeapproximately estimatedusingasmallsetofsamples.\\nSpeciﬁcally,oneachstepofthealgorithm,wecansampleaminibatchofexamples\\nB={x(1),...,x( m\\ue030)}drawnuniformlyfromthetrainingset.Theminibatchsize\\nm\\ue030istypicallychosentobearelativelysmallnumberofexamples,rangingfrom\\n1toafewhundred.Crucially,m\\ue030isusuallyheldﬁxedasthetrainingsetsizem\\ngrows.Wemayﬁtatrainingsetwithbillionsofexamplesusingupdatescomputed\\nononlyahundredexamples.\\nTheestimateofthegradientisformedas\\ng=1\\nm\\ue030∇ θm\\ue030\\ue058\\ni=1L(x() i,y() i,.θ) (5.98)\\nusingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B\\nthenfollowstheestimatedgradientdownhill:\\nθθg ← −\\ue00f, (5.99)\\nwhereisthelearningrate. \\ue00f\\n1 5 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='70f4efaf-c8bb-4ea0-b882-01c874b15e48', embedding=None, metadata={'page_label': '168', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nGradientdescentingeneralhasoftenbeenregardedassloworunreliable.In\\nthepast,theapplicationofgradientdescenttonon-convexoptimization problems\\nwasregardedasfoolhardyorunprincipled. Today,weknowthatthemachine\\nlearningmodelsdescribedinpartworkverywellwhentrainedwithgradient II\\ndescent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena\\nlocalminimuminareasonableamountoftime,butitoftenﬁndsaverylowvalue\\nofthecostfunctionquicklyenoughtobeuseful.\\nStochasticgradientdescenthasmanyimportantusesoutsidethecontextof\\ndeeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge\\ndatasets.Foraﬁxedmodelsize,thecostperSGDupdatedoesnotdependonthe\\ntrainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize\\nincreases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach\\nconvergenceusuallyincreaseswithtrainingsetsize.\\xa0However,asmapproaches\\ninﬁnity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore\\nSGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot\\nextendtheamountoftrainingtimeneededtoreachthemodel’sbestpossibletest\\nerror.Fromthispointofview,onecanarguethattheasymptoticcostoftraining\\namodelwithSGDisasafunctionof. O(1) m\\nPriortotheadventofdeeplearning,themainwaytolearnnonlinearmodels\\nwastousethekerneltrickincombinationwithalinearmodel.Manykernellearning\\nalgorithmsrequireconstructinganmm×matrixG i , j=k(x() i,x() j).Constructing\\nthismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets\\nwith\\xa0billions of\\xa0examples. In\\xa0academia, starting\\xa0in2006,deep\\xa0learning was\\ninitiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter\\nthancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof\\nthousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin\\nindustry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge\\ndatasets.\\nStochasticgradientdescentandmanyenhancements toitaredescribedfurther\\ninchapter.8\\n5.10BuildingaMachineLearningAlgorithm\\nNearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof\\nafairlysimplerecipe:combineaspeciﬁcationofadataset,acostfunction,an\\noptimization procedureandamodel.\\nForexample,thelinearregressionalgorithmcombinesadatasetconsistingof\\n1 5 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5aaaf325-c3cc-4934-9ad2-063ef59bc1cd', embedding=None, metadata={'page_label': '169', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nXyand,thecostfunction\\nJ,b(w) = − E x ,y ∼ˆ pdatalogpmodel( )y|x, (5.100)\\nthemodelspeciﬁcationpmodel(y|x) =N(y;x\\ue03ew+b,1),and,inmostcases,the\\noptimization algorithmdeﬁnedbysolvingforwherethegradientofthecostiszero\\nusingthenormalequations.\\nByrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently\\nfromtheothers,wecanobtainaverywidevarietyofalgorithms.\\nThecostfunctiontypicallyincludesatleastonetermthatcausesthelearning\\nprocesstoperformstatisticalestimation.Themostcommoncostfunctionisthe\\nnegativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum\\nlikelihoodestimation.\\nThecostfunctionmayalsoincludeadditionalterms,suchasregularization\\nterms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction\\ntoobtain\\nJ,bλ (w) = ||||w2\\n2− E x ,y ∼ˆ pdatalogpmodel( )y|x.(5.101)\\nThisstillallowsclosed-formoptimization.\\nIfwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger\\nbeoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical\\noptimization procedure,suchasgradientdescent.\\nTherecipeforconstructingalearningalgorithmbycombiningmodels,costs,and\\noptimization algorithmssupportsbothsupervisedandunsupervisedlearning.The\\nlinearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised\\nlearningcanbesupportedbydeﬁningadatasetthatcontainsonlyXandproviding\\nanappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheﬁrst\\nPCAvectorbyspecifyingthatourlossfunctionis\\nJ() = w E x ∼ˆ pdata||− ||xr(;)xw2\\n2 (5.102)\\nwhileourmodelisdeﬁnedtohavewwithnormoneandreconstructionfunction\\nr() = xw\\ue03exw.\\nInsomecases,thecostfunctionmaybeafunctionthatwecannotactually\\nevaluate,forcomputational reasons.Inthesecases,wecanstillapproximately\\nminimizeitusingiterativenumericaloptimization solongaswehavesomewayof\\napproximatingitsgradients.\\nMostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot\\nimmediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor\\n1 5 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='144c8431-0a2c-4dc6-b8a5-098210d8628b', embedding=None, metadata={'page_label': '170', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nhand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some\\nmodelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause\\ntheircostfunctionshaveﬂatregionsthatmaketheminappropriate forminimization\\nbygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms\\ncanbedescribedusingthisrecipehelpstoseethediﬀerentalgorithmsaspartofa\\ntaxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather\\nthanasalonglistofalgorithmsthateachhaveseparatejustiﬁcations.\\n5.11ChallengesMotivatingDeepLearning\\nThesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon\\nawidevarietyofimportantproblems.However,theyhavenotsucceededinsolving\\nthecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects.\\nThedevelopmentofdeeplearningwasmotivatedinpartbythefailureof\\ntraditionalalgorithmstogeneralizewellonsuchAItasks.\\nThissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes\\nexponentiallymorediﬃcultwhenworkingwithhigh-dimensionaldata,andhow\\nthemechanismsusedtoachievegeneralization intraditionalmachinelearning\\nareinsuﬃcienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such\\nspacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto\\novercometheseandotherobstacles.\\n5.11.1TheCurseofDimensionality\\nManymachinelearningproblemsbecomeexceedinglydiﬃcultwhenthenumber\\nofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof\\ndimensionality.Ofparticularconcernisthatthenumberofpossibledistinct\\nconﬁgurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables\\nincreases.\\n1 5 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e589f048-02c4-4eff-a2aa-0a42267f8f81', embedding=None, metadata={'page_label': '171', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto\\nright),thenumberofconﬁgurationsofinterestmaygrowexponentially. ( L e f t )Inthis\\none-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10\\nregionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion\\ncorrespondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly.\\nAstraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin\\neachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r )\\ndimensionsitismorediﬃculttodistinguish10diﬀerentvaluesofeachvariable.\\xa0Weneed\\ntokeeptrackofupto10×10=100regions,andweneedatleastthatmanyexamplesto\\ncoverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat\\nleastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach\\naxis,weseemtoneedO(vd)regionsandexamples.\\xa0Thisisaninstanceofthecurseof\\ndimensionality.FiguregraciouslyprovidedbyNicolasChapados.\\nThecurseofdimensionalityarisesinmanyplacesincomputerscience,and\\nespeciallysoinmachinelearning.\\nOnechallengeposedbythecurseofdimensionalityisastatisticalchallenge.\\nAsillustratedinﬁgure,astatisticalchallengearisesbecausethenumberof 5.9\\npossibleconﬁgurations ofxismuchlargerthanthenumberoftrainingexamples.\\nTounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa\\ngrid,likeintheﬁgure.Wecandescribelow-dimensional spacewithalownumber\\nofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata\\npoint,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples\\nthatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability\\ndensityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin\\nthesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples.\\nIfwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining\\nexamplesinthesamecell.\\xa0Ifwearedoingregressionwecanaveragethetarget\\nvaluesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich\\nwehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof\\nconﬁgurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell\\nhasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething\\n1 5 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b43a0a96-297f-4b8f-8ec4-898aa4f96661', embedding=None, metadata={'page_label': '172', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nmeaningfulaboutthesenewconﬁgurations? Manytraditionalmachinelearning\\nalgorithmssimplyassumethattheoutputatanewpointshouldbeapproximately\\nthesameastheoutputatthenearesttrainingpoint.\\n5.11.2LocalConstancyandSmoothnessRegularization\\nInordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior\\nbeliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen\\nthesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions\\noverparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas\\ndirectlyinﬂuencingtheitselfandonlyindirectlyactingontheparameters function\\nviatheireﬀectonthefunction.Additionally,weinformallydiscusspriorbeliefsas\\nbeingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing\\nsomeclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed\\n(orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour\\ndegreeofbeliefinvariousfunctions.\\nAmongthemostwidelyusedoftheseimplicit“priors”\\xa0isthesmoothness\\npriororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn\\nshouldnotchangeverymuchwithinasmallregion.\\nManysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and\\nasaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI-\\nleveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces\\nadditional(explicit\\xa0andimplicit)priorsinorder\\xa0toreducethegeneralization\\nerroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis\\ninsuﬃcientforthesetasks.\\nTherearemanydiﬀerentwaystoimplicitlyorexplicitlyexpressapriorbelief\\nthatthelearnedfunctionshouldbesmoothorlocallyconstant.Allofthesediﬀerent\\nmethodsaredesignedtoencouragethelearningprocesstolearnafunctionf∗that\\nsatisﬁesthecondition\\nf∗() x≈f∗(+)x\\ue00f (5.103)\\nformostconﬁgurationsxandsmallchange\\ue00f.Inotherwords,ifweknowagood\\nanswerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat\\nanswerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers\\ninsomeneighborhoodwewouldcombinethem(bysomeformofaveragingor\\ninterpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas\\npossible.\\nAnextremeexampleofthelocalconstancyapproachisthek-nearestneighbors\\nfamilyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach\\n1 5 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ebe12bc-affa-4510-9a7a-0f57b09f82ce', embedding=None, metadata={'page_label': '173', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nregioncontainingallthepointsxthathavethesamesetofknearestneighborsin\\nthetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore\\nthanthenumberoftrainingexamples.\\nWhilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining\\nexamples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated\\nwithnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal\\nkernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther\\napartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction\\nthatperformstemplatematching,bymeasuringhowcloselyatestexamplex\\nresembleseachtrainingexamplex() i.\\xa0Muchofthemodernmotivationfordeep\\nlearningisderivedfromstudyingthelimitationsoflocaltemplatematchingand\\nhowdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails\\n( ,). Bengioetal.2006b\\nDecisiontreesalsosuﬀerfromthelimitationsofexclusivelysmoothness-based\\nlearningbecausetheybreaktheinputspaceintoasmanyregionsasthereare\\nleavesanduseaseparateparameter(orsometimesmanyparametersforextensions\\nofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat\\nleastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare\\nrequiredtoﬁtthetree.Amultipleofnisneededtoachievesomelevelofstatistical\\nconﬁdenceinthepredictedoutput.\\nIngeneral,todistinguishO(k)regionsininputspace,allofthesemethods\\nrequireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters\\nassociatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario,\\nwhereeachtrainingexamplecanbeusedtodeﬁneatmostoneregion,isillustrated\\ninﬁgure.5.10\\nIsthereawaytorepresentacomplexfunctionthathasmanymoreregions\\ntobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming\\nonlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat.\\nFor\\xa0example,\\xa0imagine that\\xa0thetargetfunctionis\\xa0akind\\xa0ofcheckerboard.A\\ncheckerboardcontainsmanyvariationsbutthereisasimplestructuretothem.\\nImaginewhathappenswhenthenumberoftrainingexamplesissubstantially\\nsmallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based\\nononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould\\nbeguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame\\ncheckerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner\\ncouldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo\\nnotcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan\\nexampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe\\n1 5 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2470afc8-2c08-4a75-a560-923eb5824e0d', embedding=None, metadata={'page_label': '174', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.10:\\xa0Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\\nintoregions.\\xa0Anexample(representedherebyacircle)withineachregiondeﬁnesthe\\nregionboundary(representedherebythelines).Theyvalueassociatedwitheachexample\\ndeﬁneswhattheoutputshouldbeforallpointswithinthecorrespondingregion.\\xa0The\\nregionsdeﬁnedbynearestneighbormatchingformageometricpatterncalledaVoronoi\\ndiagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber\\noftrainingexamples.Whilethisﬁgureillustratesthebehaviorofthenearestneighbor\\nalgorithmspeciﬁcally,othermachinelearningalgorithmsthatrelyexclusivelyonthe\\nlocalsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample\\nonlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately\\nsurroundingthatexample.\\n1 5 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='31efb0a8-3289-456d-938c-526b6d599279', embedding=None, metadata={'page_label': '175', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nentirecheckerboardrightistocovereachofitscellswithatleastoneexample.\\nThesmoothnessassumptionandtheassociatednon-parametric learningalgo-\\nrithmsworkextremelywellsolongasthereareenoughexamplesforthelearning\\nalgorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys\\nofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe\\nfunctiontobelearnedissmoothenoughandvariesinfewenoughdimensions.\\nInhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina\\ndiﬀerentwayalongeachdimension.Ifthefunctionadditionallybehavesdiﬀerently\\nindiﬀerentregions,itcanbecomeextremelycomplicatedtodescribewithasetof\\ntrainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge\\nnumberofregionscomparedtothenumberofexamples),isthereanyhopeto\\ngeneralizewell?\\nTheanswertobothofthesequestions—whetheritispossibletorepresent\\nacomplicatedfunctioneﬃciently,andwhetheritispossiblefortheestimated\\nfunctiontogeneralizewelltonewinputs—isyes.Thekeyinsightisthatavery\\nlargenumberofregions,e.g.,O(2k),canbedeﬁnedwithO(k)examples,solong\\nasweintroducesomedependenciesbetweentheregionsviaadditionalassumptions\\nabouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually\\ngeneralizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c\\ndiﬀerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare\\nreasonableforabroadrangeofAItasksinordertocapturetheseadvantages.\\nOtherapproachestomachinelearningoftenmakestronger,task-speciﬁcas-\\nsumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding\\ntheassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch\\nstrong,task-speciﬁcassumptionsintoneuralnetworkssothattheycangeneralize\\ntoamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo\\ncomplextobelimitedtosimple,manuallyspeciﬁedpropertiessuchasperiodicity,\\nsowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions.\\nThecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby\\nthecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy.\\nManyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal-\\ngorithms.\\xa0Theseapparentlymildassumptionsallowanexponentialgaininthe\\nrelationshipbetweenthenumberofexamplesandthenumberofregionsthatcan\\nbedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections\\n6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep,\\ndistributedrepresentationscountertheexponentialchallengesposedbythecurse\\nofdimensionality.\\n1 6 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df5b7ce1-2de6-4ba4-82e2-f62bad7bdd2c', embedding=None, metadata={'page_label': '176', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\n5.11.3ManifoldLearning\\nAnimportantconceptunderlyingmanyideasinmachinelearningisthatofa\\nmanifold.\\nAmanifoldisaconnected\\xa0region. Mathematically ,\\xa0it\\xa0isasetofpoints,\\nassociatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the\\nmanifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience\\nthesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin\\n3-Dspace.\\nThedeﬁnitionofaneighborhoodsurroundingeachpointimpliestheexistence\\noftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition\\ntoaneighboringone.Intheexampleoftheworld’ssurfaceasamanifold,onecan\\nwalknorth,south,east,orwest.\\nAlthoughthereisaformalmathematical meaningtotheterm“manifold,”in\\nmachinelearningittendstobeusedmorelooselytodesignateaconnectedset\\nofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof\\ndegreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each\\ndimensioncorrespondstoalocaldirectionofvariation.Seeﬁgureforan5.11\\nexampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo-\\ndimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality\\nofthemanifoldtovaryfromonepointtoanother.\\xa0This oftenhappenswhena\\nmanifoldintersectsitself.Forexample,aﬁgureeightisamanifoldthathasasingle\\ndimensioninmostplacesbuttwodimensionsattheintersectionatthecenter.\\n0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . .− 1 0 .− 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 .\\nFigure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually\\nconcentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates\\ntheunderlyingmanifoldthatthelearnershouldinfer.\\n1 6 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dac03aca-6f72-492f-8052-27231a2d6589', embedding=None, metadata={'page_label': '177', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nManymachinelearningproblemsseemhopelessifweexpectthemachine\\nlearningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn.\\nManifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost\\nof Rnconsistsofinvalidinputs,\\xa0andthatinterestinginputsoccuronlyalong\\nacollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting\\nvariationsintheoutputofthelearnedfunctionoccurringonlyalongdirections\\nthatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe\\nmovefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase\\nofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis\\nprobabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe\\nsupervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis\\nhighlyconcentrated.\\nTheassumptionthatthedataliesalongalow-dimensional manifoldmaynot\\nalwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas\\nthosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis\\natleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists\\noftwocategoriesofobservations.\\nTheﬁrstobservationinfavorofthemanifoldhypothesisisthattheproba-\\nbilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis\\nhighlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs\\nfromthesedomains.\\xa0Figureshowshow,instead,uniformlysampledpoints 5.12\\nlooklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal\\nisavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat\\nrandom,whatistheprobabilitythatyouwillgetameaningfulEnglish-language\\ntext?Almostzero,again,becausemostofthelongsequencesoflettersdonot\\ncorrespondtoanaturallanguagesequence:thedistributionofnaturallanguage\\nsequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters.\\n1 6 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68cde586-cf86-4c4b-82ea-35b1b9e6c55d', embedding=None, metadata={'page_label': '178', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel\\naccordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon-\\nzeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered\\ninAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests\\nthattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe\\nvolumeofimagespace.\\nOfcourse,concentratedprobabilitydistributionsarenotsuﬃcienttoshow\\nthatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso\\nestablishthattheexamplesweencounterareconnectedtoeachotherbyother\\n1 6 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a72c3c27-acc0-44e7-a835-e8077061d823', embedding=None, metadata={'page_label': '179', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nexamples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat\\nmaybereachedbyapplyingtransformationstotraversethemanifold.Thesecond\\nargumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch\\nneighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we\\ncancertainlythinkofmanypossibletransformationsthatallowustotraceouta\\nmanifoldinimagespace:wecangraduallydimorbrightenthelights,gradually\\nmoveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof\\nobjects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost\\napplications.Forexample,themanifoldofimagesofhumanfacesmaynotbe\\nconnectedtothemanifoldofimagesofcatfaces.\\nThesethoughtexperimentssupportingthemanifoldhypothesesconveysomein-\\ntuitivereasonssupportingit.Morerigorousexperiments\\xa0(Cayton2005Narayanan,;\\nandMitter2010Schölkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal.,\\n2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,;\\nandSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof\\ninterestinAI.\\nWhenthedataliesonalow-dimensional manifold,itcanbemostnatural\\nformachinelearningalgorithmstorepresentthedataintermsofcoordinateson\\nthemanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan\\nthinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto\\nspeciﬁcaddressesintermsofaddressnumbersalongthese1-Droads,notinterms\\nofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging,\\nbutholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral\\nprincipleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13\\nadatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe\\nmethodsnecessarytolearnsuchamanifoldstructure.Inﬁgure,wewillsee 20.6\\nhowamachinelearningalgorithmcansuccessfullyaccomplishthisgoal.\\nThisconcludespart,whichhasprovidedthebasicconceptsinmathematics I\\nandmachinelearningwhichareemployedthroughouttheremainingpartsofthe\\nbook.Youarenowpreparedtoembarkuponyourstudyofdeeplearning.\\n1 6 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e127efa-533b-4071-9f29-676125241596', embedding=None, metadata={'page_label': '180', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER5.MACHINELEARNINGBASICS\\nFigure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000\\nforwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional\\nmanifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe\\nabletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6\\nfeat.\\n1 6 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5dd70ebd-05de-440d-8e5a-db28d6e3a328', embedding=None, metadata={'page_label': '181', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='P a rt I I\\nD e e p N e t w orks: Mo d e rn\\nPractices\\n166', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4157c87-f4ce-4816-b0d1-b9bb6f70db58', embedding=None, metadata={'page_label': '182', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Thispartofthebooksummarizesthestateofmoderndeeplearningasitis\\nusedtosolvepracticalapplications.\\nDeeplearninghasalonghistoryandmanyaspirations.Severalapproaches\\nhavebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoals\\nhaveyettoberealized.Theseless-developedbranchesofdeeplearningappearin\\ntheﬁnalpartofthebook.\\nThispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech-\\nnologiesthatarealreadyusedheavilyinindustry.\\nModern\\xa0deeplearning\\xa0provides\\xa0avery\\xa0powerful\\xa0framework\\xa0forsupervised\\nlearning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcan\\nrepresentfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappingan\\ninputvectortoanoutputvector,andthatareeasyforapersontodorapidly,can\\nbeaccomplishedviadeeplearning,givensuﬃcientlylargemodelsandsuﬃciently\\nlargedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed\\nasassociatingonevectortoanother,orthatarediﬃcultenoughthataperson\\nwouldrequiretimetothinkandreﬂectinordertoaccomplishthetask,remain\\nbeyondthescopeofdeeplearningfornow.\\nThispartofthebookdescribesthecoreparametricfunctionapproximation\\ntechnologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning.\\nWe\\xa0begin\\xa0by\\xa0describingthe\\xa0feedforward\\xa0deepnetworkmodelthatisusedto\\nrepresentthesefunctions.Next,wepresentadvancedtechniquesforregularization\\nandoptimization ofsuchmodels.Scalingthesemodelstolargeinputssuchashigh\\nresolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce\\ntheconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural\\nnetworkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines\\nforthepracticalmethodologyinvolvedindesigning,building,andconﬁguringan\\napplicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeep\\nlearning.\\nThesechaptersarethemostimportantforapractitioner—someone whowants\\ntobeginimplementingandusingdeeplearningalgorithmstosolvereal-world\\nproblemstoday.\\n1 6 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c40f768-fed1-417b-84c4-67634d0a4ff0', embedding=None, metadata={'page_label': '183', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 6\\nD e e p F e e d f orw ard N e t w orks\\nDeepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks,\\normultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels.\\nThegoalofafeedforwardnetworkistoapproximatesomefunction f∗.Forexample,\\nforaclassiﬁer, y= f∗(x)mapsaninputxtoacategory y.Afeedforwardnetwork\\ndeﬁnesamappingy= f(x;θ)andlearnsthevalueoftheparametersθthatresult\\ninthebestfunctionapproximation.\\nThesemodelsarecalledfeedforwardbecauseinformationﬂowsthroughthe\\nfunctionbeingevaluatedfromx,throughtheintermediate computations usedto\\ndeﬁne f,andﬁnallytotheoutputy.Therearenofeedbackconnectionsinwhich\\noutputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks\\nareextendedtoincludefeedbackconnections,theyarecalledrecurrentneural\\nnetworks,presentedinchapter.10\\nFeedforwardnetworksareofextremeimportancetomachinelearningpracti-\\ntioners.Theyformthebasisofmanyimportantcommercialapplications.For\\nexample,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea\\nspecializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual\\nsteppingstoneonthepathtorecurrentnetworks,whichpowermanynatural\\nlanguageapplications.\\nFeedforwardneuralnetworksarecallednetworksbecausetheyaretypically\\nrepresentedbycomposingtogethermanydiﬀerentfunctions.Themodelisasso-\\nciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed\\ntogether.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected\\ninachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost\\ncommonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledtheﬁrst\\nlayerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall\\n168', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5a88a2e-6f69-4bef-895e-d9332c369fac', embedding=None, metadata={'page_label': '184', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nlengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat\\nthename“deeplearning”arises.Theﬁnallayerofafeedforwardnetworkiscalled\\ntheoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch f∗(x).\\nThetrainingdataprovidesuswithnoisy,approximateexamplesof f∗(x) evaluated\\natdiﬀerenttrainingpoints.Eachexamplexisaccompanied byalabel y f≈∗(x).\\nThetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint\\nx;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis\\nnotdirectlyspeciﬁedbythetrainingdata.\\xa0Thelearningalgorithmmustdecide\\nhowtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes\\nnotsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust\\ndecidehowtousetheselayerstobestimplementanapproximation of f∗.Because\\nthetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these\\nlayersarecalledhiddenlayers.\\nFinally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby\\nneuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The\\ndimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each\\nelementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron.\\nRatherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction,\\nwecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel,\\neachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin\\nthesensethatitreceivesinputfrommanyotherunitsandcomputesitsown\\nactivationvalue.\\xa0Theideaofusingmanylayersofvector-valuedrepresentation\\nisdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute\\ntheserepresentationsisalsolooselyguidedbyneuroscientiﬁcobservationsabout\\nthefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork\\nresearchisguidedbymanymathematical andengineeringdisciplines,andthe\\ngoalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof\\nfeedforwardnetworksasfunctionapproximation machinesthataredesignedto\\nachievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe\\nknowaboutthebrain,ratherthanasmodelsofbrainfunction.\\nOnewaytounderstandfeedforwardnetworksistobeginwithlinearmodels\\nandconsiderhowtoovercometheirlimitations.\\xa0Linearmodels,suchaslogistic\\nregressionandlinearregression,areappealingbecausetheymaybeﬁteﬃciently\\nandreliably,eitherinclosedformorwithconvexoptimization. Linearmodelsalso\\nhavetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so\\nthemodelcannotunderstandtheinteractionbetweenanytwoinputvariables.\\nToextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply\\nthelinearmodelnottoxitselfbuttoatransformedinput φ(x),where φisa\\n1 6 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b7213b73-07a0-48c8-925e-fae6bfcf44c1', embedding=None, metadata={'page_label': '185', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nnonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin\\nsection,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2\\nthe φmapping.Wecanthinkof φasprovidingasetoffeaturesdescribingx,or\\nasprovidinganewrepresentationfor.x\\nThequestionisthenhowtochoosethemapping. φ\\n1.Oneoptionistouseaverygeneric φ,suchastheinﬁnite-dimens ional φthat\\nisimplicitlyusedbykernelmachinesbasedontheRBFkernel.\\xa0If φ(x)is\\nofhighenoughdimension,wecanalwayshaveenoughcapacitytoﬁtthe\\ntrainingset,butgeneralization tothetestsetoftenremainspoor.Very\\ngenericfeaturemappingsareusuallybasedonlyontheprincipleoflocal\\nsmoothnessanddonotencodeenoughpriorinformationtosolveadvanced\\nproblems.\\n2.Anotheroptionistomanuallyengineer φ.Untiltheadventofdeeplearning,\\nthiswasthedominantapproach.Thisapproachrequiresdecadesofhuman\\neﬀortfor\\xa0eachseparate\\xa0task,\\xa0withpractitioners\\xa0specializing\\xa0in diﬀerent\\ndomainssuchasspeech\\xa0recognition or\\xa0computer vision,\\xa0and\\xa0with little\\ntransferbetweendomains.\\n3.Thestrategyofdeeplearningistolearn φ.Inthisapproach,wehaveamodel\\ny= f(x;θw ,) = φ(x;θ)\\ue03ew.Wenowhaveparametersθthatweusetolearn\\nφfromabroadclassoffunctions,andparameterswthatmapfrom φ(x)to\\nthedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with\\nφdeﬁningahiddenlayer.\\xa0Thisapproachistheonlyoneofthethreethat\\ngivesupontheconvexityofthetrainingproblem,butthebeneﬁtsoutweigh\\ntheharms.Inthisapproach,weparametrizetherepresentationas φ(x;θ)\\nandusetheoptimization algorithmtoﬁndtheθthatcorrespondstoagood\\nrepresentation.Ifwewish,thisapproachcancapturethebeneﬁtoftheﬁrst\\napproachbybeinghighlygeneric—wedosobyusingaverybroadfamily\\nφ(x;θ).Thisapproachcanalsocapturethebeneﬁtofthesecondapproach.\\nHumanpractitioners canencodetheirknowledgetohelpgeneralization by\\ndesigningfamilies φ(x;θ)thattheyexpectwillperformwell.Theadvantage\\nisthatthehumandesigneronlyneedstoﬁndtherightgeneralfunction\\nfamilyratherthanﬁndingpreciselytherightfunction.\\nThisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond\\nthefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep\\nlearningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook.\\nFeedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic\\n1 7 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='984bc063-a395-405d-933e-6b6129e9e18a', embedding=None, metadata={'page_label': '186', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nmappingsfromxtoythatlackfeedbackconnections.\\xa0Othermodelspresented\\nlaterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions\\nwithfeedback,andlearningprobabilitydistributionsoverasinglevector.\\nWebeginthischapterwithasimpleexampleofafeedforwardnetwork.Next,\\nweaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork.\\nFirst,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign\\ndecisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost\\nfunction,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based\\nlearning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique\\ntofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa\\nhiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill\\nbeusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture\\nofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese\\nlayersshould\\xa0beconnectedto\\xa0each\\xa0other,\\xa0and howmanyunitsshould\\xa0bein\\neachlayer.Learningindeepneuralnetworksrequirescomputingthegradients\\nofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits\\nmoderngeneralizations ,whichcanbeusedtoeﬃcientlycomputethesegradients.\\nFinally,weclosewithsomehistoricalperspective.\\n6. 1 E x am p l e: L earni n g X O R\\nTomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan\\nexampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning\\ntheXORfunction.\\nTheXORfunction(“exclusiveor”)isanoperationontwobinaryvalues, x 1\\nand x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1\\nreturns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1\\ny= f∗(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;θ)and\\nourlearningalgorithmwilladapttheparametersθtomake fassimilaraspossible\\nto f∗.\\nInthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization.\\nWewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0]\\ue03e,[0 ,1]\\ue03e,\\n[1 ,0]\\ue03e,and[1 ,1]\\ue03e}.\\xa0Wewilltrainthenetworkonallfourofthesepoints.\\xa0The\\nonlychallengeistoﬁtthetrainingset.\\nWecantreatthisproblemasaregressionproblemanduseameansquared\\nerrorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis\\nexampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan\\n1 7 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8533120f-cb52-49aa-8f5f-447e2cec778e', embedding=None, metadata={'page_label': '187', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nappropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches\\naredescribedinsection.6.2.2.2\\nEvaluatedonourwholetrainingset,theMSElossfunctionis\\nJ() =θ1\\n4\\ue058\\nx∈ X( f∗() (;))x− fxθ2. (6.1)\\nNowwemustchoosetheformofourmodel, f(x;θ).Supposethatwechoose\\nalinearmodel,withconsistingofand.Ourmodelisdeﬁnedtobe θw b\\nf , b (;xw) = x\\ue03ew+ b . (6.2)\\nWecanminimize J(θ)inclosedformwithrespecttowand busingthenormal\\nequations.\\nAftersolvingthenormalequations,weobtainw= 0and b=1\\n2.\\xa0Thelinear\\nmodelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1\\nhowalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve\\nthisproblemistouseamodelthatlearnsadiﬀerentfeaturespaceinwhicha\\nlinearmodelisabletorepresentthesolution.\\nSpeciﬁcally,wewillintroduceaverysimplefeedforwardnetworkwithone\\nhiddenlayercontainingtwohiddenunits.Seeﬁgureforanillustrationof 6.2\\nthismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare\\ncomputedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen\\nusedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe\\nnetwork.Theoutputlayerisstilljustalinearregressionmodel,butnowitis\\nappliedtohratherthantox.Thenetworknowcontainstwofunctionschained\\ntogether:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing\\nf , , , b f (;xWcw) = ( 2 )( f( 1 )())x .\\nWhatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar,\\nanditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were\\nlinear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof\\nitsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =W\\ue03ex\\nand f( 2 )(h) =h\\ue03ew.Then f(x) =w\\ue03eW\\ue03ex.Wecouldrepresentthisfunctionas\\nf() = xx\\ue03ew\\ue030wherew\\ue030= Ww.\\nClearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural\\nnetworksdosousinganaﬃnetransformationcontrolledbylearnedparameters,\\nfollowedbyaﬁxed,nonlinearfunctioncalledanactivationfunction.Weusethat\\nstrategyhere,bydeﬁningh= g(W\\ue03ex+c) ,whereWprovidestheweightsofa\\nlineartransformationandcthebiases.Previously,todescribealinearregression\\n1 7 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d702acc-4b6c-45e4-928b-d1dac83be205', embedding=None, metadata={'page_label': '188', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n0 1\\nx 101x 2O r i g i n a l s p a c e x\\n0 1 2\\nh 101h 2L e a r n e d s p a c e h\\nFigure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers\\nprintedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint.\\n( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR\\nfunction.When x1= 0,themodel’soutputmustincreaseas x2increases.When x1= 1,\\nthemodel’soutputmustdecreaseas x 2increases.Alinearmodelmustapplyaﬁxed\\ncoeﬃcient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange\\nthecoeﬃcienton x 2andcannotsolvethisproblem. ( R i g h t )Inthetransformedspace\\nrepresentedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve\\ntheproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1\\ncollapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave\\nmappedbothx= [1 ,0]\\ue03eandx= [0 ,1]\\ue03etoasinglepointinfeaturespace,h= [1 ,0]\\ue03e.\\nThelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2.\\nInthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel\\ncapacitygreatersothatitcanﬁtthetrainingset.Inmorerealisticapplications,learned\\nrepresentationscanalsohelpthemodeltogeneralize.\\n1 7 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e97b01e2-65ca-40be-b6e4-289eccd52d9e', embedding=None, metadata={'page_label': '189', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nyy\\nhh\\nx xWwyy\\nh 1 h 1\\nx 1 x 1h 2 h 2\\nx 2 x 2\\nFigure6.2:Anexampleofafeedforwardnetwork,drawnintwodiﬀerentstyles.Speciﬁcally,\\nthisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden\\nlayercontainingtwounits. ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph.\\nThisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample\\nitcanconsumetoomuchspace. Inthisstyle,wedrawanodeinthegraphfor ( R i g h t )\\neachentirevectorrepresentingalayer’sactivations.\\xa0Thisstyleismuchmorecompact.\\nSometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat\\ndescribetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes\\nthemappingfromxtoh,andavectorwdescribesthemappingfromhto y.We\\ntypicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind\\nofdrawing.\\nmodel,weusedavectorofweightsandascalarbiasparametertodescribean\\naﬃnetransformationfromaninputvectortoanoutputscalar.Now,wedescribe\\nanaﬃnetransformationfromavectorxtoavectorh,soanentirevectorofbias\\nparametersisneeded.Theactivationfunction gistypicallychosentobeafunction\\nthatisappliedelement-wise,with h i= g(x\\ue03eW : , i+ c i).Inmodernneuralnetworks,\\nthedefaultrecommendation istousetherectiﬁedlinearunitorReLU(Jarrett\\ne t a l . e t a l . ,; ,; 2009NairandHinton2010Glorot,)deﬁnedbytheactivation 2011a\\nfunction depictedinﬁgure. g z , z () = max0{} 6.3\\nWecannowspecifyourcompletenetworkas\\nf , , , b (;xWcw) = w\\ue03emax0{ ,W\\ue03exc+}+ b . (6.3)\\nWecannowspecifyasolutiontotheXORproblem.Let\\nW=\\ue01411\\n11\\ue015\\n, (6.4)\\nc=\\ue014\\n0\\n−1\\ue015\\n, (6.5)\\n1 7 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='63eebf19-c2b2-413b-be46-e61c17d692e4', embedding=None, metadata={'page_label': '190', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n0\\nz0g z ( ) = m a x 0{ , z}\\nFigure6.3:Therectiﬁedlinearactivationfunction.Thisactivationfunctionisthedefault\\nactivationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying\\nthisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation.\\nHowever,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear\\nfunctionwithtwolinearpieces.Becauserectiﬁedlinearunitsarenearlylinear,they\\npreservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient-\\nbasedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels\\ngeneralizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild\\ncomplicatedsystemsfromminimalcomponents.\\xa0MuchasaTuringmachine’smemory\\nneedsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator\\nfromrectiﬁedlinearfunctions.\\n1 7 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fce4c910-4e16-4e3c-971a-cdf36a699494', embedding=None, metadata={'page_label': '191', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nw=\\ue0141\\n−2\\ue015\\n, (6.6)\\nand. b= 0\\nWecannowwalkthroughthewaythatthemodelprocessesabatchofinputs.\\nLetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace,\\nwithoneexampleperrow:\\nX=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f000\\n01\\n10\\n11\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.7)\\nTheﬁrststepintheneuralnetworkistomultiplytheinputmatrixbytheﬁrst\\nlayer’sweightmatrix:\\nXW=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f000\\n11\\n11\\n22\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.8)\\nNext,weaddthebiasvector,toobtainc\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00 1−\\n10\\n10\\n21\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.9)\\nInthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1\\nthisline,theoutputneedstobeginat,thenriseto,thendropbackdownto. 0 1 0\\nAlinearmodelcannotimplementsuchafunction.Toﬁnishcomputingthevalue\\nofforeachexample,weapplytherectiﬁedlineartransformation: h\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f000\\n10\\n10\\n21\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.10)\\nThistransformationhaschangedtherelationshipbetweentheexamples.Theyno\\nlongerlieonasingleline.Asshowninﬁgure,theynowlieinaspacewherea 6.1\\nlinearmodelcansolvetheproblem.\\nWeﬁnishbymultiplyingbytheweightvector:w\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f00\\n1\\n1\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb. (6.11)\\n1 7 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7362a652-07a2-4a24-b92d-e8822f613ce7', embedding=None, metadata={'page_label': '192', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nTheneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch.\\nInthisexample,wesimplyspeciﬁedthesolution,thenshowedthatitobtained\\nzeroerror.\\xa0Inarealsituation,theremightbebillionsofmodelparametersand\\nbillionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid\\nhere.Instead,agradient-basedoptimization algorithmcanﬁndparametersthat\\nproduceverylittleerror.ThesolutionwedescribedtotheXORproblemisata\\nglobalminimumofthelossfunction,sogradientdescentcouldconvergetothis\\npoint.ThereareotherequivalentsolutionstotheXORproblemthatgradient\\ndescentcouldalsoﬁnd.Theconvergencepointofgradientdescentdependsonthe\\ninitialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot\\nﬁndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented\\nhere.\\n6. 2 Gradi en t - Bas e d L earni n g\\nDesigningandtraininganeuralnetworkisnotmuchdiﬀerentfromtrainingany\\nothermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10\\nhowtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure,\\nacostfunction,andamodelfamily.\\nThelargestdiﬀerencebetweenthelinearmodelswehaveseensofarandneural\\nnetworksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss\\nfunctionstobecomenon-convex.Thismeansthatneuralnetworksareusually\\ntrainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost\\nfunctiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain\\nlinearregressionmodelsortheconvexoptimization algorithmswithglobalconver-\\ngenceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization\\nconvergesstartingfromanyinitialparameters(intheory—inpracticeitisvery\\nrobustbutcanencounternumericalproblems).Stochasticgradientdescentapplied\\ntonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive\\ntothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis\\nimportanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe\\ninitializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti-\\nmizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep\\nmodelswillbedescribedindetailinchapter,withparameterinitialization in 8\\nparticulardiscussedinsection.Forthemoment,itsuﬃcestounderstandthat 8.4\\nthetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe\\ncostfunctioninonewayoranother.\\xa0The speciﬁcalgorithmsareimprovements\\nandreﬁnementsontheideasofgradientdescent,introducedinsection,and,4.3\\n1 7 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='71590cdb-3355-4492-b2d5-448acdb6e8d1', embedding=None, metadata={'page_label': '193', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nmorespeciﬁcally,aremostoftenimprovementsofthestochasticgradientdescent\\nalgorithm,introducedinsection.5.9\\nWecanofcourse,trainmodelssuchaslinearregressionandsupportvector\\nmachineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining\\nsetisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot\\nmuchdiﬀerentfromtraininganyothermodel.Computingthegradientisslightly\\nmorecomplicatedforaneuralnetwork,butcanstillbedoneeﬃcientlyandexactly.\\nSectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5\\nalgorithmandmoderngeneralizations oftheback-propagationalgorithm.\\nAswithothermachinelearningmodels,toapplygradient-basedlearningwe\\nmustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof\\nthemodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison\\ntheneuralnetworksscenario.\\n6.2.1CostFunctions\\nAnimportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe\\ncostfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless\\nthesameasthoseforotherparametricmodels,suchaslinearmodels.\\nInmostcases,ourparametricmodeldeﬁnesadistribution p(yx|;θ)and\\nwesimplyuse\\xa0theprinciple\\xa0ofmaximumlikelihood.Thismeansweusethe\\ncross-entropybetweenthetrainingdataandthemodel’spredictionsasthecost\\nfunction.\\nSometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete\\nprobabilitydistributionovery,wemerelypredictsomestatisticofyconditioned\\non.Specializedlossfunctionsallowustotrainapredictoroftheseestimates. x\\nThetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone\\noftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave\\nalreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin\\nsection.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2\\napplicabletodeepneuralnetworksandisamongthemostpopularregularization\\nstrategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe\\ndescribedinchapter.7\\n6.2.1.1LearningConditionalDistributionswithMaximumLikelihood\\nMostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans\\nthatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed\\n1 7 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='df5bdaec-24b9-46ba-92ea-ff6fff9e6175', embedding=None, metadata={'page_label': '194', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nasthecross-entropybetweenthetrainingdataandthemodeldistribution.This\\ncostfunctionisgivenby\\nJ() = θ − E x y ,∼ ˆ pdatalog p m o de l( )yx| . (6.12)\\nThespeciﬁcformofthecostfunctionchangesfrommodeltomodel,depending\\nonthespeciﬁcformoflog p m o de l.Theexpansionoftheaboveequationtypically\\nyieldssometermsthatdonotdependonthemodelparametersandmaybedis-\\ncarded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;θ) ,I),\\nthenwerecoverthemeansquarederrorcost,\\nJ θ() =1\\n2E x y ,∼ ˆ pdata||− ||y f(;)xθ2+const , (6.13)\\nuptoascalingfactorof1\\n2andatermthatdoesnotdependon.Thediscardedθ\\nconstantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase\\nwechosenottoparametrize. Previously,wesawthattheequivalencebetween\\nmaximumlikelihoodestimationwithanoutputdistributionandminimization of\\nmeansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds\\nregardlessoftheusedtopredictthemeanoftheGaussian. f(;)xθ\\nAnadvantageofthisapproachofderivingthecostfunctionfrommaximum\\nlikelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel.\\nSpecifyingamodel p(yx|)automatically determinesacostfunction log p(yx|).\\nOnerecurringthemethroughoutneuralnetworkdesignisthatthegradientof\\nthecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide\\nforthelearningalgorithm.Functionsthatsaturate(becomeveryﬂat)undermine\\nthisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases\\nthishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe\\nhiddenunitsortheoutputunitssaturate.\\xa0Thenegativelog-likelihoodhelpsto\\navoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction\\nthatcansaturatewhenitsargumentisverynegative.The logfunctioninthe\\nnegativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill\\ndiscusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin\\nsection.6.2.2\\nOneunusualpropertyofthecross-entropycostusedtoperformmaximum\\nlikelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied\\ntothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most\\nmodelsareparametrized insuchawaythattheycannotrepresentaprobability\\nofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression\\nisanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel\\n1 7 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e88a5c3c-1718-4786-9f08-c7c6bb4e623c', embedding=None, metadata={'page_label': '195', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ncancontrolthedensityoftheoutputdistribution(forexample,bylearningthe\\nvarianceparameterofaGaussianoutputdistribution)thenitbecomespossible\\ntoassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin\\ncross-entropyapproachingnegativeinﬁnity.Regularizationtechniquesdescribed\\ninchapterprovideseveraldiﬀerentwaysofmodifyingthelearningproblemso 7\\nthatthemodelcannotreapunlimitedrewardinthisway.\\n6.2.1.2LearningConditionalStatistics\\nInsteadoflearningafullprobabilitydistribution p(yx|;θ)weoftenwanttolearn\\njustoneconditionalstatisticofgiven.yx\\nForexample,wemayhaveapredictor f(x;θ) thatwewishtopredictthemean\\nof.y\\nIfweuseasuﬃcientlypowerfulneuralnetwork,wecanthinkoftheneural\\nnetworkasbeingabletorepresentanyfunction ffromawideclassoffunctions,\\nwiththisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness\\nratherthanbyhavingaspeciﬁcparametricform.Fromthispointofview,we\\ncanviewthecostfunctionasbeingafunctionalratherthanjustafunction.A\\nfunctionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof\\nlearningaschoosingafunctionratherthanmerelychoosingasetofparameters.\\nWecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciﬁc\\nfunctionwedesire.Forexample,wecandesignthecostfunctionaltohaveits\\nminimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx.\\nSolvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical\\ntoolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2\\ntounderstandcalculusofvariationstounderstandthecontentofthischapter.At\\nthemoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe\\nusedtoderivethefollowingtworesults.\\nOurﬁrstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza-\\ntionproblem\\nf∗= argmin\\nfE x y ,∼ pdata||− ||y f()x2(6.14)\\nyields\\nf∗() = x E y∼ pdata ( ) y x|[]y , (6.15)\\nsolongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe\\ncouldtrainoninﬁnitelymanysamplesfromthetruedatageneratingdistribution,\\nminimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe\\nmeanofforeachvalueof. y x\\n1 8 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1beab68-49e4-4a30-a9f2-13e375020ced', embedding=None, metadata={'page_label': '196', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nDiﬀerentcostfunctionsgivediﬀerentstatistics.Asecondresultderivedusing\\ncalculusofvariationsisthat\\nf∗= argmin\\nfE x y ,∼ pdata||− ||y f()x 1 (6.16)\\nyieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha\\nfunctionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost\\nfunctioniscommonlycalled . meanabsoluteerror\\nUnfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor\\nresultswhenusedwithgradient-basedoptimization. Someoutputunitsthat\\nsaturateproduceverysmallgradientswhencombinedwiththesecostfunctions.\\nThisisonereasonthatthecross-entropycostfunctionismorepopularthanmean\\nsquarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean\\nentiredistribution. p( )yx|\\n6.2.2OutputUnits\\nThechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most\\nofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe\\nmodeldistribution.\\xa0Thechoiceofhowtorepresenttheoutputthendetermines\\ntheformofthecross-entropyfunction.\\nAnykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe\\nusedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe\\nmodel,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits\\nwithadditionaldetailabouttheiruseashiddenunitsinsection.6.3\\nThroughoutthissection,wesupposethatthefeedforwardnetworkprovidesa\\nsetofhiddenfeaturesdeﬁnedbyh= f(x;θ).Theroleoftheoutputlayeristhen\\ntoprovidesomeadditionaltransformationfromthefeaturestocompletethetask\\nthatthenetworkmustperform.\\n6.2.2.1LinearUnitsforGaussianOutputDistributions\\nOnesimplekindofoutputunitisanoutputunitbasedonanaﬃnetransformation\\nwithnononlinearity.Theseareoftenjustcalledlinearunits.\\nGivenfeaturesh,alayeroflinearoutputunitsproducesavectorˆy=W\\ue03eh+b.\\nLinearoutputlayersareoftenusedtoproducethemeanofaconditional\\nGaussiandistribution:\\np( ) = (;yx| NyˆyI ,) . (6.17)\\n1 8 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a474ec11-ace2-4bb9-a546-03b5b0527426', embedding=None, metadata={'page_label': '197', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nMaximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared\\nerror.\\nThemaximumlikelihoodframeworkmakesitstraightforwardtolearnthe\\ncovarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea\\nfunctionoftheinput.However,thecovariancemustbeconstrainedtobeapositive\\ndeﬁnitematrixforallinputs.Itisdiﬃculttosatisfysuchconstraintswithalinear\\noutputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance.\\nApproachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4\\nBecauselinearunitsdonotsaturate,theyposelittlediﬃcultyforgradient-\\nbasedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization\\nalgorithms.\\n6.2.2.2SigmoidUnitsforBernoulliOutputDistributions\\nManytasksrequirepredictingthevalueofabinaryvariable y.Classiﬁcation\\nproblemswithtwoclassescanbecastinthisform.\\nThemaximum-likelihoodapproachistodeﬁneaBernoullidistributionover y\\nconditionedon.x\\nABernoullidistributionisdeﬁnedbyjustasinglenumber.Theneuralnet\\nneedstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it\\nmustlieintheinterval[0,1].\\nSatisfyingthisconstraintrequiressomecarefuldesigneﬀort.Supposewewere\\ntousealinearunit,andthresholditsvaluetoobtainavalidprobability:\\nP y(= 1 ) = max |x\\ue06e\\n0min ,\\ue06e\\n1 ,w\\ue03eh+ b\\ue06f\\ue06f\\n.(6.18)\\nThiswouldindeeddeﬁneavalidconditionaldistribution,butwewouldnotbeable\\ntotrainitveryeﬀectivelywithgradientdescent.Anytimethatw\\ue03eh+ bstrayed\\noutsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto\\nitsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe\\nlearningalgorithmnolongerhasaguideforhowtoimprovethecorresponding\\nparameters.\\nInstead,itisbettertouseadiﬀerentapproachthatensuresthereisalwaysa\\nstronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased\\nonusingsigmoidoutputunitscombinedwithmaximumlikelihood.\\nAsigmoidoutputunitisdeﬁnedby\\nˆ y σ= \\ue010\\nw\\ue03eh+ b\\ue011\\n(6.19)\\n1 8 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c3fe90dc-7252-41cf-8de4-123009cd9454', embedding=None, metadata={'page_label': '198', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nwhereisthelogisticsigmoidfunctiondescribedinsection. σ 3.10\\nWecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it\\nusesalinearlayertocompute z=w\\ue03eh+ b.Next,itusesthesigmoidactivation\\nfunctiontoconvertintoaprobability. z\\nWeomitthedependenceonxforthemomenttodiscusshowtodeﬁnea\\nprobabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated\\nbyconstructinganunnormalized probabilitydistribution˜ P( y),whichdoesnot\\nsumto1.Wecanthendividebyanappropriateconstanttoobtainavalid\\nprobabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log\\nprobabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized\\nprobabilities. WethennormalizetoseethatthisyieldsaBernoullidistribution\\ncontrolledbyasigmoidaltransformationof: z\\nlog˜ P y y z () = (6.20)\\n˜ P y y z () = exp() (6.21)\\nP y() =exp() y z\\ue0501\\ny\\ue030= 0exp( y\\ue030z)(6.22)\\nP y σ y z . () = ((2−1)) (6.23)\\nProbabilitydistributionsbasedonexponentiationandnormalization arecommon\\nthroughoutthestatisticalmodelingliterature.The zvariabledeﬁningsucha\\ndistributionoverbinaryvariablesiscalleda.logit\\nThisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse\\nwithmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum\\nlikelihoodis−log P( y|x),theloginthecostfunctionundoestheexpofthe\\nsigmoid.Withoutthiseﬀect,thesaturationofthesigmoidcouldpreventgradient-\\nbased\\xa0learningfrom\\xa0makinggoodprogress.Theloss\\xa0functionfor\\xa0maximum\\nlikelihoodlearningofaBernoulliparametrized byasigmoidis\\nJ P y () = logθ − (|x) (6.24)\\n= log((2 1)) − σ y− z (6.25)\\n= ((12)) ζ − y z . (6.26)\\nThisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10\\nthelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen\\n(1−2 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready\\nhastherightanswer—when y= 1and zisverypositive,or y= 0and zisvery\\nnegative.When zhasthewrongsign,theargumenttothesoftplusfunction,\\n1 8 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35eab35b-a069-42bd-8a8e-cafa788f08d5', embedding=None, metadata={'page_label': '199', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n(1−2 y) z,maybesimpliﬁedto|| z.As|| zbecomeslargewhile zhasthewrongsign,\\nthesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The\\nderivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely\\nincorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty\\nisveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly\\ncorrectamistaken. z\\nWhenweuseotherlossfunctions,suchasmeansquarederror,thelosscan\\nsaturateanytime σ( z)saturates.Thesigmoidactivationfunctionsaturatesto0\\nwhen zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive.\\nThegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens,\\nwhetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason,\\nmaximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid\\noutputunits.\\nAnalytically,thelogarithmofthesigmoidisalwaysdeﬁnedandﬁnite,because\\nthesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing\\ntheentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations,\\ntoavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa\\nfunctionof z,ratherthanasafunctionofˆ y= σ( z).Ifthesigmoidfunction\\nunderﬂowstozero,thentakingthelogarithmofˆ yyieldsnegativeinﬁnity.\\n6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions\\nAnytimewewishtorepresentaprobabilitydistributionoveradiscretevariable\\nwith npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa\\ngeneralization ofthesigmoidfunctionwhichwasusedtorepresentaprobability\\ndistributionoverabinaryvariable.\\nSoftmaxfunctionsaremostoftenusedastheoutputofaclassiﬁer,torepresent\\ntheprobabilitydistributionover ndiﬀerentclasses.Morerarely,softmaxfunctions\\ncanbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof\\nndiﬀerentoptionsforsomeinternalvariable.\\nInthecaseofbinaryvariables,wewishedtoproduceasinglenumber\\nˆ y P y . = (= 1 )|x (6.27)\\nBecausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1\\nlogarithmofthenumbertobewell-behavedforgradient-basedoptimization of\\nthelog-likelihood,wechosetoinsteadpredictanumber z=log˜ P( y=1|x).\\nExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe\\nsigmoidfunction.\\n1 8 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cea87f96-87a0-458c-9348-609d85c4fe54', embedding=None, metadata={'page_label': '200', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nTogeneralizetothecaseofadiscretevariablewith nvalues,wenowneed\\ntoproduceavectorˆy,with ˆ y i= P( y= i|x).Werequirenotonlythateach\\nelementofˆ y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1\\nitrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor\\ntheBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear\\nlayerpredictsunnormalized logprobabilities:\\nzW= \\ue03ehb+ , (6.28)\\nwhere z i=log˜ P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand\\nnormalizetoobtainthedesired z ˆy.Formally,thesoftmaxfunctionisgivenby\\nsoftmax()z i=exp( z i)\\ue050\\njexp( z j). (6.29)\\nAswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen\\ntrainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In\\nthiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.Deﬁningthe\\nsoftmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo\\ntheofthesoftmax: exp\\nlogsoftmax()z i= z i−log\\ue058\\njexp( z j) . (6.30)\\nTheﬁrsttermofequationshowsthattheinput 6.30 z ialwayshasadirect\\ncontributiontothecostfunction.Becausethistermcannotsaturate,weknow\\nthatlearningcanproceed,evenifthecontributionof z itothesecondtermof\\nequationbecomesverysmall.Whenmaximizingthelog-likelihood,theﬁrst 6.30\\ntermencourages z itobepushedup,whilethesecondtermencouragesallofztobe\\npusheddown.Togainsomeintuitionforthesecondterm,log\\ue050\\njexp( z j),observe\\nthatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is\\nbasedontheideathatexp( z k) isinsigniﬁcantforany z kthatisnoticeablylessthan\\nmax j z j.Theintuitionwecangainfromthisapproximation isthatthenegative\\nlog-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect\\nprediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then\\nthe− z itermandthelog\\ue050\\njexp( z j)≈max j z j= z itermswillroughlycancel.\\nThisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe\\ndominatedbyotherexamplesthatarenotyetcorrectlyclassiﬁed.\\nSofarwehavediscussedonlyasingleexample.Overall,unregularized maximum\\nlikelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict\\n1 8 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2bd07fdb-e03f-4d34-a7b3-fc116bb995fa', embedding=None, metadata={'page_label': '201', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nthefractionofcountsofeachoutcomeobservedinthetrainingset:\\nsoftmax((;))zxθ i≈\\ue050m\\nj = 1 1y() j= i , x() j= x\\ue050m\\nj = 1 1x() j = x. (6.31)\\nBecausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen\\nsolongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In\\npractice,limitedmodelcapacityandimperfectoptimization willmeanthatthe\\nmodelisonlyabletoapproximatethesefractions.\\nManyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell\\nwiththesoftmaxfunction.Speciﬁcally,objectivefunctionsthatdonotusealogto\\nundotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes\\nverynegative,causingthegradienttovanish.Inparticular,squarederrorisa\\npoorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits\\noutput,evenwhenthemodelmakeshighlyconﬁdentincorrectpredictions(,Bridle\\n1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine\\nthesoftmaxfunctionitself.\\nLikethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas\\nasingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely\\npositive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These\\noutputvaluescansaturatewhenthediﬀerencesbetweeninputvaluesbecome\\nextreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax\\nalsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction.\\nToseethatthesoftmaxfunctionrespondstothediﬀerencebetweenitsinputs,\\nobservethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits\\ninputs:\\nsoftmax() = softmax(+) zz c . (6.32)\\nUsingthisproperty,wecanderiveanumericallystablevariantofthesoftmax:\\nsoftmax() = softmax( max zz−\\niz i) . (6.33)\\nThereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical\\nerrorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex-\\naminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven\\nbytheamountthatitsargumentsdeviatefrommax i z i.\\nAnoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1\\n( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput\\nsoftmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis\\nmuchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and\\n1 8 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='26c5c8ad-c345-438d-8ad4-bd760fafe503', embedding=None, metadata={'page_label': '202', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ncancausesimilardiﬃcultiesforlearningifthelossfunctionisnotdesignedto\\ncompensateforit.\\nTheargumentztothesoftmaxfunctioncanbeproducedintwodiﬀerentways.\\nThemostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput\\neveryelementofz,asdescribedaboveusingthelinearlayerz=W\\ue03eh+b.While\\nstraightforward,thisapproachactuallyoverparametrizes thedistribution.The\\nconstraintthatthe noutputsmustsumtomeansthatonly 1 n−1parametersare\\nnecessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe\\nﬁrst n−1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement\\nofzbeﬁxed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly\\nwhatthesigmoidunitdoes.Deﬁning P( y= 1|x) = σ( z)isequivalenttodeﬁning\\nP( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe n−1\\nargumentandthe nargumentapproachestothesoftmaxcandescribethesame\\nsetofprobabilitydistributions,buthavediﬀerentlearningdynamics.Inpractice,\\nthereisrarelymuchdiﬀerencebetweenusingtheoverparametrized versionorthe\\nrestrictedversion,anditissimplertoimplementtheoverparametrized version.\\nFromaneuroscientiﬁcpointofview,itisinterestingtothinkofthesoftmaxas\\nawaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the\\nsoftmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily\\ncorrespondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral\\ninhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe\\nextreme(whenthediﬀerencebetweenthemaximal a iandtheothersislargein\\nmagnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1\\nandtheothersarenearly0).\\nThename“softmax”canbesomewhatconfusing.Thefunctionismoreclosely\\nrelatedtotheargmaxfunctionthanthemaxfunction.\\xa0Theterm“soft”derives\\nfromthefactthatthesoftmaxfunctioniscontinuousanddiﬀerentiable. The\\nargmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous\\nordiﬀerentiable. Thesoftmaxfunctionthusprovidesa“softened”versionofthe\\nargmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)\\ue03ez.\\nItwouldperhapsbebettertocallthesoftmaxfunction“softargmax,”\\xa0butthe\\ncurrentnameisanentrenchedconvention.\\n6.2.2.4OtherOutputTypes\\nThelinear,\\xa0sigmoid,\\xa0andsoftmaxoutputunitsdescribedabovearethemost\\ncommon.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat\\nwewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign\\n1 8 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b94d8f3a-0b3f-4747-8b6b-c614d297425d', embedding=None, metadata={'page_label': '203', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nagoodcostfunctionfornearlyanykindofoutputlayer.\\nIngeneral,ifwedeﬁneaconditionaldistribution p(yx|;θ),theprincipleof\\nmaximumlikelihoodsuggestsweuse asourcostfunction. − | log( pyxθ;)\\nIngeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;θ).\\nTheoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead,\\nf(x;θ) =ωprovidestheparametersforadistributionover y.Ourlossfunction\\ncanthenbeinterpretedas . −log(;()) p yωx\\nForexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y,\\ngiven x.Inthesimplecase,wherethevariance σ2isaconstant,thereisaclosed\\nformexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe\\nempiricalmeanofthesquareddiﬀerencebetweenobservations yandtheirexpected\\nvalue.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting\\nspecial-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe\\ndistribution p( y|x)thatiscontrolledbyω= f(x;θ).Thenegativelog-likelihood\\n−log p(y;ω(x))willthenprovideacostfunctionwiththeappropriateterms\\nnecessarytomakeouroptimization procedureincrementally learnthevariance.In\\nthesimplecasewherethestandarddeviationdoesnotdependontheinput,we\\ncanmakeanewparameterinthenetworkthatiscopieddirectlyintoω.Thisnew\\nparametermightbe σitselforcouldbeaparameter vrepresenting σ2oritcould\\nbeaparameter βrepresenting1\\nσ2,dependingonhowwechoosetoparametrize\\nthedistribution.Wemaywishourmodeltopredictadiﬀerentamountofvariance\\nin yfordiﬀerentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe\\nheteroscedasticcase,wesimplymakethespeciﬁcationofthevariancebeoneof\\nthevaluesoutputby f( x;θ).AtypicalwaytodothisistoformulatetheGaussian\\ndistributionusingprecision,ratherthanvariance,asdescribedinequation.3.22\\nInthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix\\ndiag (6.34) ()β .\\nThisformulationworkswellwithgradientdescentbecausetheformulaforthe\\nlog-likelihoodoftheGaussiandistributionparametrized byβinvolvesonlymul-\\ntiplicationby β iandadditionoflogβ i.Thegradientofmultiplication, addition,\\nandlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the\\noutputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction\\nbecomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning,\\narbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the\\noutputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision,\\nandwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation\\ncanvanishnearzero,makingitdiﬃculttolearnparametersthataresquared.\\n1 8 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d30c7e57-a109-437e-8419-3b3bfa46a022', embedding=None, metadata={'page_label': '204', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nRegardlessofwhetherweusestandarddeviation,variance,orprecision,wemust\\nensurethatthecovariancematrixoftheGaussianispositivedeﬁnite.\\xa0Because\\ntheeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof\\nthecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis\\npositivedeﬁnite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix,\\nthentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity.\\nIfwesupposethataistherawactivationofthemodelusedtodeterminethe\\ndiagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision\\nvector:β= ζ(a) .Thissamestrategyappliesequallyifusingvarianceorstandard\\ndeviationratherthanprecisionorifusingascalartimesidentityratherthan\\ndiagonalmatrix.\\nItisraretolearnacovarianceorprecisionmatrixwithricherstructurethan\\ndiagonal.\\xa0Ifthecovarianceisfullandconditional,thenaparametrization must\\nbechosenthatguaranteespositive-deﬁnitenessofthepredictedcovariancematrix.\\nThiscanbeachievedbywriting Σ() = ()xBxB\\ue03e()x,whereBisanunconstrained\\nsquarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe\\nlikelihoodisexpensive,witha d d×matrixrequiring O( d3)computationforthe\\ndeterminantandinverseof Σ(x)(orequivalently,andmorecommonlydone,its\\neigendecompositionorthatof).Bx()\\nWeoftenwanttoperformmultimodalregression,thatis,topredictrealvalues\\nthatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldiﬀerent\\npeaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis\\nanaturalrepresentationfortheoutput( ,;,). Jacobs e t a l .1991Bishop1994\\nNeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture\\ndensitynetworks.AGaussianmixtureoutputwith ncomponentsisdeﬁnedby\\ntheconditionalprobabilitydistribution\\np( ) =yx|n\\ue058\\ni = 1p i (= c |Nx)(;yµ( ) i()x , Σ( ) i())x .(6.35)\\nTheneuralnetworkmusthavethreeoutputs:avectordeﬁning p(c= i|x),a\\nmatrixprovidingµ( ) i(x)forall i,andatensorproviding Σ( ) i(x)forall i.These\\noutputsmustsatisfydiﬀerentconstraints:\\n1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution\\noverthe ndiﬀerentcomponentsassociatedwithlatentvariable1c,andcan\\n1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t\\ny , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t\\nw e c a n i m a g i n e t h a t y w a s g e n e ra t e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a\\nra n d o m v a ria b l e .\\n1 8 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19e90f36-8d82-4394-aa20-8407b5c246f7', embedding=None, metadata={'page_label': '205', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ntypicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee\\nthattheseoutputsarepositiveandsumto1.\\n2.Meansµ( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th\\nGaussiancomponent,andareunconstrained(typicallywithnononlinearity\\natallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput\\nan n d×matrixcontainingall nofthese d-dimensionalvectors.\\xa0Learning\\nthesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan\\nlearningthemeansofadistributionwithonlyoneoutputmode.Weonly\\nwanttoupdatethemeanforthecomponentthatactuallyproducedthe\\nobservation.Inpractice,wedonotknowwhichcomponentproducedeach\\nobservation.Theexpressionforthenegativelog-likelihoodnaturallyweights\\neachexample’scontributiontothelossforeachcomponentbytheprobability\\nthatthecomponentproducedtheexample.\\n3.Covariances Σ( ) i(x):thesespecifythecovariancematrixforeachcomponent\\ni.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal\\nmatrixtoavoidneedingtocomputedeterminants. Aswithlearningthemeans\\nofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign\\npartialresponsibilityforeachpointtoeachmixturecomponent.Gradient\\ndescentwillautomatically followthecorrectprocessifgiventhecorrect\\nspeciﬁcationofthenegativelog-likelihoodunderthemixturemodel.\\nIthasbeenreportedthatgradient-basedoptimization ofconditionalGaussian\\nmixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone\\ngetsdivisions(bythevariance)whichcanbenumericallyunstable(whensome\\nvariancegetstobesmallforaparticularexample,yieldingverylargegradients).\\nOnesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1\\nthegradientsheuristically( ,). MurrayandLarochelle2014\\nGaussianmixtureoutputsareparticularlyeﬀectiveingenerativemodelsof\\nspeech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The\\nmixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput\\nmodesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining\\nahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture\\ndensitynetworkisshowninﬁgure.6.4\\nIngeneral,wemaywishtocontinuetomodellargervectorsycontainingmore\\nvariables,andtoimposericherandricherstructuresontheseoutputvariables.For\\nexample,wemaywishforourneuralnetworktooutputasequenceofcharacters\\nthatformsasentence.Inthese\\xa0cases,wemaycontinuetousetheprinciple\\nofmaximumlikelihoodappliedtoourmodel p(y;ω(x)),butthemodelweuse\\n1 9 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ac7801e-9df4-4d25-a230-bfa6e5c16355', embedding=None, metadata={'page_label': '206', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nxy\\nFigure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer.\\nTheinput xissampledfromauniformdistributionandtheoutput yissampledfrom\\np m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto\\ntheparametersoftheoutputdistribution.Theseparametersincludetheprobabilities\\ngoverningwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe\\nparametersforeachmixturecomponent.EachmixturecomponentisGaussianwith\\npredictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto\\nvarywithrespecttotheinput,andtodosoinnonlinearways. x\\ntodescribeybecomescomplexenoughtobebeyondthescopeofthischapter.\\nChapterdescribeshowtouserecurrentneuralnetworkstodeﬁnesuchmodels 10\\noversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III\\nprobabilitydistributions.\\n6. 3 Hi d d en Un i t s\\nSofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat\\narecommontomostparametricmachinelearningmodelstrainedwithgradient-\\nbasedoptimization. Nowweturntoanissuethatisuniquetofeedforwardneural\\nnetworks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe\\nmodel.\\nThedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot\\nyethavemanydeﬁnitiveguidingtheoreticalprinciples.\\nRectiﬁedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother\\ntypesofhiddenunitsareavailable.Itcanbediﬃculttodeterminewhentouse\\nwhichkind(thoughrectiﬁedlinearunitsareusuallyanacceptablechoice).\\xa0We\\n1 9 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='33140aa6-2c89-4aec-97a5-055be1962ed6', embedding=None, metadata={'page_label': '207', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ndescribeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits.\\nTheseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually\\nimpossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists\\noftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen\\ntraininganetworkwiththatkindofhiddenunitandevaluatingitsperformance\\nonavalidationset.\\nSomeofthehiddenunitsincludedinthislistarenotactuallydiﬀerentiableat\\nallinputpoints.Forexample,therectiﬁedlinearfunction g( z) =max{0 , z}isnot\\ndiﬀerentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient-\\nbasedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough\\nforthesemodelstobeusedformachinelearningtasks.\\xa0Thisisinpartbecause\\nneuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof\\nthecostfunction,butinsteadmerelyreduceitsvaluesigniﬁcantly,asshownin\\nﬁgure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8\\nexpecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable\\nfortheminimaofthecostfunctiontocorrespondtopointswithundeﬁnedgradient.\\nHiddenunitsthatarenotdiﬀerentiableareusuallynon-diﬀerentiable atonlya\\nsmallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativedeﬁned\\nbytheslopeofthefunctionimmediately totheleftof zandarightderivative\\ndeﬁnedbytheslopeofthefunctionimmediately totherightof z.Afunction\\nisdiﬀerentiableat zonlyifboththeleftderivativeandtherightderivativeare\\ndeﬁnedandequaltoeachother.Thefunctionsusedinthecontextofneural\\nnetworksusuallyhavedeﬁnedleftderivativesanddeﬁnedrightderivatives.Inthe\\ncaseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative\\nis.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1\\ntheone-sidedderivativesratherthanreportingthatthederivativeisundeﬁnedor\\nraisinganerror.\\xa0Thismaybeheuristicallyjustiﬁedbyobservingthatgradient-\\nbasedoptimization onadigitalcomputerissubjecttonumericalerroranyway.\\nWhenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying\\nvaluetrulywas.Instead,itwaslikelytobesomesmallvalue 0 \\ue00fthatwasrounded\\nto.Insomecontexts,moretheoreticallypleasingjustiﬁcationsareavailable,but 0\\ntheseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat\\ninpracticeonecansafelydisregardthenon-diﬀerentiabilityofthehiddenunit\\nactivationfunctionsdescribedbelow.\\nUnlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting\\navectorofinputsx,computinganaﬃnetransformationz=W\\ue03ex+b,and\\nthenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare\\ndistinguishedfromeachotheronlybythechoiceoftheformoftheactivation\\nfunction. g()z\\n1 9 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a71562fd-0503-4570-8c2a-e5a011b6ac57', embedding=None, metadata={'page_label': '208', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6.3.1RectiﬁedLinearUnitsandTheirGeneralizations\\nRectiﬁedlinearunitsusetheactivationfunction . g z , z () = max0{}\\nRectiﬁedlinearunitsareeasytooptimizebecausetheyaresosimilartolinear\\nunits.Theonlydiﬀerencebetweenalinearunitandarectiﬁedlinearunitis\\nthatarectiﬁedlinearunitoutputszeroacrosshalfitsdomain.\\xa0This makesthe\\nderivativesthrougharectiﬁedlinearunitremainlargewhenevertheunitisactive.\\nThegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe\\nrectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0\\noperationiseverywherethattheunitisactive.Thismeansthatthegradient 1\\ndirectionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions\\nthatintroducesecond-ordereﬀects.\\nRectiﬁedlinearunitsaretypicallyusedontopofanaﬃnetransformation:\\nhW= ( g\\ue03exb+) . (6.36)\\nWheninitializingtheparametersoftheaﬃnetransformation,itcanbeagood\\npracticetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes\\nitverylikelythattherectiﬁedlinearunitswillbeinitiallyactiveformostinputs\\ninthetrainingsetandallowthederivativestopassthrough.\\nSeveralgeneralizations ofrectiﬁedlinearunitsexist.Mostofthesegeneral-\\nizationsperformcomparablytorectiﬁedlinearunitsandoccasionallyperform\\nbetter.\\nOnedrawbacktorectiﬁedlinearunitsisthattheycannotlearnviagradient-\\nbased\\xa0methods\\xa0onexamples\\xa0for\\xa0which\\xa0their\\xa0activ ation\\xa0iszero.Avariety\\xa0of\\ngeneralizations ofrectiﬁedlinearunitsguaranteethattheyreceivegradientevery-\\nwhere.\\nThreegeneralizations ofrectiﬁedlinearunitsarebasedonusinganon-zero\\nslope α iwhen z i <0: h i= g(zα ,) i=max(0 , z i)+ α imin(0 , z i).Absolutevalue\\nrectiﬁcationﬁxes α i=−1toobtain g( z) =|| z.Itisusedforobjectrecognition\\nfromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009\\ninvariantunderapolarityreversaloftheinputillumination. Othergeneralizations\\nofrectiﬁedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l .\\n2013)ﬁxes α itoasmallvaluelike0.01whileaparametricReLUorPReLU\\ntreats α iasalearnableparameter(,). He e t a l .2015\\nMaxoutunits( ,)generalizerectiﬁedlinearunits Goodfellow e t a l .2013a\\nfurther.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez\\nintogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof\\n1 9 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a9797cd-4114-4a71-86b4-d71c5009055e', embedding=None, metadata={'page_label': '209', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\noneofthesegroups:\\ng()z i=max\\nj∈ G() iz j (6.37)\\nwhere G( ) iisthesetofindicesintotheinputsforgroup i,{( i−1) k+1 , . . . , i k}.\\nThisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple\\ndirectionsintheinputspace.x\\nAmaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces.\\nMaxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather\\nthanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan\\nlearntoapproximateanyconvexfunctionwitharbitraryﬁdelity.Inparticular,\\namaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe\\ninputxasatraditionallayerusingtherectiﬁedlinearactivationfunction,absolute\\nvaluerectiﬁcationfunction,ortheleakyorparametricReLU,orcanlearnto\\nimplementatotallydiﬀerentfunctionaltogether.Themaxoutlayerwillofcourse\\nbeparametrized diﬀerentlyfromanyoftheseotherlayertypes,sothelearning\\ndynamicswillbediﬀerenteveninthecaseswheremaxoutlearnstoimplementthe\\nsamefunctionofasoneoftheotherlayertypes. x\\nEachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone,\\nsomaxoutunitstypicallyneedmoreregularizationthanrectiﬁedlinearunits.They\\ncanworkwellwithoutregularizationifthetrainingsetislargeandthenumberof\\npiecesperunitiskeptlow(,). Cai e t a l .2013\\nMaxoutunitshaveafewotherbeneﬁts.Insomecases,onecangainsomesta-\\ntisticalandcomputational advantagesbyrequiringfewerparameters.Speciﬁcally,\\nifthefeaturescapturedby ndiﬀerentlinearﬁlterscanbesummarizedwithout\\nlosinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext\\nlayercangetbywithtimesfewerweights. k\\nBecauseeachunitisdrivenbymultipleﬁlters,maxoutunitshavesomeredun-\\ndancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting\\ninwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin\\nthepast( ,). Goodfellow e t a l .2014a\\nRectiﬁedlinearunitsandallofthesegeneralizations ofthemarebasedonthe\\nprinciplethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear.\\nThissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization\\nalsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan\\nlearnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining\\nthem,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch\\neasierwhensomelinearcomputations (withsomedirectionalderivativesbeingof\\nmagnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork\\n1 9 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c508693-0267-4243-8bb2-f36a4f4e11cf', embedding=None, metadata={'page_label': '210', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\narchitectures,theLSTM,propagatesinformationthroughtimeviasummation—a\\nparticularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther\\ninsection.10.10\\n6.3.2LogisticSigmoidandHyperbolicTangent\\nPriortotheintroduction ofrectiﬁedlinearunits,mostneuralnetworksusedthe\\nlogisticsigmoidactivationfunction\\ng z σ z () = () (6.38)\\northehyperbolictangentactivationfunction\\ng z z . () = tanh( ) (6.39)\\nTheseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 z σ z−\\nWe\\xa0havealready\\xa0seen sigmoid\\xa0unitsasoutput\\xa0units,\\xa0usedto\\xa0predictthe\\nprobabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1\\nunitssaturateacrossmostoftheirdomain—they saturatetoahighvaluewhen\\nzisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly\\nstronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof\\nsigmoidalunitscanmakegradient-basedlearningverydiﬃcult.Forthisreason,\\ntheiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse\\nasoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan\\nappropriatecostfunctioncanundothesaturationofthesigmoidintheoutput\\nlayer.\\nWhenasigmoidalactivationfunctionmustbeused,thehyperbolictangent\\nactivationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles\\ntheidentityfunctionmoreclosely,inthesensethattanh(0) = 0while σ(0) =1\\n2.\\nBecausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0\\nnetworkˆ y=w\\ue03etanh(U\\ue03etanh(V\\ue03ex))resemblestrainingalinearmodelˆ y=\\nw\\ue03eU\\ue03eV\\ue03exsolongastheactivationsofthenetworkcanbekeptsmall.This\\nmakestrainingthenetworkeasier. tanh\\nSigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed-\\nforwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome\\nautoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise\\nlinearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe\\ndrawbacksofsaturation.\\n1 9 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='799c19ce-13ee-4b19-b697-0fc757eb43e1', embedding=None, metadata={'page_label': '211', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6.3.3OtherHiddenUnits\\nManyothertypesofhiddenunitsarepossible,butareusedlessfrequently.\\nIngeneral,awidevarietyofdiﬀerentiable functionsperformperfectlywell.\\nManyunpublishedactivationfunctionsperformjustaswellasthepopularones.\\nToprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing\\nh=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan\\n1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation\\nfunctions.Duringresearchanddevelopmentofnewtechniques,itiscommon\\ntotestmanydiﬀerentactivationfunctionsandﬁndthatseveralvariationson\\nstandardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit\\ntypesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniﬁcant\\nimprovement.Newhiddenunittypesthatperformroughlycomparablytoknown\\ntypesaresocommonastobeuninteresting.\\nItwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared\\nintheliterature.Wehighlightafewespeciallyusefulanddistinctiveones.\\nOnepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof\\nthisasusingtheidentityfunctionastheactivationfunction.Wehavealready\\nseenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay\\nalsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly\\nlineartransformations,thenthenetworkasawholewillbelinear.However,it\\nisacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider\\naneuralnetworklayerwith ninputsand poutputs,h= g(W\\ue03ex+b).Wemay\\nreplacethiswithtwolayers,withonelayerusingweightmatrixUandtheother\\nusingweightmatrixV.Iftheﬁrstlayerhasnoactivationfunction,thenwehave\\nessentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The\\nfactoredapproachistocomputeh= g(V\\ue03eU\\ue03ex+b).IfUproduces qoutputs,\\nthenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p\\nparameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It\\ncomesatthecostofconstrainingthelineartransformationtobelow-rank,but\\ntheselow-rankrelationshipsareoftensuﬃcient.Linearhiddenunitsthusoﬀeran\\neﬀectivewayofreducingthenumberofparametersinanetwork.\\nSoftmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as\\ndescribedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3\\nunitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k\\npossiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden\\nunitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto\\nmanipulatememory,describedinsection.10.12\\n1 9 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a925bd83-4d2c-4d0c-b306-faae8069432e', embedding=None, metadata={'page_label': '212', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAfewotherreasonablycommonhiddenunittypesinclude:\\n•RadialbasisfunctionorRBFunit: h i=exp\\ue010\\n−1\\nσ2\\ni||W : , i−||x2\\ue011\\n.This\\nfunctionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit\\nsaturatestoformost,itcanbediﬃculttooptimize. 0x\\n•Softplus: g( a) = ζ( a) =log(1+ ea).Thisisasmoothversionoftherectiﬁer,\\nintroducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair\\nandHinton2010()fortheconditionaldistributionsofundirectedprobabilistic\\nmodels. ()comparedthesoftplusandrectiﬁerandfound Glorot e t a l .2011a\\nbetterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged.\\nThesoftplusdemonstratesthattheperformanceofhiddenunittypescan\\nbeverycounterintuitive—onemightexpectittohaveanadvantageover\\ntherectiﬁerduetobeingdiﬀerentiableeverywhereorduetosaturatingless\\ncompletely,butempiricallyitdoesnot.\\n•Hardtanh:thisisshapedsimilarlytothetanhandtherectiﬁerbutunlike\\nthelatter,itisbounded, g( a)=max(−1 ,min(1 , a)).Itwasintroduced\\nby(). Collobert2004\\nHiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden\\nunittypesremaintobediscovered.\\n6. 4 A rc h i t ec t u re D es i gn\\nAnotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture.\\nThewordarchitecturereferstotheoverallstructureofthenetwork:howmany\\nunitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother.\\nMostneuralnetworksareorganizedintogroupsofunitscalledlayers.\\xa0Most\\nneuralnetworkarchitectures arrangetheselayersinachainstructure,witheach\\nlayerbeingafunctionofthelayerthatprecededit.Inthisstructure,theﬁrstlayer\\nisgivenby\\nh( 1 )= g( 1 )\\ue010\\nW( 1 )\\ue03exb+( 1 )\\ue011\\n, (6.40)\\nthesecondlayerisgivenby\\nh( 2 )= g( 2 )\\ue010\\nW( 2 )\\ue03eh( 1 )+b( 2 )\\ue011\\n, (6.41)\\nandsoon.\\n1 9 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b055aad-22b9-4b0d-a85e-7a87045f6ea8', embedding=None, metadata={'page_label': '213', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nInthesechain-basedarchitectures,themainarchitecturalconsiderationsare\\ntochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee,\\nanetworkwithevenonehiddenlayerissuﬃcienttoﬁtthetrainingset.Deeper\\nnetworksoftenareabletousefarfewerunitsperlayerandfarfewerparameters\\nandoftengeneralizetothetestset,butarealsooftenhardertooptimize.\\xa0The\\nidealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby\\nmonitoringthevalidationseterror.\\n6.4.1UniversalApproximationPropertiesandDepth\\nAlinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can\\nbydeﬁnitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto\\ntrainbecausemanylossfunctionsresultinconvexoptimization problemswhen\\nappliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions.\\nAtﬁrstglance,wemightpresumethatlearninganonlinearfunctionrequires\\ndesigningaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn.\\nFortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi-\\nmationframework.Speciﬁcally,theuniversalapproximationtheorem(Hornik\\ne t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989\\nlayerandatleastonehiddenlayerwithany“squashing”activationfunction(such\\nasthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable\\nfunctionfromoneﬁnite-dimensional spacetoanotherwithanydesirednon-zero\\namountoferror,providedthatthenetworkisgivenenoughhiddenunits.The\\nderivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe\\nfunctionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990\\nisbeyondthescopeofthisbook;\\xa0forourpurposesitsuﬃcestosaythatany\\ncontinuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable\\nandthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay\\nalsoapproximateanyfunctionmappingfromanyﬁnitedimensionaldiscretespace\\ntoanother.Whiletheoriginaltheoremswereﬁrststatedintermsofunitswith\\nactivationfunctionsthatsaturatebothforverynegativeandforverypositive\\narguments,universalapproximation theoremshavealsobeenprovedforawider\\nclassofactivationfunctions,whichincludesthenowcommonlyusedrectiﬁedlinear\\nunit( ,). Leshno e t a l .1993\\nTheuniversalapproximationtheoremmeansthatregardlessofwhatfunction\\nwearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis\\nfunction.However,wearenotguaranteedthatthetrainingalgorithmwillbeable\\nto l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning\\ncanfailfortwodiﬀerentreasons.First,theoptimizationalgorithmusedfortraining\\n1 9 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='27a28a92-6560-4a60-92e3-e1f39103886f', embedding=None, metadata={'page_label': '214', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nmaynotbeabletoﬁndthevalueoftheparametersthatcorrespondstothedesired\\nfunction.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto\\noverﬁtting.Recallfromsectionthatthe“nofreelunch”theoremshowsthat 5.2.1\\nthereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks\\nprovideauniversalsystemforrepresentingfunctions,inthesensethat,givena\\nfunction,thereexistsafeedforwardnetworkthatapproximatesthefunction.There\\nisnouniversalprocedureforexaminingatrainingsetofspeciﬁcexamplesand\\nchoosingafunctionthatwillgeneralizetopointsnotinthetrainingset.\\nTheuniversalapproximationtheoremsaysthatthereexistsanetworklarge\\nenoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot\\nsayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993\\nsizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions.\\nUnfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly\\nwithonehiddenunitcorrespondingtoeachinputconﬁgurationthatneedstobe\\ndistinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the\\nnumberofpossiblebinaryfunctionsonvectorsv∈{0 ,1}nis22nandselecting\\nonesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof\\nfreedom.\\nInsummary,afeedforwardnetworkwithasinglelayerissuﬃcienttorepresent\\nanyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand\\ngeneralizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe\\nnumberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe\\namountofgeneralization error.\\nThereexistfamiliesoffunctionswhichcanbeapproximated eﬃcientlybyan\\narchitecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger\\nmodelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber\\nofhiddenunitsrequiredbytheshallowmodelisexponentialin n.\\xa0Suchresults\\nwereﬁrstprovedformodelsthatdonotresemblethecontinuous,diﬀerentiable\\nneuralnetworksusedformachinelearning,buthavesincebeenextendedtothese\\nmodels.Theﬁrstresultswereforcircuitsoflogicgates(,).Later Håstad1986\\nworkextendedtheseresultstolinearthresholdunitswithnon-negativeweights\\n( ,; ,),andthentonetworkswith HåstadandGoldmann1991Hajnal e t a l .1993\\ncontinuous-valuedactivations(,; ,).\\xa0Manymodern Maass1992Maass e t a l .1994\\nneuralnetworksuserectiﬁedlinearunits. ()demonstrated Leshno e t a l .1993\\nthatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions,\\nincludingrectiﬁedlinearunits,haveuniversalapproximation properties,butthese\\nresultsdonotaddressthequestionsofdepthoreﬃciency—theyspecifyonlythat\\nasuﬃcientlywiderectiﬁernetworkcouldrepresentanyfunction.Montufar e t a l .\\n1 9 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='78a8e840-86f4-4909-b648-4ec41a262fd1', embedding=None, metadata={'page_label': '215', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n()showedthatfunctionsrepresentablewithadeeprectiﬁernetcanrequire 2014\\nanexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network.\\nMoreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained\\nfromrectiﬁernonlinearities ormaxoutunits)canrepresentfunctionswithanumber\\nofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5\\nanetworkwithabsolutevaluerectiﬁcationcreatesmirrorimagesofthefunction\\ncomputedontopofsomehiddenunit,withrespecttotheinputofthathidden\\nunit.Eachhiddenunitspeciﬁeswheretofoldtheinputspaceinordertocreate\\nmirrorresponses(onbothsidesoftheabsolutevaluenonlinearity). Bycomposing\\nthesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise\\nlinearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns.\\nFigure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper\\nrectiﬁernetworksformallyby (). Montufar e t a l .2014 ( L e f t )Anabsolutevaluerectiﬁcation\\nunithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis\\nofsymmetryisgivenbythehyperplanedeﬁnedbytheweightsandbiasoftheunit.A\\nfunctioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage\\nofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r )\\nbyfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t )\\nbefoldedontopoftheﬁrst(byanotherdownstreamunit)toobtainanothersymmetry\\n(whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith\\npermissionfrom (). Montufar e t a l .2014\\nMoreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014\\nnumberoflinearregionscarvedoutbyadeeprectiﬁernetworkwith dinputs,\\ndepth,andunitsperhiddenlayer,is l n\\nO\\ue020\\ue012n\\nd\\ue013d l (− 1 )\\nnd\\ue021\\n, (6.42)\\ni.e.,exponentialinthedepth.Inthecaseofmaxoutnetworkswithﬁltersper l k\\nunit,thenumberoflinearregionsis\\nO\\ue010\\nk( 1 ) + l− d\\ue011\\n. (6.43)\\n2 0 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='185f58af-f0da-47ec-82ba-7badba650926', embedding=None, metadata={'page_label': '216', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nOfcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin\\napplicationsofmachinelearning(andinparticularforAI)sharesuchaproperty.\\nWemayalsowanttochooseadeepmodelforstatisticalreasons.\\xa0Anytime\\nwechooseaspeciﬁcmachinelearningalgorithm,weareimplicitlystatingsome\\nsetofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould\\nlearn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe\\nwanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe\\ninterpretedfromarepresentationlearningpointofviewassayingthatwebelieve\\nthelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation\\nthatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof\\nvariation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing\\nabeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof\\nmultiplesteps,whereeachstepmakesuseofthepreviousstep’soutput.\\xa0These\\nintermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe\\nanalogoustocountersorpointersthatthenetworkusestoorganizeitsinternal\\nprocessing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization\\nforawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009\\nMesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l .,\\n2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,;\\ne t a l . e t a l . ,;2014dSzegedy ,).Seeﬁgureandﬁgureforexamplesof 2014a 6.6 6.7\\nsomeoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes\\nindeedexpressausefulprioroverthespaceoffunctionsthemodellearns.\\n6.4.2OtherArchitecturalConsiderations\\nSofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe\\nmainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer.\\nInpractice,neuralnetworksshowconsiderablymorediversity.\\nManyneuralnetworkarchitectures havebeendevelopedforspeciﬁctasks.\\nSpecializedarchitecturesforcomputervisioncalledconvolutionalnetworksare\\ndescribedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9\\nrecurrentneuralnetworksforsequenceprocessing,describedinchapter,which10\\nhavetheirownarchitecturalconsiderations.\\nIngeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe\\nmostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra\\narchitecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer\\ni+2orhigher.Theseskipconnectionsmakeiteasierforthegradienttoﬂowfrom\\noutputlayerstolayersnearertheinput.\\n2 0 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cf6faa8-b49a-4648-92c9-9865f1ac5275', embedding=None, metadata={'page_label': '217', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n3 4 5 6 7 8 9 1 0 1 1\\nN u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t )\\nFigure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused\\ntotranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow\\ne t a l .().\\xa0Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth.\\xa0See 2014d\\nﬁgureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7\\ndonotyieldthesameeﬀect.\\nAnotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta\\npairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear\\ntransformationviaamatrixW,everyinputunitisconnectedtoeveryoutput\\nunit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so\\nthateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin\\ntheoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce\\nthenumberofparametersandtheamountofcomputationrequiredtoevaluate\\nthenetwork,butareoftenhighlyproblem-dependent. Forexample,convolutional\\nnetworks,describedinchapter,usespecializedpatternsofsparseconnections 9\\nthatareveryeﬀectiveforcomputervisionproblems.Inthischapter,itisdiﬃcult\\ntogivemuchmorespeciﬁcadviceconcerningthearchitectureofagenericneural\\nnetwork.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat\\nhavebeenfoundtoworkwellfordiﬀerentapplicationdomains.\\n2 0 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e325d2dc-cf0e-4b29-837a-d24aa88fa8ec', embedding=None, metadata={'page_label': '218', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n0 0 0 2 0 4 0 6 0 8 1 0 . . . . . .\\nN u m b e r o f p a r a m e t e r s × 1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional\\n3,fullyconnected\\n11,convolutional\\nFigure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis\\nlarger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber\\nofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot\\nnearlyaseﬀectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof\\nnetworkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof\\ntheconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis\\ncontextoverﬁtataround20millionparameterswhiledeeponescanbeneﬁtfromhaving\\nover60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover\\nthespaceoffunctionsthemodelcanlearn.Speciﬁcally,itexpressesabeliefthatthe\\nfunctionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult\\neitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g.,\\ncornersdeﬁnedintermsofedges)orinlearningaprogramwithsequentiallydependent\\nsteps(e.g.,ﬁrstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize\\nthem).\\n2 0 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='55fc28ff-58d7-4a2d-a492-39ab7ea24b69', embedding=None, metadata={'page_label': '219', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6. 5 Bac k - Prop a g a t i o n an d O t h er D i ﬀ eren t i at i on A l go-\\nri t h m s\\nWhenweuseafeedforwardneuralnetworktoacceptaninputxandproducean\\noutput ˆy,informationﬂowsforwardthroughthenetwork.Theinputsxprovide\\ntheinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer\\nandﬁnallyproduces ˆy.Thisiscalledforwardpropagation.Duringtraining,\\nforwardpropagationcancontinueonwarduntilitproducesascalarcost J(θ).\\nTheback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a\\nbackprop,allowstheinformationfromthecosttothenﬂowbackwardsthrough\\nthenetwork,inordertocomputethegradient.\\nComputingananalyticalexpressionforthegradientisstraightforward,but\\nnumericallyevaluatingsuchanexpressioncanbecomputationally expensive.The\\nback-propagationalgorithmdoessousingasimpleandinexpensiveprocedure.\\nThetermback-propagation isoften\\xa0misunders toodasmeaningthewhole\\nlearningalgorithmformulti-layerneuralnetworks.Actually,back-propagation\\nrefersonlytothemethodforcomputingthegradient,whileanotheralgorithm,\\nsuchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient.\\nFurthermore,back-propagation isoftenmisunderstoodasbeingspeciﬁctomulti-\\nlayerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction\\n(forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe\\nfunctionisundeﬁned).Speciﬁcally,wewilldescribehowtocomputethegradient\\n∇ x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives\\naredesired,andyisanadditionalsetofvariablesthatareinputstothefunction\\nbutwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost\\noftenrequireisthegradientofthecostfunctionwithrespecttotheparameters,\\n∇ θ J(θ).Manymachinelearningtasksinvolvecomputingotherderivatives,either\\naspartof\\xa0thelearning\\xa0process,\\xa0or\\xa0to analyzethelearned\\xa0model. The\\xa0back-\\npropagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted\\ntocomputingthegradientofthecostfunctionwithrespecttotheparameters.The\\nideaofcomputingderivativesbypropagatinginformationthroughanetworkis\\nverygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction\\nfwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly\\nusedcasewherehasasingleoutput. f\\n2 0 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5dd88e1a-3205-499a-97cf-1546e48c7028', embedding=None, metadata={'page_label': '220', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6.5.1ComputationalGraphs\\nSofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage.\\nTodescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea\\nmoreprecise language. computationalgraph\\nManywaysofformalizingcomputationasgraphsarepossible.\\nHere,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay\\nbeascalar,vector,matrix,tensor,orevenavariableofanothertype.\\nToformalizeourgraphs,wealsoneedtointroducetheideaofanoperation.\\nAnoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage\\nisaccompanied byasetofallowableoperations.Functionsmorecomplicated\\nthantheoperationsinthissetmaybedescribedbycomposingmanyoperations\\ntogether.\\nWithoutlossofgenerality,\\xa0wedeﬁneanoperationtoreturnonlyasingle\\noutputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave\\nmultipleentries,suchasavector.Softwareimplementationsofback-propagation\\nusuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour\\ndescriptionbecauseitintroducesmanyextradetailsthatarenotimportantto\\nconceptualunderstanding.\\nIfavariable yiscomputedbyapplyinganoperationtoavariable x,then\\nwedrawadirectededgefrom xto y.\\xa0Wesometimesannotatetheoutputnode\\nwiththenameoftheoperationapplied,andothertimesomitthislabelwhenthe\\noperationisclearfromcontext.\\nExamplesofcomputational graphsareshowninﬁgure.6.8\\n6.5.2ChainRuleofCalculus\\nThechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is\\nusedtocomputethederivativesoffunctionsformedbycomposingotherfunctions\\nwhosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe\\nchainrule,withaspeciﬁcorderofoperationsthatishighlyeﬃcient.\\nLet xbearealnumber,andlet fand gbothbefunctionsmappingfromareal\\nnumbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then\\nthechainrulestatesthatd z\\nd x=d z\\nd yd y\\nd x. (6.44)\\nWecangeneralizethisbeyondthescalarcase.Supposethatx∈ Rm,y∈ Rn,\\n2 0 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='15951c50-d2bd-4a9d-ada6-1e526bc7865a', embedding=None, metadata={'page_label': '221', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz z\\nxx yy\\n( a)×\\nx x ww\\n( b)u( 1 )u( 1 )\\nd o t\\nbbu( 2 )u( 2 )\\n+ˆ y ˆ y\\nσ\\n( c )XXWWU( 1 )U( 1 )\\nm a t m u l\\nbbU( 2 )U( 2 )\\n+HH\\nr e l u\\nxx ww\\n( d)ˆ yˆ y\\nd o t\\nλ λu( 1 )u( 1 )\\ns q ru( 2 )u( 2 )\\ns u mu( 3 )u( 3 )\\n×\\nFigure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) ×operationto\\ncompute z= x y.Thegraphforthelogisticregressionprediction ( b ) ˆ y= σ\\ue000\\nx\\ue03ew+ b\\ue001\\n.\\nSomeoftheintermediateexpressionsdonothavenamesinthealgebraicexpression\\nbutneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c )\\ncomputationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign\\nmatrixofrectiﬁedlinearunitactivationsHgivenadesignmatrixcontainingaminibatch\\nofinputsX.Examplesa–cappliedatmostoneoperationtoeachvariable,butit ( d )\\nispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat\\nappliesmorethanoneoperationtotheweightswofalinearregressionmodel.The\\nweightsareusedtomakeboththepredictionˆ yandtheweightdecaypenalty λ\\ue050\\niw2\\ni.\\n2 0 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8e51acf6-05b6-4584-8a49-6022f6f1faf2', embedding=None, metadata={'page_label': '222', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ngmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then\\n∂ z\\n∂ x i=\\ue058\\nj∂ z\\n∂ y j∂ y j\\n∂ x i. (6.45)\\nInvectornotation,thismaybeequivalentlywrittenas\\n∇ x z=\\ue012∂y\\n∂x\\ue013\\ue03e\\n∇ y z , (6.46)\\nwhere∂ y\\n∂ xistheJacobianmatrixof. n m× g\\nFromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying\\naJacobianmatrix∂ y\\n∂ xbyagradient∇ y z.Theback-propagation algorithmconsists\\nofperformingsuchaJacobian-gradient productforeachoperationinthegraph.\\nUsuallywedonotapplytheback-propagationalgorithmmerelytovectors,\\nbutrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe\\nsameasback-propagation withvectors.Theonlydiﬀerenceishowthenumbers\\narearrangedinagridtoformatensor.Wecouldimagineﬂatteningeachtensor\\nintoavectorbeforewerunback-propagation,computingavector-valuedgradient,\\nandthenreshapingthegradientbackintoatensor.Inthisrearrangedview,\\nback-propagationisstilljustmultiplyingJacobiansbygradients.\\nTodenotethegradientofavalue zwithrespecttoatensor X,wewrite ∇ X z,\\njustasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinates—for\\nexample,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway\\nbyusingasinglevariable itorepresentthecompletetupleofindices.Forall\\npossibleindextuples i,(∇ X z) igives∂ z\\n∂ X i.Thisisexactlythesameashowforall\\npossibleintegerindices iintoavector,(∇ x z) igives∂ z\\n∂ x i.Usingthisnotation,we\\ncanwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y\\n∇ X z=\\ue058\\nj(∇ X Y j)∂ z\\n∂ Y j. (6.47)\\n6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop\\nUsingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor\\nthegradientofascalarwithrespecttoanynodeinthecomputational graphthat\\nproducedthatscalar.However,actuallyevaluatingthatexpressioninacomputer\\nintroducessomeextraconsiderations.\\nSpeciﬁcally,manysubexpressionsmayberepeatedseveraltimeswithinthe\\noverallexpressionforthegradient.Anyprocedurethatcomputesthegradient\\n2 0 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='03a1f25f-de24-452f-8f41-3a06f86b41fb', embedding=None, metadata={'page_label': '223', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nwillneedtochoosewhethertostorethesesubexpressionsortorecomputethem\\nseveraltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin\\nﬁgure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9\\nbewasteful.\\xa0Forcomplicatedgraphs,therecanbeexponentiallymanyofthese\\nwastedcomputations, makinganaiveimplementation ofthechainruleinfeasible.\\nInothercases,computingthesamesubexpressiontwicecouldbeavalidwayto\\nreducememoryconsumptionatthecostofhigherruntime.\\nWeﬁrstbeginbyaversionoftheback-propagationalgorithmthatspeciﬁesthe\\nactualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1\\nassociatedforwardcomputation), intheorderitwillactuallybedoneandaccording\\ntotherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese\\ncomputations orviewthedescriptionofthealgorithmasasymbolicspeciﬁcation\\nofthecomputational graphforcomputingtheback-propagation. However,this\\nformulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe\\nsymbolicgraphthatperformsthegradientcomputation.\\xa0Such aformulationis\\npresentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5\\nnodesthatcontainarbitrarytensors.\\nFirstconsideracomputational graphdescribinghowtocomputeasinglescalar\\nu( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose\\ngradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ).\\xa0In\\notherwordswewishtocompute∂ u() n\\n∂ u() iforall i∈{1 ,2 , . . . , n i}.Intheapplication\\nofback-propagationtocomputinggradientsforgradientdescentoverparameters,\\nu( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i )\\ncorrespondtotheparametersofthemodel.\\nWewillassumethatthenodesofthegraphhavebeenorderedinsuchaway\\nthatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and\\ngoingupto u( ) n.Asdeﬁnedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan\\noperation f( ) iandiscomputedbyevaluatingthefunction\\nu( ) i= ( f A( ) i) (6.48)\\nwhere A( ) iisthesetofallnodesthatareparentsof u( ) i.\\nThatalgorithmspeciﬁestheforwardpropagationcomputation,whichwecould\\nputinagraph G.Inordertoperformback-propagation, wecanconstructa\\ncomputational graphthatdependsonGandaddstoitanextrasetofnodes.These\\nformasubgraph BwithonenodepernodeofG.Computation inBproceedsin\\nexactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes\\nthederivative∂ u() n\\n∂ u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone\\n2 0 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bbb3193-aebc-4a29-afe9-96f1d1cb720f', embedding=None, metadata={'page_label': '224', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs\\nu( 1 )to u( n i )toanoutput u( ) n.Thisdeﬁnesacomputational graphwhereeachnode\\ncomputesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments\\nA( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P a∈ ( u( ) i).The\\ninputtothecomputational graphisthevectorx,andissetintotheﬁrst n inodes\\nu( 1 )to u( n i ).Theoutputofthecomputational graphisreadoﬀthelast(output)\\nnode u( ) n.\\nfor i , . . . , n = 1 ido\\nu( ) i← x i\\nendfor\\nfor i n= i+1 , . . . , ndo\\nA( ) i←{ u( ) j|∈ j P a u(( ) i)}\\nu( ) i← f( ) i( A( ) i)\\nendfor\\nreturn u( ) n\\nusingthechainrulewithrespecttoscalaroutput u( ) n:\\n∂ u( ) n\\n∂ u( ) j=\\ue058\\ni j P a u :∈ (() i )∂ u( ) n\\n∂ u( ) i∂ u( ) i\\n∂ u( ) j(6.49)\\nasspeciﬁedbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach\\nedgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith\\nthecomputationof∂ u() i\\n∂ u() j.Inaddition,adotproductisperformedforeachnode,\\nbetweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren\\nof u( ) jandthevectorcontainingthepartialderivatives∂ u() i\\n∂ u() jforthesamechildren\\nnodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming\\ntheback-propagationscaleslinearlywiththenumberofedgesinG,wherethe\\ncomputationforeachedgecorrespondstocomputingapartialderivative(ofone\\nnodewithrespecttooneofitsparents)aswellasperformingonemultiplication\\nandoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which\\nisjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore\\neﬃcientimplementations.\\nTheback-propagationalgorithmisdesignedtoreducethenumberofcommon\\nsubexpressionswithoutregardtomemory.Speciﬁcally,itperformsontheorder\\nofoneJacobianproductpernodeinthegraph.\\xa0Thiscanbeseenfromthefact\\nthatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof\\nthegraphexactlyonceinordertoobtaintheassociatedpartialderivative∂ u() i\\n∂ u() j.\\n2 0 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05ecc407-90cd-47d3-9d7e-158017ae3543', embedding=None, metadata={'page_label': '225', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.2Simpliﬁedversionoftheback-propagation algorithmforcomputing\\nthederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis\\nintendedtofurtherunderstandingbyshowingasimpliﬁedcasewhereallvariables\\narescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), . . . , u( n i ).\\nThissimpliﬁedversioncomputesthederivativesofallnodesinthegraph.\\xa0The\\ncomputational costofthisalgorithmisproportional tothenumberofedgesin\\nthegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires\\naconstanttime.Thisisofthesameorderasthenumberofcomputations for\\ntheforwardpropagation. Each∂ u() i\\n∂ u() jisafunctionoftheparents u( ) jof u( ) i,thus\\nlinkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation\\ngraph.\\nRunforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1\\ntionsofthenetwork\\nInitialize grad_table,adatastructurethatwillstorethederivativesthathave\\nbeencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof\\n∂ u() n\\n∂ u() i.\\ng r a d t a b l e_ [ u( ) n] 1←\\nfor do j n= −1downto1\\nThenextlinecomputes∂ u() n\\n∂ u() j=\\ue050\\ni j P a u :∈ (() i )∂ u() n\\n∂ u() i∂ u() i\\n∂ u() jusingstoredvalues:\\ng r a d t a b l e_ [ u( ) j] ←\\ue050\\ni j P a u :∈ (() i ) g r a d t a b l e_ [ u( ) i]∂ u() i\\n∂ u() j\\nendfor\\nreturn{ g r a d t a b l e_ [ u( ) i] = 1 | i , . . . , n i}\\nBack-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions.\\nHowever,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming\\nsimpliﬁcationsonthecomputational graph,ormaybeabletoconservememoryby\\nrecomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas\\nafterdescribingtheback-propagation algorithmitself.\\n6.5.4Back-PropagationComputationinFully-ConnectedMLP\\nToclarifytheabovedeﬁnitionoftheback-propagation computation,letusconsider\\nthespeciﬁcgraphassociatedwithafully-connected multi-layerMLP.\\nAlgorithmﬁrstshowstheforwardpropagation, whichmapsparametersto 6.3\\nthesupervisedloss L(ˆyy ,)associatedwithasingle(input,target) trainingexample\\n( )xy ,,with ˆytheoutputoftheneuralnetworkwhenisprovidedininput. x\\nAlgorithm\\xa0 then\\xa0shows\\xa0thecorresponding\\xa0computation\\xa0to be\\xa0donefor 6.4\\n2 1 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e8fdde66-1d5a-4f0b-a46f-099819b437ef', embedding=None, metadata={'page_label': '226', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz z\\nxxyy\\nw wfff\\nFigure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing\\nthegradient.Let w∈ Rbetheinputtothegraph.Weusethesamefunction f: R R→\\nastheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y).\\nTocompute∂ z\\n∂ w,weapplyequationandobtain: 6.44\\n∂ z\\n∂ w(6.50)\\n=∂ z\\n∂ y∂ y\\n∂ x∂ x\\n∂ w(6.51)\\n= f\\ue030() y f\\ue030() x f\\ue030() w (6.52)\\n= f\\ue030((())) f f w f\\ue030(()) f w f\\ue030() w (6.53)\\nEquationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only\\nonceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation\\nalgorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53\\nf( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime\\nitisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the\\nback-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52\\nruntime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53\\nusefulwhenmemoryislimited.\\n2 1 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6b571ec3-9b15-4820-af9a-f97c760e6863', embedding=None, metadata={'page_label': '227', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\napplyingtheback-propagation algorithmtothisgraph.\\nAlgorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4\\nstraightforwardtounderstand.However,\\xa0theyarespecializedtoonespeciﬁc\\nproblem.\\nModernsoftwareimplementations arebasedonthegeneralizedformofback-\\npropagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6\\ntationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic\\ncomputation.\\nAlgorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand\\nthecomputationofthecostfunction.Theloss L(ˆyy ,)dependsontheoutput\\nˆyandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1\\nobtainthetotalcost J,thelossmaybeaddedtoaregularizer Ω( θ),where θ\\ncontainsalltheparameters(weightsandbiases).Algorithm showshowto 6.4\\ncomputegradientsof JwithrespecttoparametersWandb.Forsimplicity,this\\ndemonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould\\nuseaminibatch.Seesectionforamorerealisticdemonstration. 6.5.7\\nRequire:Networkdepth, l\\nRequire:W( ) i, i , . . . , l , ∈{1 }theweightmatricesofthemodel\\nRequire:b( ) i, i , . . . , l , ∈{1 }thebiasparametersofthemodel\\nRequire:x,theinputtoprocess\\nRequire:y,thetargetoutput\\nh( 0 )= x\\nfordo k , . . . , l = 1\\na( ) k= b( ) k+W( ) kh( 1 ) k−\\nh( ) k= ( fa( ) k)\\nendfor\\nˆyh= ( ) l\\nJ L= (ˆyy ,)+Ω() λ θ\\n6.5.5Symbol-to-SymbolDerivatives\\nAlgebraicexpressionsandcomputational graphsbothoperateonsymbols,or\\nvariables\\xa0thatdo\\xa0not\\xa0havespeciﬁc\\xa0values.Thesealgebraic\\xa0and graph-based\\nrepresentationsarecalledsymbolicrepresentations.Whenweactuallyuseor\\ntrainaneuralnetwork,wemustassignspeciﬁcvaluestothesesymbols.We\\nreplaceasymbolicinputtothenetworkxwithaspeciﬁcnumericvalue,suchas\\n[123765 18] . , . ,− .\\ue03e.\\n2 1 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b048ee24-16c2-4dc2-80ce-ea83e6d53417', embedding=None, metadata={'page_label': '228', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo-\\nrithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation\\nyieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe\\noutputlayerandgoingbackwardstotheﬁrsthiddenlayer.Fromthesegradients,\\nwhichcanbeinterpretedasanindicationofhoweachlayer’soutputshouldchange\\ntoreduceerror,onecanobtainthegradientontheparametersofeachlayer.The\\ngradientsonweightsandbiasescanbeimmediately usedaspartofastochas-\\nticgradientupdate(performingtheupdaterightafterthegradientshavebeen\\ncomputed)orusedwithothergradient-basedoptimization methods.\\nAftertheforwardcomputation,computethegradientontheoutputlayer:\\ng←∇ ˆ y J= ∇ ˆ y L(ˆyy ,)\\nfor do k l , l , . . . , = −1 1\\nConvert\\xa0thegradienton\\xa0thelayer’s\\xa0output\\xa0into\\xa0a\\xa0gradient\\xa0into\\xa0thepre-\\nnonlinearityactivation(element-wisemultiplicationifiselement-wise): f\\ng←∇a() k J f = g\\ue00c\\ue030(a( ) k)\\nComputegradientsonweightsandbiases(includingtheregularizationterm,\\nwhereneeded):\\n∇b() k J λ = +g ∇b() kΩ() θ\\n∇W() k J= gh( 1 ) k−\\ue03e+ λ∇W() kΩ() θ\\nPropagatethegradientsw.r.t.thenextlower-levelhiddenlayer’sactivations:\\ng←∇h(1) k − J= W( ) k\\ue03eg\\nendfor\\n2 1 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0009dcf-e3e9-4a50-8643-39cbf8d9fb9e', embedding=None, metadata={'page_label': '229', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nz z\\nxxyy\\nw wfffz z\\nxxyy\\nw wfff\\nd z\\nd yd z\\nd yf\\ue021\\nd y\\nd xd y\\nd xf\\ue021\\nd z\\nd xd z\\nd x×\\nd x\\nd wd x\\nd wf\\ue021\\nd z\\nd wd z\\nd w×\\nFigure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In\\nthisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual\\nspeciﬁcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow\\ntocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe\\nderivativesforanyspeciﬁcnumericvalues. ( L e f t )Inthisexample,webeginwithagraph\\nrepresenting z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t )\\nittoconstructthegraphfortheexpressioncorrespondingtod z\\nd w.Inthisexample,wedo\\nnotexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate\\nwhatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe\\nderivative.\\nSomeapproachestoback-propagationtakeacomputational graphandaset\\nofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical\\nvaluesdescribingthegradientatthoseinputvalues.Wecallthisapproach“symbol-\\nto-number”diﬀerentiation. ThisistheapproachusedbylibrariessuchasTorch\\n( ,)andCaﬀe(,). Collobert e t a l .2011b Jia2013\\nAnotherapproachistotakeacomputational graphandaddadditionalnodes\\ntothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This\\nistheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012\\nandTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015\\nisillustratedinﬁgure.Theprimaryadvantageofthisapproachisthat 6.10\\nthederivativesaredescribedinthesamelanguageastheoriginalexpression.\\nBecausethederivativesarejustanothercomputational graph,itispossibletorun\\nback-propagationagain,diﬀerentiating thederivativesinordertoobtainhigher\\nderivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10\\nWewillusethelatterapproachanddescribetheback-propagationalgorithmin\\n2 1 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6a98f35-2f75-47ff-9b61-dad32046ae50', embedding=None, metadata={'page_label': '230', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\ntermsofconstructingacomputational graphforthederivatives.Anysubsetofthe\\ngraphmaythenbeevaluatedusingspeciﬁcnumericalvaluesatalatertime.This\\nallowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed.\\nInstead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits\\nparents’valuesareavailable.\\nThedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol-\\nto-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas\\nperformingexactlythesamecomputations asaredoneinthegraphbuiltbythe\\nsymbol-to-symbolapproach.Thekeydiﬀerenceisthatthesymbol-to-number\\napproachdoesnotexposethegraph.\\n6.5.6GeneralBack-Propagation\\nTheback-propagationalgorithmisverysimple.Tocomputethegradientofsome\\nscalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving\\nthatthegradientwithrespectto zisgivenbyd z\\nd z=1.Wecanthencompute\\nthegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe\\ncurrentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue\\nmultiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil\\nwereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough\\ntwoormorepaths,wesimplysumthegradientsarrivingfromdiﬀerentpathsat\\nthatnode.\\nMoreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve\\nmaximumgenerality,wedescribethisvariableasbeingatensor V.\\xa0Tensorcan\\ningeneralhaveanynumberofdimensions.\\xa0Theysubsumescalars,vectors,and\\nmatrices.\\nWeassumethateachvariableisassociatedwiththefollowingsubroutines: V\\n• g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre-\\nsentedbytheedgescominginto Vinthecomputational graph.Forexample,\\ntheremaybeaPythonorC++classrepresentingthematrixmultiplication\\noperation,andtheget_operationfunction.Supposewehaveavariablethat\\niscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V)\\nreturnsapointertoaninstanceofthecorrespondingC++class.\\n• g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof\\nVinthecomputational graph.G\\n• G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V\\ninthecomputational graph.G\\n2 1 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e03ee181-c416-48c1-a43a-8932ab1681f5', embedding=None, metadata={'page_label': '231', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nEachoperationopisalsoassociatedwithabpropoperation.Thisbprop\\noperationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47\\nThisishowtheback-propagationalgorithmisabletoachievegreatgenerality.\\nEachoperationisresponsibleforknowinghowtoback-propagate throughthe\\nedgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix\\nmultiplicationoperationtocreateavariableC=AB.Supposethatthegradient\\nofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation\\nisresponsiblefordeﬁningtwoback-propagation rules,oneforeachofitsinput\\narguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto\\nAgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe\\nmatrixmultiplicationoperationmuststatethatthegradientwithrespecttoA\\nisgivenbyGB\\ue03e.Likewise,ifwecallthe b p r o pmethodtorequestthegradient\\nwithrespecttoB,thenthematrixoperationisresponsibleforimplementing the\\nb p r o pmethodandspecifyingthatthedesiredgradientisgivenbyA\\ue03eG.The\\nback-propagationalgorithmitselfdoesnotneedtoknowanydiﬀerentiation rules.It\\nonlyneedstocalleachoperation’sbpropruleswiththerightarguments.Formally,\\no p b p r o p i n p u t s . ( , , X G)mustreturn\\n\\ue058\\ni(∇ X o p f i n p u t s .( ) i) G i , (6.54)\\nwhichisjustanimplementation ofthechainruleasexpressedinequation.6.47\\nHere, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe\\nmathematical functionthattheoperationimplements, Xistheinputwhosegradient\\nwewishtocompute,andisthegradientontheoutputoftheoperation. G\\nTheop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct\\nfromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed\\ntwocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe\\nderivativewithrespecttobothinputs.Theback-propagation algorithmwilllater\\naddbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal\\nderivativeon. x\\nSoftwareimplementationsofback-propagation usuallyprovideboththeopera-\\ntionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare\\nabletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix\\nmultiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda\\nnewimplementationofback-propagationoradvanceduserswhoneedtoaddtheir\\nownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor\\nanynewoperationsmanually.\\nTheback-propagationalgorithmisformallydescribedinalgorithm .6.5\\n2 1 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4cab242-24ba-42b3-9072-f7047f6c9e21', embedding=None, metadata={'page_label': '232', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This\\nportiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens\\ninthe subroutineofalgorithm build_grad 6.6.\\nRequire: T,thetargetsetofvariableswhosegradientsmustbecomputed.\\nRequire:G,thecomputational graph\\nRequire: z,thevariabletobediﬀerentiated\\nLetG\\ue030beGprunedtocontainonlynodesthatareancestorsof zanddescendents\\nofnodesin. T\\nInitialize ,adatastructureassociatingtensorstotheirgradients grad_table\\ng r a d t a b l e_ [] 1 z←\\nfordo Vin T\\nb u i l d g r a d_ ( V , ,GG\\ue030, g r a d t a b l e_ )\\nendfor\\nReturn restrictedto grad_table T\\nInsection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2\\navoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive\\nalgorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions.\\nNowthatwehavespeciﬁedtheback-propagationalgorithm,wecanunderstandits\\ncomputational cost.Ifweassumethateachoperationevaluationhasroughlythe\\nsamecost,thenwemayanalyzethecomputational costintermsofthenumber\\nofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe\\nfundamentalunitofourcomputational graph,whichmightactuallyconsistofvery\\nmanyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix\\nmultiplicationasasingleoperation).Computingagradientinagraphwith nnodes\\nwillneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan\\nO( n2) operations.Herewearecountingoperationsinthecomputational graph,not\\nindividualoperationsexecutedbytheunderlyinghardware,soitisimportantto\\nrememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample,\\nmultiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto\\nasingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas\\nmost O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute\\nall nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute,\\nwemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm\\naddsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per\\nedgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic\\ngraphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused\\ninpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare\\n2 1 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62f77183-1f92-48e4-b396-56b7ccb90ff4', embedding=None, metadata={'page_label': '233', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nAlgorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GG\\ue030, g r a d t a b l e_ )of\\ntheback-propagationalgorithm,calledbytheback-propagationalgorithmdeﬁned\\ninalgorithm .6.5\\nRequire: V,thevariablewhosegradientshouldbeaddedtoand . Ggrad_table\\nRequire:G,thegraphtomodify.\\nRequire:G\\ue030,therestrictionoftonodesthatparticipateinthegradient. G\\nRequire:grad_table,adatastructuremappingnodestotheirgradients\\nif then Visingrad_table\\nReturn_ g r a d t a b l e[] V\\nendif\\ni←1\\nfor C V in_ g e t c o n s u m e r s( ,G\\ue030)do\\no p g e t o p e r a t i o n ←_ () C\\nD C ← b u i l d g r a d_ ( , ,GG\\ue030, g r a d t a b l e_ )\\nG( ) i← G o p b p r o p g e t i n p u t s . (_ ( C ,\\ue030) ) , , V D\\ni i←+1\\nendfor\\nG←\\ue050\\ni G( ) i\\ng r a d t a b l e_ [] = V G\\nInsertandtheoperationscreatingitinto G G\\nReturn G\\nroughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar\\nbetterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany\\nnodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting\\ntherecursivechainrule(equation)non-recursively: 6.49\\n∂ u( ) n\\n∂ u( ) j=\\ue058\\npa t h ( u( π1), u( π2), . . . , u( π t)) ,\\nf r o m π1 = t o j π t = nt\\ue059\\nk = 2∂ u( π k )\\n∂ u( π k −1 ). (6.55)\\nSincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe\\nlengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber\\nofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation\\ngraph.Thislargecostwouldbeincurredbecausethesamecomputationfor\\n∂ u() i\\n∂ u() jwouldberedonemanytimes.\\xa0Toavoidsuchrecomputation, wecanthink\\nofback-propagation asatable-ﬁllingalgorithmthattakesadvantageofstoring\\nintermediateresults∂ u() n\\n∂ u() i.Eachnodeinthegraphhasacorrespondingslotina\\ntabletostorethegradientforthatnode.Byﬁllinginthesetableentriesinorder,\\n2 1 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='576e0d96-d332-49ae-a48d-e7ea75c64965', embedding=None, metadata={'page_label': '234', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nback-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-ﬁlling\\nstrategyissometimescalled . dynamicprogramming\\n6.5.7Example:Back-PropagationforMLPTraining\\nAsanexample,wewalkthroughtheback-propagation algorithmasitisusedto\\ntrainamultilayerperceptron.\\nHerewedevelopaverysimplemultilayerperceptionwithasinglehidden\\nlayer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent.\\nTheback-propagationalgorithmisusedtocomputethegradientofthecostona\\nsingleminibatch.Speciﬁcally,weuseaminibatchofexamplesfromthetraining\\nsetformattedasadesignmatrixXandavectorofassociatedclasslabelsy.\\nThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To\\nsimplifythepresentationwedonotusebiasesinthismodel.Weassumethatour\\ngraphlanguageincludesareluoperationthatcancompute max{0 ,Z}element-\\nwise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen\\ngivenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy\\noperationthatcomputesthecross-entropybetweenthetargetsyandtheprobability\\ndistributiondeﬁnedbytheseunnormalized logprobabilities. Theresultingcross-\\nentropydeﬁnesthecost J M LE.Minimizingthiscross-entropyperformsmaximum\\nlikelihoodestimationoftheclassiﬁer.However,tomakethisexamplemorerealistic,\\nwealsoincludearegularizationterm.Thetotalcost\\nJ J= M LE+ λ\\uf8eb\\n\\uf8ed\\ue058\\ni , j\\ue010\\nW( 1 )\\ni , j\\ue0112\\n+\\ue058\\ni , j\\ue010\\nW( 2 )\\ni , j\\ue0112\\uf8f6\\n\\uf8f8 (6.56)\\nconsistsofthecross-entropyandaweightdecaytermwithcoeﬃcient λ.The\\ncomputational graphisillustratedinﬁgure.6.11\\nThecomputational graphforthegradientofthisexampleislargeenoughthat\\nitwouldbetedioustodrawortoread.Thisdemonstratesoneofthebeneﬁts\\noftheback-propagation algorithm,whichisthatitcanautomatically generate\\ngradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto\\nderivemanually.\\nWecanroughlytraceoutthebehavioroftheback-propagation algorithm\\nbylookingattheforwardpropagationgraphinﬁgure.Totrain,wewish 6.11\\ntocomputeboth∇W(1) Jand ∇W(2) J.Therearetwodiﬀerentpathsleading\\nbackwardfrom Jtotheweights:onethroughthecross-entropycost,andone\\nthroughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill\\nalwayscontribute 2 λW( ) itothegradientonW( ) i.\\n2 1 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0fec77f5-7dbd-40c5-8a41-62a83e81ffd9', embedding=None, metadata={'page_label': '235', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nXXW( 1 )W( 1 )U( 1 )U( 1 )\\nm a t m u lHH\\nr e l u\\nU( 3 )U( 3 )\\ns q ru( 4 )u( 4 )\\ns u mλ λ u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 )\\nm a t m u ly yJ M L E J M L E\\nc r o s s _ e n t r o p y\\nU( 5 )U( 5 )\\ns q ru( 6 )u( 6 )\\ns u mu( 8 )u( 8 )J J\\n+\\n×\\n+\\nFigure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample\\nofasingle-layerMLPusingthecross-entropylossandweightdecay.\\nTheotherpaththroughthecross-entropycostisslightlymorecomplicated.\\nLetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby\\nthecross_entropyoperation.Theback-propagation algorithmnowneedsto\\nexploretwodiﬀerentbranches.Ontheshorterbranch,itaddsH\\ue03eGtothe\\ngradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto\\nthematrixmultiplication operation.Theotherbranchcorrespondstothelonger\\nchaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm\\ncomputes ∇ H J=GW( 2 )\\ue03eusingtheback-propagationrulefortheﬁrstargument\\ntothematrixmultiplication operation.Next,thereluoperationusesitsback-\\npropagationruletozerooutcomponentsofthegradientcorrespondingtoentries\\nofU( 1 )thatwerelessthan.Lettheresultbecalled 0 G\\ue030.Thelaststepofthe\\nback-propagationalgorithmistousetheback-propagation ruleforthesecond\\nargumentoftheoperationtoadd matmul X\\ue03eG\\ue030tothegradientonW( 1 ).\\nAfterthesegradientshavebeencomputed,itistheresponsibilityofthegradient\\ndescentalgorithm,oranotheroptimization algorithm,tousethesegradientsto\\nupdatetheparameters.\\nFortheMLP,thecomputational costisdominatedbythecostofmatrix\\nmultiplication. Duringtheforwardpropagationstage,wemultiplybyeachweight\\n2 2 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e2042166-564e-4357-97c6-2295c5a4aa4e', embedding=None, metadata={'page_label': '236', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nmatrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During\\nthebackwardpropagationstage,wemultiplybythetransposeofeachweight\\nmatrix,whichhasthesamecomputational cost.Themainmemorycostofthe\\nalgorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer.\\nThisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas\\nreturnedtothesamepoint.Thememorycostisthus O( m n h),where misthe\\nnumberofexamplesintheminibatchand n histhenumberofhiddenunits.\\n6.5.8Complications\\nOurdescriptionoftheback-propagation algorithmhereissimplerthantheimple-\\nmentationsactuallyusedinpractice.\\nAsnotedabove,wehaverestrictedthedeﬁnitionofanoperationtobea\\nfunctionthatreturnsasingletensor.Mostsoftwareimplementations needto\\nsupportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish\\ntocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis\\nbesttocomputebothinasinglepassthroughmemory,soitismosteﬃcientto\\nimplementthisprocedureasasingleoperationwithtwooutputs.\\nWe\\xa0havenot\\xa0described\\xa0how\\xa0tocontrolthememoryconsumption\\xa0ofback-\\npropagation. Back-propagati onofteninvolvessummationofmanytensorstogether.\\nInthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then\\nallofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly\\nhighmemorybottleneckthatcanbeavoidedbymaintainingasinglebuﬀerand\\naddingeachvaluetothatbuﬀerasitiscomputed.\\nReal-worldimplementationsofback-propagation alsoneedtohandlevarious\\ndatatypes,suchas32-bitﬂoatingpoint,64-bitﬂoatingpoint,andintegervalues.\\nThepolicyforhandlingeachofthesetypestakesspecialcaretodesign.\\nSomeoperationshaveundeﬁnedgradients,anditisimportanttotrackthese\\ncasesanddeterminewhetherthegradientrequestedbytheuserisundeﬁned.\\nVariousothertechnicalitiesmakereal-worlddiﬀerentiation morecomplicated.\\nThesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey\\nintellectualtoolsneededtocomputederivatives,butitisimportanttobeaware\\nthatmanymoresubtletiesexist.\\n6.5.9DiﬀerentiationoutsidetheDeepLearningCommunity\\nThe\\xa0deeplearning\\xa0comm unityhas\\xa0beensomewhat\\xa0isolatedfrom\\xa0the\\xa0broader\\ncomputersciencecommunityandhaslargelydevelopeditsownculturalattitudes\\n2 2 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54ae82d0-801b-4404-b4ae-df59b3970537', embedding=None, metadata={'page_label': '237', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nconcerninghowtoperformdiﬀerentiation. Moregenerally,theﬁeldofautomatic\\ndiﬀerentiationisconcernedwithhowtocomputederivativesalgorithmically .\\nTheback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic\\ndiﬀerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse\\nmodeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain\\nruleindiﬀerentorders.Ingeneral,\\xa0determining theorderofevaluationthat\\nresultsinthelowestcomputational costisadiﬃcultproblem.Findingtheoptimal\\nsequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008\\ninthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast\\nexpensiveform.\\nForexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities\\nandvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities. Suppose\\nwedeﬁne\\nq i=exp( z i)\\ue050\\niexp( z i), (6.57)\\nwherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision\\noperations,\\xa0and\\xa0construct\\xa0a cross-entropyloss J=−\\ue050\\ni p ilog q i.Ahuman\\nmathematician canobservethatthederivativeof Jwithrespectto z itakesavery\\nsimpleform: q i− p i.Theback-propagation algorithmisnotcapableofsimplifying\\nthegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof\\nthelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware\\nlibrariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012\\nperformsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed\\nbythepureback-propagation algorithm.\\nWhentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative\\n∂ u() i\\n∂ u() jcanbecomputedwithaconstantamountofcomputation,back-propagation\\nguaranteesthatthenumberofcomputations forthegradientcomputationisof\\nthesameorderasthenumberofcomputations fortheforwardcomputation: this\\ncanbeseeninalgorithm becauseeachlocalpartialderivative 6.2∂ u() i\\n∂ u() jneedsto\\nbecomputedonlyoncealongwithanassociatedmultiplication andadditionfor\\ntherecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49\\ntherefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe\\ncomputational graphconstructedbyback-propagation,andthisisanNP-complete\\ntask.\\xa0ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon\\nmatchingknownsimpliﬁcationpatternsinordertoiterativelyattempttosimplify\\nthegraph.Wedeﬁnedback-propagation onlyforthecomputationofagradientofa\\nscalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either\\nof kdiﬀerentscalarnodesinthegraph,orofatensor-valuednodecontaining k\\nvalues).Anaiveimplementation maythenneed ktimesmorecomputation: for\\n2 2 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dc929ee2-27df-4834-be11-11841505c1ef', embedding=None, metadata={'page_label': '238', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\neachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation\\ncomputes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof\\nthegraphislargerthanthenumberofinputs,itissometimespreferabletouse\\nanotherformofautomaticdiﬀerentiationcalledforwardmodeaccumulation.\\nForwardmodecomputationhasbeenproposedforobtainingreal-timecomputation\\nofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989\\nalsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading\\noﬀcomputational eﬃciencyformemory.Therelationshipbetweenforwardmode\\nandbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus\\nright-multiplyingasequenceofmatrices,suchas\\nABCD , (6.58)\\nwherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD\\nisacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha\\nsingleoutputandmanyinputs,andstartingthemultiplications fromtheend\\nandgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto\\nthebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea\\nseriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore\\nexpensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun\\nthemultiplications left-to-right,correspondingtotheforwardmode.\\nInmanycommunitiesoutsideofmachinelearning,itismorecommontoim-\\nplementdiﬀerentiationsoftwarethatactsdirectlyontraditionalprogramming\\nlanguagecode,suchasPythonorCcode,andautomatically generatesprograms\\nthatdiﬀerentiatefunctionswrittenintheselanguages.Inthedeeplearningcom-\\nmunity,computational graphsareusuallyrepresentedbyexplicitdatastructures\\ncreatedbyspecializedlibraries.Thespecializedapproachhasthedrawbackof\\nrequiringthelibrarydevelopertodeﬁnethebpropmethodsforeveryoperation\\nandlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeﬁned.\\nHowever,thespecializedapproachalsohasthebeneﬁtofallowingcustomized\\nback-propagationrulestobedevelopedforeachoperation,allowingthedeveloper\\ntoimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure\\nwouldpresumablybeunabletoreplicate.\\nBack-propagationisthereforenottheonlywayortheoptimalwayofcomputing\\nthegradient,butitisaverypracticalmethodthatcontinuestoservethedeep\\nlearningcommunityverywell.Inthefuture,diﬀerentiation technologyfordeep\\nnetworksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances\\ninthebroaderﬁeldofautomaticdiﬀerentiation.\\n2 2 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='548ad66b-8f45-43be-9d33-fb4c6af6e771', embedding=None, metadata={'page_label': '239', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\n6.5.10Higher-OrderDerivatives\\nSomesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe\\ndeeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow.\\nTheselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor\\nderivativesastheyusetodescribetheoriginalfunctionbeingdiﬀerentiated.This\\nmeansthatthesymbolicdiﬀerentiation machinerycanbeappliedtoderivatives.\\nInthecontextofdeeplearning,itisraretocomputeasinglesecondderivative\\nofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian\\nmatrix.Ifwehaveafunction f: Rn→ R,thentheHessianmatrixisofsize n n×.\\nIntypicaldeeplearningapplications, nwillbethenumberofparametersinthe\\nmodel,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis\\nthusinfeasibletoevenrepresent.\\nInsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach\\nistouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor\\nperformingvariousoperationslikeapproximately invertingamatrixorﬁnding\\napproximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation\\notherthanmatrix-vector products.\\nInordertouseKrylovmethodsontheHessian,weonlyneedtobeableto\\ncomputetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A\\nstraightforwardtechnique( ,)fordoingsoistocompute Christianson1992\\nHv= ∇ x\\ue068\\n(∇ x f x())\\ue03ev\\ue069\\n. (6.59)\\nBothofthegradientcomputations inthisexpressionmaybecomputedautomati-\\ncallybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression\\ntakesthegradientofafunctionoftheinnergradientexpression.\\nIfvisitselfavectorproducedbyacomputational graph,itisimportantto\\nspecifythattheautomaticdiﬀerentiationsoftwareshouldnotdiﬀerentiatethrough\\nthegraphthatproduced.v\\nWhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith\\nHessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 , . . . , n ,where\\ne( ) iistheone-hotvectorwith e( ) i\\ni= 1andallotherentriesequalto0.\\n6. 6 Hi s t or i c a l Not es\\nFeedforwardnetworkscanbeseenaseﬃcientnonlinearfunctionapproximators\\nbasedonusinggradientdescenttominimizetheerrorinafunctionapproximation.\\n2 2 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c8f7e716-191b-4ba2-a61c-1943680cb416', embedding=None, metadata={'page_label': '240', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nFromthispointofview,themodernfeedforwardnetworkistheculminationof\\ncenturiesofprogressonthegeneralfunctionapproximationtask.\\nThechainrulethatunderliestheback-propagation algorithmwasinvented\\ninthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676L’Hôpital1696\\nlongbeenusedtosolveoptimization problemsinclosedform,butgradientdescent\\nwasnotintroducedasatechniqueforiterativelyapproximating thesolutionto\\noptimization problemsuntilthe19thcentury(Cauchy1847,).\\nBeginninginthe1940s,thesefunctionapproximation techniqueswereusedto\\nmotivatemachinelearningmodelssuchastheperceptron.However,theearliest\\nmodelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout\\nseveraloftheﬂawsofthelinearmodelfamily,suchasitsinabilitytolearnthe\\nXORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach.\\nLearningnonlinearfunctionsrequiredthedevelopmentofamultilayerper-\\nceptronandameansofcomputingthegradientthroughsuchamodel.Eﬃcient\\napplicationsofthechainrulebasedondynamicprogramming begantoappear\\ninthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand\\nDenham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor\\nsensitivityanalysis(,). Linnainmaa1976Werbos1981()proposedapplyingthese\\ntechniquestotrainingartiﬁcialneuralnetworks.Theideawasﬁnallydeveloped\\ninpracticeafterbeingindependentlyrediscoveredindiﬀerentways(,;LeCun1985\\nParker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro-\\ncessingpresentedtheresultsofsomeoftheﬁrstsuccessfulexperimentswith\\nback-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b\\ntothepopularization ofback-propagation andinitiatedaveryactiveperiodof\\nresearchinmulti-layerneuralnetworks.\\xa0However,theideasputforwardbythe\\nauthorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond\\nback-propagation.\\xa0Theyincludecrucialideasaboutthepossiblecomputational\\nimplementationofseveralcentralaspectsofcognitionandlearning,whichcame\\nunderthenameof“connectionism” becauseoftheimportancethisschoolofthought\\nplacesontheconnectionsbetweenneuronsasthelocusoflearningandmemory.\\nInparticular,theseideasincludethenotionofdistributedrepresentation(Hinton\\ne t a l .,).1986\\nFollowingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop-\\nularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning\\ntechniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat\\nbeganin2006.\\nThecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub-\\nstantiallysincethe1980s.\\xa0Thesameback-propagationalgorithmandthesame\\n2 2 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a759b08-b8d6-4c0c-9411-bccad1a2135f', embedding=None, metadata={'page_label': '241', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\napproachestogradientdescentarestillinuse.Mostoftheimprovementinneural\\nnetworkperformancefrom1986to2015canbeattributedtotwofactors.First,\\nlargerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa\\nchallengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger,\\nduetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a\\nsmallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural\\nnetworksnoticeably.\\nOneofthesealgorithmicchangeswasthereplacementofmeansquarederror\\nwiththecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin\\nthe1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe\\nprincipleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity\\nandthemachinelearningcommunity.Theuseofcross-entropylossesgreatly\\nimprovedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which\\nhadpreviouslysuﬀeredfromsaturationandslowlearningwhenusingthemean\\nsquarederrorloss.\\nTheothermajoralgorithmicchangethathasgreatlyimprovedtheperformance\\noffeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise\\nlinearhiddenunits,suchasrectiﬁedlinearunits.Rectiﬁcationusingthemax{0 , z}\\nfunctionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast\\nasfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly\\nmodelsdid\\xa0notuserectiﬁed\\xa0linearunits,\\xa0but\\xa0insteadappliedrectiﬁcation\\xa0to\\nnonlinearfunctions.Despitetheearlypopularityofrectiﬁcation,rectiﬁcationwas\\nlargelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter\\nwhenneuralnetworksareverysmall.Asoftheearly2000s,rectiﬁedlinearunits\\nwereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith\\nnon-diﬀerentiablepointsmustbeavoided.Thisbegantochangeinabout2009.\\nJarrett2009 e t a l .()observedthat“usingarectifyingnonlinearityisthesinglemost\\nimportantfactorinimprovingtheperformanceofarecognitionsystem”among\\nseveraldiﬀerentfactorsofneuralnetworkarchitecturedesign.\\nForsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009\\nlinearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers.\\nRandomweightsaresuﬃcienttopropagateusefulinformationthrougharectiﬁed\\nlinearnetwork,allowingtheclassiﬁerlayeratthetoptolearnhowtomapdiﬀerent\\nfeaturevectorstoclassidentities.\\nWhenmoredataisavailable,learningbeginstoextractenoughusefulknowledge\\ntoexceedtheperformanceofrandomlychosenparameters. () Glorot e t a l .2011a\\nshowedthatlearningisfareasierindeeprectiﬁedlinearnetworksthanindeep\\nnetworksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions.\\n2 2 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fafa1cb6-c775-4999-934e-4b1aff9fbf49', embedding=None, metadata={'page_label': '242', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER6.DEEPFEEDFORWARDNETWORKS\\nRectiﬁedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat\\nneurosciencehascontinuedtohave\\xa0aninﬂuenceonthe\\xa0developmentofdeep\\nlearningalgorithms. ()motivaterectiﬁedlinearunitsfrom Glorot e t a l .2011a\\nbiologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture\\nthesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare\\ncompletelyinactive.2)Forsomeinputs,abiologicalneuron’soutputisproportional\\ntoitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere\\ntheyareinactive(i.e.,theyshouldhavesparseactivations).\\nWhenthemodernresurgenceofdeeplearningbeganin2006,feedforward\\nnetworkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely\\nbelievedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted\\nbyothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe\\nrightresourcesandengineeringpractices,feedforwardnetworksperformverywell.\\nToday,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop\\nprobabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial\\nnetworks,describedinchapter.Ratherthanbeingviewedasanunreliable 20\\ntechnologythatmustbesupportedbyothertechniques,gradient-basedlearningin\\nfeedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat\\nmaybeappliedtomanyothermachinelearningtasks.In2006,thecommunity\\nusedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it\\nismorecommontousesupervisedlearningtosupportunsupervisedlearning.\\nFeedforwardnetworkscontinuetohaveunfulﬁlledpotential.Inthefuture,we\\nexpecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization\\nalgorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This\\nchapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe\\nsubsequentchapters,weturntohowtousethesemodels—howtoregularizeand\\ntrainthem.\\n2 2 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1a60cf3f-e1cc-4f02-8886-2491cd3ecee1', embedding=None, metadata={'page_label': '243', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 7\\nRegularization f or D e e p L e ar n i n g\\nAcentralprobleminmachinelearningishowtomakeanalgorithmthatwill\\nperformwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies\\nusedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly\\nattheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively\\nasregularization.\\xa0As wewillseethereareagreatmanyformsofregularization\\navailabletothedeeplearningpractitioner. Infact,\\xa0developingmoreeﬀective\\nregularizationstrategieshasbeenoneofthemajorresearcheﬀortsintheﬁeld.\\nChapterintroducedthebasicconceptsofgeneralization, underﬁtting,overﬁt- 5\\nting,bias,varianceandregularization. Ifyouarenotalreadyfamiliarwiththese\\nnotions,pleaserefertothatchapterbeforecontinuingwiththisone.\\nInthischapter,wedescriberegularizationinmoredetail,focusingonregular-\\nizationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks\\ntoformdeepmodels.\\nSomesectionsofthischapterdealwithstandardconceptsinmachinelearning.\\nIfyouarealreadyfamiliarwiththeseconcepts,\\xa0feelfreetoskiptherelevant\\nsections.However,mostofthischapterisconcernedwiththeextensionofthese\\nbasicconceptstotheparticularcaseofneuralnetworks.\\nInsection,wedeﬁnedregularizationas“anymodiﬁcationwemaketo 5.2.2\\nalearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot\\nitstrainingerror.”Therearemanyregularizationstrategies.Someputextra\\nconstraints\\xa0ona\\xa0machine\\xa0learning\\xa0model, such\\xa0asadding\\xa0restrictionson\\xa0the\\nparametervalues.Someaddextratermsintheobjectivefunctionthatcanbe\\nthoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen\\ncarefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance\\n228', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='316a7dc7-9b1d-4c9e-8754-64f38b939c7a', embedding=None, metadata={'page_label': '244', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nonthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode\\nspeciﬁckindsofpriorknowledge.Othertimes,theseconstraintsandpenalties\\naredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto\\npromotegeneralization. Sometimespenaltiesandconstraintsarenecessarytomake\\nanunderdetermined problemdetermined.Otherformsofregularization,knownas\\nensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata.\\nInthecontextofdeeplearning,mostregularizationstrategiesarebasedon\\nregularizingestimators.Regularizationofanestimatorworksbytradingincreased\\nbiasforreducedvariance.Aneﬀectiveregularizerisonethatmakesaproﬁtable\\ntrade,reducingvariancesigniﬁcantlywhilenotoverlyincreasingthebias.Whenwe\\ndiscussedgeneralization andoverﬁttinginchapter,wefocusedonthreesituations, 5\\nwherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating\\nprocess—correspondingtounderﬁttingandinducingbias,or(2)matchedthetrue\\ndatageneratingprocess,or(3)includedthegeneratingprocessbutalsomany\\notherpossiblegeneratingprocesses—theoverﬁttingregimewherevariancerather\\nthanbiasdominatestheestimationerror.Thegoalofregularizationistotakea\\nmodelfromthethirdregimeintothesecondregime.\\nInpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe\\ntargetfunctionorthetruedatageneratingprocess,orevenacloseapproximation\\nofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso\\nwecanneverknowforsureifthemodelfamilybeingestimatedincludesthe\\ngeneratingprocessornot.However,mostapplicationsofdeeplearningalgorithms\\naretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside\\nthemodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely\\ncomplicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue\\ngenerationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome\\nextent,wearealwaystryingtoﬁtasquarepeg(thedatageneratingprocess)into\\naroundhole(ourmodelfamily).\\nWhatthismeansisthatcontrollingthecomplexityofthemodelisnota\\nsimplematterofﬁndingthemodeloftherightsize,withtherightnumberof\\nparameters.Instead,wemightﬁnd—andindeedinpracticaldeeplearningscenarios,\\nwealmostalwaysdoﬁnd—thatthebestﬁttingmodel(inthesenseofminimizing\\ngeneralization error)isalargemodelthathasbeenregularizedappropriately .\\nWenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized\\nmodel.\\n2 2 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab020a9e-d89a-412f-a878-d724da4e634a', embedding=None, metadata={'page_label': '245', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n7.1ParameterNormPenalties\\nRegularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear\\nmodelssuchaslinearregressionandlogisticregressionallowsimple,straightforward,\\nandeﬀectiveregularizationstrategies.\\nManyregularizationapproachesarebasedonlimitingthecapacityofmodels,\\nsuchasneuralnetworks,linearregression,orlogisticregression,byaddingapa-\\nrameternormpenalty Ω(θ)totheobjectivefunction J.Wedenotetheregularized\\nobjectivefunctionby˜ J:\\n˜ J , J , α (;θXy) = (;θXy)+Ω()θ (7.1)\\nwhere α∈[0 ,∞)isahyperparameter thatweightstherelativecontributionofthe\\nnormpenaltyterm,,relativetothestandardobjectivefunction Ω J.Setting αto0\\nresultsinnoregularization. Largervaluesof αcorrespondtomoreregularization.\\nWhenourtrainingalgorithmminimizestheregularizedobjectivefunction ˜ Jit\\nwilldecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure\\nofthesizeoftheparametersθ(orsomesubsetoftheparameters).Diﬀerent\\nchoicesfortheparameternormcanresultindiﬀerentsolutionsbeingpreferred. Ω\\nInthissection,wediscusstheeﬀectsofthevariousnormswhenusedaspenalties\\nonthemodelparameters.\\nBeforedelvingintotheregularizationbehaviorofdiﬀerentnorms,wenotethat\\nforneuralnetworks,wetypicallychoosetouseaparameternormpenaltythatΩ\\npenalizes oftheaﬃnetransformationateachlayerandleaves onlytheweights\\nthebiasesunregularized. Thebiasestypicallyrequirelessdatatoﬁtaccurately\\nthantheweights.\\xa0Eachweightspeciﬁeshowtwovariablesinteract.\\xa0Fittingthe\\nweightwellrequiresobservingbothvariablesinavarietyofconditions.Each\\nbiascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch\\nvariancebyleavingthebiasesunregularized. Also,regularizingthebiasparameters\\ncanintroduceasigniﬁcantamountofunderﬁtting. Wethereforeusethevectorw\\ntoindicatealloftheweightsthatshouldbeaﬀectedbyanormpenalty,whilethe\\nvectorθdenotesalloftheparameters,includingbothwandtheunregularized\\nparameters.\\nInthecontextofneuralnetworks,itissometimesdesirabletouseaseparate\\npenaltywithadiﬀerent αcoeﬃcientforeachlayerofthenetwork.Becauseitcan\\nbeexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill\\nreasonabletousethesameweightdecayatalllayersjusttoreducethesizeof\\nsearchspace.\\n2 3 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5cfb80b5-f688-42cd-8cf8-fcba0541193f', embedding=None, metadata={'page_label': '246', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n7 . 1 . 1 L2P a ra m et e r Regu l a ri z a t i o n\\nWehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2\\nofparameternormpenalty:the L2parameternormpenaltycommonlyknownas\\nweightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1\\nbyaddingaregularizationtermΩ(θ) =1\\n2\\ue06b\\ue06bw2\\n2totheobjectivefunction.Inother\\nacademiccommunities, L2regularizationisalsoknownasridgeregressionor\\nTikhonovregularization.\\nWecangainsomeinsightintothebehaviorofweightdecayregularization\\nbystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe\\npresentation,weassumenobiasparameter,soθisjustw.Suchamodelhasthe\\nfollowingtotalobjectivefunction:\\n˜ J , (;wXy) =α\\n2w\\ue03ewwXy +( J; ,) , (7.2)\\nwiththecorrespondingparametergradient\\n∇ w˜ J , α (;wXy) = w+∇ w J , . (;wXy) (7.3)\\nTotakeasinglegradientsteptoupdatetheweights,weperformthisupdate:\\nwww ← − \\ue00f α( +∇ w J , . (;wXy)) (7.4)\\nWrittenanotherway,theupdateis:\\nww ← −(1 \\ue00f α)−∇ \\ue00f w J , . (;wXy) (7.5)\\nWecanseethattheadditionoftheweightdecaytermhasmodiﬁedthelearning\\nruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep,\\njustbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin\\nasinglestep.Butwhathappensovertheentirecourseoftraining?\\nWewillfurthersimplifytheanalysisbymakingaquadraticapproximation\\ntotheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat\\nobtainsminimalunregularized trainingcost,w∗=argminw J(w).Iftheobjective\\nfunctionistrulyquadratic,asinthecaseofﬁttingalinearregressionmodelwith\\n1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i ﬁ c p o i n t i n s p a c e\\na n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e ﬀ e c t , b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e\\nc l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f\\nt h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e . S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e\\nm o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n .\\n2 3 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f529f175-3928-4478-981b-fa347871ab13', embedding=None, metadata={'page_label': '247', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nmeansquarederror,thentheapproximationisperfect.Theapproximation ˆ Jis\\ngivenby\\nˆ J J () = θ (w∗)+1\\n2(ww−∗)\\ue03eHww (−∗) , (7.6)\\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Thereis\\nnoﬁrst-orderterminthisquadraticapproximation, becausew∗isdeﬁnedtobea\\nminimum,wherethegradientvanishes.Likewise,becausew∗isthelocationofa\\nminimumof,wecanconcludethatispositivesemideﬁnite. J H\\nTheminimumofˆ Joccurswhereitsgradient\\n∇ wˆ J() = (wHww−∗) (7.7)\\nisequalto. 0\\nTostudytheeﬀectofweightdecay,wemodifyequationbyaddingthe 7.7\\nweightdecaygradient.Wecannowsolvefortheminimumoftheregularized\\nversionofˆ J.Weusethevariable ˜wtorepresentthelocationoftheminimum.\\nα˜wH+ (˜ww−∗) = 0 (7.8)\\n(+ )H αI˜wHw = ∗(7.9)\\n˜wHI = (+ α)− 1Hw∗. (7.10)\\nAs αapproaches0,theregularizedsolution ˜wapproachesw∗.Butwhat\\nhappensas αgrows?BecauseHisrealandsymmetric,wecandecomposeit\\nintoadiagonalmatrix Λandanorthonormal basisofeigenvectors,Q,suchthat\\nHQQ = Λ\\ue03e.Applyingthedecompositiontoequation,weobtain:7.10\\n˜wQQ = ( Λ\\ue03e+ ) αI− 1QQ Λ\\ue03ew∗(7.11)\\n=\\ue068\\nQIQ (+ Λ α)\\ue03e\\ue069− 1\\nQQ Λ\\ue03ew∗(7.12)\\n= (+ )Q Λ αI− 1ΛQ\\ue03ew∗. (7.13)\\nWeseethattheeﬀectofweightdecayistorescalew∗alongtheaxesdeﬁnedby\\ntheeigenvectorsofH.Speciﬁcally,thecomponentofw∗thatisalignedwiththe\\ni-theigenvectorofHisrescaledbyafactorofλ i\\nλ i + α.(Youmaywishtoreview\\nhowthiskindofscalingworks,ﬁrstexplainedinﬁgure).2.3\\nAlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample,\\nwhere λ i\\ue01d α,theeﬀectofregularizationisrelativelysmall.However,components\\nwith λ i\\ue01c αwillbeshrunktohavenearlyzeromagnitude.Thiseﬀectisillustrated\\ninﬁgure.7.1\\n2 3 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5e58ddfe-d36d-478c-b8e1-4e29cde319ea', embedding=None, metadata={'page_label': '248', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nw 1w 2w∗\\n˜ w\\nFigure7.1:Anillustrationoftheeﬀectof L2(orweightdecay)regularizationonthevalue\\noftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized\\nobjective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At\\nthepoint˜w,thesecompetingobjectivesreachanequilibrium.Intheﬁrstdimension,the\\neigenvalueoftheHessianof Jissmall.\\xa0Theobjectivefunctiondoesnotincreasemuch\\nwhenmovinghorizontallyawayfromw∗.Becausetheobjectivefunctiondoesnotexpress\\nastrongpreferencealongthisdirection,theregularizerhasastrongeﬀectonthisaxis.\\nTheregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction\\nisverysensitivetomovementsawayfromw∗.Thecorrespondingeigenvalueislarge,\\nindicatinghighcurvature.Asaresult,weightdecayaﬀectsthepositionof w2relatively\\nlittle.\\nOnlydirectionsalongwhichtheparameterscontributesigniﬁcantlytoreducing\\ntheobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot\\ncontributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian\\ntellsusthatmovementinthisdirectionwillnotsigniﬁcantlyincreasethegradient.\\nComponentsoftheweightvectorcorrespondingtosuchunimportant directions\\naredecayedawaythroughtheuseoftheregularizationthroughouttraining.\\nSofarwehavediscussedweightdecayintermsofitseﬀectontheoptimization\\nofanabstract,general,quadraticcostfunction.Howdotheseeﬀectsrelateto\\nmachinelearninginparticular?Wecanﬁndoutbystudyinglinearregression,a\\nmodelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe\\nsamekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill\\nbeabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow\\nphrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis\\n2 3 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2aaa4b0d-f9ea-4cc0-b8da-12ccf94132ac', embedding=None, metadata={'page_label': '249', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nthesumofsquarederrors:\\n( )Xwy−\\ue03e( )Xwy− . (7.14)\\nWhenweadd L2regularization, theobjectivefunctionchangesto\\n( )Xwy−\\ue03e( )+Xwy−1\\n2αw\\ue03ew . (7.15)\\nThischangesthenormalequationsforthesolutionfrom\\nwX= (\\ue03eX)− 1X\\ue03ey (7.16)\\nto\\nwX= (\\ue03eXI+ α)− 1X\\ue03ey . (7.17)\\nThematrixX\\ue03eXinequationisproportionaltothecovariancematrix 7.161\\nmX\\ue03eX.\\nUsing L2regularizationreplacesthismatrixwith\\ue000\\nX\\ue03eXI+ α\\ue001− 1inequation.7.17\\nThenewmatrixisthesameastheoriginalone,butwiththeadditionof αtothe\\ndiagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach\\ninputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm\\nto“perceive”theinputXashavinghighervariance,whichmakesitshrinkthe\\nweightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto\\nthisaddedvariance.\\n7 . 1 . 2 L1Regu l a ri z a t i o n\\nWhile L2weightdecayisthemostcommonformofweightdecay,thereareother\\nwaystopenalizethesizeofthemodelparameters.\\xa0Anotheroptionistouse L1\\nregularization.\\nFormally, L1regularizationonthemodelparameter isdeﬁnedas:w\\nΩ() = θ ||||w 1=\\ue058\\ni| w i| , (7.18)\\nthatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill\\nnowdiscusstheeﬀectof L1regularizationonthesimplelinearregressionmodel,\\nwithnobiasparameter,thatwestudiedinouranalysisof L2regularization. In\\nparticular,weareinterestedindelineatingthediﬀerencesbetween L1and L2forms\\n2As with L2re g u l a riz a t i o n , w e c o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t\\nz e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o. In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d\\ni n t ro d u c e t h e t e rmΩ() = θ ||− w w( ) o|| 1=\\ue050\\ni| w i− w( ) o\\ni| .\\n2 3 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='937d2fe8-e77c-44a6-80e2-99d02cb0ead3', embedding=None, metadata={'page_label': '250', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nofregularization. Aswith L2weightdecay, L1weightdecaycontrolsthestrength\\noftheregularizationbyscalingthepenaltyusingapositivehyperparameter Ω α.\\nThus,theregularizedobjectivefunction ˜ J , (;wXy)isgivenby\\n˜ J , α (;wXy) = ||||w 1+(; ) JwXy , , (7.19)\\nwiththecorrespondinggradient(actually,sub-gradient):\\n∇ w˜ J , α (;wXy) = sign( )+w ∇ w J ,(Xyw;) (7.20)\\nwhere issimplythesignofappliedelement-wise. sign( )w w\\nByinspectingequation,wecanseeimmediately thattheeﬀectof 7.20 L1\\nregularizationisquitediﬀerentfromthatof L2regularization. Speciﬁcally,wecan\\nseethattheregularizationcontributiontothegradientnolongerscaleslinearly\\nwitheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One\\nconsequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean\\nalgebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2\\nregularization.\\nOursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent\\nviaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor\\nseriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient\\ninthissettingisgivenby\\n∇ wˆ J() = (wHww−∗) , (7.21)\\nwhere,again,istheHessianmatrixofwithrespecttoevaluatedat H J ww∗.\\nBecausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase\\nofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption\\nthattheHessianisdiagonal,H=diag([ H 1 1 , , . . . , H n , n]),whereeach H i , i >0.\\nThisassumptionholdsifthedataforthelinearregressionproblemhasbeen\\npreprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe\\naccomplishedusingPCA.\\nOurquadraticapproximationofthe L1regularizedobjectivefunctiondecom-\\nposesintoasumovertheparameters:\\nˆ J , J (;wXy) = (w∗; )+Xy ,\\ue058\\ni\\ue0141\\n2H i , i(w i−w∗\\ni)2+ α w| i|\\ue015\\n.(7.22)\\nTheproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution\\n(foreachdimension),withthefollowingform: i\\nw i= sign( w∗\\ni)max\\ue01a\\n| w∗\\ni|−α\\nH i , i,0\\ue01b\\n. (7.23)\\n2 3 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7a509bae-9188-4705-b01f-38aaf8ad1f8a', embedding=None, metadata={'page_label': '251', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nConsiderthesituationwhere w∗\\ni > i 0forall.Therearetwopossibleoutcomes:\\n1.Thecasewhere w∗\\ni≤α\\nH i , i.Heretheoptimalvalueof w iundertheregularized\\nobjectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,)\\ntotheregularizedobjective˜ J(w;Xy ,)isoverwhelmed—indirection i—by\\nthe L1regularizationwhichpushesthevalueof w itozero.\\n2.Thecasewhere w∗\\ni >α\\nH i , i.Inthiscase,theregularizationdoesnotmovethe\\noptimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya\\ndistanceequaltoα\\nH i , i.\\nAsimilarprocesshappenswhen w∗\\ni <0,butwiththe L1penaltymaking w iless\\nnegativebyα\\nH i , i,or0.\\nIncomparisonto L2regularization, L1regularizationresultsinasolutionthat\\nismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters\\nhaveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively\\ndiﬀerentbehaviorthanariseswith L2regularization. Equationgavethe7.13\\nsolution ˜ wfor L2regularization. Ifwerevisitthatequationusingtheassumption\\nofadiagonalandpositivedeﬁniteHessianHthatweintroducedforouranalysisof\\nL1regularization,weﬁndthat˜ w i=H i , i\\nH i , i + αw∗\\ni.If w∗\\niwasnonzero,then ˜ w iremains\\nnonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters\\ntobecomesparse,while L1regularizationmaydosoforlargeenough. α\\nThesparsitypropertyinducedby L1regularizationhasbeenusedextensively\\nasafeatureselectionmechanism.Featureselectionsimpliﬁesamachinelearning\\nproblembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In\\nparticular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995\\nselectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast\\nsquarescostfunction.The L1penaltycausesasubsetoftheweightstobecome\\nzero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded.\\nInsection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1\\nasMAPBayesianinference,andthatinparticular, L2regularizationisequivalent\\ntoMAPBayesianinferencewithaGaussianpriorontheweights.\\xa0For L1regu-\\nlarization,thepenalty αΩ(w)= α\\ue050\\ni| w i|usedtoregularizeacostfunctionis\\nequivalenttothelog-priortermthatismaximizedbyMAPBayesianinference\\nwhenthepriorisanisotropicLaplacedistribution(equation)over3.26w∈ Rn:\\nlog() = pw\\ue058\\nilogLaplace( w i;0 ,1\\nα) = −|||| αw 1+log log2 n α n− .(7.24)\\n2 3 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80abf4de-4f61-47eb-b648-85d998dabfcc', embedding=None, metadata={'page_label': '252', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nFromthepointofviewoflearningviamaximization withrespecttow,wecan\\nignorethe termsbecausetheydonotdependon. log log2 α− w\\n7.2NormPenaltiesasConstrainedOptimization\\nConsiderthecostfunctionregularizedbyaparameternormpenalty:\\n˜ J , J , α . (;θXy) = (;θXy)+Ω()θ (7.25)\\nRecallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4\\nbyconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective\\nfunctionplusasetofpenalties.Eachpenaltyisaproductbetweenacoeﬃcient,\\ncalledaKarush–Kuhn–Tucker(KKT)multiplier,andafunctionrepresenting\\nwhethertheconstraintissatisﬁed.IfwewantedtoconstrainΩ(θ)tobelessthan\\nsomeconstant,wecouldconstructageneralizedLagrangefunction k\\nL − (; ) = (; )+(Ω() θ , αXy , JθXy , αθ k .) (7.26)\\nThesolutiontotheconstrainedproblemisgivenby\\nθ∗= argmin\\nθmax\\nα , α≥ 0L()θ , α . (7.27)\\nAsdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 θ\\nand α.Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2\\nconstraint.Manydiﬀerentproceduresarepossible—somemayusegradientdescent,\\nwhileothersmayuseanalyticalsolutionsforwherethegradientiszero—butinall\\nprocedures αmustincreasewheneverΩ(θ) > kanddecreasewheneverΩ(θ) < k.\\nAllpositive αencourage Ω(θ)toshrink.Theoptimalvalue α∗willencourage Ω(θ)\\ntoshrink,butnotsostronglytomakebecomelessthan. Ω()θ k\\nTogainsomeinsightintotheeﬀectoftheconstraint,wecanﬁx α∗andview\\ntheproblemasjustafunctionof:θ\\nθ∗= argmin\\nθL(θ , α∗) = argmin\\nθJ , α (;θXy)+∗Ω()θ .(7.28)\\nThisisexactlythesameastheregularizedtrainingproblemofminimizing ˜ J.\\nWecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe\\nweights.Ifisthe Ω L2norm,thentheweightsareconstrainedtolieinan L2\\nball.\\xa0Ifisthe Ω L1norm,thentheweightsareconstrainedtolieinaregionof\\n2 3 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6147c64c-2082-4391-aba2-ca83e702af5a', embedding=None, metadata={'page_label': '253', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nlimited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe\\nimposebyusingweightdecaywithcoeﬃcient α∗becausethevalueof α∗doesnot\\ndirectlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship\\nbetween kand α∗dependsontheformof J.Whilewedonotknowtheexactsize\\noftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing α\\ninordertogroworshrinktheconstraintregion.Larger αwillresultinasmaller\\nconstraintregion.Smallerwillresultinalargerconstraintregion. α\\nSometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As\\ndescribedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4\\ndescenttotakeastepdownhillon J(θ)andthenprojectθbacktothenearest\\npointthatsatisﬁesΩ(θ) < k.Thiscanbeusefulifwehaveanideaofwhatvalue\\nof kisappropriateanddonotwanttospendtimesearchingforthevalueof αthat\\ncorrespondstothis. k\\nAnotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing\\nconstraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization\\nprocedurestogetstuckinlocalminimacorrespondingtosmallθ.Whentraining\\nneuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral\\n“deadunits.”Theseareunitsthatdonotcontributemuchtothebehaviorofthe\\nfunctionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare\\nallverysmall.\\xa0Whentrainingwithapenaltyonthenormoftheweights,these\\nconﬁgurations canbelocallyoptimal,evenifitispossibletosigniﬁcantlyreduce\\nJbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection\\ncanworkmuchbetterinthesecasesbecausetheydonotencouragetheweights\\ntoapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly\\nhaveaneﬀectwhentheweightsbecomelargeandattempttoleavetheconstraint\\nregion.\\nFinally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose\\nsomestabilityontheoptimization procedure.Whenusinghighlearningrates,it\\nispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce\\nlargegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates\\nconsistentlyincreasethesizeoftheweights,thenθrapidlymovesawayfrom\\ntheoriginuntilnumericaloverﬂowoccurs.Explicitconstraintswithreprojection\\npreventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights\\nwithoutbound. ()recommendusingconstraintscombinedwith Hintonetal.2012c\\nahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining\\nsomestability.\\nInparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro\\nandShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix\\n2 3 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd840e05-136f-4c9f-abfc-a96d006b97ef', embedding=None, metadata={'page_label': '254', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire\\nweightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone\\nhiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa\\npenaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha\\nseparateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT\\nmultiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit\\nobeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas\\nanexplicitconstraintwithreprojection.\\n7.3RegularizationandUnder-ConstrainedProblems\\nInsomecases,regularizationisnecessaryformachinelearningproblemstobeprop-\\nerlydeﬁned.Manylinearmodelsinmachinelearning,includinglinearregression\\nandPCA,dependoninvertingthematrixX\\ue03eX.Thisisnotpossiblewhenever\\nX\\ue03eXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri-\\nbutiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin\\nsomedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures\\n(columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting\\nX\\ue03eXI+ αinstead.Thisregularizedmatrixisguaranteedtobeinvertible.\\nTheselinearproblemshaveclosedformsolutionswhentherelevantmatrix\\nisinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe\\nunderdetermined. Anexampleislogisticregressionappliedtoaproblemwhere\\ntheclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect\\nclassiﬁcation,then2wwillalsoachieveperfectclassiﬁcationandhigherlikelihood.\\nAniterativeoptimization procedurelikestochasticgradientdescentwillcontinually\\nincreasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical\\nimplementationofgradientdescentwilleventuallyreachsuﬃcientlylargeweights\\ntocausenumericaloverﬂow,atwhichpointitsbehaviorwilldependonhowthe\\nprogrammerhasdecidedtohandlevaluesthatarenotrealnumbers.\\nMostformsofregularizationareabletoguaranteetheconvergenceofiterative\\nmethodsappliedtounderdetermined problems.\\xa0Forexample,weightdecaywill\\ncausegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe\\nslopeofthelikelihoodisequaltotheweightdecaycoeﬃcient.\\nTheideaofusingregularizationtosolveunderdetermined problemsextends\\nbeyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra\\nproblems.\\nAswesawinsection,wecansolveunderdetermined linearequationsusing 2.9\\n2 3 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e42e1655-06ed-41a9-90c1-61c6b0a1ca7b', embedding=None, metadata={'page_label': '255', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ntheMoore-Penrosepseudoinverse.Recallthatonedeﬁnitionofthepseudoinverse\\nX+ofamatrixisX\\nX+=lim\\nα\\ue026 0(X\\ue03eXI+ α)− 1X\\ue03e. (7.29)\\nWecannowrecognizeequationasperforminglinearregressionwithweight 7.29\\ndecay.Speciﬁcally,equationisthelimitofequationastheregularization 7.29 7.17\\ncoeﬃcientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing\\nunderdetermined problemsusingregularization.\\n7.4DatasetAugmentation\\nThebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton\\nmoredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway\\ntogetaroundthisproblemistocreatefakedataandaddittothetrainingset.\\nForsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew\\nfakedata.\\nThisapproachiseasiestforclassiﬁcation.Aclassiﬁerneedstotakeacompli-\\ncated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y.\\nThismeansthatthemaintaskfacingaclassiﬁeristobeinvarianttoawidevariety\\noftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming\\ntheinputsinourtrainingset. x\\nThisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it\\nisdiﬃculttogeneratenewfakedataforadensityestimationtaskunlesswehave\\nalreadysolvedthedensityestimationproblem.\\nDatasetaugmentationhasbeenaparticularlyeﬀectivetechniqueforaspeciﬁc\\nclassiﬁcationproblem:objectrecognition.Imagesarehighdimensionalandinclude\\nanenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated.\\nOperationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan\\noftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto\\nbepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques\\ndescribedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9\\ntheimagehavealsoprovenquiteeﬀective.\\nOnemustbecarefulnottoapplytransformationsthatwouldchangethecorrect\\nclass.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe\\ndiﬀerencebetween‘b’and‘d’andthediﬀerencebetween‘6’and‘9’,sohorizontal\\nﬂipsand180◦rotationsarenotappropriatewaysofaugmentingdatasetsforthese\\ntasks.\\n2 4 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b5611034-aaa6-47f4-a042-d3154e0c7f46', embedding=None, metadata={'page_label': '256', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nTherearealsotransformationsthatwewouldlikeourclassiﬁerstobeinvariant\\nto,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot\\nbeimplementedasasimplegeometricoperationontheinputpixels.\\nDatasetaugmentationiseﬀectiveforspeechrecognitiontasksaswell(Jaitly\\nandHinton2013,).\\nInjectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,)\\ncanalsobeseenasaformofdataaugmentation.Formanyclassiﬁcationand\\nevensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall\\nrandomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust\\ntonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness\\nofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir\\ninputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch\\nasthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks\\nwhenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset\\naugmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed\\nthatthisapproachcanbehighlyeﬀectiveprovidedthatthemagnitudeofthe\\nnoiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe\\ndescribedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12\\nmultiplyingbynoise.\\nWhencomparingmachinelearningbenchmarkresults,itisimportanttotake\\ntheeﬀectofdatasetaugmentationintoaccount.Often,hand-designeddataset\\naugmentationschemescandramaticallyreducethegeneralization errorofamachine\\nlearningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm\\ntoanother,itisnecessarytoperformcontrolledexperiments.Whencomparing\\nmachinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary\\ntomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed\\ndatasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith\\nnodatasetaugmentationandalgorithmBperformswellwhencombinedwith\\nnumeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe\\nsynthetictransformationscausedtheimprovedperformance,ratherthantheuse\\nofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment\\nhasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine\\nlearningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset\\naugmentation.Usually,operationsthataregenerallyapplicable(suchasadding\\nGaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm,\\nwhileoperationsthatarespeciﬁctooneapplicationdomain(suchasrandomly\\ncroppinganimage)areconsideredtobeseparatepre-processingsteps.\\n2 4 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6cc75f72-254d-4343-bbe7-83d69c6b36ff', embedding=None, metadata={'page_label': '257', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n7.5NoiseRobustness\\nSectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4\\naugmentationstrategy.Forsomemodels,theadditionofnoisewithinﬁnitesimal\\nvarianceattheinputofthemodelisequivalenttoimposingapenaltyonthe\\nnormoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab\\nrememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking\\ntheparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise\\nappliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate\\ndiscussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12\\nofthatapproach.\\nAnotherwaythatnoisehasbeenusedintheserviceofregularizingmodels\\nisbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe\\ncontextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,).\\xa0Thiscan\\nbeinterpretedasa\\xa0stochasticimplementation of\\xa0Bayesianinference\\xa0overthe\\nweights.\\xa0TheBayesiantreatmentoflearningwouldconsiderthemodelweights\\ntobeuncertainandrepresentableviaaprobabilitydistributionthatreﬂectsthis\\nuncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreﬂect\\nthisuncertainty.\\nNoiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome\\nassumptions)toamoretraditionalformofregularization, encouragingstabilityof\\nthefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain\\nafunction ˆ y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares\\ncostfunctionbetweenthemodelpredictions ˆ y()xandthetruevalues: y\\nJ= E p x , y ( )\\ue002(ˆ y y ()x−)2\\ue003\\n. (7.30)\\nThetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}.\\nWenowassumethatwitheachinputpresentationwealsoincludearandom\\nperturbation \\ue00f W∼N(\\ue00f; 0 , ηI)ofthenetworkweights.Letusimaginethatwe\\nhaveastandard l-layerMLP.Wedenotetheperturbedmodelasˆ y \\ue00f W(x).Despite\\ntheinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe\\noutputofthenetwork.Theobjectivefunctionthusbecomes:\\n˜ J W= E p , y , ( x \\ue00f W )\\ue068\\n(ˆ y \\ue00f W() )x− y2\\ue069\\n(7.31)\\n= E p , y , ( x \\ue00f W )\\ue002\\nˆ y2\\n\\ue00f W()2ˆx− y y \\ue00f W()+x y2\\ue003\\n.(7.32)\\nForsmall η,theminimization of Jwithaddedweightnoise(withcovariance\\nηI)isequivalenttominimization of Jwithanadditionalregularizationterm:\\n2 4 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='51c10971-f8ee-4594-9a27-97b3fa573614', embedding=None, metadata={'page_label': '258', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nη E p , y ( x )\\ue002\\ue06b∇ Wˆ y()x\\ue06b2\\ue003\\n.Thisformofregularizationencouragestheparametersto\\ngotoregionsofparameterspacewheresmallperturbationsoftheweightshave\\narelativelysmallinﬂuenceontheoutput.Inotherwords,itpushesthemodel\\nintoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe\\nweights,ﬁndingpointsthatarenotmerelyminima,butminimasurroundedby\\nﬂatregions(HochreiterandSchmidhuber1995,).Inthesimpliﬁedcaseoflinear\\nregression(where,forinstance, ˆ y(x) =w\\ue03ex+ b),thisregularizationtermcollapses\\ninto η E p ( ) x\\ue002\\n\\ue06b\\ue06bx2\\ue003\\n,whichisnotafunctionofparametersandthereforedoesnot\\ncontributetothegradientof˜ J Wwithrespecttothemodelparameters.\\n7 . 5 . 1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s\\nMostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto\\nmaximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly\\nmodelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall\\nconstant \\ue00f,thetrainingsetlabel yiscorrectwithprobability 1− \\ue00f,andotherwise\\nanyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto\\nincorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing\\nnoisesamples.Forexample,labelsmoothingregularizesamodelbasedona\\nsoftmaxwith koutputvaluesbyreplacingthehardandclassiﬁcationtargets 0 1\\nwithtargetsof\\ue00f\\nk− 1and1− \\ue00f,respectively.Thestandardcross-entropylossmay\\nthenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax\\nclassiﬁerandhardtargetsmayactuallyneverconverge—thesoftmaxcannever\\npredictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1\\nandlargerweights,makingmoreextremepredictionsforever.Itispossibleto\\npreventthisscenariousingotherregularizationstrategieslikeweightdecay.Label\\nsmoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout\\ndiscouragingcorrectclassiﬁcation.Thisstrategyhasbeenusedsincethe1980s\\nandcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy\\netal.,).2015\\n7.6Semi-SupervisedLearning\\nIntheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x)\\nandlabeledexamplesfrom P( x y ,)areusedtoestimate P( y x|)orpredict yfrom\\nx.\\nInthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto\\nlearningarepresentationh= f(x) .Thegoalistolearnarepresentationso\\n2 4 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e1ea5569-2e22-44e5-a38a-168184ddea0c', embedding=None, metadata={'page_label': '259', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nthatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised\\nlearningcanprovideusefulcuesforhowtogroupexamplesinrepresentation\\nspace.Examplesthatclustertightlyintheinputspaceshouldbemappedto\\nsimilarrepresentations.Alinearclassiﬁerinthenewspacemayachievebetter\\ngeneralization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A\\nlong-standingvariantofthisapproachistheapplicationofprincipalcomponents\\nanalysisasapre-processingstepbeforeapplyingaclassiﬁer(ontheprojected\\ndata).\\nInsteadofhavingseparateunsupervisedandsupervisedcomponentsinthe\\nmodel,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or\\nP( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan\\nthentrade-oﬀthesupervisedcriterion −log P( y x|)withtheunsupervisedor\\ngenerativeone(suchas−log P( x)or−log P( x y ,)).Thegenerativecriterionthen\\nexpressesaparticularformofpriorbeliefaboutthesolutiontothesupervised\\nlearningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is\\nconnectedtothestructureof P( y x|)inawaythatiscapturedbytheshared\\nparametrization. Bycontrollinghowmuchofthegenerativecriterionisincluded\\ninthetotalcriterion,onecanﬁndabettertrade-oﬀthanwithapurelygenerative\\norapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand\\nBengio2008,).\\nSalakhutdinovandHinton2008()describeamethodforlearningthekernel\\nfunctionofakernelmachineusedforregression,inwhichtheusageofunlabeled\\nexamplesformodeling improvesquitesigniﬁcantly. P() x P( ) y x|\\nSee ()formoreinformationaboutsemi-supervisedlearning. Chapelle etal.2006\\n7.7Multi-TaskLearning\\nMulti-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993\\ntheexamples(whichcanbeseenassoftconstraintsimposedontheparameters)\\narisingoutofseveraltasks.\\xa0Inthesamewaythatadditionaltrainingexamples\\nputmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize\\nwell,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore\\nconstrainedtowardsgoodvalues(assumingthesharingisjustiﬁed),oftenyielding\\nbettergeneralization.\\nFigureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2\\ndiﬀerentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell\\nassomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof\\n2 4 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='17276e47-d909-4498-b7e4-50530a5b73a2', embedding=None, metadata={'page_label': '260', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nfactors.Themodelcangenerallybedividedintotwokindsofpartsandassociated\\nparameters:\\n1.Task-speciﬁcparameters(whichonlybeneﬁtfromtheexamplesoftheirtask\\ntoachievegoodgeneralization). Thesearetheupperlayersoftheneural\\nnetworkinﬁgure.7.2\\n2.Genericparameters,sharedacrossallthetasks(whichbeneﬁtfromthe\\npooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork\\ninﬁgure.7.2\\nh( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 )\\nh( s h a r e d )h( s h a r e d )\\nxx\\nFigure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks\\nandthisﬁgureillustratesthecommonsituationwherethetasksshareacommoninputbut\\ninvolvediﬀerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit\\nissupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows)\\ncanbesharedacrosssuchtasks,whiletask-speciﬁcparameters(associatedrespectively\\nwiththeweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga\\nsharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon\\npooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated\\nwithasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level\\nhiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and\\ny(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In\\ntheunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe\\nassociatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof\\ntheinputvariationsbutarenotrelevantforpredicting y(1)or y(2).\\nImprovedgeneralization andgeneralization errorbounds(,)canbe Baxter1995\\nachievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe\\n2 4 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52f5752a-9129-4d91-b127-77250b01902a', embedding=None, metadata={'page_label': '261', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n0 50 100 150 200 250\\nTime(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s\\nV a l i d a t i o n s e t l o s s\\nFigure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover\\ntime(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis\\nexample,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective\\ndecreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto\\nincreaseagain,forminganasymmetricU-shapedcurve.\\ngreatlyimproved(inproportionwiththeincreasednumberofexamplesforthe\\nsharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis\\nwillhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween\\nthediﬀerenttasksarevalid,meaningthatthereissomethingsharedacrosssome\\nofthetasks.\\nFromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe\\nfollowing:amongthefactorsthat\\xa0explainthevariations\\xa0observed\\xa0inthedata\\nassociatedwiththediﬀerenttasks,somearesharedacrosstwoormoretasks.\\n7.8EarlyStopping\\nWhentraininglargemodelswithsuﬃcientrepresentationalcapacitytooverﬁt\\nthetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but\\nvalidationseterrorbeginstoriseagain.Seeﬁgureforanexampleofthis 7.3\\nbehavior.Thisbehavioroccursveryreliably.\\nThismeanswecanobtainamodelwithbettervalidationseterror(andthus,\\nhopefullybettertestseterror)byreturningtotheparametersettingatthepointin\\ntimewiththelowestvalidationseterror.Everytimetheerroronthevalidationset\\nimproves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm\\nterminates,wereturntheseparameters,ratherthanthelatestparameters.The\\n2 4 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='788ab0e0-d0af-465a-a1f5-1e87551381aa', embedding=None, metadata={'page_label': '262', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nalgorithmterminateswhennoparametershaveimprovedoverthebestrecorded\\nvalidationerrorforsomepre-speciﬁednumberofiterations.Thisprocedureis\\nspeciﬁedmoreformallyinalgorithm .7.1\\nAlgorithm\\xa07.1Theearlystopping\\xa0meta-algorithmfor\\xa0determiningthe\\xa0best\\namountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks\\nwellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe\\nvalidationset.\\nLetbethenumberofstepsbetweenevaluations. n\\nLet pbethe“patience,”thenumberoftimestoobserveworseningvalidationset\\nerrorbeforegivingup.\\nLetθ obetheinitialparameters.\\nθθ← o\\ni←0\\nj←0\\nv←∞\\nθ∗←θ\\ni∗← i\\nwhiledo j < p\\nUpdatebyrunningthetrainingalgorithmforsteps. θ n\\ni i n ←+\\nv\\ue030←ValidationSetError ()θ\\nif v\\ue030< vthen\\nj←0\\nθ∗←θ\\ni∗← i\\nv v←\\ue030\\nelse\\nj j←+1\\nendif\\nendwhile\\nBestparametersareθ∗,bestnumberoftrainingstepsis i∗\\nThisstrategyisknownasearlystopping.Itisprobablythemostcommonly\\nusedformofregularizationindeeplearning.Itspopularityisduebothtoits\\neﬀectivenessanditssimplicity.\\nOnewaytothinkofearlystoppingisasaveryeﬃcienthyperparameter selection\\nalgorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter.\\nWecanseeinﬁgurethatthishyperparameter hasaU-shapedvalidationset 7.3\\n2 4 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='366820ec-a28d-4584-9971-d85e878c23ea', embedding=None, metadata={'page_label': '263', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nperformancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha\\nU-shapedvalidationsetperformancecurve,asillustratedinﬁgure.Inthecaseof 5.3\\nearlystopping,wearecontrollingtheeﬀectivecapacityofthemodelbydetermining\\nhowmanystepsitcantaketoﬁtthetrainingset.Mosthyperparametersmustbe\\nchosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter\\natthestartoftraining,thenruntrainingforseveralstepstoseeitseﬀect.The\\n“trainingtime”\\xa0hyperparam eterisuniqueinthatbydeﬁnitionasinglerunof\\ntrainingtriesoutmanyvaluesofthehyperparameter.Theonlysigniﬁcantcost\\ntochoosingthishyperparameter automatically viaearlystoppingisrunningthe\\nvalidationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein\\nparalleltothetrainingprocessonaseparatemachine,separateCPU,orseparate\\nGPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe\\ncostoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis\\nsmallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless\\nfrequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime.\\nAnadditionalcosttoearlystoppingistheneedtomaintainacopyofthe\\nbestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore\\ntheseparametersinaslowerandlargerformofmemory(forexample,trainingin\\nGPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk\\ndrive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring\\ntraining,theseoccasionalslowwriteshavelittleeﬀectonthetotaltrainingtime.\\nEarlystoppingisaveryunobtrusiveformofregularization, inthatitrequires\\nalmostnochangeintheunderlyingtrainingprocedure,theobjectivefunction,\\northesetofallowableparametervalues.Thismeansthatitiseasytouseearly\\nstoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight\\ndecay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe\\nnetworkinabadlocalminimumcorrespondingtoasolutionwithpathologically\\nsmallweights.\\nEarlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza-\\ntionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective\\nfunctiontoencouragebettergeneralization, itisrareforthebestgeneralization to\\noccuratalocalminimumofthetrainingobjective.\\nEarlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot\\nfedtothemodel.Tobestexploitthisextradata,onecanperformextratraining\\naftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra\\ntrainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies\\nonecanuseforthissecondtrainingprocedure.\\nOnestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2\\n2 4 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f291f4c7-393b-49fc-b98e-89dc5716513e', embedding=None, metadata={'page_label': '264', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas\\ntheearlystoppingproceduredeterminedwasoptimalintheﬁrstpass.Thereare\\nsomesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood\\nwayofknowingwhethertoretrainforthesamenumberofparameterupdatesor\\nthesamenumberofpassesthroughthedataset.Onthesecondroundoftraining,\\neachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe\\ntrainingsetisbigger.\\nAlgorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong\\ntotrain,thenretrainingonallthedata.\\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\\nrespectively.\\nRunearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand\\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\\nreturns i∗,theoptimalnumberofsteps.\\nSettorandomvaluesagain. θ\\nTrainonX( ) t r a i nandy( ) t r a i nfor i∗steps.\\nAnotherstrategyforusingallofthedataistokeeptheparametersobtained\\nfromtheﬁrstroundoftrainingandthencontinuetrainingbutnowusingallof\\nthedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms\\nofanumberofsteps.\\xa0Instead,wecanmonitortheaveragelossfunctiononthe\\nvalidationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining\\nsetobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids\\nthehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For\\nexample,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill\\neverreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate.\\nThisprocedureispresentedmoreformallyinalgorithm .7.3\\nEarlystoppingisalsousefulbecauseitreducesthecomputational costofthe\\ntrainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber\\noftrainingiterations,italsohasthebeneﬁtofprovidingregularizationwithout\\nrequiringtheadditionofpenaltytermstothecostfunctionorthecomputationof\\nthegradientsofsuchadditionalterms.\\nHowearlystoppingactsasaregularizer:Sofarwehavestatedthatearly\\nstoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is\\nshowinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What\\n2 4 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bd1d0cb5-b685-41ff-ad38-4340dd502591', embedding=None, metadata={'page_label': '265', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nAlgorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec-\\ntivevaluewestarttooverﬁt,thencontinuetraininguntilthatvalueisreached.\\nLetX( ) t r a i nandy( ) t r a i nbethetrainingset.\\nSplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d ))\\nrespectively.\\nRunearlystopping(algorithm )startingfromrandom 7.1 θusingX( ) s ubtr a i nand\\ny( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This\\nupdates.θ\\n\\ue00f J , ←(θX( ) s ubtr a i n,y( ) s ubtr a i n)\\nwhile J ,(θX( v a l i d ),y( v a l i d )) > \\ue00fdo\\nTrainonX( ) t r a i nandy( ) t r a i nforsteps. n\\nendwhile\\nistheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop\\n()and ()arguedthatearlystoppinghastheeﬀectof 1995aSjöbergandLjung1995\\nrestrictingtheoptimization proceduretoarelativelysmallvolumeofparameter\\nspaceintheneighborhoodoftheinitialparametervalueθ o,asillustratedin\\nﬁgure.Morespeciﬁcally,imaginetaking 7.4 τoptimization steps(corresponding\\nto τtrainingiterations)andwithlearningrate \\ue00f.Wecanviewtheproduct \\ue00f τ\\nasameasureofeﬀectivecapacity.Assumingthegradientisbounded,restricting\\nboththenumberofiterationsandthelearningratelimitsthevolumeofparameter\\nspacereachablefromθ o.Inthissense, \\ue00f τbehavesasifitwerethereciprocalof\\nthecoeﬃcientusedforweightdecay.\\nIndeed,wecanshowhow—inthecaseofasimplelinearmodelwithaquadratic\\nerrorfunctionandsimplegradientdescent—earlystoppingisequivalentto L2\\nregularization.\\nInordertocomparewithclassical L2regularization, weexamineasimple\\nsettingwheretheonlyparametersarelinearweights(θ=w).Wecanmodel\\nthecostfunction Jwithaquadraticapproximationintheneighborhoodofthe\\nempiricallyoptimalvalueoftheweightsw∗:\\nˆ J J () = θ (w∗)+1\\n2(ww−∗)\\ue03eHww (−∗) , (7.33)\\nwhereHistheHessianmatrixof Jwithrespecttowevaluatedatw∗.Giventhe\\nassumptionthatw∗isaminimumof J(w),weknowthatHispositivesemideﬁnite.\\nUnderalocalTaylorseriesapproximation,thegradientisgivenby:\\n∇ wˆ J() = (wHww−∗) . (7.34)\\n2 5 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b35cca9-d06b-492d-a699-561d57adadbe', embedding=None, metadata={'page_label': '266', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nw 1w 2w∗\\n˜ w\\nw 1w 2w∗\\n˜ w\\nFigure7.4:Anillustrationoftheeﬀectofearlystopping. ( L e f t )Thesolidcontourlines\\nindicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory\\ntakenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw∗that\\nminimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpoint˜w.\\n( R i g h t )Anillustrationoftheeﬀectof L2regularizationforcomparison.Thedashedcircles\\nindicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie\\nnearertheoriginthantheminimumoftheunregularizedcost.\\nWearegoingtostudythetrajectoryfollowedbytheparametervectorduring\\ntraining.Forsimplicity,letussettheinitialparametervectortotheorigin,3that\\nisw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby\\nanalyzinggradientdescentonˆ J:\\nw( ) τ= w( 1 ) τ−−∇ \\ue00f wˆ J(w( 1 ) τ−) (7.35)\\n= w( 1 ) τ−− \\ue00fHw(( 1 ) τ−−w∗) (7.36)\\nw( ) τ−w∗= ( )(IH− \\ue00fw( 1 ) τ−−w∗) . (7.37)\\nLetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting\\ntheeigendecompositionofH:H=QQ Λ\\ue03e,where ΛisadiagonalmatrixandQ\\nisanorthonormalbasisofeigenvectors.\\nw( ) τ−w∗= (IQQ − \\ue00f Λ\\ue03e)(w( 1 ) τ−−w∗)(7.38)\\nQ\\ue03e(w( ) τ−w∗) = ( )I− \\ue00f ΛQ\\ue03e(w( 1 ) τ−−w∗) (7.39)\\n3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e\\na l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n . Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 . 2\\ni n i t i a l v a l u e w( 0 ).\\n2 5 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='508bb5b3-2cdd-416d-8fe7-3ad0533b81fc', embedding=None, metadata={'page_label': '267', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nAssumingthatw( 0 )=0andthat \\ue00fischosentobesmallenoughtoguarantee\\n|1− \\ue00f λ i| <1,theparametertrajectoryduringtrainingafter τparameterupdates\\nisasfollows:\\nQ\\ue03ew( ) τ= [ ( )I−I− \\ue00f Λτ]Q\\ue03ew∗. (7.40)\\nNow,theexpressionforQ\\ue03e˜winequationfor7.13 L2regularizationcanberear-\\nrangedas:\\nQ\\ue03e˜wI = (+ Λ α)− 1ΛQ\\ue03ew∗(7.41)\\nQ\\ue03e˜wII = [−(+ Λ α)− 1α]Q\\ue03ew∗(7.42)\\nComparingequationandequation,weseethatifthehyperparameters 7.40 7.42 \\ue00f,\\nα τ,andarechosensuchthat\\n( )I− \\ue00f Λτ= (+ ) Λ αI− 1α , (7.43)\\nthen L2regularizationandearlystoppingcanbeseentobeequivalent(atleast\\nunderthequadraticapproximation oftheobjectivefunction).Goingevenfurther,\\nbytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude\\nthatifall λ iaresmall(thatis, \\ue00f λ i\\ue01c1and λ i /α\\ue01c1)then\\nτ≈1\\n\\ue00f α, (7.44)\\nα≈1\\nτ \\ue00f. (7.45)\\nThatis,undertheseassumptions,thenumberoftrainingiterations τplaysarole\\ninverselyproportionaltothe L2regularizationparameter,andtheinverseof τ \\ue00f\\nplaystheroleoftheweightdecaycoeﬃcient.\\nParametervaluescorrespondingtodirectionsofsigniﬁcantcurvature(ofthe\\nobjectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse,\\ninthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond\\ntodirectionsofsigniﬁcantcurvaturetendtolearnearlyrelativetoparameters\\ncorrespondingtodirectionsoflesscurvature.\\nThederivationsinthissectionhaveshownthatatrajectoryoflength τends\\natapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early\\nstoppingisofcoursemorethanthemererestrictionofthetrajectorylength;\\ninstead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin\\nordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping\\nthereforehastheadvantageoverweightdecaythatearlystoppingautomatically\\ndeterminesthecorrectamountofregularizationwhileweightdecayrequiresmany\\ntrainingexperimentswithdiﬀerentvaluesofitshyperparameter.\\n2 5 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f3d8e1c2-7951-4f37-84fd-ecb20d29ea10', embedding=None, metadata={'page_label': '268', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n7.9ParameterTyingandParameterSharing\\nThusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties\\ntotheparameters,wehavealwaysdonesowithrespecttoaﬁxedregionorpoint.\\nForexample, L2regularization(orweightdecay)penalizesmodelparametersfor\\ndeviatingfromtheﬁxedvalueofzero.However,sometimeswemayneedother\\nwaystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters.\\nSometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake\\nbutweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere\\nshouldbesomedependencies betweenthemodelparameters.\\nAcommontypeofdependencythatweoftenwanttoexpressisthatcertain\\nparametersshouldbeclosetooneanother.Considerthefollowingscenario:we\\nhavetwomodelsperformingthesameclassiﬁcationtask(withthesamesetof\\nclasses)butwithsomewhatdiﬀerentinputdistributions.Formally,wehavemodel\\nAwithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels\\nmaptheinput\\xa0totwo\\xa0diﬀerent,\\xa0but\\xa0related outputs:ˆ y( ) A= f(w( ) A,x)and\\nˆ y( ) B= ( gw( ) B,x).\\nLetusimaginethatthetasksaresimilarenough(perhapswithsimilarinput\\nandoutputdistributions)thatwebelievethemodelparametersshouldbeclose\\ntoeachother: ∀ i, w( ) A\\nishouldbecloseto w( ) B\\ni.Wecanleveragethisinformation\\nthroughregularization. Speciﬁcally,wecanuseaparameternormpenaltyofthe\\nform: Ω(w( ) A,w( ) B)=\\ue06bw( ) A−w( ) B\\ue06b2\\n2.\\xa0Hereweusedan L2penalty,butother\\nchoicesarealsopossible.\\nThiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006\\ntheparametersofonemodel,trainedasaclassiﬁerinasupervisedparadigm,to\\nbeclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm\\n(tocapturethedistributionoftheobservedinputdata).Thearchitectures were\\nconstructedsuchthatmanyoftheparametersintheclassiﬁermodelcouldbe\\npairedtocorrespondingparametersintheunsupervisedmodel.\\nWhileaparameternormpenaltyisonewaytoregularizeparameterstobe\\nclosetooneanother,themorepopularwayistouseconstraints:toforcesets\\nofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas\\nparametersharing,becauseweinterpretthevariousmodelsormodelcomponents\\nassharingauniquesetofparameters.Asigniﬁcantadvantageofparametersharing\\noverregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya\\nsubsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain\\nmodels—suchastheconvolutionalneuralnetwork—thiscanleadtosigniﬁcant\\nreductioninthememoryfootprintofthemodel.\\n2 5 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='162c4c0a-9085-407e-9a55-fe05c66dd5ca', embedding=None, metadata={'page_label': '269', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse\\nofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied\\ntocomputervision.\\nNaturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation.\\nForexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel\\ntotheright.CNNstakethispropertyintoaccountbysharingparametersacross\\nmultipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights)\\niscomputedoverdiﬀerentlocationsintheinput.Thismeansthatwecanﬁnda\\ncatwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn\\ni+1intheimage.\\nParametersharinghasallowedCNNstodramaticallylowerthenumberofunique\\nmodelparametersandtosigniﬁcantlyincreasenetworksizeswithoutrequiringa\\ncorrespondingincreaseintrainingdata.\\xa0Itremainsoneofthebestexamplesof\\nhowtoeﬀectivelyincorporatedomainknowledgeintothenetworkarchitecture.\\nCNNswillbediscussedinmoredetailinchapter.9\\n7.10SparseRepresentations\\nWeightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another\\nstrategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork,\\nencouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated\\npenaltyonthemodelparameters.\\nWehave\\xa0alreadydiscussed\\xa0(insection)how7.1.2 L1penalizationinduces\\nasparseparametrization—meaning thatmanyoftheparametersbecomezero\\n(orcloseto\\xa0zero).Representationalsparsity,\\xa0on\\xa0theother\\xa0hand,\\xa0des cribesa\\nrepresentationwheremanyoftheelementsoftherepresentationarezero(orclose\\ntozero).Asimpliﬁedviewofthisdistinctioncanbeillustratedinthecontextof\\nlinearregression:\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f018\\n5\\n15\\n−9\\n−3\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0400 20 0 −\\n00 10 3 0 −\\n050 0 0 0\\n100 10 4 − −\\n100 0 50 −\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f02\\n3\\n−2\\n−5\\n1\\n4\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\ny∈ RmA∈ Rm n×x∈ Rn(7.46)\\n2 5 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='46edf918-5018-4092-a184-c49b32a5fdc2', embedding=None, metadata={'page_label': '270', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−14\\n1\\n19\\n2\\n23\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb=\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f03 12 54 1 − −\\n4 2 3 11 3 − −\\n− − − 15 4 2 3 2\\n3 1 2 30 3 − −\\n− − − − 54 22 5 1\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f00\\n2\\n0\\n0\\n−3\\n0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\ny∈ RmB∈ Rm n×h∈ Rn(7.47)\\nIntheﬁrstexpression,wehaveanexampleofasparselyparametrized linear\\nregressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta-\\ntionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents\\ntheinformationpresentin,butdoessowithasparsevector. x\\nRepresentationalregularizationisaccomplishedbythesamesortsofmechanisms\\nthatwehaveusedinparameterregularization.\\nNormpenaltyregularizationofrepresentationsisperformedbyaddingtothe\\nlossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted\\nΩ()h.Asbefore,wedenotetheregularizedlossfunctionby˜ J:\\n˜ J , J , α (;θXy) = (;θXy)+Ω()h (7.48)\\nwhere α∈[0 ,∞)weightstherelativecontributionofthenormpenaltyterm,with\\nlargervaluesofcorrespondingtomoreregularization. α\\nJustasan L1penaltyontheparametersinducesparametersparsity,an L1\\npenaltyontheelementsoftherepresentationinducesrepresentationalsparsity:\\nΩ(h) =||||h 1=\\ue050\\ni| h i|.\\xa0Ofcourse,the L1penaltyisonlyonechoiceofpenalty\\nthatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom\\naStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011\\nandKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008\\nusefulforrepresentationswithelementsconstrainedtolieontheunitinterval.\\nLee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies\\nbasedonregularizingtheaverageactivationacrossseveralexamples,1\\nm\\ue050\\nih( ) i,to\\nbenearsometargetvalue,suchasavectorwith.01foreachentry.\\nOtherapproachesobtainrepresentationalsparsitywithahardconstrainton\\ntheactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal.,\\n1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained\\noptimization problem\\nargmin\\nh h ,\\ue06b\\ue06b 0 < k\\ue06b− \\ue06bxWh2, (7.49)\\nwhere \\ue06b\\ue06bh 0isthenumberofnon-zeroentriesofh.\\xa0Thisproblemcanbesolved\\neﬃcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled\\n2 5 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c445062d-0065-42a5-abdf-a072cd8681bd', embedding=None, metadata={'page_label': '271', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nOMP- kwiththevalueof kspeciﬁedtoindicatethenumberofnon-zerofeatures\\nallowed. ()demonstratedthatOMP-canbeaveryeﬀective CoatesandNg2011 1\\nfeatureextractorfordeeparchitectures.\\nEssentiallyanymodelthathashiddenunitscanbemadesparse.Throughout\\nthisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof\\ncontexts.\\n7.11BaggingandOtherEnsembleMethods\\nBagging(shortforbootstrapaggregating)isatechniqueforreducinggen-\\neralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994\\ntrainseveraldiﬀerentmodelsseparately,thenhaveallofthemodelsvoteonthe\\noutputfortestexamples.Thisisanexampleofageneralstrategyinmachine\\nlearningcalledmodelaveraging.Techniquesemployingthisstrategyareknown\\nasensemblemethods.\\nThereasonthatmodelaveragingworksisthatdiﬀerentmodelswillusually\\nnotmakeallthesameerrorsonthetestset.\\nConsiderforexampleasetof kregressionmodels.Supposethateachmodel\\nmakesanerror \\ue00f ioneachexample,\\xa0withtheerrorsdrawnfromazero-mean\\nmultivariatenormaldistributionwithvariances E[ \\ue00f2\\ni] = vandcovariances E[ \\ue00f i \\ue00f j] =\\nc.\\xa0Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis\\n1\\nk\\ue050\\ni \\ue00f i.Theexpectedsquarederroroftheensemblepredictoris\\nE\\uf8ee\\n\\uf8f0\\ue020\\n1\\nk\\ue058\\ni\\ue00f i\\ue0212\\uf8f9\\n\\uf8fb=1\\nk2E\\uf8ee\\n\\uf8f0\\ue058\\ni\\uf8eb\\n\\uf8ed \\ue00f2\\ni+\\ue058\\nj i\\ue036=\\ue00f i \\ue00f j\\uf8f6\\n\\uf8f8\\uf8f9\\n\\uf8fb(7.50)\\n=1\\nkv+k−1\\nkc . (7.51)\\nInthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared\\nerrorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere\\ntheerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe\\nensembleisonly1\\nkv.Thismeansthattheexpectedsquarederroroftheensemble\\ndecreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble\\nwillperformatleastaswellasanyofitsmembers,andifthemembersmake\\nindependenterrors,theensemblewillperformsigniﬁcantlybetterthanitsmembers.\\nDiﬀerentensemblemethodsconstructtheensembleofmodelsindiﬀerentways.\\nForexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely\\n2 5 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e79007c9-69de-42e4-897b-8960a0e0ec9e', embedding=None, metadata={'page_label': '272', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n8\\n8F i r s t \\xa0 e nse m b l e \\xa0 m e m b e r\\nSe c ond\\xa0e nse m b l e \\xa0 m e m b e rO r i gi nal \\xa0 data s e t\\nF i r s t \\xa0 r e s am pl e d \\xa0 d a t a s e t\\nSe c ond\\xa0re s am p l e d \\xa0 d a t a s e t\\nFigure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron\\nthedatasetdepictedabove,containingan8,a6anda9.Supposewemaketwodiﬀerent\\nresampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets\\nbysamplingwithreplacement.Theﬁrstdatasetomitsthe9andrepeatsthe8.Onthis\\ndataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On\\ntheseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns\\nthatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual\\nclassiﬁcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust,\\nachievingmaximalconﬁdenceonlywhenbothloopsofthe8arepresent.\\ndiﬀerentkindofmodelusingadiﬀerentalgorithmorobjectivefunction.Bagging\\nisamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective\\nfunctiontobereusedseveraltimes.\\nSpeciﬁcally,bagginginvolvesconstructing kdiﬀerentdatasets.Eachdataset\\nhasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis\\nconstructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans\\nthat,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe\\noriginaldatasetandalsocontainsseveralduplicateexamples(onaveragearound\\n2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining\\nset,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset\\ni.Thediﬀerencesbetweenwhichexamplesareincludedineachdatasetresultin\\ndiﬀerencesbetweenthetrainedmodels.Seeﬁgureforanexample.7.5\\nNeuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan\\noftenbeneﬁtfrommodelaveragingevenifallofthemodelsaretrainedonthesame\\ndataset.Diﬀerencesinrandominitialization, randomselectionofminibatches,\\ndiﬀerencesinhyperparameters,ordiﬀerentoutcomesofnon-determinis ticimple-\\nmentationsofneuralnetworksareoftenenoughtocausediﬀerentmembersofthe\\n2 5 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1884e1a2-b5d4-4ebb-ae1c-7082bf617f58', embedding=None, metadata={'page_label': '273', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nensembletomakepartiallyindependenterrors.\\nModelaveragingisanextremelypowerfulandreliablemethodforreducing\\ngeneralization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms\\nforscientiﬁcpapers,becauseanymachinelearningalgorithmcanbeneﬁtsubstan-\\ntiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory.\\nForthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel.\\nMachinelearningcontestsareusuallywonbymethodsusingmodelaverag-\\ningoverdozensofmodels.ArecentprominentexampleistheNetﬂixGrand\\nPrize(Koren2009,).\\nNotalltechniquesforconstructingensemblesaredesignedtomaketheensemble\\nmoreregularizedthantheindividualmodels.Forexample,atechniquecalled\\nboosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher\\ncapacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles\\nofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural\\nnetworkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual\\nneuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a\\nunitstotheneuralnetwork.\\n7.12Dropout\\nDropout(Srivastava2014etal.,)providesacomputationally inexpensivebut\\npowerfulmethodofregularizingabroadfamilyofmodels.Toaﬁrstapproximation,\\ndropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles\\nofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels,\\nandevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical\\nwheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch\\nnetworksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles\\nofﬁvetotenneuralnetworks— ()usedsixtowintheILSVRC— Szegedy etal.2014a\\nbutmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive\\napproximationtotrainingandevaluatingabaggedensembleofexponentiallymany\\nneuralnetworks.\\nSpeciﬁcally,dropouttrainstheensembleconsistingofallsub-networksthat\\ncanbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork,\\nasillustratedinﬁgure.Inmostmodernneuralnetworks,basedonaseriesof 7.6\\naﬃnetransformationsandnonlinearities, wecaneﬀectivelyremoveaunitfroma\\nnetworkbymultiplyingitsoutputvaluebyzero.\\xa0Thisprocedurerequiressome\\nslightmodiﬁcationformodelssuchasradialbasisfunctionnetworks,whichtake\\n2 5 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='683e5ad3-a94c-47a3-9d18-bda38778a6b7', embedding=None, metadata={'page_label': '274', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nthediﬀerencebetweentheunit’sstateandsomereferencevalue.Here,wepresent\\nthedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan\\nbetriviallymodiﬁedtoworkwithotheroperationsthatremoveaunitfromthe\\nnetwork.\\nRecallthattolearnwithbagging,wedeﬁne kdiﬀerentmodels,construct k\\ndiﬀerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen\\ntrainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan\\nexponentiallylargenumberofneuralnetworks.Speciﬁcally,totrainwithdropout,\\nweuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas\\nstochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we\\nrandomlysampleadiﬀerentbinarymasktoapplytoalloftheinputandhidden\\nunitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof\\ntheothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe\\nincluded)isahyperparameter ﬁxedbeforetrainingbegins.\\xa0Itisnotafunction\\nofthecurrentvalueofthemodelparametersortheinputexample.Typically,\\naninputunitisincludedwithprobability0.8andahiddenunitisincludedwith\\nprobability0.5.Wethenrunforwardpropagation, back-propagation,andthe\\nlearningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7\\nwithdropout.\\nMoreformally,supposethatamaskvectorµspeciﬁeswhichunitstoinclude,\\nand J(θµ ,)deﬁnesthecostofthemodeldeﬁnedbyparametersθandmaskµ.\\nThendropouttrainingconsistsinminimizing E µ J(θµ ,).Theexpectationcontains\\nexponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient\\nbysamplingvaluesof.µ\\nDropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof\\nbagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare\\nparameters,witheachmodelinheritingadiﬀerentsubsetofparametersfromthe\\nparentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan\\nexponentialnumberofmodelswithatractableamountofmemory.Inthecaseof\\nbagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe\\ncaseofdropout,typicallymostmodelsarenotexplicitlytrainedatall—usually,\\nthemodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub-\\nnetworkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible\\nsub-networksareeachtrainedforasinglestep,andtheparametersharingcauses\\ntheremainingsub-networkstoarriveatgoodsettingsoftheparameters.These\\naretheonlydiﬀerences.Beyondthese,dropoutfollowsthebaggingalgorithm.For\\nexample,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof\\ntheoriginaltrainingsetsampledwithreplacement.\\n2 5 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1d02c66-e78b-4b5c-a14c-5a5c9606100a', embedding=None, metadata={'page_label': '275', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nyy\\nh 1 h 1 h 2 h 2\\nx 1 x 1 x 2 x 2yy\\nh 1 h 1 h 2 h 2\\nx 1 x 1 x 2 x 2yy\\nh 1 h 1 h 2 h 2\\nx 2 x 2yy\\nh 1 h 1 h 2 h 2\\nx 1 x 1yy\\nh 2 h 2\\nx 1 x 1 x 2 x 2\\nyy\\nh 1 h 1\\nx 1 x 1 x 2 x 2yy\\nh 1 h 1 h 2 h 2yy\\nx 1 x 1 x 2 x 2yy\\nh 2 h 2\\nx 2 x 2\\nyy\\nh 1 h 1\\nx 1 x 1yy\\nh 1 h 1\\nx 2 x 2yy\\nh 2 h 2\\nx 1 x 1yy\\nx 1 x 1\\nyy\\nx 2 x 2yy\\nh 2 h 2yy\\nh 1 h 1yyB ase \\xa0 ne t w or k\\nE nse m bl e \\xa0 of \\xa0 s u b n e t w or k s\\nFigure\\xa07.6:Dropout\\xa0trainsan\\xa0ensemble\\xa0consistingof\\xa0allsub-networks\\xa0that\\xa0canbe\\nconstructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we\\nbeginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen\\npossiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed\\nbydroppingoutdiﬀerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample,\\nalargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting\\ntheinputtotheoutput.Thisproblembecomesinsigniﬁcantfornetworkswithwider\\nlayers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes\\nsmaller.\\n2 6 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='17936749-3c09-4904-84b0-7cb3b11227fc', embedding=None, metadata={'page_label': '276', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nˆ x 1ˆ x 1\\nµ x 1 µ x 1 x 1 x 1ˆ x 2ˆ x 2\\nx 2 x 2 µ x 2 µ x 2h 1 h 1 h 2 h 2µ h 1 µ h 1 µ h 2 µ h 2ˆ h 1ˆ h 1ˆ h 2ˆ h 2yyyy\\nh 1 h 1 h 2 h 2\\nx 1 x 1 x 2 x 2\\nFigure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing\\ndropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one\\nhiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom )\\npropagationwithdropout,werandomlysampleavectorµwithoneentryforeachinput\\norhiddenunitinthenetwork.Theentriesofµarebinaryandaresampledindependently\\nfromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5\\nforthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby\\nthecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe\\nnetworkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom\\nﬁgureandrunningforwardpropagationthroughit. 7.6\\n2 6 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='058e6269-fd2c-4b2d-b303-28cb7d5612bc', embedding=None, metadata={'page_label': '277', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nTomakeaprediction,abaggedensemblemustaccumulatevotesfromallof\\nitsmembers.Werefertothisprocessasinferenceinthiscontext.\\xa0Sofar,our\\ndescriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly\\nprobabilistic.Now,weassumethatthemodel’sroleistooutputaprobability\\ndistribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution\\np( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall\\nofthesedistributions,\\n1\\nkk\\ue058\\ni = 1p( ) i( ) y|x . (7.52)\\nInthecaseofdropout,eachsub-modeldeﬁnedbymaskvectorµdeﬁnesaprob-\\nabilitydistribution p( y ,|xµ).Thearithmeticmeanoverallmasksisgiven\\nby\\ue058\\nµp p y , ()µ(|xµ) (7.53)\\nwhere p(µ)istheprobabilitydistributionthatwasusedtosampleµattraining\\ntime.\\nBecausethissumincludesanexponentialnumberofterms,itisintractable\\ntoevaluateexceptincaseswherethestructureofthemodelpermitssomeform\\nofsimpliﬁcation.Sofar,deepneuralnetsarenotknowntopermitanytractable\\nsimpliﬁcation.Instead,\\xa0wecan\\xa0approximatetheinferencewithsampling,\\xa0by\\naveragingtogethertheoutputfrommanymasks.Even10-20masksareoften\\nsuﬃcienttoobtaingoodperformance.\\nHowever,thereisanevenbetterapproach,thatallowsustoobtainagood\\napproximationtothepredictionsoftheentireensemble,atthecostofonlyone\\nforwardpropagation. Todoso,wechangetousingthegeometricmeanratherthan\\nthearithmeticmeanoftheensemblemembers’predicteddistributions.Warde-\\nFarley2014etal.()presentargumentsandempiricalevidencethatthegeometric\\nmeanperformscomparablytothearithmeticmeaninthiscontext.\\nThegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe\\naprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution,\\nweimposetherequirementthatnoneofthesub-modelsassignsprobability0toany\\nevent,andwerenormalizetheresultingdistribution.Theunnormalized probability\\ndistributiondeﬁneddirectlybythegeometricmeanisgivenby\\n˜ p e nse m bl e( ) = y|x 2d\\ue073\\ue059\\nµp y , (|xµ) (7.54)\\nwhere disthenumberofunitsthatmaybedropped.Hereweuseauniform\\ndistributionoverµtosimplifythepresentation,butnon-uniformdistributionsare\\n2 6 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc024d32-9c00-4cdd-92a6-07bcd6047d71', embedding=None, metadata={'page_label': '278', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nalsopossible.Tomakepredictionswemustre-normalizetheensemble:\\np e nse m bl e( ) = y|x˜ p e nse m bl e( ) y|x\\ue050\\ny\\ue030˜ p e nse m bl e( y\\ue030|x). (7.55)\\nAkeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c\\nmate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but\\nwiththeweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit\\ni.Themotivationforthismodiﬁcationistocapturetherightexpectedvalueofthe\\noutputfromthatunit.Wecallthisapproachtheweightscalinginferencerule.\\nThereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate\\ninferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell.\\nBecauseweusuallyuseaninclusionprobabilityof1\\n2,theweightscalingrule\\nusuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 \\nthemodelasusual.Anotherwaytoachievethesameresultistomultiplythe\\nstatesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2\\ntheexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected\\ntotalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare\\nmissingonaverage.\\nFormanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight\\nscalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression\\nclassiﬁerwithinputvariablesrepresentedbythevector: n v\\nP y (= y | v) = softmax\\ue010\\nW\\ue03ev+b\\ue011\\ny. (7.56)\\nWecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe\\ninputwithabinaryvector: d\\nP y (= y | v;) = dsoftmax\\ue010\\nW\\ue03e( )+d\\ue00c vb\\ue011\\ny.(7.57)\\nTheensemblepredictorisdeﬁnedbyre-normalizingthegeometricmeanoverall\\nensemblemembers’predictions:\\nP e nse m bl e(= ) =y y| v˜ P e nse m bl e(= )y y| v\\ue050\\ny\\ue030˜ P e nse m bl e(= y y\\ue030| v)(7.58)\\nwhere\\n˜ P e nse m bl e(= ) =y y| v2n\\ue073\\ue059\\nd∈{} 0 1 ,nP y . (= y | v;)d (7.59)\\n2 6 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3b9fc8a7-5770-43cb-87d6-3a6b0bdefbe9', embedding=None, metadata={'page_label': '279', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nToseethattheweightscalingruleisexact,wecansimplify ˜ P e nse m bl e:\\n˜ P e nse m bl e(= ) =y y| v2n\\ue073\\ue059\\nd∈{} 0 1 ,nP y (= y | v;)d(7.60)\\n= 2n\\ue073\\ue059\\nd∈{} 0 1 ,nsoftmax (W\\ue03e( )+)d\\ue00c vby (7.61)\\n= 2n\\ue076\\ue075\\ue075\\ue074\\ue059\\nd∈{} 0 1 ,nexp\\ue000\\nW\\ue03ey , :( )+d\\ue00c v b y\\ue001\\n\\ue050\\ny\\ue030exp\\ue010\\nW\\ue03e\\ny\\ue030 , :( )+d\\ue00c v b y\\ue030\\ue011 (7.62)\\n=2n\\ue071\\ue051\\nd∈{} 0 1 ,nexp\\ue000\\nW\\ue03ey , :( )+d\\ue00c v b y\\ue001\\n2n\\ue072\\ue051\\nd∈{} 0 1 ,n\\ue050\\ny\\ue030exp\\ue010\\nW\\ue03e\\ny\\ue030 , :( )+d\\ue00c v b y\\ue030\\ue011(7.63)\\nBecause˜ Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat\\nareconstantwithrespectto: y\\n˜ P e nse m bl e(= ) y y| v∝2n\\ue073\\ue059\\nd∈{} 0 1 ,nexp\\ue000\\nW\\ue03ey , :( )+d\\ue00c v b y\\ue001\\n(7.64)\\n= exp\\uf8eb\\n\\uf8ed1\\n2n\\ue058\\nd∈{} 0 1 ,nW\\ue03e\\ny , :( )+d\\ue00c v b y\\uf8f6\\n\\uf8f8 (7.65)\\n= exp\\ue0121\\n2W\\ue03e\\ny , : v+ b y\\ue013\\n. (7.66)\\nSubstitutingthisbackintoequationweobtainasoftmaxclassiﬁerwithweights 7.58\\n1\\n2W.\\nTheweightscalingruleisalsoexactinothersettings,includingregression\\nnetworkswithconditionallynormaloutputs,anddeepnetworksthathavehidden\\nlayerswithoutnonlinearities. However,theweightscalingruleisonlyanapproxi-\\nmationfordeepmodelsthathavenonlinearities. Thoughtheapproximationhas\\nnotbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow\\netal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a\\nbetter(intermsofclassiﬁcationaccuracy)thanMonteCarloapproximations tothe\\nensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas\\nallowedtosampleupto1,000sub-networks. ()found GalandGhahramani2015\\nthatsomemodelsobtainbetterclassiﬁcationaccuracyusingtwentysamplesand\\n2 6 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b20d923e-2919-45ed-a6b0-a16bc466bde5', embedding=None, metadata={'page_label': '280', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ntheMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference\\napproximationisproblem-dependent.\\nSrivastava2014etal.()showedthatdropoutismoreeﬀectivethanother\\nstandardcomputationally inexpensiveregularizers,suchasweightdecay,ﬁlter\\nnormconstraintsandsparseactivityregularization. Dropoutmayalsobecombined\\nwithotherformsofregularizationtoyieldafurtherimprovement.\\nOneadvantageofdropoutisthatitisverycomputationally cheap.Using\\ndropoutduringtrainingrequiresonly O( n)computationperexampleperupdate,\\ntogenerate nrandombinarynumbersandmultiplythembythestate.Depending\\nontheimplementation,itmayalsorequire O( n)memorytostorethesebinary\\nnumbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel\\nhasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay\\nthecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon\\nexamples.\\nAnothersigniﬁcantadvantageofdropoutisthatitdoesnotsigniﬁcantlylimit\\nthetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly\\nanymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic\\ngradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels\\nsuchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent\\nneuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother\\nregularizationstrategiesofcomparablepowerimposemoresevererestrictionson\\nthearchitectureofthemodel.\\nThoughthecostper-stepofapplyingdropouttoaspeciﬁcmodelisnegligible,\\nthecostofusingdropoutinacompletesystemcanbesigniﬁcant.Becausedropout\\nisaregularizationtechnique,itreducestheeﬀectivecapacityofamodel.Tooﬀset\\nthiseﬀect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation\\nseterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch\\nlargermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge\\ndatasets,regularizationconferslittlereductioningeneralization error.\\xa0Inthese\\ncases,thecomputational costofusingdropoutandlargermodelsmayoutweigh\\nthebeneﬁtofregularization.\\nWhenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless\\neﬀective.Bayesian\\xa0neuralnetworks(,\\xa0)outperform\\xa0dropout\\xa0onthe Neal1996\\nAlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011\\nareavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable,\\nunsupervisedfeaturelearningcangainanadvantageoverdropout.\\nWager2013etal.()showedthat,whenappliedtolinearregression,dropout\\nisequivalentto L2weightdecay,withadiﬀerentweightdecaycoeﬃcientfor\\n2 6 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d9622ac-16c0-4737-9ee3-b18d6fabd33b', embedding=None, metadata={'page_label': '281', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\neachinputfeature.Themagnitudeofeachfeature’sweightdecaycoeﬃcientis\\ndeterminedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep\\nmodels,dropoutisnotequivalenttoweightdecay.\\nThestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe\\napproach’ssuccess.Itisjustameansofapproximating thesumoverallsub-\\nmodels.WangandManning2013()derivedanalyticalapproximationstothis\\nmarginalization. Theirapproximation,knownasfastdropoutresultedinfaster\\nconvergencetimeduetothereducedstochasticityinthecomputationofthe\\ngradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled\\n(butalsomorecomputationally expensive)approximation totheaverageoverall\\nsub-networksthantheweightscalingapproximation.Fastdropouthasbeenused\\ntonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork\\nproblems,buthasnotyetyieldedasigniﬁcantimprovementorbeenappliedtoa\\nlargeproblem.\\nJustasstochasticityisnotnecessarytoachievetheregularizing\\xa0eﬀect of\\ndropout,itisalsonotsuﬃcient.Todemonstratethis,Warde-Farley2014etal.()\\ndesignedcontrolexperimentsusingamethodcalleddropoutboostingthatthey\\ndesignedtouseexactlythesamemasknoiseastraditionaldropoutbutlack\\nitsregularizingeﬀect.Dropoutboostingtrainstheentireensembletojointly\\nmaximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional\\ndropoutisanalogoustobagging,\\xa0this approachisanalogoustoboosting.As\\nintended,experimentswithdropoutboostingshowalmostnoregularizationeﬀect\\ncomparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat\\ntheinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof\\ndropoutasrobustnesstonoise.Theregularizationeﬀectofthebaggedensembleis\\nonlyachievedwhenthestochasticallysampledensemblemembersaretrainedto\\nperformwellindependently ofeachother.\\nDropouthasinspiredotherstochasticapproachestotrainingexponentially\\nlargeensemblesofmodelsthatshareweights.\\xa0DropConnectisaspecialcaseof\\ndropoutwhereeachproductbetweenasinglescalarweightandasinglehidden\\nunitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic\\npoolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3\\nofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiﬀerent\\nspatiallocationsofeachfeaturemap.\\xa0Sofar,dropoutremainsthemostwidely\\nusedimplicitensemblemethod.\\nOneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic\\nbehaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions\\nimplementsaformofbaggingwithparametersharing.Earlier,\\xa0wedescribed\\n2 6 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='02fb138b-ed61-4e66-b0cd-a2deab341855', embedding=None, metadata={'page_label': '282', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ndropoutas\\xa0bagginganensembleofmodelsformedbyincludingor\\xa0excluding\\nunits.However,thereisnoneedforthismodelaveragingstrategytobebasedon\\ninclusionandexclusion.Inprinciple,anykindofrandommodiﬁcationisadmissible.\\nInpractice,wemustchoosemodiﬁcationfamiliesthatneuralnetworksareable\\ntolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast\\napproximateinferencerule.Wecanthinkofanyformofmodiﬁcationparametrized\\nbyavectorµastraininganensembleconsistingof p( y ,|xµ)forallpossible\\nvaluesofµ.Thereisnorequirementthatµhaveaﬁnitenumberofvalues.For\\nexample,µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe\\nweightsbyµ∼N( 1 , I)canoutperformdropoutbasedonbinarymasks.Because\\nE[µ] = 1thestandardnetworkautomatically implementsapproximate inference\\nintheensemble,withoutneedinganyweightscaling.\\nSofarwehavedescribeddropoutpurelyasameansofperformingeﬃcient,\\napproximatebagging.However,thereisanotherviewofdropoutthatgoesfurther\\nthanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble\\nofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto\\nperformwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits\\nmustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal.\\n()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c\\nswappinggenesbetweentwodiﬀerentorganisms,createsevolutionarypressurefor\\ngenestobecomenotjustgood,buttobecomereadilyswappedbetweendiﬀerent\\norganisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir\\nenvironmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures\\nofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe\\nnotmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts.\\xa0Warde-\\nFarley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand\\nconcludedthatdropoutoﬀersadditionalimprovementstogeneralization error\\nbeyondthoseobtainedbyensemblesofindependentmodels.\\nItisimportanttounderstandthatalargeportionofthepowerofdropout\\narisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This\\ncanbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation\\ncontentoftheinputratherthandestructionoftherawvaluesoftheinput.For\\nexample,ifthemodellearnsahiddenunit h ithatdetectsafacebyﬁndingthenose,\\nthendropping h icorrespondstoerasingtheinformationthatthereisanosein\\ntheimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe\\npresenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth.\\nTraditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare\\nnotabletorandomlyerasetheinformationaboutanosefromanimageofaface\\nunlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin\\n2 6 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ff127508-df01-46d9-8212-586ef56ce1c9', embedding=None, metadata={'page_label': '283', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\ntheimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues\\nallowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput\\ndistributionthatthemodelhasacquiredsofar.\\nAnotherimportantaspectofdropoutisthatthenoiseismultiplicative. Ifthe\\nnoisewereadditivewithﬁxedscale,thenarectiﬁedlinearhiddenunit h iwith\\naddednoise \\ue00fcouldsimplylearntohave h ibecomeverylargeinordertomake\\ntheaddednoise \\ue00finsigniﬁcantbycomparison.Multiplicativenoisedoesnotallow\\nsuchapathologicalsolutiontothenoiserobustnessproblem.\\nAnotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel\\ninawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden\\nunitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove\\noptimization, butthenoisecanhavearegularizingeﬀect,andsometimesmakes\\ndropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1\\n7.13AdversarialTraining\\nInmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen\\nevaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese\\nmodelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder\\ntoprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan\\nsearchforexamplesthatthemodelmisclassiﬁes. ()foundthat Szegedy etal.2014b\\nevenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100%\\nerrorrateonexamplesthatareintentionallyconstructedbyusinganoptimization\\nproceduretosearchforaninputx\\ue030nearadatapointxsuchthatthemodel\\noutputisverydiﬀerentatx\\ue030.Inmanycases,x\\ue030canbesosimilartoxthata\\nhumanobservercannottellthediﬀerencebetweentheoriginalexampleandthe\\nadversarialexample,butthenetworkcanmakehighlydiﬀerentpredictions.See\\nﬁgureforanexample.7.8\\nAdversarialexampleshavemanyimplications,forexample,incomputersecurity,\\nthatarebeyondthescopeofthischapter.\\xa0However,theyareinterestinginthe\\ncontextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d.\\ntestsetviaadversarialtraining—trainingonadversariallyperturbedexamples\\nfromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,).\\nGoodfellow2014betal.()showedthatoneoftheprimarycausesofthese\\nadversarial\\xa0examplesis\\xa0excessive\\xa0linearity.Neural\\xa0networks\\xa0arebuilt\\xa0out\\xa0of\\nprimarilylinearbuildingblocks.\\xa0Insomeexperimentstheoverallfunctionthey\\nimplementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy\\n2 6 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2320a635-bd5d-4f57-a8cf-e3840c0a2d4d', embedding=None, metadata={'page_label': '284', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\n+ .007× =\\nx sign(∇ x J(θx , , y))x+\\n\\ue00fsign(∇ x J(θx , , y))\\ny=“panda” “nematode”“gibbon”\\nw/57.7%\\nconﬁdencew/8.2%\\nconﬁdencew/99.3%\\nconﬁdence\\nFigure7.8:\\xa0AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet\\n( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a\\nelementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith\\nrespecttotheinput,wecanchangeGoogLeNet’sclassiﬁcationoftheimage.Reproduced\\nwithpermissionfrom (). Goodfellow e t a l .2014b\\ntooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly\\nifithasnumerousinputs.Ifwechangeeachinputby \\ue00f,thenalinearfunction\\nwithweightswcanchangebyasmuchas \\ue00f||||w 1,whichcanbeaverylarge\\namountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly\\nsensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant\\nintheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly\\nintroducingalocalconstancypriorintosupervisedneuralnets.\\nAdversarialtraininghelpstoillustratethepowerofusingalargefunction\\nfamilyincombinationwithaggressiveregularization. Purelylinearmodels,like\\nlogisticregression,arenotabletoresistadversarialexamplesbecausetheyare\\nforcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange\\nfromnearlylineartonearlylocallyconstantandthushavetheﬂexibilitytocapture\\nlineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation.\\nAdversarialexamplesalsoprovideameansofaccomplishingsemi-supervised\\nlearning.Atapointxthatisnotassociatedwithalabelinthedataset,the\\nmodelitselfassignssomelabel ˆ y.Themodel’slabel ˆ ymaynotbethetruelabel,\\nbutifthemodelishighquality,thenˆ yhasahighprobabilityofprovidingthe\\ntruelabel.Wecanseekanadversarialexamplex\\ue030thatcausestheclassiﬁerto\\noutputalabel y\\ue030with y\\ue030\\ue036=ˆ y.Adversarialexamplesgeneratedusingnotthetrue\\nlabelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial\\nexamples(Miyato2015etal.,).Theclassiﬁermaythenbetrainedtoassignthe\\nsamelabeltoxandx\\ue030.Thisencouragestheclassiﬁertolearnafunctionthatis\\n2 6 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fe564c20-c785-4ad9-a200-37a72c45841e', embedding=None, metadata={'page_label': '285', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nrobusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata\\nlies.Theassumptionmotivatingthisapproachisthatdiﬀerentclassesusuallylie\\nondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump\\nfromoneclassmanifoldtoanotherclassmanifold.\\n7.14Tangent\\xa0Distance,\\xa0TangentProp,and\\xa0Manifold\\nTangentClassiﬁer\\nManymachinelearningalgorithmsaimtoovercomethecurseofdimensionality\\nbyassumingthatthedataliesnearalow-dimensional manifold,asdescribedin\\nsection.5.11.3\\nOneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe\\ntangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998\\nnearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean\\ndistancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich\\nprobabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand\\nthatexamplesonthesamemanifoldsharethesamecategory.Sincetheclassiﬁer\\nshouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement\\nonthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween\\npointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey\\nrespectivelybelong.Althoughthatmaybecomputationally diﬃcult(itwould\\nrequiresolvinganoptimization problem,toﬁndthenearestpairofpointson M 1\\nand M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits\\ntangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween\\natangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional\\nlinearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires\\nonetospecifythetangentvectors.\\nInarelatedspirit,thetangentpropalgorithm( ,)(ﬁgure) Simardetal.19927.9\\ntrainsaneuralnetclassiﬁerwithanextrapenaltytomakeeachoutput f(x)of\\ntheneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof\\nvariationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe\\nsameclassconcentrate.Localinvarianceisachievedbyrequiring ∇ x f(x)tobe\\northogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat\\nthedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga\\nregularizationpenalty:Ω\\nΩ() = f\\ue058\\ni\\ue010\\n(∇ x f())x\\ue03ev( ) i\\ue0112\\n. (7.67)\\n2 7 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='01308f99-9b31-4da1-87d9-571ebd87e1e7', embedding=None, metadata={'page_label': '286', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nThisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for\\nmostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone\\noutput f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm,\\nthetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof\\ntheeﬀectoftransformationssuchastranslation,rotation,andscalinginimages.\\nTangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992\\nbutalsointhecontextofreinforcementlearning(,). Thrun1995\\nTangentpropagation is\\xa0closelyrelated\\xa0todataset\\xa0augmentation.In\\xa0both\\ncases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask\\nbyspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe\\nnetwork.Thediﬀerenceisthatinthecaseofdatasetaugmentation, thenetworkis\\nexplicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying\\nmorethananinﬁnitesimalamountofthesetransformations.Tangentpropagation\\ndoesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically\\nregularizesthemodeltoresistperturbationinthedirectionscorrespondingto\\nthe\\xa0speciﬁed\\xa0transformation.While\\xa0thisanalytical\\xa0approac h\\xa0isintellectually\\nelegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist\\ninﬁnitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto\\nlargerperturbations.Second,theinﬁnitesimalapproachposesdiﬃcultiesformodels\\nbasedonrectiﬁedlinearunits.Thesemodelscanonlyshrinktheirderivatives\\nbyturningunitsoﬀorshrinkingtheirweights.Theyarenotabletoshrinktheir\\nderivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh\\nunitscan.Datasetaugmentation workswellwithrectiﬁedlinearunitsbecause\\ndiﬀerentsubsetsofrectiﬁedunitscanactivatefordiﬀerenttransformedversionsof\\neachoriginalinput.\\nTangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun,\\n1992)andadversarialtraining( ,; ,). Szegedy etal.2014bGoodfellowetal.2014b\\nDoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining\\nﬁndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame\\noutputontheseasontheoriginalinputs.Tangentpropagation anddataset\\naugmentationusingmanuallyspeciﬁedtransformationsbothrequirethatthe\\nmodelshouldbeinvarianttocertainspeciﬁeddirectionsofchangeintheinput.\\nDoublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe\\ninvarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all\\nasdatasetaugmentationisthenon-inﬁnitesimalversionoftangentpropagation,\\nadversarialtrainingisthenon-inﬁnitesimalversionofdoublebackprop.\\nThemanifoldtangentclassiﬁer(,),eliminatestheneedto Rifaietal.2011c\\nknowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14\\n2 7 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0ac1231a-e857-4d25-a310-2b7e2070b8e5', embedding=None, metadata={'page_label': '287', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nx 1x 2N o r m a lT a ng e nt\\nFigure7.9:\\xa0Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l .\\n1992 Rifai2011c )andmanifoldtangentclassiﬁer( e t a l .,),whichbothregularizethe\\nclassiﬁeroutputfunction f(x).Eachcurverepresentsthemanifoldforadiﬀerentclass,\\nillustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace.\\nOnonecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe\\nclassmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe\\nclassmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany\\ntangentdirectionsandmanynormaldirections.Weexpecttheclassiﬁcationfunctionto\\nchangerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas\\nitmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent\\nclassiﬁerregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent\\npropagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent\\ndirections(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass\\nmanifold)whilethemanifoldtangentclassiﬁerestimatesthemanifoldtangentdirections\\nbytraininganautoencodertoﬁtthetrainingdata.Theuseofautoencoderstoestimate\\nmanifoldswillbedescribedinchapter.14\\nestimatethemanifoldtangentvectors.Themanifoldtangentclassiﬁermakesuse\\nofthistechniquetoavoidneedinguser-speciﬁedtangentvectors.\\xa0Asillustrated\\ninﬁgure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10\\nthatariseoutofthegeometryofimages(suchastranslation,rotationandscaling)\\nandincludefactorsthatmustbelearnedbecausetheyareobject-speciﬁc(suchas\\nmovingbodyparts).Thealgorithmproposedwiththemanifoldtangentclassiﬁer\\nisthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby\\nunsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiﬁer\\nasintangentprop(equation).7.67\\nThischapterhasdescribedmostofthegeneralstrategiesusedtoregularize\\nneuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch\\n2 7 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a8ac10f-3883-4d15-9d04-3b342b8a1c87', embedding=None, metadata={'page_label': '288', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER7.REGULARIZATIONFORDEEPLEARNING\\nwillberevisitedperiodicallybymostoftheremainingchapters.Anothercentral\\nthemeofmachinelearningisoptimization, describednext.\\n2 7 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3e441f6f-a872-4b94-b30d-323c85e3818c', embedding=None, metadata={'page_label': '289', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 8\\nOptimizationforTrainingDeep\\nModels\\nDeeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample,\\nperforminginferenceinmodelssuchasPCAinvolvessolvinganoptimization\\nproblem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms.\\nOfallofthemanyoptimization problemsinvolvedindeeplearning,themost\\ndiﬃcultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof\\ntimeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural\\nnetworktrainingproblem.Becausethisproblemissoimportantandsoexpensive,\\naspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit.\\nThischapterpresentstheseoptimization techniquesforneuralnetworktraining.\\nIfyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization,\\nwesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4\\noptimization ingeneral.\\nThischapterfocusesononeparticularcaseofoptimization: ﬁndingtheparam-\\netersθofaneuralnetworkthatsigniﬁcantlyreduceacostfunction J(θ),which\\ntypicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas\\nwellasadditionalregularizationterms.\\nWebeginwithadescriptionofhowoptimization usedasatrainingalgorithm\\nforamachinelearningtaskdiﬀersfrompureoptimization. Next,wepresentseveral\\noftheconcretechallengesthatmakeoptimization ofneuralnetworksdiﬃcult.We\\nthendeﬁneseveralpracticalalgorithms,includingbothoptimization algorithms\\nthemselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms\\nadapttheirlearningratesduringtrainingorleverageinformationcontainedin\\n274', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9595ccc5-37b8-4e7a-bfe5-9b6a230ed6d9', embedding=None, metadata={'page_label': '290', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nthesecondderivativesofthecostfunction.Finally,weconcludewithareviewof\\nseveraloptimization strategiesthatareformedbycombiningsimpleoptimization\\nalgorithmsintohigher-levelprocedures.\\n8.1HowLearningDiﬀersfromPureOptimization\\nOptimization algorithmsusedfortrainingofdeepmodelsdiﬀerfromtraditional\\noptimization algorithmsinseveralways.Machinelearningusuallyactsindirectly.\\nInmostmachinelearningscenarios,wecareaboutsomeperformancemeasure\\nP,thatisdeﬁnedwithrespecttothetestsetandmayalsobeintractable.We\\nthereforeoptimize Ponlyindirectly.Wereduceadiﬀerentcostfunction J(θ)in\\nthehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization,\\nwhereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining\\ndeepmodelsalsotypicallyincludesomespecializationonthespeciﬁcstructureof\\nmachinelearningobjectivefunctions.\\nTypically,thecostfunctioncanbewrittenasanaverageoverthetrainingset,\\nsuchas\\nJ() = θ E ( ) ˆ x ,y ∼ pdataL f , y , ((;)xθ) (8.1)\\nwhere Listheper-examplelossfunction, f(x;θ)isthepredictedoutputwhen\\ntheinputisx,ˆ p da t aistheempiricaldistribution.Inthesupervisedlearningcase,\\nyisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized\\nsupervisedcase,wheretheargumentsto Lare f(x;θ)and y.However,itistrivial\\ntoextendthisdevelopment,forexample,toincludeθorxasarguments,orto\\nexclude yasarguments,inordertodevelopvariousformsofregularizationor\\nunsupervisedlearning.\\nEquationdeﬁnesanobjectivefunctionwithrespecttothetrainingset.We 8.1\\nwouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe\\nexpectationistakenacrossthedatageneratingdistribution p da t aratherthanjust\\novertheﬁnitetrainingset:\\nJ∗() = θ E ( ) x ,y ∼ pdataL f , y . ((;)xθ) (8.2)\\n8.1.1EmpiricalRiskMinimization\\nThegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization\\nerrorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere\\nthattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew\\nthetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task\\n2 7 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='08be14ad-9514-4a5e-b031-bd0a7c78b7e8', embedding=None, metadata={'page_label': '291', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nsolvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y)\\nbutonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem.\\nThesimplestwaytoconvertamachinelearningproblembackintoanop-\\ntimizationproblemistominimizetheexpectedlossonthetrainingset.This\\nmeansreplacingthetruedistribution p(x , y) withtheempiricaldistributionˆ p(x , y)\\ndeﬁnedbythetrainingset.Wenowminimizetheempiricalrisk\\nE x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1\\nmm \\ue058\\ni = 1L f((x( ) i;)θ , y( ) i)(8.3)\\nwhereisthenumberoftrainingexamples. m\\nThetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown\\nasempiricalriskminimization.Inthissetting,machinelearningisstillvery\\nsimilartostraightforwardoptimization. Ratherthanoptimizingtheriskdirectly,\\nweoptimizetheempiricalrisk,andhopethattheriskdecreasessigniﬁcantlyas\\nwell.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk\\ncanbeexpectedtodecreasebyvariousamounts.\\nHowever,empiricalriskminimization ispronetooverﬁtting.Modelswith\\nhighcapacitycansimplymemorizethetrainingset.Inmanycases,empirical\\nriskminimization isnotreallyfeasible.Themosteﬀectivemodernoptimization\\nalgorithmsarebasedongradientdescent,butmanyusefullossfunctions,such\\nas0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeﬁned\\neverywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we\\nrarelyuseempiricalriskminimization. Instead,wemustuseaslightlydiﬀerent\\napproach,inwhichthequantitythatweactuallyoptimizeisevenmorediﬀerent\\nfromthequantitythatwetrulywanttooptimize.\\n8.1.2SurrogateLossFunctionsandEarlyStopping\\nSometimes,thelossfunctionweactuallycareabout(sayclassiﬁcationerror)isnot\\nonethatcanbeoptimizedeﬃciently.Forexample,exactlyminimizingexpected0-1\\nlossistypicallyintractable(exponentialintheinputdimension),evenforalinear\\nclassiﬁer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes\\nasurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages.\\nForexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa\\nsurrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate\\ntheconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan\\ndothatwell,thenitcanpicktheclassesthatyieldtheleastclassiﬁcationerrorin\\nexpectation.\\n2 7 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='691c1eae-a7c4-40f8-9fc0-c92b2ad8a973', embedding=None, metadata={'page_label': '292', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nInsomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn\\nmore.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong\\ntimeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe\\nlog-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero,\\nonecanimprovetherobustnessoftheclassiﬁerbyfurtherpushingtheclassesapart\\nfromeachother,obtainingamoreconﬁdentandreliableclassiﬁer,thusextracting\\nmoreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply\\nminimizingtheaverage0-1lossonthetrainingset.\\nAveryimportantdiﬀerencebetweenoptimization ingeneralandoptimization\\nasweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt\\natalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes\\nasurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly\\nstopping(section)issatisﬁed.Typicallytheearlystoppingcriterionisbased 7.8\\nonthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset,\\nandisdesignedtocausethealgorithmtohaltwheneveroverﬁttingbeginstooccur.\\nTrainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives,\\nwhichisverydiﬀerentfromthepureoptimization setting,whereanoptimization\\nalgorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall.\\n8.1.3BatchandMinibatchAlgorithms\\nOneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral\\noptimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum\\noverthetrainingexamples.Optimization algorithmsformachinelearningtypically\\ncomputeeachupdatetotheparametersbasedonanexpectedvalueofthecost\\nfunctionestimatedusingonlyasubsetofthetermsofthefullcostfunction.\\nForexample,maximumlikelihoodestimationproblems,whenviewedinlog\\nspace,decomposeintoasumovereachexample:\\nθ M L= argmax\\nθm \\ue058\\ni = 1log p m o de l(x( ) i, y( ) i;)θ . (8.4)\\nMaximizingthissumisequivalenttomaximizingtheexpectationoverthe\\nempiricaldistributiondeﬁnedbythetrainingset:\\nJ() = θ E x ,y ∼ ˆ pdatalog p m o de l(;)x , yθ . (8.5)\\nMostofthepropertiesoftheobjectivefunction Jusedbymostofouropti-\\nmizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the\\n2 7 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7517b195-9fe7-444d-a166-f95015410731', embedding=None, metadata={'page_label': '293', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nmostcommonlyusedpropertyisthegradient:\\n∇ θ J() = θ E x ,y ∼ ˆ pdata∇ θlog p m o de l(;)x , yθ . (8.6)\\nComputing\\xa0this expectation\\xa0exactly\\xa0isvery\\xa0expensive\\xa0because\\xa0it\\xa0requires\\nevaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan\\ncomputetheseexpectationsbyrandomlysamplingasmallnumberofexamples\\nfromthedataset,thentakingtheaverageoveronlythoseexamples.\\nRecallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n\\nsamplesisgivenby σ /√n ,where σisthetruestandarddeviationofthevalueof\\nthesamples.Thedenominator of√nshowsthattherearelessthanlinearreturns\\ntousingmoreexamplestoestimatethegradient.Comparetwohypothetical\\nestimatesofthegradient,onebasedon100examplesandanotherbasedon10,000\\nexamples.Thelatterrequires100timesmorecomputationthantheformer,but\\nreducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization\\nalgorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof\\nnumberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates\\nofthegradientratherthanslowlycomputingtheexactgradient.\\nAnotherconsiderationmotivatingstatisticalestimationofthegradientfroma\\nsmallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all\\nmsamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling-\\nbasedestimateofthegradientcouldcomputethecorrectgradientwithasingle\\nsample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we\\nareunlikelytotrulyencounterthisworst-casesituation,butwemayﬁndlarge\\nnumbersofexamplesthatallmakeverysimilarcontributionstothegradient.\\nOptimization algorithmsthatusetheentiretrainingsetarecalledbatchor\\ndeterministicgradientmethods,becausetheyprocessallofthetrainingexamples\\nsimultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing\\nbecausetheword“batch”isalsooftenusedtodescribetheminibatchusedby\\nminibatchstochasticgradientdescent.Typicallytheterm“batchgradientdescent”\\nimpliestheuseofthefulltrainingset,whiletheuseoftheterm“batch”todescribe\\nagroupofexamplesdoesnot.\\xa0Forexample,itisverycommontousetheterm\\n“batchsize”todescribethesizeofaminibatch.\\nOptimization algorithmsthatuseonlyasingleexampleatatimearesometimes\\ncalledstochasticorsometimesonlinemethods.Thetermonlineisusually\\nreservedforthecasewheretheexamplesaredrawnfromastreamofcontinually\\ncreatedexamplesratherthanfromaﬁxed-sizetrainingsetoverwhichseveral\\npassesaremade.\\nMostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore\\n2 7 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9f46507e-9628-49ea-8302-5d9a4e304037', embedding=None, metadata={'page_label': '294', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nthanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled\\nminibatchorminibatchstochasticmethodsanditisnowcommontosimply\\ncallthemstochasticmethods.\\nThecanonicalexampleofastochasticmethodisstochasticgradientdescent,\\npresentedindetailinsection.8.3.1\\nMinibatchsizesaregenerallydrivenbythefollowingfactors:\\n•Largerbatchesprovideamoreaccurateestimateofthegradient,butwith\\nlessthanlinearreturns.\\n•Multicorearchitectures areusuallyunderutilized byextremelysmallbatches.\\nThismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere\\nisnoreductioninthetimetoprocessaminibatch.\\n•Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically\\nthecase),thentheamountofmemoryscaleswiththebatchsize.Formany\\nhardwaresetupsthisisthelimitingfactorinbatchsize.\\n•Somekindsofhardwareachievebetterruntimewithspeciﬁcsizesofarrays.\\nEspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooﬀer\\nbetterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16\\nsometimesbeingattemptedforlargemodels.\\n•Smallbatchescanoﬀeraregularizingeﬀect( ,), WilsonandMartinez2003\\nperhapsduetothenoisetheyaddtothelearningprocess.Generalization\\nerrorisoftenbestforabatchsizeof1.\\xa0Trainingwithsuchasmallbatch\\nsizemightrequireasmalllearningratetomaintainstabilityduetothehigh\\nvarianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh\\nduetotheneedtomakemoresteps,bothbecauseofthereducedlearning\\nrateandbecauseittakesmorestepstoobservetheentiretrainingset.\\nDiﬀerentkindsofalgorithmsusediﬀerentkindsofinformationfromthemini-\\nbatchindiﬀerentways.Somealgorithmsaremoresensitivetosamplingerrorthan\\nothers,eitherbecausetheyuseinformationthatisdiﬃculttoestimateaccurately\\nwithfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling\\nerrorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare\\nusuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order\\nmethods,whichusealsotheHessianmatrixHandcomputeupdatessuchas\\nH− 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch\\nsizesarerequiredtominimizeﬂuctuationsintheestimatesofH− 1g.Suppose\\nthatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by\\n2 7 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='357c9e64-db71-4446-9e80-92ac38b8e186', embedding=None, metadata={'page_label': '295', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nHoritsinverseampliﬁespre-existingerrors,inthiscase,estimationerrorsing.\\nVerysmallchangesintheestimateofgcanthuscauselargechangesintheupdate\\nH− 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly\\napproximately,sotheupdateH− 1gwillcontainevenmoreerrorthanwewould\\npredictfromapplyingapoorlyconditionedoperationtotheestimateof.g\\nItisalsocrucialthattheminibatchesbeselectedrandomly.Computingan\\nunbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose\\nsamplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe\\nindependentfromeachother,sotwosubsequentminibatchesofexamplesshould\\nalsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged\\ninawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight\\nhaveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This\\nlistmightbearrangedsothatﬁrstwehaveﬁvebloodsamplestakenatdiﬀerent\\ntimesfromtheﬁrstpatient,thenwehavethreebloodsamplestakenfromthe\\nsecondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe\\nweretodrawexamplesinorderfromthislist,theneachofourminibatcheswould\\nbeextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe\\nmanypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset\\nholdssomesigniﬁcance,itisnecessarytoshuﬄetheexamplesbeforeselecting\\nminibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof\\nexamplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly\\natrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice\\nitisusuallysuﬃcienttoshuﬄetheorderofthedatasetonceandthenstoreitin\\nshuﬄedfashion.Thiswillimposeaﬁxedsetofpossibleminibatchesofconsecutive\\nexamplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel\\nwillbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining\\ndata.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea\\nsigniﬁcantdetrimentaleﬀect.Failingtoevershuﬄetheexamplesinanywaycan\\nseriouslyreducetheeﬀectivenessofthealgorithm.\\nManyoptimization problemsinmachinelearningdecomposeoverexamples\\nwellenoughthatwecancomputeentireseparateupdatesoverdiﬀerentexamples\\ninparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for\\noneminibatchofexamplesXatthesametimethatwecomputetheupdatefor\\nseveralotherminibatches.Suchasynchronousparalleldistributedapproachesare\\ndiscussedfurtherinsection.12.1.3\\nAninterestingmotivationforminibatchstochasticgradientdescentisthatit\\nfollowsthegradientofthetruegeneralizationerror(equation)solongasno 8.2\\nexamplesarerepeated.Mostimplementations ofminibatchstochasticgradient\\n2 8 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5663ed03-d297-4c42-befd-dd6f99243d6d', embedding=None, metadata={'page_label': '296', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\ndescentshuﬄethedatasetonceandthenpassthroughitmultipletimes.Onthe\\nﬁrstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue\\ngeneralization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis\\nformedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining\\nnewfairsamplesfromthedatageneratingdistribution.\\nThefactthatstochasticgradientdescentminimizesgeneralization erroris\\neasiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn\\nfromastreamofdata.Inotherwords,insteadofreceivingaﬁxed-sizetraining\\nset,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant,\\nwitheveryexample (x , y)comingfromthedatageneratingdistribution p da t a(x , y).\\nInthisscenario,examplesareneverrepeated;everyexperienceisafairsample\\nfrom p da t a.\\nTheequivalenceiseasiesttoderivewhenbothxand yarediscrete.\\xa0Inthis\\ncase,thegeneralization error(equation)canbewrittenasasum 8.2\\nJ∗() =θ\\ue058\\nx\\ue058\\nyp da t a()((;)) x , y L fxθ , y , (8.7)\\nwiththeexactgradient\\ng= ∇ θ J∗() =θ\\ue058\\nx\\ue058\\nyp da t a()x , y∇ θ L f , y . ((;)xθ)(8.8)\\nWehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa-\\ntionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L\\nbesidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous,\\nundermildassumptionsregarding p da t aand. L\\nHence,\\xa0wecanobtainanunbiasedestimatoroftheexactgradientof\\xa0the\\ngeneralization errorbysamplingaminibatchofexamples {x( 1 ), . . .x( ) m}withcor-\\nrespondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing\\nthegradientofthelosswithrespecttotheparametersforthatminibatch:\\nˆg=1\\nm∇ θ\\ue058\\niL f((x( ) i;)θ , y( ) i) . (8.9)\\nUpdatinginthedirectionof θ ˆgperformsSGDonthegeneralization error.\\nOfcourse,\\xa0thisinterpretation only\\xa0applies whenexamplesarenotreused.\\nNonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset,\\nunlessthetrainingsetisextremelylarge.\\xa0When multiplesuchepochsareused,\\nonlytheﬁrstepochfollowstheunbiasedgradientofthegeneralization error,but\\n2 8 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c4753dc5-ada8-4c83-bcda-2a9b7ebb1968', embedding=None, metadata={'page_label': '297', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nofcourse,theadditionalepochsusuallyprovideenoughbeneﬁtduetodecreased\\ntrainingerrortooﬀsettheharmtheycausebyincreasingthegapbetweentraining\\nerrorandtesterror.\\nWithsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it\\nisbecomingmorecommonformachinelearningapplicationstouseeachtraining\\nexampleonlyonceoreventomakeanincompletepassthroughthetraining\\nset.Whenusinganextremelylargetrainingset,overﬁttingisnotanissue,so\\nunderﬁttingandcomputational eﬃciencybecomethepredominant concerns.See\\nalso ()foradiscussionoftheeﬀectofcomputational BottouandBousquet2008\\nbottlenecksongeneralization error,asthenumberoftrainingexamplesgrows.\\n8.2ChallengesinNeuralNetworkOptimization\\nOptimization ingeneralisanextremelydiﬃculttask.Traditionally,machine\\nlearninghasavoidedthediﬃcultyofgeneraloptimization bycarefullydesigning\\ntheobjectivefunctionandconstraintstoensurethattheoptimization problemis\\nconvex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex\\ncase.Evenconvexoptimization isnotwithoutitscomplications. Inthissection,\\nwesummarizeseveralofthemostprominentchallengesinvolvedinoptimization\\nfortrainingdeepmodels.\\n8.2.1Ill-Conditioning\\nSomechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost\\nprominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral\\nprobleminmostnumericaloptimization, convexorotherwise,andisdescribedin\\nmoredetailinsection.4.3.1\\nTheill-conditioning problemisgenerallybelievedtobepresentinneural\\nnetworktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget\\n“stuck”inthesensethatevenverysmallstepsincreasethecostfunction.\\nRecallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9\\ncostfunctionpredictsthatagradientdescentstepofwilladd − \\ue00fg\\n1\\n2\\ue00f2g\\ue03eHgg− \\ue00f\\ue03eg (8.10)\\ntothecost.Ill-conditioningofthegradientbecomesaproblemwhen1\\n2\\ue00f2g\\ue03eHg\\nexceeds \\ue00fg\\ue03eg.\\xa0Todeterminewhetherill-conditioning isdetrimentaltoaneural\\nnetwork\\xa0training task,\\xa0one\\xa0canmonitorthe\\xa0squaredgradientnormg\\ue03egand\\n2 8 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='03a936da-fa39-418c-b644-7e6aa764bcd2', embedding=None, metadata={'page_label': '298', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n−50050100150200250\\nTrainingtime(epochs)−20246810121416Gradient norm\\n0 50100150200250\\nTrainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .Classiﬁcationerrorrate\\nFigure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis\\nexample,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused\\nforobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient\\nevaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm\\nisplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid\\ncurve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould\\nexpectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t )\\ngradient,thetrainingprocessisreasonablysuccessful.Thevalidationsetclassiﬁcation\\nerrordecreasestoalowlevel.\\ntheg\\ue03eHgterm.Inmanycases,thegradientnormdoesnotshrinksigniﬁcantly\\nthroughoutlearning,buttheg\\ue03eHgtermgrowsbymorethananorderofmagnitude.\\nTheresultisthatlearningbecomesveryslowdespitethepresenceofastrong\\ngradientbecausethelearningratemustbeshrunktocompensateforevenstronger\\ncurvature.Figureshowsanexampleofthegradientincreasingsigniﬁcantly 8.1\\nduringthesuccessfultrainingofaneuralnetwork.\\nThoughill-conditioning ispresentinothersettingsbesidesneuralnetwork\\ntraining,someofthetechniquesusedtocombatitinothercontextsareless\\napplicabletoneuralnetworks.Forexample,Newton’smethodisanexcellenttool\\nforminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin\\nthesubsequentsectionswewillarguethatNewton’smethodrequiressigniﬁcant\\nmodiﬁcationbeforeitcanbeappliedtoneuralnetworks.\\n8.2.2LocalMinima\\nOneofthemostprominentfeaturesofaconvexoptimization problemisthatit\\ncanbereducedtotheproblemofﬁndingalocalminimum.Anylocalminimumis\\n2 8 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e15e1448-a048-4484-a67e-3b438bb6e85a', embedding=None, metadata={'page_label': '299', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nguaranteedtobeaglobalminimum.Someconvexfunctionshaveaﬂatregionat\\nthebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch\\naﬂatregionisanacceptablesolution.Whenoptimizingaconvexfunction,we\\nknowthatwehavereachedagoodsolutionifweﬁndacriticalpointofanykind.\\nWithnon-convexfunctions,suchasneuralnets,itispossibletohavemany\\nlocalminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave\\nanextremelylargenumberoflocalminima.However,aswewillsee,thisisnot\\nnecessarilyamajorproblem.\\nNeuralnetworksandanymodelswithmultipleequivalentlyparametrized latent\\nvariablesallhavemultiplelocalminimabecauseofthemodelidentiﬁability\\nproblem.Amodelissaidtobeidentiﬁableifasuﬃcientlylargetrainingsetcan\\nruleoutallbutonesettingofthemodel’sparameters.Modelswithlatentvariables\\nareoftennotidentiﬁablebecausewecanobtainequivalentmodelsbyexchanging\\nlatentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand\\nmodifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming\\nweightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe\\nhave mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden\\nunits.Thiskindofnon-identiﬁabilit yisknownasweightspacesymmetry.\\nInadditiontoweightspacesymmetry,manykindsofneuralnetworkshave\\nadditionalcausesofnon-identiﬁabilit y.Forexample,inanyrectiﬁedlinearor\\nmaxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby\\nαifwealsoscaleallofitsoutgoingweightsby1\\nα.Thismeansthat—ifthecost\\nfunctiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe\\nweightsratherthanthemodels’outputs—everylocalminimumofarectiﬁedlinear\\normaxoutnetworkliesonan( m n×)-dimensionalhyperbolaofequivalentlocal\\nminima.\\nThesemodelidentiﬁabilityissuesmeanthattherecanbeanextremelylarge\\norevenuncountablyinﬁniteamountoflocalminimainaneuralnetworkcost\\nfunction.However,alloftheselocalminimaarisingfromnon-identiﬁabilit yare\\nequivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare\\nnotaproblematicformofnon-convexity.\\nLocalminimacanbeproblematiciftheyhavehighcostincomparisontothe\\nglobalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden\\nunits,thathavelocalminimawithhighercostthantheglobalminimum(Sontag\\nandSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima\\nwithhighcostarecommon,thiscouldposeaseriousproblemforgradient-based\\noptimization algorithms.\\nItremainsanopenquestionwhethertherearemanylocalminimaofhighcost\\n2 8 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d6ec26d-b381-40f4-9903-fae40df7e010', embedding=None, metadata={'page_label': '300', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nfornetworksofpracticalinterestandwhetheroptimization algorithmsencounter\\nthem.Formanyyears,mostpractitioners believedthatlocalminimawerea\\ncommonproblemplaguingneuralnetworkoptimization. Today,thatdoesnot\\nappeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts\\nnowsuspectthat,forsuﬃcientlylargeneuralnetworks,mostlocalminimahavea\\nlowcostfunctionvalue,andthatitisnotimportanttoﬁndatrueglobalminimum\\nratherthantoﬁndapointinparameterspacethathaslowbutnotminimalcost\\n(,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska\\netal.,).2014\\nManypractitioners attributenearlyalldiﬃcultywithneuralnetworkoptimiza-\\ntiontolocalminima.Weencouragepractitioners tocarefullytestforspeciﬁc\\nproblems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe\\nnormofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto\\ninsigniﬁcantsize,theproblemisneitherlocalminimanoranyotherkindofcritical\\npoint.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional\\nspaces,itcanbeverydiﬃculttopositivelyestablishthatlocalminimaarethe\\nproblem.Manystructuresotherthanlocalminimaalsohavesmallgradients.\\n8.2.3Plateaus,SaddlePointsandOtherFlatRegions\\nFormanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima)\\nareinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle\\npoint.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint,\\nwhileothershavealowercost.\\xa0Atasaddlepoint,theHessianmatrixhasboth\\npositiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith\\npositiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying\\nalongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas\\nbeingalocalminimumalongonecross-sectionofthecostfunctionandalocal\\nmaximumalonganothercross-section.Seeﬁgureforanillustration. 4.5\\nManyclasses\\xa0ofrandomfunctionsexhibitthefollowingbehavior:inlow-\\ndimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local\\nminimaarerareandsaddlepointsaremorecommon.Forafunction f: Rn→ Rof\\nthistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows\\nexponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe\\nthattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues.\\xa0The\\nHessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues.\\nImaginethatthesignofeacheigenvalueisgeneratedbyﬂippingacoin.Inasingle\\ndimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads\\nonce.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill\\n2 8 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='379c8c5d-c820-4781-9314-0832e4b1cea2', embedding=None, metadata={'page_label': '301', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nbeheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014\\nAnamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe\\nHessianbecomemorelikelytobepositiveaswereachregionsoflowercost.\\xa0In\\nourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup\\nheads ntimesifweareatacriticalpointwithlowcost.\\xa0Thismeansthatlocal\\nminimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith\\nhighcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely\\nhighcostaremorelikelytobelocalmaxima.\\nThishappensformanyclassesofrandomfunctions.Doesithappenforneural\\nnetworks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989\\n(feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin\\nchapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14\\nlocalminimawithhighercostthantheglobalminimum.Theyobservedwithout\\nproofthattheseresultsextendtodeepernetworkswithoutnonlinearities. The\\noutputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful\\ntostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis\\nanon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust\\nmultiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013\\ntothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin\\nthesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof\\ndeepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014\\nexperimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery\\nmanyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional\\ntheoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom\\nfunctionsrelatedtoneuralnetworksdoessoaswell.\\nWhataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo-\\nrithms?Forﬁrst-orderoptimization algorithmsthatuseonlygradientinformation,\\nthesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle\\npoint.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape\\nsaddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015\\nseverallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample\\ngiveninﬁgure.Thesevisualizationsshowaﬂatteningofthecostfunctionnear 8.2\\naprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe\\ngradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015\\nalsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe\\nrepelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation\\nmaybediﬀerentformorerealisticusesofgradientdescent.\\nForNewton’smethod,\\xa0itisclearthatsaddlepointsconstituteaproblem.\\n2 8 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7aed690-79bf-4cd6-ad3c-6640a372cd2b', embedding=None, metadata={'page_label': '302', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nP r o j e c t i o n2o f θ\\nP r o j e c t i o n 1 o f θJ(\\n)θ\\nFigure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted\\nwithpermissionfromGoodfellow2015 e t a l .().\\xa0Thesevisualizationsappearsimilarfor\\nfeedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied\\ntorealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these\\nvisualizationsusuallydonotshowmanyconspicuousobstacles.\\xa0Priortothesuccessof\\nstochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012,\\nneuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex\\nstructurethanisrevealedbytheseprojections.\\xa0Theprimaryobstaclerevealedbythis\\nprojectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as\\nindicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily.\\nMostoftrainingtimeisspenttraversingtherelativelyﬂatvalleyofthecostfunction,\\nwhichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix\\ninthisregion,orsimplytheneedtocircumnavigatethetall“mountain”visibleinthe\\nﬁgureviaanindirectarcingpath.\\n2 8 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e5767d48-4340-4973-a188-f7e70b918f8f', embedding=None, metadata={'page_label': '303', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nGradientdescentisdesignedtomove“downhill”andisnotexplicitlydesigned\\ntoseekacriticalpoint.Newton’smethod,however,isdesignedtosolvefora\\npointwherethegradientiszero.Withoutappropriatemodiﬁcation,itcanjump\\ntoasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces\\npresumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing\\ngradientdescentforneuralnetworktraining. ()introduceda Dauphinetal.2014\\nsaddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit\\nimprovessigniﬁcantlyoverthetraditionalversion.Second-order methodsremain\\ndiﬃculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds\\npromiseifitcouldbescaled.\\nThereareotherkindsofpointswithzerogradientbesidesminimaandsaddle\\npoints.Therearealsomaxima,\\xa0whic haremuchlikesaddlepointsfromthe\\nperspectiveofoptimization—many algorithmsarenotattractedtothem,\\xa0but\\nunmodiﬁedNewton’smethodis.Maximaofmanyclassesofrandomfunctions\\nbecomeexponentiallyrareinhighdimensionalspace,justlikeminimado.\\nTheremayalsobewide,ﬂatregionsofconstantvalue.Intheselocations,the\\ngradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor\\nproblemsforallnumericaloptimization algorithms.Inaconvexproblem,awide,\\nﬂatregionmustconsistentirelyofglobalminima,butinageneraloptimization\\nproblem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction.\\n8.2.4CliﬀsandExplodingGradients\\nNeuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling\\ncliﬀs,asillustratedinﬁgure.Theseresultfromthemultiplicationofseveral 8.3\\nlargeweightstogether.Onthefaceofanextremelysteepcliﬀstructure,the\\ngradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoﬀ\\nofthecliﬀstructurealtogether.\\n2 8 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='769f9c4e-ba39-4175-bb0b-250245c8a679', embedding=None, metadata={'page_label': '304', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n\\ue077\\n\\ue062\\ue04a\\ue077\\ue03b \\ue062\\n\\ue028\\ue029\\nFigure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor\\nrecurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting\\nfromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery\\nhighderivativesinsomeplaces.Whentheparametersgetclosetosuchacliﬀregion,a\\ngradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe\\noptimizationworkthathadbeendone.\\xa0FigureadaptedwithpermissionfromPascanu\\ne t a l .().2013\\nThecliﬀcanbedangerouswhetherweapproachitfromaboveorfrombelow,\\nbutfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient\\nclippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1\\nthegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection\\nwithinaninﬁnitesimalregion.Whenthetraditionalgradientdescentalgorithm\\nproposestomakeaverylargestep,thegradientclippingheuristicintervenesto\\nreducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion\\nwherethegradientindicatesthedirectionofapproximately steepestdescent.Cliﬀ\\nstructuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks,\\nbecausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor\\nforeachtimestep.Longtemporalsequencesthusincuranextremeamountof\\nmultiplication.\\n8.2.5Long-TermDependencies\\nAnotherdiﬃcultythatneuralnetworkoptimization algorithmsmustovercome\\narises\\xa0when\\xa0thecomputational\\xa0gra ph\\xa0becomes\\xa0extremely\\xa0deep.Feedforward\\nnetworkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent\\nnetworks,describedinchapter,whichconstructverydeepcomputational graphs 10\\n2 8 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e7c2138-a164-4565-b8a0-79b913b45e11', embedding=None, metadata={'page_label': '305', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nbyrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\\nsequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\\npronounceddiﬃculties.\\nForexample,supposethatacomputational graphcontainsapaththatconsists\\nofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul-\\ntiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(λ)V− 1.\\nInthissimplecase,itisstraightforwardtoseethat\\nWt=\\ue000\\nVλVdiag()− 1\\ue001t= ()VdiagλtV− 1. (8.11)\\nAnyeigenvalues λ ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1\\naregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1\\nvanishingandexplodinggradientproblemreferstothefactthatgradients\\nthroughsuchagrapharealsoscaledaccordingtodiag(λ)t.Vanishinggradients\\nmakeitdiﬃculttoknowwhichdirectiontheparametersshouldmovetoimprove\\nthecostfunction,whileexplodinggradientscanmakelearningunstable.Thecliﬀ\\nstructuresdescribedearlierthatmotivategradientclippingareanexampleofthe\\nexplodinggradientphenomenon.\\nTherepeatedmultiplication byWateachtimestepdescribedhereisvery\\nsimilartothepowermethodalgorithmusedtoﬁndthelargesteigenvalueof\\namatrixWandthecorrespondingeigenvector.Fromthispointofviewitis\\nnotsurprisingthatx\\ue03eWtwilleventuallydiscardallcomponentsofxthatare\\northogonaltotheprincipaleigenvectorof.W\\nRecurrentnetworksusethesamematrixWateachtimestep,butfeedforward\\nnetworksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe\\nvanishingandexplodinggradientproblem(,). Sussillo2014\\nWedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks\\nuntilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7\\n8.2.6InexactGradients\\nMostoptimization algorithmsaredesignedwiththeassumptionthatwehave\\naccesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave\\nanoisyorevenbiasedestimateofthesequantities.\\xa0Nearlyeverydeeplearning\\nalgorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch\\noftrainingexamplestocomputethegradient.\\nInothercases,theobjectivefunctionwewanttominimizeisactuallyintractable.\\nWhentheobjectivefunctionisintractable,typicallyitsgradientisintractableas\\nwell.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise\\n2 9 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='38f165bc-10e2-4f22-bd0c-ad826c3df226', embedding=None, metadata={'page_label': '306', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nwiththemoreadvancedmodelsinpart.Forexample,contrastivedivergence III\\ngivesatechniqueforapproximatingthegradientoftheintractablelog-likelihood\\nofaBoltzmannmachine.\\nVariousneuralnetworkoptimization algorithmsaredesignedtoaccountfor\\nimperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing\\nasurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss.\\n8.2.7PoorCorrespondencebetweenLocalandGlobalStructure\\nManyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe\\nlossfunctionatasinglepoint—itcanbediﬃculttomakeasinglestepif J(θ)is\\npoorlyconditionedatthecurrentpointθ,orifθliesonacliﬀ,orifθisasaddle\\npointhidingtheopportunitytomakeprogressdownhillfromthegradient.\\nItispossibletoovercomealloftheseproblemsatasinglepointandstill\\nperformpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes\\nnotpointtowarddistantregionsofmuchlowercost.\\nGoodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto\\nthelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2\\nthelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda\\nmountain-shapedstructure.\\nMuchofresearchintothediﬃcultiesofoptimization hasfocusedonwhether\\ntrainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin\\npracticeneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1\\nshowsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed,\\nsuchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction\\n−log p( y|x;θ)canlackaglobalminimumpointandinsteadasymptotically\\napproachsomevalueasthemodelbecomesmoreconﬁdent.Foraclassiﬁerwith\\ndiscrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan\\nbecomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery\\nexampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof\\nzero.Likewise,amodelofrealvalues p( y|x) =N( y; f(θ) , β− 1)canhavenegative\\nlog-likelihoodthatasymptotestonegativeinﬁnity—if f(θ)isabletocorrectly\\npredictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease\\nβwithoutbound.Seeﬁgureforanexampleofafailureoflocaloptimization to 8.4\\nﬁndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle\\npoints.\\nFutureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat\\ninﬂuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome\\n2 9 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bb09e178-ad7f-4f80-82bb-83da61573ac1', embedding=None, metadata={'page_label': '307', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nθJ ( ) θ\\nFigure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes\\nnotpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur,\\neveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction\\ncontainsonlyasymptotestowardlowvalues,notminima.Themaincauseofdiﬃcultyin\\nthiscaseisbeinginitializedonthewrongsideofthe“mountain”andnotbeingableto\\ntraverseit.\\xa0Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate\\nsuchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin\\nexcessivetrainingtime,asillustratedinﬁgure.8.2\\noftheprocess.\\nManyexistingresearchdirectionsareaimedatﬁndinggoodinitialpointsfor\\nproblemsthathavediﬃcultglobalstructure,ratherthandevelopingalgorithms\\nthatusenon-localmoves.\\nGradientdescentandessentiallyalllearningalgorithmsthatareeﬀectivefor\\ntrainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious\\nsectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves\\ncanbediﬃculttocompute.Wemaybeabletocomputesomepropertiesofthe\\nobjectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance\\ninourestimateofthecorrectdirection.Inthesecases,localdescentmayormay\\nnotdeﬁneareasonablyshortpathtoavalidsolution,butwearenotactually\\nabletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues\\nsuchaspoorconditioningordiscontinuousgradients,causingtheregionwhere\\nthegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In\\nthesecases,localdescentwithstepsofsize \\ue00fmaydeﬁneareasonablyshortpath\\ntothesolution,butweareonlyabletocomputethelocaldescentdirectionwith\\nstepsofsize δ \\ue00f\\ue01c.Inthesecases,localdescentmayormaynotdeﬁneapath\\ntothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa\\n2 9 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1aa0adf6-7d6e-408f-95d8-ed9105ff5306', embedding=None, metadata={'page_label': '308', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nhighcomputational cost.Sometimeslocalinformationprovidesusnoguide,when\\nthefunctionhasawideﬂatregion,orifwemanagetolandexactlyonacritical\\npoint(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly\\nforcriticalpoints,suchasNewton’smethod).Inthesecases,localdescentdoes\\nnotdeﬁneapathtoasolutionatall.Inothercases,localmovescanbetoogreedy\\nandleadusalongapaththatmovesdownhillbutawayfromanysolution,asin\\nﬁgure,oralonganunnecessarilylongtrajectorytothesolution,asinﬁgure. 8.4 8.2\\nCurrently,wedonotunderstandwhichoftheseproblemsaremostrelevantto\\nmakingneuralnetworkoptimization diﬃcult,andthisisanactiveareaofresearch.\\nRegardlessofwhichoftheseproblemsaremostsigniﬁcant,allofthemmightbe\\navoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution\\nbyapaththatlocaldescentcanfollow,andifweareabletoinitializelearning\\nwithinthatwell-behavedregion.\\xa0Thislastviewsuggestsresearchintochoosing\\ngoodinitialpointsfortraditionaloptimization algorithmstouse.\\n8.2.8TheoreticalLimitsofOptimization\\nSeveraltheoreticalresultsshowthattherearelimitsontheperformanceofany\\noptimization algorithmwemightdesignforneuralnetworks(BlumandRivest,\\n1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave\\nlittlebearingontheuseofneuralnetworksinpractice.\\nSometheoreticalresultsapplyonlytothecasewheretheunitsofaneural\\nnetworkoutput\\xa0discretevalues.However,\\xa0most\\xa0neuralnetworkunitsoutput\\nsmoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some\\ntheoreticalresultsshowthatthereexistproblemclassesthatareintractable,but\\nitcanbediﬃculttotellwhetheraparticularproblemfallsintothatclass.Other\\nresultsshowthatﬁndingasolutionforanetworkofagivensizeisintractable,but\\ninpracticewecanﬁndasolutioneasilybyusingalargernetworkforwhichmany\\nmoreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe\\ncontextofneuralnetworktraining,weusuallydonotcareaboutﬁndingtheexact\\nminimumofafunction,butseekonlytoreduceitsvaluesuﬃcientlytoobtaingood\\ngeneralization error.\\xa0Theoretical analysisofwhetheranoptimization algorithm\\ncanaccomplishthisgoalisextremelydiﬃcult.Developingmorerealisticbounds\\nontheperformanceofoptimization algorithmsthereforeremainsanimportant\\ngoalformachinelearningresearch.\\n2 9 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ab3d8427-bd0e-4d85-af31-f02ca8ce0d89', embedding=None, metadata={'page_label': '309', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n8.3BasicAlgorithms\\nWehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3\\nfollowsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated\\nconsiderablybyusingstochasticgradientdescenttofollowthegradientofrandomly\\nselectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3\\n8.3.1StochasticGradientDescent\\nStochasticgradientdescent(SGD)anditsvariantsareprobablythemostused\\noptimization algorithmsformachinelearningingeneralandfordeeplearning\\ninparticular.\\xa0As discussedinsection,itispossibletoobtainanunbiased 8.1.3\\nestimateofthegradientbytakingtheaveragegradientonaminibatchof m\\nexamplesdrawni.i.dfromthedatageneratingdistribution.\\nAlgorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1\\nAlgorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k\\nRequire:Learningrate \\ue00f k.\\nRequire:Initialparameterθ\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradientestimate: ˆg←+1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nApplyupdate:θθ← − \\ue00fˆg\\nendwhile\\nAcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we\\nhavedescribedSGDasusingaﬁxedlearningrate \\ue00f.Inpractice,itisnecessaryto\\ngraduallydecreasethelearningrateovertime,sowenowdenotethelearningrate\\natiterationas k \\ue00f k.\\nThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the\\nrandomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive\\nataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes\\nsmallandthen 0whenweapproachandreachaminimumusingbatchgradient\\ndescent,sobatchgradientdescentcanuseaﬁxedlearningrate.Asuﬃcient\\nconditiontoguaranteeconvergenceofSGDisthat\\n∞\\ue058\\nk = 1\\ue00f k= and ∞ , (8.12)\\n2 9 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5305c422-011d-4186-ac4c-cb9358d86738', embedding=None, metadata={'page_label': '310', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n∞\\ue058\\nk = 1\\ue00f2\\nk < .∞ (8.13)\\nInpractice,itiscommontodecaythelearningratelinearlyuntiliteration: τ\\n\\ue00f k= (1 )− α \\ue00f 0+ α \\ue00f τ (8.14)\\nwith α=k\\nτ.Afteriteration,itiscommontoleaveconstant. τ \\ue00f\\nThelearningratemaybechosenbytrialanderror,butitisusuallybest\\ntochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa\\nfunctionoftime.Thisismoreofanartthanascience,andmostguidanceonthis\\nsubjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule,\\ntheparameterstochooseare \\ue00f 0, \\ue00f τ,and τ.Usually τmaybesettothenumberof\\niterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually\\n\\ue00f τshouldbesettoroughlythevalueof 1% \\ue00f 0.Themainquestionishowtoset \\ue00f 0.\\nIfitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost\\nfunctionoftenincreasingsigniﬁcantly.Gentleoscillationsareﬁne,especiallyif\\ntrainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe\\nuseofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe\\ninitiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue.\\nTypically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe\\nﬁnalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance\\naftertheﬁrst100iterationsorso.Therefore,itisusuallybesttomonitortheﬁrst\\nseveraliterationsandusealearningratethatishigherthanthebest-performing\\nlearningrateatthistime,butnotsohighthatitcausessevereinstability.\\nThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient-\\nbasedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe\\nnumberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber\\noftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay\\nconvergetowithinsomeﬁxedtoleranceofitsﬁnaltestseterrorbeforeithas\\nprocessedtheentiretrainingset.\\nTostudytheconvergencerateofanoptimization algorithmitiscommonto\\nmeasuretheexcesserror J(θ)−min θ J(θ),whichistheamountthatthecurrent\\ncostfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex\\nproblem,theexcesserroris O(1√\\nk)after kiterations,whileinthestronglyconvex\\ncaseitis O(1\\nk).Theseboundscannotbeimprovedunlessextraconditionsare\\nassumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic\\ngradientdescentintheory.However,theCramér-Raobound(,;, Cramér1946Rao\\n1945)statesthatgeneralization errorcannotdecreasefasterthan O(1\\nk).Bottou\\n2 9 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b204eaed-399e-4eac-8023-ad6396b66908', embedding=None, metadata={'page_label': '311', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nandBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue\\nanoptimization algorithmthatconvergesfasterthan O(1\\nk)formachinelearning\\ntasks—fasterconvergencepresumablycorrespondstooverﬁtting.Moreover,the\\nasymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent\\nhasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake\\nrapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples\\noutweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin\\ntheremainderofthischapterachievebeneﬁtsthatmatterinpracticebutarelost\\nintheconstantfactorsobscuredbythe O(1\\nk)asymptoticanalysis.Onecanalso\\ntradeoﬀthebeneﬁtsofbothbatchandstochasticgradientdescentbygradually\\nincreasingtheminibatchsizeduringthecourseoflearning.\\nFormoreinformationonSGD,see(). Bottou1998\\n8.3.2Momentum\\nWhilestochasticgradientdescentremainsaverypopularoptimization strategy,\\nlearningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,)\\nisdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut\\nconsistentgradients,ornoisygradients.Themomentumalgorithmaccumulates\\nanexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove\\nintheirdirection.Theeﬀectofmomentumisillustratedinﬁgure.8.5\\nFormally,themomentumalgorithmintroducesavariablevthatplaystherole\\nofvelocity—itisthedirectionandspeedatwhichtheparametersmovethrough\\nparameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe\\nnegativegradient.Thenamemomentumderivesfromaphysicalanalogy,in\\nwhichthenegativegradientisaforcemovingaparticlethroughparameterspace,\\naccordingtoNewton’slawsofmotion.Momentuminphysicsismasstimesvelocity.\\nInthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv\\nmayalsoberegardedasthemomentumoftheparticle.Ahyperparameter α∈[0 ,1)\\ndetermineshowquicklythecontributionsofpreviousgradientsexponentiallydecay.\\nTheupdateruleisgivenby:\\nvv← α−∇ \\ue00f θ\\ue020\\n1\\nmm \\ue058\\ni = 1L((fx( ) i;)θ ,y( ) i)\\ue021\\n, (8.15)\\nθθv ← + . (8.16)\\nThevelocityvaccumulatesthegradientelements∇ θ\\ue0001\\nm\\ue050m\\ni = 1 L((fx( ) i;)θ ,y( ) i)\\ue001\\n.\\nThelarger αisrelativeto \\ue00f,themorepreviousgradientsaﬀectthecurrentdirection.\\nTheSGDalgorithmwithmomentumisgiveninalgorithm .8.2\\n2 9 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='998ad1b2-2a65-4916-b9c9-1852abbebb49', embedding=None, metadata={'page_label': '312', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n− − − 3 0 2 0 1 0 0 1 0 2 0− 3 0− 2 0− 1 001 02 0\\nFigure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe\\nHessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum\\novercomestheﬁrstofthesetwoproblems.Thecontourlinesdepictaquadraticloss\\nfunctionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe\\ncontoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis\\nfunction.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient\\ndescentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective\\nlookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses\\nthecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe\\nnarrowaxisofthecanyon.Comparealsoﬁgure,whichshowsthebehaviorofgradient 4.6\\ndescentwithoutmomentum.\\n2 9 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b27d1c11-366d-4f08-92d3-fd10564419c2', embedding=None, metadata={'page_label': '313', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nPreviously,thesizeofthestepwassimplythenormofthegradientmultiplied\\nbythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow\\nalignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive\\ngradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways\\nobservesgradientg,thenitwillaccelerateinthedirectionof−g,untilreachinga\\nterminalvelocitywherethesizeofeachstepis\\n\\ue00f||||g\\n1− α. (8.17)\\nItisthushelpfultothinkofthemomentumhyperparameterintermsof1\\n1 − α.For\\nexample, α= .9correspondstomultiplyingthemaximumspeedbyrelativeto 10\\nthegradientdescentalgorithm.\\nCommonvaluesof αusedinpracticeinclude .5, .9,and .99.Likethelearning\\nrate, αmayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand\\nislaterraised.Itislessimportanttoadapt αovertimethantoshrink \\ue00fovertime.\\nAlgorithm8.2Stochasticgradientdescent(SGD)withmomentum\\nRequire:Learningrate,momentumparameter. \\ue00f α\\nRequire:Initialparameter,initialvelocity. θ v\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradientestimate:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nComputevelocityupdate:vvg ← α− \\ue00f\\nApplyupdate:θθv ← +\\nendwhile\\nWecanviewthemomentumalgorithmassimulatingaparticlesubjectto\\ncontinuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild\\nintuitionforhowthemomentumandgradientdescentalgorithmsbehave.\\nThepositionoftheparticleatanypointintimeisgivenbyθ( t).Theparticle\\nexperiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t\\nf() = t∂2\\n∂ t2θ() t . (8.18)\\nRatherthanviewingthisasasecond-orderdiﬀerentialequationoftheposition,\\nwecanintroducethevariablev( t)representingthevelocityoftheparticleattime\\ntandrewritetheNewtoniandynamicsasaﬁrst-orderdiﬀerentialequation:\\nv() = t∂\\n∂ tθ() t , (8.19)\\n2 9 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='383b0add-6e6c-4a09-91d3-7eecef431553', embedding=None, metadata={'page_label': '314', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nf() = t∂\\n∂ tv() t . (8.20)\\nThemomentumalgorithmthenconsistsofsolvingthediﬀerentialequationsvia\\nnumericalsimulation.Asimplenumericalmethodforsolvingdiﬀerentialequations\\nisEuler’smethod,whichsimplyconsistsofsimulatingthedynamicsdeﬁnedby\\ntheequationbytakingsmall,ﬁnitestepsinthedirectionofeachgradient.\\nThisexplainsthebasicformofthemomentumupdate,butwhatspeciﬁcallyare\\ntheforces?Oneforceisproportionaltothenegativegradientofthecostfunction:\\n−∇ θ J(θ).Thisforcepushestheparticledownhillalongthecostfunctionsurface.\\nThegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach\\ngradient,buttheNewtonianscenariousedbythemomentumalgorithminstead\\nusesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle\\nasbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa\\nsteeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection\\nuntilitbeginstogouphillagain.\\nOneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction,\\nthentheparticlemightnevercometorest.Imagineahockeypuckslidingdown\\nonesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever,\\nassumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone\\notherforce,proportionalto−v( t).Inphysicsterminology,thisforcecorresponds\\ntoviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas\\nsyrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually\\nconvergetoalocalminimum.\\nWhydoweuse−v( t)andviscousdraginparticular?\\xa0Partofthereasonto\\nuse−v( t)ismathematical convenience—anintegerpowerofthevelocityiseasy\\ntoworkwith.However,otherphysicalsystemshaveotherkindsofdragbased\\nonotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough\\ntheairexperiencesturbulentdrag,withforceproportionaltothesquareofthe\\nvelocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha\\nforceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag,\\nproportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis\\nsmall.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle\\nwithanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag\\nwillmoveawayfromitsinitialpositionforever,withthedistancefromthestarting\\npointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity.\\nIfweuseapowerofzero,representingdryfriction,thentheforceistoostrong.\\nWhentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the\\nconstantforceduetofrictioncancausetheparticletocometorestbeforereaching\\nalocalminimum.Viscousdragavoidsbothoftheseproblems—itisweakenough\\n2 9 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45bad067-a518-4a77-ae20-e11c6b610344', embedding=None, metadata={'page_label': '315', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nthatthegradientcancontinuetocausemotionuntilaminimumisreached,but\\nstrongenoughtopreventmotionifthegradientdoesnotjustifymoving.\\n8.3.3NesterovMomentum\\nSutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas\\ninspiredbyNesterov’sacceleratedgradientmethod(,,).The Nesterov19832004\\nupdaterulesinthiscasearegivenby:\\nvv← α−∇ \\ue00f θ\\ue022\\n1\\nmm \\ue058\\ni = 1L\\ue010\\nfx(( ) i;+ )θ αv ,y( ) i\\ue011\\ue023\\n,(8.21)\\nθθv ← + , (8.22)\\nwheretheparameters αand \\ue00fplayasimilarroleasinthestandardmomentum\\nmethod.ThediﬀerencebetweenNesterovmomentumandstandardmomentumis\\nwherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated\\nafterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum\\nasattemptingtoaddacorrectionfactortothestandardmethodofmomentum.\\nThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3\\nIntheconvexbatchgradientcase,Nesterovmomentumbringstherateof\\nconvergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown\\nbyNesterov1983().Unfortunately,\\xa0inthestochasticgradientcase,Nesterov\\nmomentumdoesnotimprovetherateofconvergence.\\nAlgorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum\\nRequire:Learningrate,momentumparameter. \\ue00f α\\nRequire:Initialparameter,initialvelocity. θ v\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondinglabelsy( ) i.\\nApplyinterimupdate: ˜θθv ← + α\\nComputegradient(atinterimpoint):g←1\\nm∇ ˜ θ\\ue050\\ni L f((x( ) i;˜θy) ,( ) i)\\nComputevelocityupdate:vvg ← α− \\ue00f\\nApplyupdate:θθv ← +\\nendwhile\\n3 0 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='620645d9-364f-4424-a2cb-f123666b23ea', embedding=None, metadata={'page_label': '316', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n8.4ParameterInitializationStrategies\\nSomeoptimization algorithmsarenotiterativebynatureandsimplysolvefora\\nsolutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when\\nappliedtotherightclassofoptimization problems,convergetoacceptablesolutions\\ninanacceptableamountoftimeregardlessofinitialization. Deeplearningtraining\\nalgorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep\\nlearningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify\\nsomeinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep\\nmodelsisasuﬃcientlydiﬃculttaskthatmostalgorithmsarestronglyaﬀectedby\\nthechoiceofinitialization. Theinitialpointcandeterminewhetherthealgorithm\\nconvergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm\\nencountersnumericaldiﬃcultiesandfailsaltogether.Whenlearningdoesconverge,\\ntheinitialpointcandeterminehowquicklylearningconvergesandwhetherit\\nconvergestoapointwithhigh\\xa0orlowcost.Also,\\xa0pointsofcomparablecost\\ncanhavewildlyvaryinggeneralization error,andtheinitialpointcanaﬀectthe\\ngeneralization aswell.\\nModerninitialization strategiesaresimpleandheuristic.Designingimproved\\ninitialization strategiesisadiﬃculttaskbecauseneuralnetworkoptimization is\\nnotyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome\\nnicepropertieswhenthenetworkisinitialized.However,wedonothaveagood\\nunderstandingofwhichofthesepropertiesarepreservedunderwhichcircumstances\\nafterlearningbeginstoproceed.Afurtherdiﬃcultyisthatsomeinitialpoints\\nmaybebeneﬁcialfromtheviewpointofoptimization butdetrimentalfromthe\\nviewpointofgeneralization. Ourunderstandingofhowtheinitialpointaﬀects\\ngeneralization isespeciallyprimitive,oﬀeringlittletonoguidanceforhowtoselect\\ntheinitialpoint.\\nPerhapstheonlypropertyknownwithcompletecertaintyisthattheinitial\\nparametersneedto“breaksymmetry”\\xa0betweendiﬀerentunits.Iftwohidden\\nunitswiththesameactivationfunctionareconnectedtothesameinputs,then\\ntheseunitsmusthavediﬀerentinitialparameters.\\xa0Iftheyhavethesameinitial\\nparameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost\\nandmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe\\nmodelortrainingalgorithmiscapableofusingstochasticitytocomputediﬀerent\\nupdatesfordiﬀerentunits(forexample,ifonetrainswithdropout),itisusually\\nbesttoinitializeeachunittocomputeadiﬀerentfunctionfromalloftheother\\nunits.Thismayhelptomakesurethatnoinputpatternsarelostinthenull\\nspaceofforwardpropagationandnogradientpatternsarelostinthenullspace\\nofback-propagation. Thegoalofhavingeachunitcomputeadiﬀerentfunction\\n3 0 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0462bd76-f346-4666-a530-a7220513d6b6', embedding=None, metadata={'page_label': '317', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nmotivatesrandominitialization oftheparameters.Wecouldexplicitlysearch\\nforalargesetofbasisfunctionsthatareallmutuallydiﬀerentfromeachother,\\nbutthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat\\nmostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization\\nonaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery\\ndiﬀerentfunctionfromeachotherunit.Randominitialization fromahigh-entropy\\ndistributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely\\ntoassignanyunitstocomputethesamefunctionaseachother.\\nTypically,wesetthebiasesforeachunittoheuristicallychosenconstants,and\\ninitializeonlytheweightsrandomly.Extraparameters,forexample,parameters\\nencodingtheconditionalvarianceofaprediction,areusuallysettoheuristically\\nchosenconstantsmuchlikethebiasesare.\\nWealmostalwaysinitializealltheweightsin\\xa0themodel\\xa0tovalues\\xa0drawn\\nrandomly\\xa0froma\\xa0Gaussian\\xa0oruniform\\xa0distribution.Thechoiceof\\xa0Gaussian\\noruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen\\nexhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea\\nlargeeﬀectonboththeoutcomeoftheoptimization procedureandontheability\\nofthenetworktogeneralize.\\nLargerinitialweightswillyieldastrongersymmetrybreakingeﬀect,helping\\ntoavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor\\nback-propagationthroughthelinearcomponentofeachlayer—largervaluesinthe\\nmatrixresultinlargeroutputsofmatrixmultiplication. Initialweightsthatare\\ntoolargemay,however,resultinexplodingvaluesduringforwardpropagationor\\nback-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos\\n(suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior\\nofthedeterministicforwardpropagationprocedureappearsrandom).\\xa0Tosome\\nextent,theexplodinggradientproblemcanbemitigatedbygradientclipping\\n(thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep).\\nLargeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction\\ntosaturate,causingcompletelossofgradientthroughsaturatedunits.These\\ncompetingfactorsdeterminetheidealinitialscaleoftheweights.\\nTheperspectivesofregularizationandoptimization cangiveverydiﬀerent\\ninsightsintohowweshouldinitializeanetwork.Theoptimization perspective\\nsuggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess-\\nfully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse\\nofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall\\nincrementalchangestotheweightsandtendstohaltinareasthatarenearerto\\ntheinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or\\n3 0 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='812e750a-9df2-43a2-a6cc-f3597d0238f5', embedding=None, metadata={'page_label': '318', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nduetotriggeringsomeearlystoppingcriterionbasedonoverﬁtting)expressesa\\npriorthattheﬁnalparametersshouldbeclosetotheinitialparameters.Recall\\nfromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8\\ndecayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis\\nnotthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout\\ntheeﬀectofinitialization. Wecanthinkofinitializingtheparametersθtoθ 0as\\nbeingsimilartoimposingaGaussianprior p(θ)withmeanθ 0.Fromthispoint\\nofview,itmakessensetochooseθ 0tobenear0.Thispriorsaysthatitismore\\nlikelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units\\ninteractonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong\\npreferenceforthemtointeract.Ontheotherhand,ifweinitializeθ 0tolarge\\nvalues,thenourpriorspeciﬁeswhichunitsshouldinteractwitheachother,and\\nhowtheyshouldinteract.\\nSomeheuristicsareavailableforchoosingtheinitialscaleoftheweights.One\\nheuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand\\nnoutputsbysamplingeachweightfrom U(−1√m,1√m),whileGlorotandBengio\\n()suggestusingthe 2010 normalizedinitialization\\nW i , j∼ U\\ue020\\n−\\ue072\\n6\\nm n+,\\ue072\\n6\\nm n+\\ue021\\n. (8.23)\\nThislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing\\nalllayerstohavethesameactivationvarianceandthegoalofinitializingall\\nlayerstohavethesamegradientvariance.Theformulaisderivedusingthe\\nassumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications,\\nwithnononlinearities. Realneuralnetworksobviouslyviolatethisassumption,\\nbutmanystrategiesdesignedforthelinearmodelperformreasonablywellonits\\nnonlinearcounterparts.\\nSaxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with\\nacarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied\\nateachlayer.Theyderivespeciﬁcvaluesofthescalingfactorfordiﬀerenttypesof\\nnonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya\\nmodelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities.\\nUndersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof\\ntrainingiterationsrequiredtoreachconvergenceisindependentofdepth.\\nIncreasingthescalingfactor gpushesthenetworktowardtheregimewhere\\nactivationsincreaseinnormastheypropagateforwardthroughthenetworkand\\ngradientsincreaseinnormastheypropagatebackward.\\xa0 ()showed Sussillo2014\\nthatsettingthegainfactorcorrectlyissuﬃcienttotrainnetworksasdeepas\\n3 0 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2577cd98-bde4-4db1-ad2e-8aa6aca7a7f7', embedding=None, metadata={'page_label': '319', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n1,000layers,withoutneedingtouseorthogonalinitializations.\\xa0A keyinsightof\\nthisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow\\norshrinkoneachstepofforwardorback-propagation, followingarandomwalk\\nbehavior.Thisisbecausefeedforwardnetworksuseadiﬀerentweightmatrixat\\neachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward\\nnetworkscanmostlyavoidthevanishingandexplodinggradientsproblemthat\\nariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5\\nUnfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto\\noptimalperformance.Thismaybeforthreediﬀerentreasons.First,wemay\\nbeusingthewrongcriteria—itmaynotactuallybebeneﬁcialtopreservethe\\nnormofasignalthroughouttheentirenetwork.Second,thepropertiesimposed\\natinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the\\ncriteriamightsucceedatimprovingthespeedofoptimization butinadvertently\\nincreasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe\\nweightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut\\nnotexactlyequaltothetheoreticalpredictions.\\nOnedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe\\nsamestandarddeviation,suchas1√m,isthateveryindividualweightbecomes\\nextremelysmallwhenthelayersbecomelarge. ()introducedan Martens2010\\nalternativeinitialization schemecalledsparseinitializationinwhicheachunitis\\ninitializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount\\nofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe\\nmagnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps\\ntoachievemorediversityamongtheunitsatinitialization time.However,italso\\nimposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian\\nvalues.Becauseittakesalongtimeforgradientdescenttoshrink“incorrect”large\\nvalues,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits\\nthathaveseveralﬁltersthatmustbecarefullycoordinatedwitheachother.\\nWhencomputational resourcesallowit,itisusuallyagoodideatotreatthe\\ninitialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese\\nscalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2\\nasrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization\\ncanalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor\\nthebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto\\nlookattherangeorstandarddeviationofactivationsorgradientsonasingle\\nminibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe\\nminibatchwillshrinkastheactivationspropagateforwardthroughthenetwork.\\nByrepeatedlyidentifyingtheﬁrstlayerwithunacceptably smallactivationsand\\n3 0 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d57ff4af-4a8a-41f2-92e8-b831e2216ff4', embedding=None, metadata={'page_label': '320', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nincreasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable\\ninitialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe\\nusefultolookattherangeorstandarddeviationofthegradientsaswellasthe\\nactivations.\\xa0Thisprocedurecaninprinciplebeautomatedandisgenerallyless\\ncomputationally costlythanhyperparameter optimization basedonvalidationset\\nerrorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona\\nsinglebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation\\nset.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciﬁedmore\\nformallyandstudiedby (). MishkinandMatas2015\\nSo\\xa0far\\xa0we\\xa0have\\xa0focused\\xa0on\\xa0the\\xa0initialization\\xa0ofthe\\xa0weights.Fortunately,\\ninitialization ofotherparametersistypicallyeasier.\\nTheapproachforsettingthebiasesmustbecoordinatedwiththeapproach\\nforsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight\\ninitialization schemes.Thereareafewsituationswherewemaysetsomebiasesto\\nnon-zerovalues:\\n•Ifabiasisforanoutputunit,thenitisoftenbeneﬁcialtoinitializethebiasto\\nobtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat\\ntheinitialweightsaresmallenoughthattheoutputoftheunitisdetermined\\nonlybythebias.Thisjustiﬁessettingthebiastotheinverseoftheactivation\\nfunctionappliedtothemarginalstatisticsoftheoutputinthetrainingset.\\nForexample,iftheoutputisadistributionoverclassesandthisdistribution\\nisahighlyskeweddistributionwiththemarginalprobabilityofclass igiven\\nbyelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving\\ntheequationsoftmax (b) =c.Thisappliesnotonlytoclassiﬁersbutalsoto\\nmodelswewillencounterinPart,suchasautoencodersandBoltzmann III\\nmachines.Thesemodelshavelayerswhoseoutputshouldresembletheinput\\ndatax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto\\nmatchthemarginaldistributionover.x\\n•Sometimeswemay\\xa0wanttochoosethebiastoavoidcausingtoo\\xa0much\\nsaturationatinitialization. Forexample,wemaysetthebiasofaReLU\\nhiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization.\\nThisapproachisnotcompatiblewithweightinitialization schemesthatdo\\nnotexpectstronginputfromthebiasesthough.Forexample,\\xa0itisnot\\nrecommendedforusewithrandomwalkinitialization (,). Sussillo2014\\n•Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina\\nfunction.Insuchsituations,wehaveaunitwithoutput uandanotherunit\\nh∈[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h.\\xa0We\\n3 0 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b6abbcfe-d3fc-4934-8021-2c7571258dc5', embedding=None, metadata={'page_label': '321', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\ncanview hasagatethatdetermineswhether u h u≈or u h≈0.\\xa0Inthese\\nsituations,wewanttosetthebiasfor hsothat h≈1mostofthetimeat\\ninitialization. Otherwise udoesnothaveachancetolearn.Forexample,\\nJozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1\\ntheLSTMmodel,describedinsection.10.10\\nAnothercommontypeofparameterisavarianceorprecisionparameter.For\\nexample,wecanperformlinearregressionwithaconditionalvarianceestimate\\nusingthemodel\\np y y (| Nx) = (|wTx+1) b , /β (8.24)\\nwhere βisaprecisionparameter.Wecanusuallyinitializevarianceorprecision\\nparametersto1safely.Anotherapproachistoassumetheinitialweightsareclose\\nenoughtozerothatthebiasesmaybesetwhileignoringtheeﬀectoftheweights,\\nthensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset\\nthevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset.\\nBesidesthesesimpleconstantorrandommethodsofinitializingmodelparame-\\nters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon\\nstrategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III\\ntheparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs.\\nOnecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming\\nsupervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that\\noﬀersfasterconvergencethanarandominitialization. Someoftheseinitialization\\nstrategiesmayyieldfasterconvergenceandbettergeneralization becausethey\\nencodeinformationaboutthedistributionintheinitialparametersofthemodel.\\nOthersapparentlyperformwellprimarilybecausetheysettheparameterstohave\\ntherightscaleorsetdiﬀerentunitstocomputediﬀerentfunctionsfromeachother.\\n8.5AlgorithmswithAdaptiveLearningRates\\nNeuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone\\nofthehyperparameters thatisthemostdiﬃculttosetbecauseithasasigniﬁcant\\nimpactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2\\ncostisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive\\ntoothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but\\ndoessoattheexpenseofintroducinganotherhyperparameter. Inthefaceofthis,\\nitisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof\\nsensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning\\n3 0 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ceed0fde-e0cc-4999-aab4-17ca3d5fe5ab', embedding=None, metadata={'page_label': '322', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nrateforeachparameter,andautomatically adapttheselearningratesthroughout\\nthecourseoflearning.\\nThe algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988\\ntoadaptingindividuallearningratesformodelparametersduringtraining.The\\napproachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect\\ntoagivenmodelparameter,remainsthesamesign,thenthelearningrateshould\\nincrease.Ifthepartialderivativewithrespecttothatparameterchangessign,\\nthenthelearningrateshoulddecrease.\\xa0Ofcourse,thiskindofrulecanonlybe\\nappliedtofullbatchoptimization.\\nMorerecently,anumberofincremental(ormini-batch-bas ed)methodshave\\nbeenintroducedthatadaptthelearningratesofmodelparameters.Thissection\\nwillbrieﬂyreviewafewofthesealgorithms.\\n8.5.1AdaGrad\\nTheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4\\nratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare\\nrootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011\\nparameterswiththelargestpartialderivativeofthelosshaveacorrespondingly\\nrapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives\\nhavearelativelysmalldecreaseintheirlearningrate.Theneteﬀectisgreater\\nprogressinthemoregentlyslopeddirectionsofparameterspace.\\nInthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome\\ndesirabletheoreticalproperties.However,empiricallyithasbeenfoundthat—for\\ntrainingdeepneuralnetworkmodels—theaccumulation ofsquaredgradientsfrom\\nthebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe\\neﬀectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning\\nmodels.\\n8.5.2RMSProp\\nTheRMSPropalgorithm(,)modiﬁesAdaGradtoperformbetterin Hinton2012\\nthenon-convexsettingbychangingthegradientaccumulation intoanexponentially\\nweightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied\\ntoaconvexfunction.\\xa0When appliedtoanon-convexfunctiontotrainaneural\\nnetwork,thelearningtrajectorymaypassthroughmanydiﬀerentstructuresand\\neventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe\\nlearningrateaccordingtotheentirehistoryofthesquaredgradientandmay\\n3 0 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e0cf702e-4ad3-4ae3-b812-b3b9e98debe0', embedding=None, metadata={'page_label': '323', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.4TheAdaGradalgorithm\\nRequire:Globallearningrate \\ue00f\\nRequire:Initialparameterθ\\nRequire:Smallconstant,perhaps δ 10− 7,fornumericalstability\\nInitializegradientaccumulationvariabler= 0\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradient:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nAccumulatesquaredgradient:rrgg ←+\\ue00c\\nComputeupdate: ∆θ←−\\ue00f\\nδ +√r\\ue00cg.(Divisionandsquarerootapplied\\nelement-wise)\\nApplyupdate:θθθ ← +∆\\nendwhile\\nhavemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure.\\nRMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe\\nextremepastsothatitcanconvergerapidlyafterﬁndingaconvexbowl,asifit\\nwereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl.\\nRMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5\\nNesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6\\nmovingaverageintroducesanewhyperparameter, ρ,thatcontrolsthelengthscale\\nofthemovingaverage.\\nEmpirically,RMSProphasbeenshowntobeaneﬀectiveandpracticalop-\\ntimizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to\\noptimization methodsbeingemployedroutinelybydeeplearningpractitioners.\\n8.5.3Adam\\nAdam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014\\nalgorithmandispresentedinalgorithm .Thename“Adam”\\xa0derivesfrom 8.7\\nthephrase“adaptivemoments.”Inthecontextoftheearlieralgorithms,itis\\nperhapsbestseenasavariantonthecombinationofRMSPropandmomentum\\nwithafewimportantdistinctions.First,inAdam,momentumisincorporated\\ndirectlyasanestimateoftheﬁrstordermoment(withexponentialweighting)of\\nthegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto\\napplymomentumtotherescaledgradients.Theuseofmomentumincombination\\nwithrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes\\n3 0 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='06eabca5-393d-4278-bcd8-36d7b47ded0a', embedding=None, metadata={'page_label': '324', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.5TheRMSPropalgorithm\\nRequire:Globallearningrate,decayrate. \\ue00f ρ\\nRequire:Initialparameterθ\\nRequire:Smallconstant δ,\\xa0usually 10− 6,\\xa0usedtostabilizedivision\\xa0bysmall\\nnumbers.\\nInitializeaccumulation variablesr= 0\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradient:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nAccumulatesquaredgradient:rrgg ← ρ+(1 )− ρ\\ue00c\\nComputeparameterupdate: ∆θ=−\\ue00f√\\nδ + r\\ue00cg.(1√\\nδ + rappliedelement-wise)\\nApplyupdate:θθθ ← +∆\\nendwhile\\nbiascorrectionstotheestimatesofboththeﬁrst-ordermoments(themomentum\\nterm)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization\\nattheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7\\n(uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus,\\nunlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias\\nearlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice\\nofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom\\nthesuggesteddefault.\\n8.5.4ChoosingtheRightOptimizationAlgorithm\\nInthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress\\nthechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach\\nmodelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone\\nchoose?\\nUnfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014\\npresentedavaluablecomparisonofalargenumberofoptimization algorithms\\nacrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof\\nalgorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta)\\nperformedfairlyrobustly,nosinglebestalgorithmhasemerged.\\nCurrently,themostpopularoptimization algorithmsactivelyinuseinclude\\nSGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta\\nandAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend\\n3 0 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c5a13d08-be0a-4049-89bb-6994666df3c9', embedding=None, metadata={'page_label': '325', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.6RMSPropalgorithmwithNesterovmomentum\\nRequire:Globallearningrate,decayrate,momentumcoeﬃcient. \\ue00f ρ α\\nRequire:Initialparameter,initialvelocity. θ v\\nInitializeaccumulation variabler= 0\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputeinterimupdate: ˜θθv ← + α\\nComputegradient:g←1\\nm∇ ˜ θ\\ue050\\ni L f((x( ) i;˜θy) ,( ) i)\\nAccumulategradient:rrgg ← ρ+(1 )− ρ\\ue00c\\nComputevelocityupdate:vv← α−\\ue00f√r\\ue00cg.(1√rappliedelement-wise)\\nApplyupdate:θθv ← +\\nendwhile\\nlargelyontheuser’sfamiliaritywiththealgorithm(foreaseofhyperparameter\\ntuning).\\n8.6ApproximateSecond-OrderMethods\\nInthissectionwediscusstheapplicationofsecond-ordermethodstothetraining\\nofdeepnetworks.See ()foranearliertreatmentofthissubject. LeCunetal.1998a\\nForsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical\\nrisk:\\nJ() = θ E x ,y ∼ ˆ pdata ( ) x , y[((;))] = L fxθ , y1\\nmm \\ue058\\ni = 1L f((x( ) i;)θ , y( ) i) .(8.25)\\nHoweverthemethodswediscusshereextendreadilytomoregeneralobjective\\nfunctionsthat,forinstance,includeparameterregularizationtermssuchasthose\\ndiscussedinchapter.7\\n8.6.1Newton’sMethod\\nInsection,weintroducedsecond-ordergradientmethods.Incontrasttoﬁrst- 4.3\\nordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove\\noptimization. Themostwidelyusedsecond-ordermethodisNewton’smethod.We\\nnowdescribeNewton’smethodinmoredetail,withemphasisonitsapplicationto\\nneuralnetworktraining.\\n3 1 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='86d8e026-cc48-4779-a491-2b3c7c3fa33b', embedding=None, metadata={'page_label': '326', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.7TheAdamalgorithm\\nRequire:Stepsize(Suggesteddefault: ) \\ue00f 0001 .\\nRequire:Exponentialdecayratesformomentestimates, ρ 1and ρ 2in[0 ,1).\\n(Suggesteddefaults:andrespectively) 09 . 0999 .\\nRequire:Smallconstant δusedfornumericalstabilization.(Suggesteddefault:\\n10− 8)\\nRequire:Initialparametersθ\\nInitialize1stand2ndmomentvariables ,s= 0r= 0\\nInitializetimestep t= 0\\nwhile do stoppingcriterionnotmet\\nSampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with\\ncorrespondingtargetsy( ) i.\\nComputegradient:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nt t←+1\\nUpdatebiasedﬁrstmomentestimate:s← ρ 1s+(1− ρ 1)g\\nUpdatebiasedsecondmomentestimate:r← ρ 2r+(1− ρ 2)gg\\ue00c\\nCorrectbiasinﬁrstmoment:ˆs←s\\n1 − ρt\\n1\\nCorrectbiasinsecondmoment:ˆr←r\\n1 − ρt\\n2\\nComputeupdate: ∆= θ− \\ue00fˆs√\\nˆ r + δ(operationsappliedelement-wise)\\nApplyupdate:θθθ ← +∆\\nendwhile\\nNewton’smethodisanoptimization schemebasedonusingasecond-orderTay-\\nlorseriesexpansiontoapproximate J(θ)nearsomepointθ 0,ignoringderivatives\\nofhigherorder:\\nJ J () θ≈(θ 0)+(θθ− 0)\\ue03e∇ θ J(θ 0)+1\\n2(θθ− 0)\\ue03eHθθ (− 0) ,(8.26)\\nwhereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Ifwethensolvefor\\nthecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule:\\nθ∗= θ 0−H− 1∇ θ J(θ 0) (8.27)\\nThusforalocallyquadraticfunction(withpositivedeﬁniteH),byrescaling\\nthegradientbyH− 1,Newton’smethodjumpsdirectlytotheminimum.\\xa0If the\\nobjectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this\\nupdatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton’s\\nmethod,giveninalgorithm .8.8\\n3 1 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a8e46e84-13f6-426d-9bd8-dea7aa633afc', embedding=None, metadata={'page_label': '327', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nAlgorithm8.8Newton’smethodwithobjective J(θ) =\\n1\\nm\\ue050m\\ni = 1 L f((x( ) i;)θ , y( ) i).\\nRequire:Initialparameterθ 0\\nRequire:Trainingsetofexamples m\\nwhile do stoppingcriterionnotmet\\nComputegradient:g←1\\nm∇ θ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nComputeHessian:H←1\\nm∇2\\nθ\\ue050\\ni L f((x( ) i;)θ ,y( ) i)\\nComputeHessianinverse:H− 1\\nComputeupdate: ∆= θ−H− 1g\\nApplyupdate:θθθ = +∆\\nendwhile\\nForsurfacesthatarenotquadratic,aslongastheHessianremainspositive\\ndeﬁnite,Newton’smethodcanbeappliediteratively.Thisimpliesatwo-step\\niterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat-\\ningthequadraticapproximation).\\xa0Second, updatetheparametersaccordingto\\nequation.8.27\\nInsection,wediscussedhowNewton’smethodisappropriateonlywhen 8.2.3\\ntheHessianispositivedeﬁnite.Indeeplearning,thesurfaceoftheobjective\\nfunctionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that\\nareproblematicforNewton’smethod.\\xa0IftheeigenvaluesoftheHessianarenot\\nallpositive,forexample,nearasaddlepoint,thenNewton’smethodcanactually\\ncauseupdatestomoveinthewrongdirection.Thissituationcanbeavoided\\nbyregularizingtheHessian.Commonregularizationstrategiesincludeaddinga\\nconstant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes α\\nθ∗= θ 0−[(( H fθ 0))+ ] αI− 1∇ θ f(θ 0) . (8.28)\\nThisregularizationstrategyisusedinapproximations toNewton’smethod,such\\nastheLevenberg–Marquardt algorithm(Levenberg1944Marquardt1963 ,;,),and\\nworksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively\\nclosetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the\\nvalueof αwouldhavetobesuﬃcientlylargetooﬀsetthenegativeeigenvalues.\\nHowever,as αincreasesinsize,theHessianbecomesdominatedbythe αIdiagonal\\nandthedirectionchosenbyNewton’smethodconvergestothestandardgradient\\ndividedby α.\\xa0Whenstrongnegativecurvatureispresent, αmayneedtobeso\\nlargethatNewton’smethodwouldmakesmallerstepsthangradientdescentwith\\naproperlychosenlearningrate.\\nBeyondthechallengescreatedbycertainfeaturesoftheobjectivefunction,\\n3 1 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='24a4bdba-8d89-4e21-a8a6-16eda13b9132', embedding=None, metadata={'page_label': '328', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nsuchassaddlepoints,theapplicationofNewton’smethodfortraininglargeneural\\nnetworksislimitedbythesigniﬁcantcomputational burdenitimposes.The\\nnumberofelementsintheHessianissquaredinthenumberofparameters,sowith\\nkparameters(andforevenverysmallneuralnetworksthenumberofparameters\\nkcanbeinthemillions),Newton’smethodwouldrequiretheinversionofa k k×\\nmatrix—with computational complexityof O( k3).Also,sincetheparameterswill\\nchangewitheveryupdate,theinverseHessianhastobecomputed ateverytraining\\niteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters\\ncanbepracticallytrainedviaNewton’smethod.Intheremainderofthissection,\\nwewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton’s\\nmethodwhileside-steppingthecomputational hurdles.\\n8.6.2ConjugateGradients\\nConjugategradientsisamethodtoeﬃcientlyavoidthecalculationoftheinverse\\nHessianbyiterativelydescendingconjugatedirections.Theinspirationforthis\\napproachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest\\ndescent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3\\nthedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6\\nsteepestdescent,whenappliedinaquadraticbowl,progressesinaratherineﬀective\\nback-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection,\\nwhengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline\\nsearchdirection.\\nLettheprevioussearchdirectionbed t − 1.Attheminimum,wheretheline\\nsearchterminates,thedirectionalderivativeiszeroindirectiond t − 1:∇ θ J(θ)·\\nd t − 1=0.Sincethegradientatthispointdeﬁnesthecurrentsearchdirection,\\nd t=∇ θ J(θ) willhavenocontributioninthedirectiond t − 1.Thusd tisorthogonal\\ntod t − 1.Thisrelationshipbetweend t − 1andd tisillustratedinﬁgurefor8.6\\nmultipleiterationsofsteepestdescent.Asdemonstratedintheﬁgure,thechoiceof\\northogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious\\nsearchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby\\ndescendingtotheminimuminthecurrentgradientdirection,wemustre-minimize\\ntheobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat\\ntheendofeachlinesearchweare,inasense,undoingprogresswehavealready\\nmadeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients\\nseekstoaddressthisproblem.\\nInthemethodofconjugategradients,weseektoﬁndasearchdirectionthat\\nisconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress\\nmadeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes\\n3 1 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f045aacb-5d83-4831-a968-e664cdb4f162', embedding=None, metadata={'page_label': '329', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8. OPTIMIZATIONFORTRAININGDEEPMODELS\\n\\x00\\ue033\\ue030 \\x00\\ue032\\ue030 \\x00\\ue031\\ue030 \\ue030 \\ue031\\ue030 \\ue032\\ue030\\x00\\ue033\\ue030\\x00\\ue032\\ue030\\x00\\ue031\\ue030\\ue030\\ue031\\ue030\\ue032\\ue030\\nFigure 8.6: The method of steepest descent applied to a quadratic cost surface. The\\nmethod of steepest descent involves jumping to the point of lowest cost along the line\\ndeﬁnedbythegradientattheinitialpointoneachstep. Thisresolvessomeoftheproblems\\nseen with using a ﬁxed learning rate in ﬁgure , but evenwith the optimal step size 4.6\\nthealgorithm stillmakesback-and-forthprogress toward theoptimum. Bydeﬁnition, at\\ntheminimumof theobjective alongagivendirection, thegradientatthe ﬁnalpointis\\northogonalto thatdirection.\\ntheform:\\nd t= ∇ θ J β( ) +θ td t−1 (8.29)\\nwhere β tisacoeﬃcientwhosemagnitudecontrolshowmuchofthedirection, d t−1,\\nweshouldaddbacktothecurrentsearchdirection.\\nTwodirections,d tandd t−1,aredeﬁnedasconjugateif d\\ue03e\\ntHd t−1= 0,where\\nHistheHessianmatrix.\\nThestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe\\neigenvectorsofHtochoose β t,whichwouldnotsatisfyourgoalofdeveloping\\namethodthatismorecomputationallyviablethanNewton’smethodforlarge\\nproblems.\\xa0Canwecalculatetheconjugatedirectionswithoutresortingtothese\\ncalculations? Fortunatelytheanswertothatisyes.\\nTwopopularmethodsforcomputingthe β tare:\\n1. Fletcher-Reeves:\\nβ t=∇ θ J(θ t)\\ue03e∇ θ J(θ t)\\n∇ θ J(θ t−1)\\ue03e∇ θ J(θ t−1)(8.30)\\n314', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c62efdcc-fc9f-4e56-b658-74043a4d5b75', embedding=None, metadata={'page_label': '330', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8. OPTIMIZATIONFORTRAININGDEEPMODELS\\n2. Polak-Ribière:\\nβ t=(∇ θ J(θ t)− ∇ θ J(θ t−1))\\ue03e∇ θ J(θ t)\\n∇ θ J(θ t−1)\\ue03e∇ θ J(θ t−1)(8.31)\\nForaquadraticsurface,theconjugatedirectionsensurethatthegradientalong\\nthepreviousdirectiondoesnotincreaseinmagnitude. Wethereforestayatthe\\nminimumalong thepreviousdirections. As aconsequence, in a k-dimensional\\nparameterspace,theconjugategradientmethodrequiresatmost klinesearchesto\\nachievetheminimum. Theconjugategradientalgorithmisgiveninalgorithm . 8.9\\nAlgorithm8.9 Theconjugategradientmethod\\nRequire:Initialparameters θ 0\\nRequire:Trainingsetof examples m\\nInitializeρ 0= 0\\nInitialize g 0= 0\\nInitialize t= 1\\nwhile do stoppingcriterionnotmet\\nInitializethegradient g t= 0\\nComputegradient: g t←1\\nm∇ θ\\ue050\\ni L f( (x( ) i; )θ ,y( ) i)\\nCompute β t=(g t −g t−1 )\\ue03eg t\\ng\\ue03e\\nt−1g t−1(Polak-Ribière)\\n(Nonlinearconjugategradient: optionallyreset β ttozero,forexampleif tis\\namultipleofsomeconstant ,suchas ) k k = 5\\nComputesearchdirection: ρ t= −g t+ β tρ t−1\\nPerformlinesearchtoﬁnd: \\ue00f∗= argmin \\ue00f1\\nm\\ue050m\\ni=1 L f( (x( ) i;θ t+ \\ue00fρ t),y( ) i)\\n(On a truly quadratic cost function, analytically solve for \\ue00f∗rather than\\nexplicitlysearchingforit)\\nApplyupdate:θ t+1= θ t+ \\ue00f∗ρ t\\nt t←+ 1\\nendwhile\\nNonlinearConjugateGradients: So far wehave discussed the method of\\nconjugategradientsasitisappliedtoquadraticobjectivefunctions.\\xa0Ofcourse,\\nourprimaryinterestinthischapteristoexploreoptimizationmethodsfortraining\\nneuralnetworksandotherrelateddeeplearningmodelswherethecorresponding\\nobjective function is far from quadratic. Perhaps surprisingly, the method of\\nconjugategradientsisstillapplicableinthissetting,thoughwithsomemodiﬁcation.\\nWithoutanyassurancethattheobjectiveisquadratic,theconjugatedirections\\n315', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='697920eb-9814-4643-ae6c-52056483aede', embedding=None, metadata={'page_label': '331', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\narenolongerassuredtoremainattheminimumoftheobjectiveforprevious\\ndirections.Asaresult,thenonlinearconjugategradientsalgorithmincludes\\noccasionalresetswherethemethodofconjugategradientsisrestartedwithline\\nsearchalongtheunalteredgradient.\\nPractitionersreportreasonableresultsinapplicationsofthenonlinearconjugate\\ngradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneﬁcialto\\ninitializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore\\ncommencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate\\ngradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch\\nversionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal.\\n2011).\\xa0Adaptationsofconjugategradientsspeciﬁcallyforneuralnetworkshave\\nbeenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller\\n1993).\\n8.6.3BFGS\\nTheBroyden–Fletcher–Goldfarb–Shanno(BFGS)algorithmattemptsto\\nbringsomeoftheadvantagesofNewton’smethodwithoutthecomputational\\nburden.In\\xa0thatrespect,\\xa0BFGSissimilartotheconjugategradientmethod.\\nHowever,BFGStakesamoredirectapproachtotheapproximation ofNewton’s\\nupdate.RecallthatNewton’supdateisgivenby\\nθ∗= θ 0−H− 1∇ θ J(θ 0) , (8.32)\\nwhereHistheHessianof Jwithrespecttoθevaluatedatθ 0.Theprimary\\ncomputational diﬃcultyinapplyingNewton’supdateisthecalculationofthe\\ninverseHessianH− 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich\\ntheBFGSalgorithmisthemostprominent)istoapproximate theinversewith\\namatrixM tthatisiterativelyreﬁnedbylowrankupdatestobecomeabetter\\napproximationofH− 1.\\nThespeciﬁcationandderivationoftheBFGSapproximationisgiveninmany\\ntextbooksonoptimization, includingLuenberger1984().\\nOncetheinverseHessianapproximationM tisupdated,thedirectionofdescent\\nρ tisdeterminedbyρ t=M tg t.Alinesearchisperformedinthisdirectionto\\ndeterminethesizeofthestep, \\ue00f∗,takeninthisdirection.Theﬁnalupdatetothe\\nparametersisgivenby:\\nθ t + 1= θ t+ \\ue00f∗ρ t . (8.33)\\nLikethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof\\nlinesearcheswiththedirectionincorporatingsecond-orderinformation. However\\n3 1 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc91eb5a-d5fa-4219-9264-ee484e2fe590', embedding=None, metadata={'page_label': '332', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nunlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent\\nonthelinesearchﬁndingapointveryclosetothetrueminimumalongtheline.\\nThus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend\\nlesstimereﬁningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust\\nstoretheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS\\nimpracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof\\nparameters.\\nLimited\\xa0Memory\\xa0BFGS\\xa0(or\\xa0L-BFGS)The\\xa0memory costs\\xa0ofthe\\xa0BFGS\\nalgorithmcanbesigniﬁcantlydecreasedbyavoidingstoringthecompleteinverse\\nHessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM\\nusingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption\\nthatM( 1 ) t −istheidentitymatrix,ratherthanstoringtheapproximation fromone\\nsteptothenext.Ifusedwithexactlinesearches,thedirectionsdeﬁnedbyL-BFGS\\naremutuallyconjugate.However,unlikethemethodofconjugategradients,this\\nprocedureremainswellbehavedwhentheminimumofthelinesearchisreached\\nonlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe\\ngeneralizedtoincludemoreinformationabouttheHessianbystoringsomeofthe\\nvectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n()\\n8.7OptimizationStrategiesandMeta-Algorithms\\nManyoptimization techniquesarenotexactlyalgorithms,\\xa0butrathergeneral\\ntemplatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe\\nincorporatedintomanydiﬀerentalgorithms.\\n8.7.1BatchNormalization\\nBatchnormalization ( ,)isoneofthemostexcitingrecent IoﬀeandSzegedy2015\\ninnovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization\\nalgorithmatall.Instead,itisamethodofadaptivereparametrization, motivated\\nbythediﬃcultyoftrainingverydeepmodels.\\nVerydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The\\ngradienttellshowtoupdateeachparameter,undertheassumptionthattheother\\nlayersdonotchange.Inpractice,weupdateallofthelayerssimultaneously.\\nWhenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions\\ncomposedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed\\nundertheassumptionthattheotherfunctionsremainconstant.Asasimple\\n3 1 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='098bd028-2d9b-472a-a632-1031bead25f4', embedding=None, metadata={'page_label': '333', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nexample,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer\\nanddoesnotuseanactivationfunctionateachhiddenlayer:ˆ y= x w 1 w 2 w 3 . . . w l.\\nHere, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i − 1 w i.\\nTheoutput ˆ yisalinearfunctionoftheinput x,butanonlinearfunctionofthe\\nweights w i.Supposeourcostfunctionhasputagradientofon1 ˆ y,sowewishto\\ndecreaseˆ yslightly.Theback-propagationalgorithmcanthencomputeagradient\\ng=∇ wˆ y.Considerwhathappenswhenwemakeanupdatewwg ← − \\ue00f.The\\nﬁrst-orderTaylorseriesapproximation ofˆ ypredictsthatthevalueofˆ ywilldecrease\\nby \\ue00fg\\ue03eg.Ifwewantedtodecreaseˆ yby .1,thisﬁrst-orderinformationavailablein\\nthegradientsuggestswecouldsetthelearningrate \\ue00fto. 1\\ng\\ue03eg.However,theactual\\nupdatewillincludesecond-orderandthird-ordereﬀects,onuptoeﬀectsoforder l.\\nThenewvalueofˆ yisgivenby\\nx w( 1− \\ue00f g 1)( w 2− \\ue00f g 2)( . . . w l− \\ue00f g l) . (8.34)\\nAnexampleofonesecond-ordertermarisingfromthisupdateis \\ue00f2g 1 g 2\\ue051l\\ni = 3 w i.\\nThistermmightbenegligibleif\\ue051l\\ni = 3 w iissmall,ormightbeexponentiallylarge\\niftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1\\ntochooseanappropriatelearningrate,becausetheeﬀectsofanupdatetothe\\nparametersforonelayerdependssostronglyonalloftheotherlayers.Second-order\\noptimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese\\nsecond-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks,\\nevenhigher-orderinteractionscanbesigniﬁcant.Evensecond-orderoptimization\\nalgorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent\\nthemfromtrulyaccountingforallsigniﬁcantsecond-orderinteractions. Building\\nan n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe\\ndoinstead?\\nBatchnormalization providesanelegantwayofreparametrizing almostanydeep\\nnetwork.Thereparametrization signiﬁcantlyreducestheproblemofcoordinating\\nupdatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput\\norhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer\\ntonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample\\nappearinginarowofthematrix.Tonormalize,wereplaceitwith H\\nH\\ue030=Hµ−\\nσ, (8.35)\\nwhereµisavectorcontainingthemeanofeachunitandσisavectorcontaining\\nthestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting\\nthevectorµandthevectorσtobeappliedtoeveryrowofthematrixH.Within\\neachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting µ j\\n3 1 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='40702e97-0df9-49fa-bd02-f9e700bd71a7', embedding=None, metadata={'page_label': '334', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nanddividingby σ j.TherestofthenetworkthenoperatesonH\\ue030inexactlythe\\nsamewaythattheoriginalnetworkoperatedon.H\\nAttrainingtime,\\nµ=1\\nm\\ue058\\niH i , : (8.36)\\nand\\nσ=\\ue073\\nδ+1\\nm\\ue058\\ni( )Hµ−2\\ni , (8.37)\\nwhere δisasmallpositivevaluesuchas10− 8imposedtoavoidencountering\\ntheundeﬁnedgradientof√zat z=0.Crucially,\\xa0weback-propagatethrough\\ntheseoperationsforcomputingthemeanandthestandarddeviation,andfor\\napplyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose\\nanoperation\\xa0that actssimplytoincreasethestandard\\xa0deviationormeanof\\nh i;thenormalization operationsremovetheeﬀectofsuchanactionandzero\\noutitscomponentinthegradient.Thiswasamajorinnovationofthebatch\\nnormalization approach.\\xa0Previous approacheshadinvolvedaddingpenaltiesto\\nthecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor\\ninvolvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep.\\nTheformerapproachusuallyresultedinimperfectnormalization andthelatter\\nusuallyresultedinsigniﬁcantwastedtimeasthelearningalgorithmrepeatedly\\nproposedchangingthemeanandvarianceandthenormalization steprepeatedly\\nundidthischange.Batchnormalization reparametrizes themodeltomakesome\\nunitsalwaysbestandardizedbydeﬁnition,deftlysidesteppingbothproblems.\\nAttesttime,µandσmaybereplacedbyrunningaveragesthatwerecollected\\nduringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample,\\nwithoutneedingtousedeﬁnitionsofµandσthatdependonanentireminibatch.\\nRevisitingtheˆ y= x w 1 w 2 . . . w lexample,weseethatwecanmostlyresolvethe\\ndiﬃcultiesinlearningthismodelbynormalizing h l − 1.Supposethat xisdrawn\\nfromaunitGaussian.Then h l − 1willalsocomefromaGaussian,becausethe\\ntransformationfrom xto h lislinear.However, h l − 1willnolongerhavezeromean\\nandunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized\\nˆh l − 1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany\\nupdatetothelowerlayers,ˆh l − 1willremainaunitGaussian.Theoutput ˆ ymay\\nthenbelearnedasasimplelinearfunction ˆ y= w lˆ h l − 1.Learninginthismodelis\\nnowverysimplebecausetheparametersatthelowerlayerssimplydonothavean\\neﬀectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian.\\xa0In\\nsomecornercases,thelowerlayerscanhaveaneﬀect.Changingoneofthelower\\nlayerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0\\n3 1 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='804ed151-f447-4fbe-a0cd-e7818b59a672', embedding=None, metadata={'page_label': '335', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nofoneofthelowerweightscanﬂiptherelationshipbetweenˆ h l − 1and y.\\xa0These\\nsituationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave\\nanextremeeﬀectonthestatisticsof h l − 1.Batchnormalization hasthusmade\\nthismodelsigniﬁcantlyeasiertolearn.\\xa0Inthisexample,theeaseoflearningof\\ncoursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample,\\nthelowerlayersnolongerhaveanyharmfuleﬀect,buttheyalsonolongerhave\\nanybeneﬁcialeﬀect.Thisisbecausewehavenormalizedouttheﬁrstandsecond\\norderstatistics,whichisallthatalinearnetworkcaninﬂuence.Inadeepneural\\nnetworkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear\\ntransformationsofthedata,sotheyremainuseful.Batchnormalization actsto\\nstandardizeonlythemeanandvarianceofeachunitinordertostabilizelearning,\\nbutallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle\\nunittochange.\\nBecausetheﬁnallayerofthenetworkisabletolearnalineartransformation,\\nwemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina\\nlayer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015\\ntheinspirationforbatchnormalization. Unfortunately,\\xa0eliminating alllinear\\ninteractionsismuchmoreexpensivethanstandardizingthemeanandstandard\\ndeviationofeachindividualunit,andsofarbatchnormalization remainsthemost\\npracticalapproach.\\nNormalizingthemeanandstandarddeviationofaunitcanreducetheexpressive\\npowerofthe\\xa0neuralnetworkcontainingthatunit.Inordertomaintainthe\\nexpressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit\\nactivationsHwithγH\\ue030+βratherthansimplythenormalizedH\\ue030.Thevariables\\nγandβarelearnedparametersthatallowthenewvariabletohaveanymean\\nandstandarddeviation.Atﬁrstglance,thismayseemuseless—whydidweset\\nthemeanto 0,andthenintroduceaparameterthatallowsittobesetbackto\\nanyarbitraryvalueβ?Theansweristhatthenewparametrization canrepresent\\nthesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew\\nparametrization hasdiﬀerentlearningdynamics.Intheoldparametrization, the\\nmeanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters\\ninthelayersbelowH.Inthenewparametrization, themeanofγH\\ue030+βis\\ndeterminedsolelybyβ.Thenewparametrization ismucheasiertolearnwith\\ngradientdescent.\\nMostneuralnetworklayerstaketheformof φ(XW+b)where φissome\\nﬁxednonlinearactivationfunctionsuchastherectiﬁedlineartransformation.It\\nisnaturaltowonderwhetherweshouldapplybatchnormalization totheinput\\nX,ortothetransformedvalueXW+b. ()recommend IoﬀeandSzegedy2015\\n3 2 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c048c2ef-121f-4d6e-a978-4e52f0ba6e5b', embedding=None, metadata={'page_label': '336', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nthelatter.Morespeciﬁcally,XW+bshouldbereplacedbyanormalizedversion\\nofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith\\nthe βparameterappliedbythebatchnormalization reparametrization. Theinput\\ntoalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe\\nrectiﬁedlinearfunctioninapreviouslayer.\\xa0Thestatisticsoftheinputarethus\\nmorenon-Gaussianandlessamenabletostandardizationbylinearoperations.\\nInconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9\\nsamenormalizing µand σateveryspatiallocationwithinafeaturemap,sothat\\nthestatisticsofthefeaturemapremainthesameregardlessofspatiallocation.\\n8.7.2CoordinateDescent\\nInsomecases,itmaybepossibletosolveanoptimization problemquicklyby\\nbreakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle\\nvariable x i,\\xa0then\\xa0minimize\\xa0it with\\xa0respect\\xa0to\\xa0another variable x jand\\xa0soon,\\nrepeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local)\\nminimum.Thispracticeisknownascoordinatedescent,becauseweoptimize\\nonecoordinateatatime.\\xa0Moregenerally,blockcoordinatedescentrefersto\\nminimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm\\n“coordinatedescent”isoftenusedtorefertoblockcoordinatedescentaswellas\\nthestrictlyindividualcoordinatedescent.\\nCoordinatedescentmakesthemostsensewhenthediﬀerentvariablesinthe\\noptimization problemcanbeclearlyseparatedintogroupsthatplayrelatively\\nisolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis\\nsigniﬁcantlymoreeﬃcientthanoptimization withrespecttoallofthevariables.\\nForexample,considerthecostfunction\\nJ ,(HW) =\\ue058\\ni , j| H i , j|+\\ue058\\ni , j\\ue010\\nXW−\\ue03eH\\ue0112\\ni , j.(8.38)\\nThisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis\\ntoﬁndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues\\nHtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso\\ninvolveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder\\ntopreventthepathologicalsolutionwithextremelysmallandlarge.HW\\nThefunction Jisnotconvex.However,\\xa0wecandividetheinputstothe\\ntrainingalgorithmintotwosets:thedictionaryparametersWandthecode\\nrepresentationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof\\nthesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives\\n3 2 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='032c6c42-a93a-45c6-9c34-a262d5673248', embedding=None, metadata={'page_label': '337', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nusanoptimization strategythatallowsustouseeﬃcientconvexoptimization\\nalgorithms,byalternatingbetweenoptimizingWwithHﬁxed,thenoptimizing\\nHWwithﬁxed.\\nCoordinatedescentisnotaverygoodstrategywhenthevalueofonevariable\\nstronglyinﬂuencestheoptimalvalueofanothervariable,asinthefunction f(x) =\\n( x 1− x 2)2+ α\\ue000\\nx2\\n1+ x2\\n2\\ue001\\nwhere αisapositiveconstant.Theﬁrsttermencourages\\nthetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem\\ntobenearzero.Thesolutionistosetbothtozero.Newton’smethodcansolve\\ntheprobleminasinglestepbecauseitisapositivedeﬁnitequadraticproblem.\\nHowever,forsmall α,coordinatedescentwillmakeveryslowprogressbecausethe\\nﬁrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiﬀers\\nsigniﬁcantlyfromthecurrentvalueoftheothervariable.\\n8.7.3PolyakAveraging\\nPolyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral\\npoints\\xa0inthe\\xa0trajectory\\xa0through parameter\\xa0spacevisited\\xa0by\\xa0anoptimization\\nalgorithm.\\xa0If titerationsofgradientdescentvisitpointsθ( 1 ), . . . ,θ( ) t,thenthe\\noutputofthePolyakaveragingalgorithmisˆθ( ) t=1\\nt\\ue050\\niθ( ) i.\\xa0Onsomeproblem\\nclasses,suchasgradientdescentappliedtoconvexproblems,thisapproachhas\\nstrongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiﬁcation\\nismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe\\noptimization algorithmmayleapbackandforthacrossavalleyseveraltimes\\nwithoutevervisitingapointnearthebottomofthevalley.Theaverageofallof\\nthelocationsoneithersideshouldbeclosetothebottomofthevalleythough.\\nInnon-convexproblems,thepathtakenbytheoptimization trajectorycanbe\\nverycomplicatedandvisitmanydiﬀerentregions.Includingpointsinparameter\\nspacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge\\nbarriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult,\\nwhenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean\\nexponentiallydecayingrunningaverage:\\nˆθ( ) t= αˆθ( 1 ) t −+(1 )− αθ( ) t. (8.39)\\nTherunningaverageapproachisusedinnumerousapplications.SeeSzegedy\\netal.()forarecentexample. 2015\\n3 2 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2090ef2d-f5ae-4c07-b250-60cb4bc10f7d', embedding=None, metadata={'page_label': '338', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n8.7.4SupervisedPretraining\\nSometimes,directlytrainingamodeltosolveaspeciﬁctaskcanbetooambitious\\nifthemodeliscomplexandhardtooptimizeorifthetaskisverydiﬃcult.Itis\\nsometimesmoreeﬀectivetotrainasimplermodeltosolvethetask,thenmake\\nthemodelmorecomplex.Itcanalsobemoreeﬀectivetotrainthemodeltosolve\\nasimplertask,thenmoveontoconfronttheﬁnaltask.Thesestrategiesthat\\ninvolvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof\\ntrainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas\\npretraining.\\nGreedyalgorithmsbreakaproblemintomanycomponents,thensolvefor\\ntheoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe\\nindividuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete\\nsolution.However,greedyalgorithmscanbecomputationally muchcheaperthan\\nalgorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution\\nisoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya\\nﬁne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal\\nsolutiontothefullproblem.Initializingthejointoptimization algorithmwitha\\ngreedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit\\nﬁnds.\\nPretraining,andespeciallygreedypretraining,algorithmsareubiquitousin\\ndeeplearning.Inthissection,wedescribespeciﬁcallythosepretrainingalgorithms\\nthatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning\\nproblems.Thisapproachisknownas . greedysupervisedpretraining\\nIntheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007\\neachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof\\nthelayersintheﬁnalneuralnetwork.Anexampleofgreedysupervisedpretraining\\nisillustratedinﬁgure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7\\nofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained\\nhiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman\\n()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015\\ntheﬁrstfourandlastthreelayersfromthisnetworktoinitializeevendeeper\\nnetworks(withuptonineteenlayersofweights).Themiddlelayersofthenew,\\nverydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained.\\nAnotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs\\ntrainedMLPs,aswellastherawinput,asinputsforeachaddedstage.\\nWhy\\xa0would\\xa0greedy\\xa0sup ervised\\xa0pretraining help?The\\xa0hypothesis \\xa0initially\\ndiscussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007\\n3 2 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6db986b-5d56-4c53-9422-346bf2d4ac1b', embedding=None, metadata={'page_label': '339', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\ny y\\nh( 1 )h( 1 )\\nx x\\n( a )U( 1 )U( 1 )\\nW( 1 )W( 1 ) y yh( 1 )h( 1 )\\nx x\\n( b )U( 1 )U( 1 )W( 1 )W( 1 )\\ny yh( 1 )h( 1 )\\nx x\\n( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )\\ny y U( 2 )U( 2 ) W( 2 )W( 2 )\\ny yh( 1 )h( 1 )\\nx x\\n( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y\\nU( 2 )U( 2 )\\nW( 2 )W( 2 )\\nFigure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,). Bengio e t a l .2007\\n( a )Westartbytrainingasuﬃcientlyshallowarchitecture.Anotherdrawingofthe ( b )\\nsamearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c )\\ndiscardthehidden-to-outputlayer.Wesendtheoutputoftheﬁrsthiddenlayerasinput\\ntoanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective\\nastheﬁrstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas\\nmanylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. ( d )\\nTofurtherimprovetheoptimization,wecanjointlyﬁne-tuneallthelayers,eitheronlyat\\ntheendorateachstageofthisprocess.\\n3 2 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='03929143-ab08-4878-ae5d-3e965513c3e8', embedding=None, metadata={'page_label': '340', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nintermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin\\ntermsofoptimization andintermsofgeneralization.\\nAnapproachrelatedtosupervisedpretrainingextendstheideatothecontext\\noftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8\\nlayersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories)\\nandtheninitializeasame-sizenetworkwiththeﬁrst klayersoftheﬁrstnet.All\\nthelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are\\nthenjointlytrainedtoperformadiﬀerentsetoftasks(anothersubsetofthe1000\\nImageNetobjectcategories),withfewertrainingexamplesthanfortheﬁrstsetof\\ntasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin\\nsection.15.2\\nAnotherrelatedlineofworkistheFitNets( ,)approach. Romeroetal.2015\\nThisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat\\nenoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen\\nbecomesateacherforasecondnetwork,designatedthestudent.Thestudent\\nnetworkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe\\ndiﬃculttotrainwithSGDundernormalcircumstances.Thetrainingofthe\\nstudentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict\\ntheoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer\\noftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe\\nhiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional\\nparametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork\\nfromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting\\ntheﬁnalclassiﬁcationtarget,theobjectiveistopredictthemiddlehiddenlayer\\noftheteachernetwork.\\xa0Thelowerlayersofthestudentnetworksthushavetwo\\nobjectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as\\nwellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin\\nanddeepnetworkappearstobemorediﬃculttotrainthanawideandshallow\\nnetwork,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower\\ncomputational costifitisthinenoughtohavefarfewerparameters.Without\\nthehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe\\nexperiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus\\nbeoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiﬃcultto\\ntrain,butotheroptimization techniquesorchangesinthearchitecturemayalso\\nsolvetheproblem.\\n3 2 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dddaaff1-dd6d-425e-adad-07ae868e329f', embedding=None, metadata={'page_label': '341', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\n8.7.5DesigningModelstoAidOptimization\\nToimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization\\nalgorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave\\ncomefromdesigningthemodelstobeeasiertooptimize.\\nInprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein\\njaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely\\ndiﬃcult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto\\noptimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin\\nneuralnetworklearningoverthepast30yearshavebeenobtainedbychanging\\nthemodelfamilyratherthanchangingtheoptimization procedure.Stochastic\\ngradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe\\n1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications.\\nSpeciﬁcally,modernneuralnetworksreﬂectadesignchoicetouselineartrans-\\nformationsbetweenlayersandactivationfunctionsthatarediﬀerentiable almost\\neverywhereandhavesigniﬁcantslopeinlargeportionsoftheirdomain.\\xa0Inpar-\\nticular,modelinnovationsliketheLSTM,rectiﬁedlinearunitsandmaxoutunits\\nhaveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep\\nnetworksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake\\noptimization easier.Thegradientﬂowsthroughmanylayersprovidedthatthe\\nJacobianofthelineartransformationhasreasonablesingularvalues.\\xa0Moreover,\\nlinearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel’s\\noutputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient\\nwhichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords,\\nmodernneuralnetshavebeendesignedsothattheirlocalgradientinformation\\ncorrespondsreasonablywelltomovingtowardadistantsolution.\\nOthermodeldesignstrategiescanhelptomakeoptimization easier.For\\nexample,linearpathsorskipconnectionsbetweenlayersreducethelengthof\\ntheshortestpathfromthelower\\xa0layer’sparameters\\xa0totheoutput,\\xa0and thus\\nmitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea\\ntoskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe\\nintermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a\\nanddeeply-supervisednets(,).These“auxiliaryheads”aretrained Leeetal.2014\\ntoperformthesametaskastheprimaryoutputatthetopofthenetworkinorder\\ntoensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete\\ntheauxiliaryheadsmaybediscarded.\\xa0Thisisanalternativetothepretraining\\nstrategies,whichwereintroducedintheprevioussection.Inthisway,onecan\\ntrainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat\\nintermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey\\n3 2 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a5168d76-ad27-4a7c-927c-ca1ec8f9e5f7', embedding=None, metadata={'page_label': '342', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nshoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers.\\n8.7.6ContinuationMethodsandCurriculumLearning\\nAsarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7\\nglobalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter\\nestimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis\\nproblemistoattempttoinitializetheparametersinaregionthatisconnected\\ntothesolutionbyashortpaththroughparameterspacethatlocaldescentcan\\ndiscover.\\nContinuationmethodsareafamilyofstrategiesthatcanmakeoptimization\\neasierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof\\nitstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis\\ntoconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto\\nminimizeacostfunction J(θ),wewillconstructnewcostfunctions { J( 0 ), . . . , J( ) n}.\\nThesecostfunctionsaredesignedtobeincreasinglydiﬃcult,with J( 0 )beingfairly\\neasytominimize,and J( ) n,themostdiﬃcult,being J(θ),thetruecostfunction\\nmotivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we\\nmeanthatitiswellbehavedovermoreofθspace.Arandominitialization ismore\\nlikelytolandintheregionwherelocaldescentcanminimizethecostfunction\\nsuccessfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned\\nsothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby\\nsolvinganeasyproblemthenreﬁnethesolutiontosolveincrementally harder\\nproblemsuntilwearriveatasolutiontothetrueunderlyingproblem.\\nTraditionalcontinuationmethods(predatingtheuseofcontinuationmethods\\nforneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction.\\nSeeWu1997()foranexampleofsuchamethodandareviewofsomerelated\\nmethods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing,\\nwhichaddsnoisetotheparameters(Kirkpatrick\\xa01983etal.,).Continuation\\nmethodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher\\n()foranoverviewofrecentliterature,especiallyforAIapplications. 2015\\nContinuationmethodstraditionallyweremostlydesignedwiththegoalof\\novercomingthechallengeoflocalminima.Speciﬁcally,theyweredesignedto\\nreachaglobalminimumdespitethepresenceofmanylocalminima.Todoso,\\nthesecontinuationmethodswouldconstructeasiercostfunctionsby“blurring”the\\noriginalcostfunction.Thisblurringoperationcanbedonebyapproximating\\nJ( ) i() = θ Eθ\\ue030∼ N ( θ\\ue030; θ , σ()2 i) J(θ\\ue030) (8.40)\\nviasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions\\n3 2 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea3012c2-2d4a-48a2-922f-0de0c91674d3', embedding=None, metadata={'page_label': '343', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nbecomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves\\nenoughinformationaboutthelocationofaglobalminimumthatwecanﬁndthe\\nglobalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This\\napproachcanbreakdowninthreediﬀerentways.First,itmightsuccessfullydeﬁne\\naseriesofcostfunctionswheretheﬁrstisconvexandtheoptimumtracksfrom\\nonefunctiontothenextarrivingattheglobalminimum,butitmightrequireso\\nmanyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh.\\nNP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods\\nareapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond\\ntothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex,\\nnomatterhowmuchitisblurred.Considerforexamplethefunction J(θ) =−θ\\ue03eθ.\\nSecond,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum\\nofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe\\noriginalcostfunction.\\nThoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe\\nproblemoflocalminima,localminimaarenolongerbelievedtobetheprimary\\nproblemforneuralnetworkoptimization. Fortunately,continuationmethodscan\\nstillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan\\neliminateﬂatregions,decreasevarianceingradientestimates,improveconditioning\\noftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates\\neasiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections\\nandprogresstowardaglobalsolution.\\nBengio2009etal.()observedthatanapproachcalledcurriculumlearning\\norshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis\\nbasedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts\\nandprogresstolearningmorecomplexconceptsthatdependonthesesimpler\\nconcepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal\\ntraining(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine\\nlearning(,;,;,). () Solomonoﬀ1989Elman1993Sanger1994Bengioetal.2009\\njustiﬁedthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby\\nincreasingtheinﬂuenceofsimplerexamples(eitherbyassigningtheircontributions\\ntothecostfunctionlargercoeﬃcients,orbysamplingthemmorefrequently),and\\nexperimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga\\ncurriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning\\nhasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,;\\nCollobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer\\nvision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013\\ntasks.Curriculumlearningwasalsoveriﬁedasbeingconsistentwiththewayin\\nwhichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011\\n3 2 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68560da3-d810-4535-97f6-1b483b72a393', embedding=None, metadata={'page_label': '344', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\\nmoreprototypicalexamplesandthenhelpthelearnerreﬁnethedecisionsurface\\nwiththelessobviouscases.Curriculum-based strategiesaremoreeﬀectivefor\\nteachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan\\nalsoincreasetheeﬀectivenessofotherteachingstrategies( , BasuandChristensen\\n2013).\\nAnotherimportantcontributiontoresearchoncurriculumlearningaroseinthe\\ncontextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies:\\nZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha\\nstochasticcurriculum,inwhicharandommixofeasyanddiﬃcultexamplesisalways\\npresentedtothelearner,butwheretheaverageproportionofthemorediﬃcult\\nexamples(here,thosewithlonger-termdependencies)isgraduallyincreased.With\\nadeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining\\nfromthefulltrainingset)wasobserved.\\nWehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto\\nregularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof\\ntheneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand\\nprocessinputdatathathasspecialstructure.Theoptimization methodsdiscussed\\ninthischapterareoftendirectlyapplicabletothesespecializedarchitectures with\\nlittleornomodiﬁcation.\\n3 2 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7dbbaaab-a4b1-407b-9a7d-07f008963f1c', embedding=None, metadata={'page_label': '345', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 9\\nC on v ol u t i on al N e t w orks\\nCon v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al\\nnet w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata\\nthathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan\\nbethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata,\\nwhichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen\\ntremendouslysuccessfulinpracticalapplications.Thename“convolutionalneural\\nnetwork”indicatesthatthenetworkemploysamathematical operationcalled\\nc o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional\\nnetworksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix\\nmultiplicationinatleastoneoftheirlayers.\\nInthis\\xa0chapter,\\xa0wewillﬁrst\\xa0describewhatconvolutionis.Next,\\xa0wewill\\nexplainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen\\ndescribeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks\\nemploy.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot\\ncorrespondpreciselytothedeﬁnitionofconvolutionasusedinotherﬁeldssuch\\nasengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe\\nconvolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We\\nwillalso\\xa0show\\xa0how\\xa0convolutionmaybeappliedtomanykindsofdata,\\xa0with\\ndiﬀerentnumbersofdimensions.Wethendiscussmeansofmakingconvolution\\nmoreeﬃcient.Convolutionalnetworksstandoutasanexampleofneuroscientiﬁc\\nprinciplesinﬂuencingdeeplearning.Wewilldiscusstheseneuroscientiﬁcprinciples,\\nthenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed\\ninthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto\\nchoosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris\\ntodescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11\\n330', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4234ce1f-e8e7-4d0f-97fc-af72e5a16bff', embedding=None, metadata={'page_label': '346', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\ndescribesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances.\\nResearchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew\\nbestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths,\\nrenderingitimpracticaltodescribethebestarchitectureinprint.However,the\\nbestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed\\nhere.\\n9.1TheConvolutionOperation\\nInitsmostgeneralform,convolutionisanoperationontwofunctionsofareal-\\nvaluedargument.Tomotivatethedeﬁnitionofconvolution,westartwithexamples\\noftwofunctionswemightuse.\\nSupposewearetrackingthelocationofaspaceshipwithalasersensor.Our\\nlasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime\\nt.Both xand tarereal-valued,i.e.,wecangetadiﬀerentreadingfromthelaser\\nsensoratanyinstantintime.\\nNowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy\\nestimateofthespaceship’sposition,wewouldliketoaveragetogetherseveral\\nmeasurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill\\nwantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements.\\nWecandothiswithaweightingfunction w( a),where aistheageofameasurement.\\nIfweapplysuchaweightedaverageoperationateverymoment,weobtainanew\\nfunctionprovidingasmoothedestimateofthepositionofthespaceship: s\\ns t() =\\ue05a\\nx a w t a d a ()( −) (9.1)\\nThisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically\\ndenotedwithanasterisk:\\ns t x w t () = ( ∗)() (9.2)\\nInourexample, wneedstobeavalidprobabilitydensityfunction,orthe\\noutputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0\\noritwilllookintothefuture,whichispresumablybeyondourcapabilities.These\\nlimitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeﬁned\\nforanyfunctionsforwhichtheaboveintegralisdeﬁned,andmaybeusedforother\\npurposesbesidestakingweightedaverages.\\nInconvolutionalnetworkterminology,theﬁrstargument(inthisexample,the\\nfunction x)totheconvolutionisoftenreferredtoasthe i nputandthesecond\\n3 3 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='459baf84-9931-43ee-8822-7a3a6d78a876', embedding=None, metadata={'page_label': '347', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nargument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes\\nreferredtoasthe . f e at ur e m ap\\nInourexample,theideaofalasersensorthatcanprovidemeasurements\\nateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona\\ncomputer,timewillbediscretized,andoursensorwillprovidedataatregular\\nintervals.Inourexample,itmightbemorerealistictoassumethatourlaser\\nprovidesameasurementoncepersecond.Thetimeindex tcanthentakeononly\\nintegervalues.Ifwenowassumethat xand waredeﬁnedonlyoninteger t,we\\ncandeﬁnethediscreteconvolution:\\ns t x w t () = ( ∗)() =∞\\ue058\\na = − ∞x a w t a ()( −) (9.3)\\nInmachinelearningapplications,theinputisusuallyamultidimensional array\\nofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare\\nadaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays\\nastensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored\\nseparately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe\\nﬁnitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe\\ncanimplementtheinﬁnitesummationasasummationoveraﬁnitenumberof\\narrayelements.\\nFinally,weoftenuseconvolutionsovermorethanoneaxisatatime.For\\nexample,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant\\ntouseatwo-dimensionalkernel: K\\nS i , j I K i , j () = ( ∗)() =\\ue058\\nm\\ue058\\nnI m , n K i m , j n . ( )( − −)(9.4)\\nConvolutioniscommutative,meaningwecanequivalentlywrite:\\nS i , j K I i , j () = ( ∗)() =\\ue058\\nm\\ue058\\nnI i m , j n K m , n . ( − −)( )(9.5)\\nUsuallythelatterformulaismorestraightforwardtoimplementinamachine\\nlearninglibrary,becausethereislessvariationintherangeofvalidvaluesof m\\nand. n\\nThecommutativepropertyofconvolutionarisesbecausewehave ﬂi pp e dthe\\nkernelrelativetotheinput,inthesensethatas mincreases,theindexintothe\\ninputincreases,buttheindexintothekerneldecreases.Theonlyreasontoﬂip\\nthekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty\\n3 3 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cdbe720d-f457-4743-a8f4-0b7be7d7fc56', embedding=None, metadata={'page_label': '348', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nisusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural\\nnetworkimplementation.Instead,manyneuralnetworklibrariesimplementa\\nrelatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution\\nbutwithoutﬂippingthekernel:\\nS i , j I K i , j () = ( ∗)() =\\ue058\\nm\\ue058\\nnI i m , j n K m , n . (+ +)( )(9.6)\\nManymachinelearninglibrariesimplementcross-correlationbutcallitconvolution.\\nInthistextwewillfollowthisconventionofcallingbothoperationsconvolution,\\nandspecifywhetherwemeantoﬂipthekernelornotincontextswherekernel\\nﬂippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill\\nlearntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm\\nbasedonconvolutionwithkernelﬂippingwilllearnakernelthatisﬂippedrelative\\ntothekernellearnedbyanalgorithmwithouttheﬂipping.Itisalsorarefor\\nconvolutiontobeusedaloneinmachinelearning;insteadconvolutionisused\\nsimultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes\\nnotcommuteregardlessofwhethertheconvolutionoperationﬂipsitskernelor\\nnot.\\nSeeﬁgureforanexampleofconvolution(withoutkernelﬂipping)applied 9.1\\ntoa2-Dtensor.\\nDiscreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the\\nmatrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample,\\nforunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe\\nequaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z\\nm at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto\\nconvolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto\\neachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix\\nwhoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch\\nsmallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith\\nmatrixmultiplication anddoesnotdependonspeciﬁcpropertiesofthematrix\\nstructureshouldworkwithconvolution,withoutrequiringanyfurtherchanges\\ntotheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof\\nfurtherspecializationsinordertodealwithlargeinputseﬃciently,buttheseare\\nnotstrictlynecessaryfromatheoreticalperspective.\\n3 3 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99e2fd63-8bf3-46a3-97d8-05a11dd5974d', embedding=None, metadata={'page_label': '349', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\na b c d\\ne f g h\\ni j k lw x\\ny z\\na w + b x +\\ne y + f za w + b x +\\ne y + f zb w + c x +\\nf y + g zb w + c x +\\nf y + g zc w + d x +\\ng y + h zc w + d x +\\ng y + h z\\ne w + f x +\\ni y + j ze w + f x +\\ni y + j zf w + g x +\\nj y + k zf w + g x +\\nj y + k zg w + h x +\\nk y + l zg w + h x +\\nk y + l zI nput\\nK e r ne l\\nO ut put\\nFigure9.1:Anexampleof2-Dconvolutionwithoutkernel-ﬂipping.Inthiscasewerestrict\\ntheoutputtoonlypositionswherethekernelliesentirelywithintheimage,called“valid”\\nconvolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left\\nelementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding\\nupper-leftregionoftheinputtensor.\\n3 3 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10a20ef9-c1c4-4488-befb-f261577f2422', embedding=None, metadata={'page_label': '350', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\n9.2Motivation\\nConvolutionleveragesthreeimportantideasthatcanhelpimproveamachine\\nlearningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t\\nr e pr e se n t at i o ns.Moreover,\\xa0convolutionprovidesameansforworkingwith\\ninputsofvariablesize.Wenowdescribeeachoftheseideasinturn.\\nTraditionalneuralnetworklayersusematrixmultiplicationbyamatrixof\\nparameterswithaseparateparameterdescribingtheinteractionbetweeneachinput\\nunitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput\\nunit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also\\nreferredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby\\nmakingthekernelsmallerthantheinput.Forexample,whenprocessinganimage,\\ntheinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall,\\nmeaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof\\npixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe\\nmemoryrequirementsofthemodelandimprovesitsstatisticaleﬃciency.Italso\\nmeansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements\\nineﬃciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then\\nmatrixmultiplication requires m n ×parametersandthealgorithmsusedinpractice\\nhave O( m n ×)runtime(perexample).Ifwelimitthenumberofconnections\\neachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly\\nk n ×parametersand O( k n ×)runtime.Formanypracticalapplications,itis\\npossibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping\\nkseveralordersofmagnitudesmallerthan m.\\xa0Forgraphicaldemonstrationsof\\nsparseconnectivity,seeﬁgureandﬁgure.Inadeepconvolutionalnetwork, 9.2 9.3\\nunitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput,\\nasshowninﬁgure.Thisallowsthenetworktoeﬃcientlydescribecomplicated 9.4\\ninteractionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple\\nbuildingblocksthateachdescribeonlysparseinteractions.\\nP ar amet e r shar i ngreferstousingthesameparameterformorethanone\\nfunctioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix\\nisusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby\\noneelementoftheinputandthenneverrevisited.Asasynonymforparameter\\nsharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe\\nweightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In\\naconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition\\noftheinput(exceptperhapssomeoftheboundarypixels,\\xa0dependingonthe\\ndesigndecisionsregardingtheboundary).Theparametersharingusedbythe\\nconvolutionoperationmeansthatratherthanlearningaseparatesetofparameters\\n3 3 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd931e2b-78eb-4129-bb0d-4a5ebf3dd2c1', embedding=None, metadata={'page_label': '351', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nFigure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3,\\nandalsohighlighttheoutputunitsin sthatareaﬀectedbythisunit. ( T o p )When sis\\nformedbyconvolutionwithakernelofwidth,onlythreeoutputsareaﬀectedby 3 x.\\n( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s\\nalloftheoutputsareaﬀectedby x 3.\\n3 3 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8fdf62af-f959-43c6-a28d-e850692b6d39', embedding=None, metadata={'page_label': '352', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nFigure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e : \\xa0Wehighlightoneoutputunit, s 3,\\nandalsohighlighttheinputunitsin xthataﬀectthisunit.Theseunitsareknown\\nasthereceptiveﬁeldof s 3. ( T o p )When sisformedbyconvolutionwithakernelof\\nwidth,onlythreeinputsaﬀect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication,\\nconnectivityisnolongersparse,soalloftheinputsaﬀect s 3.\\nx 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3\\nx 4 x 4h 4 h 4\\nx 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5\\nFigure9.4:Thereceptiveﬁeldoftheunitsinthedeeperlayersofaconvolutionalnetwork\\nislargerthanthereceptiveﬁeldoftheunitsintheshallowlayers.Thiseﬀectincreasesif\\nthenetworkincludesarchitecturalfeatureslikestridedconvolution(ﬁgure)orpooling 9.12\\n(section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare\\nverysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe\\ninputimage.\\n3 3 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f23536e-bf15-4bdb-945b-e5d9aeed0038', embedding=None, metadata={'page_label': '353', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5\\nFigure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular\\nparameterintwodiﬀerentmodels.\\xa0 ( T o p )Theblackarrowsindicateusesofthecentral\\nelementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this\\nsingleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom )\\ntheuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel\\nhasnoparametersharingsotheparameterisusedonlyonce.\\nforeverylocation,welearnonlyoneset.Thisdoesnotaﬀecttheruntimeof\\nforwardpropagation—it isstill O( k n ×)—butitdoesfurtherreducethestorage\\nrequirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders\\nofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis\\npracticallyinsigniﬁcantcomparedto m n ×.Convolutionisthusdramatically more\\neﬃcientthandensematrixmultiplication intermsofthememoryrequirements\\nandstatisticaleﬃciency.Foragraphicaldepictionofhowparametersharingworks,\\nseeﬁgure.9.5\\nAsanexampleofbothoftheseﬁrsttwoprinciplesinaction,ﬁgureshows9.6\\nhowsparseconnectivityandparametersharingcandramatically improvethe\\neﬃciencyofalinearfunctionfordetectingedgesinanimage.\\nInthecaseofconvolution,theparticularformofparametersharingcausesthe\\nlayertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis\\nequivariantmeansthatiftheinputchanges,theoutputchangesinthesameway.\\nSpeciﬁcally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)).\\nInthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput,\\ni.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I\\nbeafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction\\n3 3 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='64c463ff-bcff-460e-b6e5-80f41456bb29', embedding=None, metadata={'page_label': '354', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nmappingoneimagefunctiontoanotherimagefunction,suchthat I\\ue030= g( I)is\\ntheimagefunctionwith I\\ue030( x , y)= I( x −1 , y).Thisshiftseverypixelof Ione\\nunittotheright.Ifweapplythistransformationto I,thenapplyconvolution,\\ntheresultwillbethesameasifweappliedconvolutionto I\\ue030,thenappliedthe\\ntransformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans\\nthatconvolutionproducesasortoftimelinethatshowswhendiﬀerentfeatures\\nappearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact\\nsamerepresentationofitwillappearintheoutput,justlaterintime.Similarly\\nwithimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin\\ntheinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe\\nsameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction\\nofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput\\nlocations.Forexample,whenprocessingimages,itisusefultodetectedgesin\\ntheﬁrstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless\\neverywhereintheimage,soitispracticaltoshareparametersacrosstheentire\\nimage.Insomecases,wemaynotwishtoshareparametersacrosstheentire\\nimage.Forexample,ifweareprocessingimagesthatarecroppedtobecentered\\nonanindividual’sface,weprobablywanttoextractdiﬀerentfeaturesatdiﬀerent\\nlocations—thepartofthenetworkprocessingthetopofthefaceneedstolookfor\\neyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto\\nlookforachin.\\nConvolutionisnotnaturallyequivarianttosomeothertransformations,such\\naschangesinthescaleorrotationofanimage.Othermechanismsarenecessary\\nforhandlingthesekindsoftransformations.\\nFinally,somekindsofdatacannotbeprocessedbyneuralnetworksdeﬁnedby\\nmatrixmultiplication withaﬁxed-shapematrix.Convolutionenablesprocessing\\nofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7\\n9.3Pooling\\nAtypicallayerofaconvolutionalnetworkconsistsofthreestages(seeﬁgure).9.7\\nIntheﬁrststage,thelayerperformsseveralconvolutionsinparalleltoproducea\\nsetoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough\\nanonlinearactivationfunction,suchastherectiﬁedlinearactivationfunction.\\nThisstageissometimescalledthe det e c t o rstage.\\xa0Inthethirdstage,weusea\\np o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther.\\nApoolingfunctionreplacestheoutputofthenetatacertainlocationwitha\\nsummarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou\\n3 3 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b12154c-cf16-46f6-8277-b8f217f9c3e1', embedding=None, metadata={'page_label': '355', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.6: E ﬃ c i e n c y o f e d g e d e t e c t i o n.\\xa0Theimageontherightwasformedbytaking\\neachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe\\nleft.\\xa0Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage,\\nwhichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall.\\nTheinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This\\ntransformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and\\nrequires319 ×280 ×3=267 ,960ﬂoatingpointoperations(twomultiplicationsand\\noneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame\\ntransformationwithamatrixmultiplicationwouldtake320 ×280 ×319 ×280,orover\\neightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeﬃcientfor\\nrepresentingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm\\nperformsoversixteenbillionﬂoatingpointoperations,makingconvolutionroughly60,000\\ntimesmoreeﬃcientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe\\nzero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication\\nandconvolutionwouldrequirethesamenumberofﬂoatingpointoperationstocompute.\\nThematrixwouldstillneedtocontain2 ×319 ×280=178 ,640entries.Convolution\\nisanextremelyeﬃcientwayofdescribingtransformationsthatapplythesamelinear\\ntransformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula\\nGoodfellow)\\n3 4 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e212bf8d-1317-4576-9393-da502a2c66e7', embedding=None, metadata={'page_label': '356', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nConvolutional\\xa0Layer\\nInput\\xa0to\\xa0layerConvolution\\xa0stage:\\nAne\\xa0transform ﬃDetector\\xa0stage:\\nNonlinearity\\ne.g.,\\xa0rectiﬁed\\xa0linearPooling\\xa0stageNext\\xa0layer\\nInput\\xa0to\\xa0layersConvolution\\xa0layer:\\nAne\\xa0transform\\xa0 ﬃDetector\\xa0layer:\\xa0Nonlinearity\\ne.g.,\\xa0rectiﬁed\\xa0linearPooling\\xa0layerNext\\xa0layerComplex\\xa0layer\\xa0terminology Simple\\xa0layer\\xa0terminology\\nFigure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo\\ncommonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology,\\ntheconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with\\neachlayerhavingmany“stages.”Inthisterminology,thereisaone-to-onemapping\\nbetweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology.\\n( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple\\nlayers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat\\nnotevery“layer”hasparameters.\\n3 4 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d379b0db-9a80-4f0f-ac81-6f08c94b5f27', embedding=None, metadata={'page_label': '357', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nandChellappa1988,)operationreportsthemaximumoutputwithinarectangular\\nneighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular\\nneighborhood,the L2normofarectangularneighborhood,oraweightedaverage\\nbasedonthedistancefromthecentralpixel.\\nInallcases,poolinghelpstomaketherepresentationbecomeapproximately\\ni n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat\\nifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled\\noutputsdonotchange.Seeﬁgureforanexampleofhowthisworks. 9.8 Invariance\\ntolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether\\nsomefeatureispresentthanexactlywhereitis.Forexample,whendetermining\\nwhetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith\\npixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside\\nofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore\\nimportanttopreservethelocationofafeature.Forexample,ifwewanttoﬁnda\\ncornerdeﬁnedbytwoedgesmeetingataspeciﬁcorientation,weneedtopreserve\\nthelocationoftheedgeswellenoughtotestwhethertheymeet.\\nTheuseofpoolingcanbeviewedasaddinganinﬁnitelystrongpriorthat\\nthefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis\\nassumptioniscorrect,itcangreatlyimprovethestatisticaleﬃciencyofthenetwork.\\nPoolingoverspatialregionsproducesinvariancetotranslation,butifwepool\\novertheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn\\nwhichtransformationstobecomeinvariantto(seeﬁgure).9.9\\nBecausepoolingsummarizestheresponsesoverawholeneighborhood,itis\\npossibletousefewerpoolingunitsthandetectorunits,byreportingsummary\\nstatisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See\\nﬁgureforanexample.Thisimprovesthecomputational eﬃciencyofthe 9.10\\nnetworkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When\\nthenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas\\nwhenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this\\nreductionintheinputsizecanalsoresultinimprovedstatisticaleﬃciencyand\\nreducedmemoryrequirementsforstoringtheparameters.\\nFormanytasks,poolingisessentialforhandlinginputsofvaryingsize.\\xa0For\\nexample,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiﬁcation\\nlayermusthaveaﬁxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan\\noﬀsetbetweenpoolingregionssothattheclassiﬁcationlayeralwaysreceivesthe\\nsamenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the\\nﬁnalpoolinglayerofthenetworkmaybedeﬁnedtooutputfoursetsofsummary\\nstatistics,oneforeachquadrantofanimage,regardlessoftheimagesize.\\n3 4 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='040991ea-6215-457c-88d0-3ec59e92ab06', embedding=None, metadata={'page_label': '358', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\n0. 1 1. 0. 21. 1. 1.\\n0. 10. 2\\n. . . . . .. . . . . .\\n0. 3 0. 1 1.1. 0. 3 1.\\n0. 21.\\n. . . . . .. . . . . .D E T E C T O R \\xa0 S T A GEP O O L I N G\\xa0 ST A GE\\nP O O L I N G\\xa0 ST A GE\\nD E T E C T O R \\xa0 S T A GE\\nFigure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput\\nofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop\\nrowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions\\nandapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom )\\ntheinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas\\nchanged,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling\\nunitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation.\\n3 4 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9cdd7578-1005-43f8-8c44-98b8d4e06855', embedding=None, metadata={'page_label': '359', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nL ar ge \\xa0 r e s pon s e\\ni n\\xa0 po ol i ng\\xa0uni tL ar ge \\xa0 r e s pon s e\\ni n\\xa0 po ol i ng\\xa0uni t\\nL ar ge\\nr e s ponse\\ni n\\xa0 de t e c t or\\nuni t \\xa0 1L ar ge\\nr e s ponse\\ni n\\xa0 de t e c t or\\nuni t \\xa0 3\\nFigure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures\\nthatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof\\ntheinput.Hereweshowhowasetofthreelearnedﬁltersandamaxpoolingunitcanlearn\\ntobecomeinvarianttorotation.Allthreeﬁltersareintendedtodetectahand-written5.\\nEachﬁlterattemptstomatchaslightlydiﬀerentorientationofthe5.Whena5appearsin\\ntheinput,thecorrespondingﬁlterwillmatchitandcausealargeactivationinadetector\\nunit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit\\nwasactivated.Weshowherehowthenetworkprocessestwodiﬀerentinputs,resulting\\nintwodiﬀerentdetectorunitsbeingactivated.Theeﬀectonthepoolingunitisroughly\\nthesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l .,\\n2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally\\ninvarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother\\ntransformations.\\n0. 1 1. 0. 21. 0. 2\\n0. 10. 1\\n0. 0 0. 1\\nFigure9.10: P o o l i n g w i t h d o w n s a m p l i n g.Hereweusemax-poolingwithapoolwidthof\\nthreeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor\\noftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note\\nthattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot\\nwanttoignoresomeofthedetectorunits.\\n3 4 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c7f0b053-a6bd-4a9a-979d-6fa7371debec', embedding=None, metadata={'page_label': '360', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nSometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould\\nuseinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010\\npoolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe\\nlocationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011\\ndiﬀerentsetofpoolingregionsforeachimage.Anotherapproachistolearna\\nsinglepoolingstructurethatisthenappliedtoallimages(,). Jiaetal.2012\\nPoolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse\\ntop-downinformation, suchasBoltzmannmachinesandautoencoders.These\\nissueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III\\nPoolinginconvolutionalBoltzmannmachinesispresentedinsection.\\xa0The20.6\\ninverse-likeoperationsonpoolingunitsneededinsomediﬀerentiablenetworkswill\\nbecoveredinsection.20.10.6\\nSomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassiﬁcation\\nusingconvolutionandpoolingareshowninﬁgure.9.11\\n9.4Convolutionand\\xa0Pooling\\xa0asan\\xa0InﬁnitelyStrong\\nPrior\\nRecalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2\\naprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs\\naboutwhatmodelsarereasonable,beforewehaveseenanydata.\\nPriorscanbeconsideredweakorstrongdependingonhowconcentratedthe\\nprobabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh\\nentropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows\\nthedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow\\nentropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa\\nmoreactiveroleindeterminingwheretheparametersendup.\\nAninﬁnitelystrongpriorplaceszeroprobabilityonsomeparametersandsays\\nthattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch\\nsupportthedatagivestothosevalues.\\nWecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet,\\nbutwithaninﬁnitelystrongprioroveritsweights.Thisinﬁnitelystrongprior\\nsaysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits\\nneighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero,\\nexceptforinthesmall,spatiallycontiguousreceptiveﬁeldassignedtothathidden\\nunit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinﬁnitely\\nstrongpriorprobabilitydistributionovertheparametersofalayer.Thisprior\\n3 4 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2c22e115-e23a-45d9-82f6-5d70c5b8bf00', embedding=None, metadata={'page_label': '361', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nInput\\xa0image:\\xa0\\n256x256x3Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa0256x256x64Output\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n64x64x64Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa064x64x64Output\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n16x16x64Output\\xa0of\\xa0reshape\\xa0to\\xa0\\nvector:\\n16,384\\xa0unitsOutput\\xa0of\\xa0matrix\\xa0\\nmultiply:\\xa01,000\\xa0unitsOutput\\xa0of\\xa0softmax:\\xa0\\n1,000\\xa0class\\xa0\\nprobabilities\\nInput\\xa0image:\\xa0\\n256x256x3Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa0256x256x64Output\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n64x64x64Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa064x64x64Output\\xa0of\\xa0pooling\\xa0to\\xa0\\n3x3\\xa0grid:\\xa03x3x64Output\\xa0of\\xa0reshape\\xa0to\\xa0\\nvector:\\n576\\xa0unitsOutput\\xa0of\\xa0matrix\\xa0\\nmultiply:\\xa01,000\\xa0unitsOutput\\xa0of\\xa0softmax:\\xa0\\n1,000\\xa0class\\xa0\\nprobabilities\\nInput\\xa0image:\\xa0\\n256x256x3Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa0256x256x64Output\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n64x64x64Output\\xa0of\\xa0\\nconvolution\\xa0+\\xa0\\nReLU:\\xa064x64x64Output\\xa0of\\xa0\\nconvolution:\\n16x16x1,000Output\\xa0of\\xa0average\\xa0\\npooling:\\xa01x1x1,000Output\\xa0of\\xa0softmax:\\xa0\\n1,000\\xa0class\\xa0\\nprobabilities\\nOutput\\xa0of\\xa0pooling\\xa0\\nwith\\xa0stride\\xa04:\\xa0\\n16x16x64\\nFigure9.11:Examplesofarchitecturesforclassiﬁcationwithconvolutionalnetworks.The\\nspeciﬁcstridesanddepthsusedinthisﬁgurearenotadvisableforrealuse;theyare\\ndesignedtobeveryshallowinordertoﬁtontothepage.\\xa0Realconvolutionalnetworks\\nalsoofteninvolvesigniﬁcantamountsofbranching,unlikethechainstructuresused\\nhereforsimplicity. ( L e f t )Aconvolutionalnetworkthatprocessesaﬁxedimagesize.\\nAfteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe\\nconvolutionalfeaturemapisreshapedtoﬂattenoutthespatialdimensions.Therest\\nofthenetworkisanordinaryfeedforwardnetworkclassiﬁer,asdescribedinchapter.6\\n( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains\\nafullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools\\nbutaﬁxednumberofpools,inordertoprovideaﬁxed-sizevectorof576unitstothe\\nfullyconnectedportionofthenetwork.\\xa0Aconvolutionalnetworkthatdoesnot ( R i g h t )\\nhaveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone\\nfeaturemapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto\\noccurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides\\ntheargumenttothesoftmaxclassiﬁeratthetop.\\n3 4 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4fce5396-820d-41ee-8001-9345b07f37f3', embedding=None, metadata={'page_label': '362', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nsaysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis\\nequivarianttotranslation.Likewise,theuseofpoolingisaninﬁnitelystrongprior\\nthateachunitshouldbeinvarianttosmalltranslations.\\nOfcourse,implementing aconvolutionalnetasafullyconnectednetwithan\\ninﬁnitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking\\nofaconvolutionalnetasafullyconnectednetwithaninﬁnitelystrongpriorcan\\ngiveussomeinsightsintohowconvolutionalnetswork.\\nOnekeyinsightisthatconvolutionandpoolingcancauseunderﬁtting. Like\\nanyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade\\nbythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial\\ninformation, thenusingpoolingonallfeaturescanincreasethetrainingerror.\\nSomeconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a\\nusepoolingonsomechannelsbutnotonotherchannels,inordertogetboth\\nhighlyinvariantfeaturesandfeaturesthatwillnotunderﬁtwhenthetranslation\\ninvariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom\\nverydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe\\ninappropriate.\\nAnotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu-\\ntionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning\\nperformance.Modelsthatdonotuseconvolutionwouldbeabletolearneven\\nifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there\\nareseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust\\ndiscovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge\\nofspatialrelationshipshard-codedintothembytheirdesigner.\\n9.5VariantsoftheBasicConvolutionFunction\\nWhendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo\\nnotreferexactlytothestandarddiscreteconvolutionoperationasitisusually\\nunderstoodinthemathematical literature.Thefunctionsusedinpracticediﬀer\\nslightly.Herewedescribethesediﬀerencesindetail,andhighlightsomeuseful\\npropertiesofthefunctionsusedinneuralnetworks.\\nFirst,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually\\nactuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin\\nparallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind\\noffeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour\\nnetworktoextractmanykindsoffeatures,atmanylocations.\\n3 4 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='555b73d8-8b39-4ebc-a62c-fb62ec445007', embedding=None, metadata={'page_label': '363', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nAdditionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa\\ngridofvector-valuedobservations.\\xa0Forexample,acolorimagehasared,green\\nandblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput\\ntothesecondlayeristheoutputoftheﬁrstlayer,whichusuallyhastheoutput\\nofmanydiﬀerentconvolutionsateachposition.Whenworkingwithimages,we\\nusuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with\\noneindexintothediﬀerentchannelsandtwoindicesintothespatialcoordinates\\nofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey\\nwillactuallyuse4-Dtensors,withthefourthaxisindexingdiﬀerentexamplesin\\nthebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity.\\nBecauseconvolutionalnetworksusuallyusemulti-channelconvolution,the\\nlinearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif\\nkernel-ﬂippingisused.Thesemulti-channeloperationsareonlycommutativeif\\neachoperationhasthesamenumberofoutputchannelsasinputchannels.\\nAssumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection\\nstrengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe\\ninput,withanoﬀsetof krowsand lcolumnsbetweentheoutputunitandthe\\ninputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving\\nthevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour\\noutputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K\\nacrosswithoutﬂipping,then V K\\nZ i , j , k=\\ue058\\nl , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n (9.7)\\nwherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing\\noperationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto\\narraysusingafortheﬁrstentry.Thisnecessitatesthe 1 −1intheaboveformula.\\nProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0\\ntheaboveexpressionevensimpler.\\nWemaywanttoskipoversomepositionsofthekernelinordertoreducethe\\ncomputational cost(attheexpenseofnotextractingourfeaturesasﬁnely).We\\ncanthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If\\nwewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan\\ndeﬁneadownsampledconvolutionfunctionsuchthat c\\nZ i , j , k= ( ) c K V , , s i , j , k=\\ue058\\nl , m , n\\ue002\\nVl , j s m , k s n ( − × 1 ) + ( − × 1 ) + K i , l , m , n\\ue003\\n.(9.8)\\nWereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible\\n3 4 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2e9c007-6d1b-4860-8952-f6f601e8535a', embedding=None, metadata={'page_label': '364', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\ntodeﬁneaseparatestrideforeachdirectionofmotion.Seeﬁgureforan9.12\\nillustration.\\nOneessentialfeatureofanyconvolutionalnetworkimplementationistheability\\ntoimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature,\\nthewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth\\nateachlayer.\\xa0Zeropaddingtheinputallowsustocontrolthekernelwidthand\\nthesizeoftheoutputindependently.Withoutzeropadding,weareforcedto\\nchoosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall\\nkernels—bothscenariosthatsigniﬁcantlylimittheexpressivepowerofthenetwork.\\nSeeﬁgureforanexample. 9.13\\nThreespecialcasesofthezero-paddingsettingareworthmentioning.Oneis\\ntheextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution\\nkernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely\\nwithintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In\\nthiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin\\ntheinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However,\\nthesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand\\nthekernelhaswidth k,theoutputwillbeofwidth m k −+1.\\xa0Therateofthis\\nshrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis\\ngreaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded\\ninthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill\\neventuallydropto1 ×1,atwhichpointadditionallayerscannotmeaningfully\\nbeconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis\\nwhenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto\\nthesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the\\nnetworkcancontainasmanyconvolutionallayersastheavailablehardwarecan\\nsupport,sincetheoperationofconvolutiondoesnotmodifythearchitectural\\npossibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder\\ninﬂuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake\\ntheborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe\\notherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough\\nzeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting\\ninanoutputimageofwidth m+ k −1.Inthiscase,theoutputpixelsnearthe\\nborderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This\\ncanmakeitdiﬃculttolearnasinglekernelthatperformswellatallpositionsin\\ntheconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in\\ntermsoftestsetclassiﬁcationaccuracy)liessomewherebetween“valid”and“same”\\nconvolution.\\n3 4 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91217b6b-50db-42aa-b2d9-1a974988fd2b', embedding=None, metadata={'page_label': '365', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2\\nx 4 x 4 x 5 x 5s 3 s 3\\nx 1 x 1 x 2 x 2 x 3 x 3z 2 z 2 z 1 z 1 z 3 z 3\\nx 4 x 4z 4 z 4\\nx 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d\\nc onv ol ut i on\\nD ow nsampl i n g\\nC onv ol ut i on\\nFigure\\xa09.12:Convolution\\xa0witha\\xa0stride.Inthisexample,we\\xa0use\\xa0astride\\xa0oftwo.\\n( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation.\\xa0 ( Bot-\\nt o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto\\nconvolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach\\ninvolvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues\\nthatarethendiscarded.\\n3 5 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c284da9-921a-4222-81a0-a927eb48a3f5', embedding=None, metadata={'page_label': '366', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\n. . . . . .. . .\\n. . . . . .. . . . . .. . . . . .\\nFigure9.13: T h e e ﬀ e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork\\nwithakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so\\nonlytheconvolutionoperationitselfshrinksthenetworksize. ( T o p )Inthisconvolutional\\nnetwork,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto\\nshrinkbyﬁvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly\\nabletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel,\\nsoarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan\\nbemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome\\nshrinkingisinevitableinthiskindofarchitecture. Byaddingﬁveimplicitzeroes ( Bottom )\\ntoeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto\\nmakeanarbitrarilydeepconvolutionalnetwork.\\n3 5 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5fe4b0b6-dee5-4b8b-bc68-8384d08b23fc', embedding=None, metadata={'page_label': '367', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nInsomecases,wedonotactuallywanttouseconvolution,butratherlocally\\nconnectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989\\ngraphofourMLPisthesame,buteveryconnectionhasitsownweight,speciﬁed\\nbya6-Dtensor W.\\xa0Theindicesinto Warerespectively: i,theoutputchannel,\\nj,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowoﬀset\\nwithintheinput,and n,thecolumnoﬀsetwithintheinput.Thelinearpartofa\\nlocallyconnectedlayeristhengivenby\\nZ i , j , k=\\ue058\\nl , m , n[ V l , j m , k n + − 1 + − 1 w i , j , k, l , m , n] . (9.9)\\nThisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper-\\nationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters\\nacrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14\\nconnections.\\nLocallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe\\nafunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame\\nfeatureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage\\nisapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe\\nimage.\\nItcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers\\ninwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput\\nchannel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon\\nwaytodothisistomaketheﬁrst moutputchannelsconnecttoonlytheﬁrst\\nninputchannels,thesecond moutputchannelsconnecttoonlythesecond n\\ninputchannels,andsoon.Seeﬁgureforanexample.Modelinginteractions 9.15\\nbetweenfewchannelsallowsthenetworktohavefewerparametersinorderto\\nreducememoryconsumptionandincreasestatisticaleﬃciency,andalsoreduces\\ntheamountofcomputationneededtoperformforwardandback-propagation. It\\naccomplishesthesegoalswithoutreducingthenumberofhiddenunits.\\nT i l e d c o n v o l ut i o n( ,;,)oﬀersacom- GregorandLeCun2010aLeetal.2010\\npromisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan\\nlearningaseparatesetofweightsatspatiallocation,welearnasetofkernels every\\nthatwerotatethroughaswemovethroughspace.Thismeansthatimmediately\\nneighboringlocationswillhavediﬀerentﬁlters,likeinalocallyconnectedlayer,\\nbutthememoryrequirementsforstoringtheparameterswillincreaseonlybya\\nfactorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput\\nfeaturemap.Seeﬁgureforacomparisonoflocallyconnectedlayers,tiled 9.16\\nconvolution,andstandardconvolution.\\n3 5 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05aaf897-f666-4d57-8e20-7c7b3e512cdc', embedding=None, metadata={'page_label': '368', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9. CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\nx 1 x 1 x 2 x 2s 1 s 1 s 3 s 3\\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\na\\xa0 \\xa0b a\\xa0 \\xa0b a\\xa0 \\xa0b a\\xa0 \\xa0b a\\xa0a\\xa0 \\xa0b c\\xa0 \\xa0d e\\xa0 \\xa0f g\\xa0 \\xa0h \\xa0i\\xa0\\nx 4 x 4 x 3 x 3s 4 s 4 s 2 s 2\\nFigure 9.14: Comparison of localconnections, convolution, andfull connections.\\n(T op)A locally connected layer with a patch size of two pixels. Each edge islabeled with\\na unique letterto show thateach edge isassociated with itsown weight parameter.\\n(Center)A convolutional layerwith a kernel width of two pixels. This model hasexactly\\nthesame connectivityas thelocallyconnected layer. Thediﬀerence liesnot inwhichunits\\ninteractwitheachother,butinhowtheparametersareshared. Thelocallyconnectedlayer\\nhas no parameter sharing. The convolutionallayer uses thesame two weights repeatedly\\nacross the entire input,as indicated bythe repetition ofthe letters labeling each edge.\\n(Bottom)A fullyconnected layer resemblesa locallyconnected layer in the sensethat each\\nedge has its own parameter (there are too many to label explicitly with letters in this\\ndiagram). However,it does nothave the restrictedconnectivity ofthe locally connected\\nlayer.\\n353', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='46f03a0f-a91e-4e92-9c6c-6eb1e833950b', embedding=None, metadata={'page_label': '369', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nI nput\\xa0T e nsorO ut put\\xa0T e nsor\\nS p a t i a l \\xa0 c o o r d i n a t e sC h a n n e l \\xa0 c o o r d i n a t e s\\nFigure9.15:\\xa0Aconvolutionalnetworkwiththeﬁrsttwooutputchannelsconnectedto\\nonlytheﬁrsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly\\nthesecondtwoinputchannels.\\n3 5 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='00293850-ee13-4519-ac1c-47489c7e566c', embedding=None, metadata={'page_label': '370', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9. CONVOLUTIONALNETWORKS\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\na\\xa0 \\xa0b a\\xa0 \\xa0b a\\xa0 \\xa0b a\\xa0 \\xa0b a\\xa0a\\xa0 \\xa0b c\\xa0 \\xa0d e\\xa0 \\xa0f g\\xa0 \\xa0h \\xa0i\\xa0\\nx 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3\\nx 4 x 4s 4 s 4\\nx 5 x 5s 5 s 5\\na\\xa0 \\xa0b c\\xa0 \\xa0d a\\xa0 \\xa0b c\\xa0 \\xa0d a\\xa0\\nFigure 9.16: A comparison of locally connected layers,tiled convolution, and standard\\nconvolution. All three have thesame sets of connectionsbetweenunits, when thesame\\nsize of kernel is used. This diagram illustrates the use of a kernel that is two pixels wide.\\nThe diﬀerences between the methodslies in how they shareparameters. (T op)A locally\\nconnectedlayerhasno sharingatall. Weindicate thateachconnection hasits ownweight\\nby labelingeach connectionwith a unique letter. Tiled convolution has a set of (Center)\\ntdiﬀerent kernels. Here we illustrate the case of t= 2.\\xa0One of these kernels has edges\\nlabeled“a” and “b,” whilethe other hasedges labeled“c” and “d.”\\xa0Each timewemove one\\npixel to the right in theoutput, we move on to usinga diﬀerent kernel. This means that,\\nlike the locally connected layer, neighboring units in the outputhave diﬀerent parameters.\\nUnlike the locally connected layer, after we have gone through all tavailable kernels,\\nwe cycle back to the ﬁrst kernel. If two output units are separated by a multiple of t\\nsteps, then they share parameters. Traditional convolution is equivalent to tiled (Bottom)\\nconvolutionwith t= 1. Thereis onlyone kerneland itis appliedeverywhere,as indicated\\nin the diagramby using thekernel with weights labeled “a” and “b” everywhere.\\n355', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b2b9637d-6366-4e84-9ac7-cc4df1845ac4', embedding=None, metadata={'page_label': '371', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nTodeﬁnetiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof\\nthedimensionscorrespondtodiﬀerentlocationsintheoutputmap.Ratherthan\\nhavingaseparateindexforeachlocationintheoutputmap,outputlocationscycle\\nthroughasetof tdiﬀerentchoicesofkernelstackineachdirection.If tisequalto\\ntheoutputwidth,thisisthesameasalocallyconnectedlayer.\\nZ i , j , k=\\ue058\\nl , m , nV l , j m , k n + − 1 + − 1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10)\\nwhereis\\xa0themodulooperation,with % t% t=0(, t+1)% t=1,etc.It\\xa0is\\nstraightforwardtogeneralizethisequationtouseadiﬀerenttilingrangeforeach\\ndimension.\\nBothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting\\ninteractionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby\\ndiﬀerentﬁlters.Iftheseﬁlterslearntodetectdiﬀerenttransformedversionsof\\nthesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe\\nlearnedtransformation(seeﬁgure).Convolutionallayersarehard-codedtobe 9.9\\ninvariantspeciﬁcallytotranslation.\\nOtheroperationsbesidesconvolutionareusuallynecessarytoimplementa\\nconvolutionalnetwork.Toperformlearning,onemustbeabletocomputethe\\ngradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs.\\nInsomesimplecases,\\xa0thisoperationcanbeperformedusingtheconvolution\\noperation,butmanycasesofinterest,includingthecaseofstridegreaterthan1,\\ndonothavethisproperty.\\nRecallthatconvolutionisalinearoperationandcanthusbedescribedasa\\nmatrixmultiplication (ifweﬁrstreshapetheinputtensorintoaﬂatvector).The\\nmatrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand\\neachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview\\nhelpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional\\nnetwork.\\nMultiplication bythetransposeofthematrixdeﬁnedbyconvolutionisone\\nsuchoperation.Thisistheoperationneededtoback-propagate errorderivatives\\nthroughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks\\nthathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe\\nwishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simard etal.1992\\nReconstructingthevisibleunitsisanoperationcommonlyusedinthemodels\\ndescribedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III\\nTransposeconvolutionisnecessarytoconstructconvolutionalversionsofthose\\nmodels.Likethekernelgradientoperation,thisinputgradientoperationcanbe\\n3 5 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='08207506-66c1-4bc8-bf61-e46f78544ed3', embedding=None, metadata={'page_label': '372', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nimplementedusingaconvolutioninsomecases,butinthegeneralcaserequires\\nathirdoperationtobeimplemented.Caremustbetakentocoordinatethis\\ntransposeoperationwiththeforwardpropagation. Thesizeoftheoutputthatthe\\ntransposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof\\ntheforwardpropagationoperation,aswellasthesizeoftheforwardpropagation’s\\noutputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan\\nresultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly\\ntoldwhatthesizeoftheoriginalinputwas.\\nThesethreeoperations—convolution,backpropfromoutputtoweights,and\\nbackpropfromoutputtoinputs—aresuﬃcienttocomputeallofthegradients\\nneededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain\\nconvolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof\\nconvolution.\\xa0See ()forafullderivationoftheequationsinthe Goodfellow2010\\nfullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese\\nequationswork,wepresentthetwodimensional,singleexampleversionhere.\\nSupposewewanttotrainaconvolutionalnetworkthatincorporatesstrided\\nconvolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas\\ndeﬁnedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8\\nfunction J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto\\noutput Z,whichisthenpropagatedthroughtherestofthenetworkandusedto\\ncomputethecostfunction J.Duringback-propagation, wewillreceiveatensor G\\nsuchthat G i , j , k=∂\\n∂ Z i , j , kJ , . ( V K)\\nTotrainthenetwork,weneedtocomputethederivativeswithrespecttothe\\nweightsinthekernel.Todoso,wecanuseafunction\\ng , , s ( G V) i , j , k, l=∂\\n∂ K i , j , k, lJ ,( V K) =\\ue058\\nm , nG i , m , n V j , m s k, n s l ( − × 1 ) + ( − × 1 ) + .(9.11)\\nIfthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute\\nthegradientwithrespectto Vinordertoback-propagate theerrorfartherdown.\\nTodoso,wecanuseafunction\\nh , , s ( K G) i , j , k=∂\\n∂ V i , j , kJ ,( V K) (9.12)\\n=\\ue058\\nl , m\\ns . t .\\n( 1 ) + = l − × s m j\\ue058\\nn , p\\ns . t .\\n( 1 ) + = n − × s p k\\ue058\\nqK q , i , m , p G q , l , n .(9.13)\\nAutoencodernetworks,\\xa0describedinchapter,\\xa0arefeedforwardnetworks 14\\ntrainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm,\\n3 5 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c71ce428-fdda-467f-8553-13afe3af5aab', embedding=None, metadata={'page_label': '373', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nthatcopiesitsinput xtoanapproximatereconstruction rusingthefunction\\nW\\ue03eW x.Itiscommonformore\\xa0general autoencoders\\xa0tousemultiplication\\nbythetransposeoftheweightmatrixjustasPCAdoes.\\xa0Tomakesuchmodels\\nconvolutional,wecanusethefunction htoperformthetransposeoftheconvolution\\noperation.Supposewehavehiddenunits Hinthesameformatas Zandwedeﬁne\\nareconstruction\\nR K H = ( h , , s .) (9.14)\\nInordertotraintheautoencoder,wewillreceivethegradientwithrespect\\nto Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith\\nrespectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain\\nthegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto\\ndiﬀerentiatethrough gusing cand h,buttheseoperationsarenotneededforthe\\nback-propagationalgorithmonanystandardnetworkarchitectures.\\nGenerally,wedonotuseonlyalinearoperationinordertotransformfrom\\ntheinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome\\nbiastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion\\nofhowtoshareparametersamongthebiases.\\xa0Forlocallyconnectedlayersitis\\nnaturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto\\nsharethebiaseswiththesametilingpatternasthekernels.Forconvolutional\\nlayers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross\\nalllocationswithineachconvolutionmap.However,iftheinputisofknown,ﬁxed\\nsize,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap.\\nSeparatingthebiasesmayslightlyreducethestatisticaleﬃciencyofthemodel,but\\nalsoallowsthemodeltocorrectfordiﬀerencesintheimagestatisticsatdiﬀerent\\nlocations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe\\nedgeoftheimagereceivelesstotalinputandmayneedlargerbiases.\\n9.6StructuredOutputs\\nConvolutionalnetworkscanbeusedtooutputahigh-dimensional,structured\\nobject,ratherthanjustpredictingaclasslabelforaclassiﬁcationtaskorareal\\nvalueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya\\nstandardconvolutionallayer.Forexample,themodelmightemitatensor S,where\\nS i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass\\ni.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks\\nthatfollowtheoutlinesofindividualobjects.\\nOneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe\\n3 5 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ebd50964-2073-4fc8-b249-824dad9421f6', embedding=None, metadata={'page_label': '374', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nˆ Y( 1 )ˆ Y( 1 )ˆ Y( 2 )ˆ Y( 2 )ˆ Y( 3 )ˆ Y( 3 )\\nH( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 )\\nXXU U UV V V W W\\nFigure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The\\ninputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X\\nchannels(red,green,blue).Thegoalistooutputatensoroflabelsˆ Y,withaprobability\\ndistributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows,\\nimagecolumns,andthediﬀerentclasses.Ratherthanoutputtingˆ Yinasingleshot,the\\nrecurrentnetworkiterativelyreﬁnesitsestimateˆ Ybyusingapreviousestimateofˆ Y\\nasinputforcreatinganewestimate.\\xa0Thesameparametersareusedforeachupdated\\nestimate,andtheestimatecanbereﬁnedasmanytimesaswewish.Thetensorof\\nconvolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe\\ninputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe\\nhiddenvalues.Onallbuttheﬁrststep,thekernels Wareconvolvedoverˆ Ytoprovide\\ninputtothehiddenlayer.Ontheﬁrsttimestep,thistermisreplacedbyzero.Because\\nthesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as\\ndescribedinchapter.10\\ninputplane,asshowninﬁgure.Inthekindsofarchitectures typicallyusedfor 9.13\\nclassiﬁcationofasingleobjectinanimage,thegreatestreductioninthespatial\\ndimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In\\nordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling\\naltogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007\\ngridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015\\nuseapoolingoperatorwithunitstride.\\nOnestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess\\noftheimagelabels,thenreﬁnethisinitialguessusingtheinteractionsbetween\\nneighboringpixels.Repeatingthisreﬁnementstepseveraltimescorrespondsto\\nusingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof\\nthedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007\\nbythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular\\nkindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17\\nthearchitectureofsucharecurrentconvolutionalnetwork.\\n3 5 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='58ec91f6-6546-4030-801d-5bbf3512cd94', embedding=None, metadata={'page_label': '375', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nOnceapredictionforeachpixelismade,variousmethodscanbeusedto\\nfurtherprocessthesepredictionsinordertoobtainasegmentationoftheimage\\nintoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,).\\nThegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe\\nassociatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic\\nrelationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork\\ncanbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining\\nobjective(,; ,). Ningetal.2005Thompsonetal.2014\\n9.7DataTypes\\nThedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels,\\neachchannelbeingtheobservationofadiﬀerentquantityatsomepointinspace\\nortime.Seetableforexamplesofdatatypeswithdiﬀerentdimensionalities 9.1\\nandnumberofchannels.\\nForanexampleofconvolutionalnetworksappliedtovideo,seeChenetal.\\n().2010\\nSofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest\\ndatahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks\\nisthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof\\ninputsimplycannotberepresentedbytraditional,matrixmultiplication-based\\nneuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks\\nevenwhencomputational costandoverﬁttingarenotsigniﬁcantissues.\\nForexample,consideracollectionofimages,whereeachimagehasadiﬀerent\\nwidthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof\\nﬁxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda\\ndiﬀerentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe\\nconvolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix\\nmultiplication; thesameconvolutionkernelinducesadiﬀerentsizeofdoublyblock\\ncirculantmatrixforeachsizeofinput.\\xa0Sometimes theoutputofthenetworkis\\nallowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign\\naclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis\\nnecessary.Inothercases,thenetworkmustproducesomeﬁxed-sizeoutput,for\\nexampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase\\nwemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose\\npoolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto\\nmaintainaﬁxednumberofpooledoutputs.Someexamplesofthiskindofstrategy\\nareshowninﬁgure.9.11\\n3 6 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='14cbed13-0fb7-4b17-a4d1-e00480400e45', embedding=None, metadata={'page_label': '376', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nSinglechannel Multi-channel\\n1-DAudio\\xa0waveform:The\\xa0axis\\xa0we\\nconvolveovercorrespondsto\\ntime.Wediscretizetimeand\\nmeasuretheamplitudeofthe\\nwaveformoncepertimestep.Skeletonanimationdata:Anima-\\ntionsof3-Dcomputer-rendered\\ncharactersaregeneratedbyalter-\\ningtheposeofa“skeleton”over\\ntime.Ateachpointintime,the\\nposeofthecharacterisdescribed\\nbyaspeciﬁcationoftheanglesof\\neachofthejointsinthecharac-\\nter’sskeleton.Eachchannelin\\nthedatawefeedtotheconvolu-\\ntionalmodelrepresentstheangle\\naboutoneaxisofonejoint.\\n2-DAudiodatathathasbeenprepro-\\ncessedwithaFouriertransform:\\nWecantransformtheaudiowave-\\nformintoa2Dtensorwithdif-\\nferentrowscorrespondingtodif-\\nferentfrequencies\\xa0anddiﬀerent\\ncolumnscorrespondingtodiﬀer-\\nentpointsintime.Usingconvolu-\\ntioninthetimemakesthemodel\\nequivarianttoshiftsintime.Us-\\ningconvolutionacrossthefre-\\nquencyaxismakesthemodel\\nequivarianttofrequency,sothat\\nthesamemelodyplayedinadif-\\nferentoctaveproducesthesame\\nrepresentationbutatadiﬀerent\\nheightinthenetwork’soutput.Colorimagedata:Onechannel\\ncontainstheredpixels,onethe\\ngreen\\xa0pixels,\\xa0and\\xa0one\\xa0theblue\\npixels.Theconvolutionkernel\\nmovesoverboththehorizontal\\nandverticalaxesofthe\\xa0image,\\nconferringtranslationequivari-\\nanceinbothdirections.\\n3-DVolumetricdata:Acommon\\nsourceofthiskindofdataismed-\\nicalimagingtechnology,suchas\\nCTscans.Colorvideodata:Oneaxiscorre-\\nspondstotime,onetotheheight\\nofthevideoframe,andoneto\\nthewidthofthevideoframe.\\nTable9.1:Examplesofdiﬀerentformatsofdatathatcanbeusedwithconvolutional\\nnetworks.\\n3 6 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='188c27fb-e8d2-4624-9957-e302fef3da59', embedding=None, metadata={'page_label': '377', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nNotethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes\\nsenseforinputsthathavevariablesizebecausetheycontainvaryingamounts\\nofobservationofthesamekindofthing—diﬀeren tlengthsofrecordingsover\\ntime,diﬀerentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake\\nsenseiftheinputhasvariablesizebecauseitcanoptionallyincludediﬀerent\\nkindsofobservations.Forexample,ifweareprocessingcollegeapplications,and\\nourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery\\napplicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe\\nsameweightsoverboththefeaturescorrespondingtothegradesandthefeatures\\ncorrespondingtothetestscores.\\n9.8EﬃcientConvolutionAlgorithms\\nModernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore\\nthanonemillionunits.Powerfulimplementations exploitingparallelcomputation\\nresources,asdiscussedinsection,areessential.\\xa0However,inmanycasesit 12.1\\nisalsopossibletospeedupconvolutionbyselectinganappropriateconvolution\\nalgorithm.\\nConvolutionisequivalenttoconvertingboththeinputandthekerneltothe\\nfrequencydomainusingaFouriertransform,performingpoint-wisemultiplication\\nofthetwosignals,\\xa0andconvertingbacktothetimedomainusinganinverse\\nFouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive\\nimplementationofdiscreteconvolution.\\nWhena d-dimensionalkernelcanbeexpressedas\\xa0theouterproductof d\\nvectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe\\nkernelisseparable,naiveconvolutionisineﬃcient.Itisequivalenttocompose d\\none-dimensional convolutionswitheachofthesevectors.Thecomposedapproach\\nissigniﬁcantlyfasterthanperformingone d-dimensionalconvolutionwiththeir\\nouterproduct.Thekernelalsotakesfewerparameterstorepresentasvectors.\\nIfthekernelis welementswideineachdimension,thennaivemultidimensional\\nconvolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable\\nconvolutionrequires O( w d ×)runtimeandparameterstoragespace.Ofcourse,\\nnoteveryconvolutioncanberepresentedinthisway.\\nDevisingfasterwaysofperformingconvolutionorapproximateconvolution\\nwithoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech-\\nniquesthatimprovetheeﬃciencyofonlyforwardpropagationareusefulbecause\\ninthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof\\nanetworkthantoitstraining.\\n3 6 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6afbf539-4198-47fd-bbba-8006468d4093', embedding=None, metadata={'page_label': '378', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\n9.9RandomorUnsupervisedFeatures\\nTypically,themostexpensivepartofconvolutionalnetworktrainingislearningthe\\nfeatures.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber\\noffeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof\\npooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient\\nsteprequiresacompleterunofforwardpropagationandbackwardpropagation\\nthroughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork\\ntrainingistousefeaturesthatarenottrainedinasupervisedfashion.\\nTherearethreebasicstrategiesforobtaining\\xa0con volutionkernelswithout\\nsupervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo\\ndesignthembyhand,forexamplebysettingeachkerneltodetectedgesata\\ncertainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised\\ncriterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall\\nimagepatches,thenuseeachlearnedcentroidasaconvolutionkernel.\\xa0PartIII\\ndescribesmanymoreunsupervisedlearningapproaches.Learningthefeatures\\nwithanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe\\nclassiﬁerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor\\ntheentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe\\nlastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem,\\nassumingthelastlayerissomethinglikelogisticregressionoranSVM.\\nRandomﬁltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett\\netal. etal. etal. ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal.\\n()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011\\nbecomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights.\\nTheyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof\\naconvolutionalnetwork:ﬁrstevaluatetheperformanceofseveralconvolutional\\nnetworkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese\\narchitecturesandtraintheentirearchitectureusingamoreexpensiveapproach.\\nAnintermediate approachistolearnthefeatures,butusingmethodsthatdo\\nnotrequirefullforwardandback-propagationateverygradientstep.Aswith\\nmultilayerperceptrons,weusegreedylayer-wisepretraining,totraintheﬁrstlayer\\ninisolation,thenextractallfeaturesfromtheﬁrstlayeronlyonce,thentrainthe\\nsecondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8\\nhowtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII\\ntogreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The\\ncanonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe\\nconvolutionaldeepbeliefnetwork(,).Convolutionalnetworksoﬀer Leeetal.2009\\n3 6 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='27150d82-0702-4f28-bd61-55b146c9942e', embedding=None, metadata={'page_label': '379', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible\\nwithmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata\\ntime,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means.\\nWecanthenusetheparametersfromthispatch-basedmodeltodeﬁnethekernels\\nofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning\\ntotrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining\\nprocess.Usingthisapproach,wecantrainverylargemodelsandincurahigh\\ncomputational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal.\\n2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular\\nfromroughly2007–2013,whenlabeleddatasetsweresmallandcomputational\\npowerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina\\npurelysupervisedfashion,usingfullforwardandback-propagation throughthe\\nentirenetworkoneachtrainingiteration.\\nAswithotherapproachestounsupervisedpretraining,itremainsdiﬃcultto\\nteaseapartthecauseofsomeofthebeneﬁtsseenwiththisapproach.Unsupervised\\npretrainingmayoﬀersomeregularizationrelativetosupervisedtraining,oritmay\\nsimplyallowustotrainmuchlargerarchitectures duetothereducedcomputational\\ncostofthelearningrule.\\n9.10TheNeuroscientiﬁcBasisforConvolutionalNet-\\nworks\\nConvolutional\\xa0networksare\\xa0perhaps\\xa0the greatest\\xa0successstory\\xa0ofbiologically\\ninspiredartiﬁcialintelligence.Thoughconvolutionalnetworkshavebeenguided\\nbymanyotherﬁelds,someofthekeydesignprinciplesofneuralnetworkswere\\ndrawnfromneuroscience.\\nThehistoryofconvolutionalnetworksbeginswithneuroscientiﬁcexperiments\\nlongbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists\\nDavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany\\nofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland\\nWiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith\\naNobelprize.Theirﬁndingsthathavehadthegreatestinﬂuenceoncontemporary\\ndeeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin\\ncats.Theyobservedhowneuronsinthecat’sbrainrespondedtoimagesprojected\\ninpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas\\nthatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciﬁc\\npatternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto\\notherpatterns.\\n3 6 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f02286b2-3488-46d5-bcd5-1a7f2f4fa891', embedding=None, metadata={'page_label': '380', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nTheirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare\\nbeyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan\\nfocusonasimpliﬁed,cartoonviewofbrainfunction.\\nInthissimpliﬁedview,wefocusonapartofthebraincalledV1,alsoknown\\nasthe pr i m ar y v i sual c o r t e x.\\xa0V1istheﬁrstareaofthebrainthatbeginsto\\nperformsigniﬁcantlyadvancedprocessingofvisualinput.\\xa0Inthiscartoonview,\\nimagesareformedbylightarrivingintheeyeandstimulatingtheretina,the\\nlight-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform\\nsomesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis\\nrepresented.Theimagethenpassesthroughtheopticnerveandabrainregion\\ncalledthelateralgeniculatenucleus.\\xa0Themainrole,asfarasweareconcerned\\nhere,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom\\ntheeyetoV1,whichislocatedatthebackofthehead.\\nAconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1:\\n1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure\\nmirroring\\xa0the structure\\xa0of\\xa0theimage\\xa0in\\xa0the retina.For\\xa0example,\\xa0light\\narrivingatthelowerhalfoftheretinaaﬀectsonlythecorrespondinghalfof\\nV1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures\\ndeﬁnedintermsoftwodimensionalmaps.\\n2.V1containsmany si m pl e c e l l s.Asimplecell’sactivitycantosomeextent\\nbecharacterizedbyalinear\\xa0function oftheimagein\\xa0asmall,\\xa0spatially\\nlocalizedreceptiveﬁeld.Thedetectorunitsofaconvolutionalnetworkare\\ndesignedtoemulatethesepropertiesofsimplecells.\\n3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat\\naresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant\\ntosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits\\nofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges\\ninlightingthatcannotbecapturedsimplybypoolingoverspatiallocations.\\nTheseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies\\ninconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellow etal.2013a\\nThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame\\nbasicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof\\nthevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly\\nappliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical\\nlayersofthebrain,weeventuallyﬁndcellsthatrespondtosomespeciﬁcconcept\\nandareinvarianttomanytransformationsoftheinput.Thesecellshavebeen\\n3 6 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85d4f55b-df44-4c55-a5ca-c31f98172ed1', embedding=None, metadata={'page_label': '381', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nnicknamed“grandmother cells”—theideaisthatapersoncouldhaveaneuronthat\\nactivateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe\\nappearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof\\nherfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin\\nshadow,etc.\\nThesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain,\\ninaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005\\ntestedwhetherindividualneuronswouldrespondtophotosoffamousindividuals.\\nTheyfoundwhathascometobecalledthe“HalleBerryneuron”:anindividual\\nneuronthatisactivatedbytheconceptofHalleBerry.Thisneuronﬁreswhena\\npersonseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining\\nthewords“HalleBerry.”Ofcourse,thishasnothingtodowithHalleBerryherself;\\notherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc.\\nThesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern\\nconvolutionalnetworks,whichwouldnotautomatically generalizetoidentifying\\napersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional\\nnetwork’slastlayeroffeaturesisabrainareacalledtheinferotemporal cortex\\n(IT).Whenviewinganobject,informationﬂowsfromtheretina,throughthe\\nLGN,toV1,thenonwardtoV2,thenV4,thenIT.Thishappenswithintheﬁrst\\n100msofglimpsinganobject.\\xa0Ifapersonisallowedtocontinuelookingatthe\\nobjectformoretime,theninformationwillbegintoﬂowbackwardsasthebrain\\nusestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas.\\nHowever,ifweinterrupttheperson’sgaze,andobserveonlytheﬁringratesthat\\nresultfromtheﬁrst100msofmostlyfeedforwardactivation,thenITprovestobe\\nverysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT\\nﬁringrates,andalsoperformverysimilarlyto(timelimited)humansonobject\\nrecognitiontasks(,). DiCarlo2013\\nThatbeingsaid,therearemanydiﬀerencesbetweenconvolutionalnetworks\\nandthemammalianvisionsystem.Someofthesediﬀerencesarewellknown\\ntocomputational neuroscientists,butoutsidethescopeofthisbook.Someof\\nthesediﬀerencesarenotyetknown,becausemanybasicquestionsabouthowthe\\nmammalianvisionsystemworksremainunanswered.Asabrieflist:\\n•Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe\\nf o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat\\narmslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution,\\nthisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches\\ntogetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually\\nreceivelargefullresolutionphotographsasinput.Thehumanbrainmakes\\n3 6 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1340de5d-0cce-41f5-897e-0a82931a2661', embedding=None, metadata={'page_label': '382', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nseveraleyemovementscalled sac c adestoglimpsethemostvisuallysalient\\nortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms\\nintodeeplearningmodelsisanactiveresearchdirection.Inthecontextof\\ndeeplearning,attentionmechanismshavebeenmostsuccessfulfornatural\\nlanguageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1\\nwithfoveationmechanismshavebeendevelopedbutsofarhavenotbecome\\nthedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,).\\n•Thehumanvisualsystemisintegratedwithmanyothersenses,suchas\\nhearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks\\nsofararepurelyvisual.\\n•Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis\\nabletounderstandentirescenesincludingmanyobjectsandrelationships\\nbetweenobjects,andprocessesrich3-Dgeometricinformationneededfor\\nourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen\\nappliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy.\\n•EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher\\nlevels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut\\nhasnotyetbeenshowntooﬀeracompellingimprovement.\\n•WhilefeedforwardITﬁringratescapturemuchofthesameinformationas\\nconvolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate\\ncomputations are.Thebrainprobablyusesverydiﬀerentactivationand\\npoolingfunctions.Anindividualneuron’sactivationprobablyisnotwell-\\ncharacterizedbyasinglelinearﬁlterresponse.ArecentmodelofV1involves\\nmultiplequadraticﬁltersforeachneuron(,).Indeedour Rustetal.2005\\ncartoonpictureof“simplecells”\\xa0and“complexcells”\\xa0mightcreateanon-\\nexistentdistinction;simplecellsandcomplexcellsmightbothbethesame\\nkindofcellbutwiththeir“parameters”enablingacontinuumofbehaviors\\nrangingfromwhatwecall“simple”towhatwecall“complex.”\\nItis\\xa0alsoworthmentioningthatneuroscience\\xa0hastold\\xa0usrelativelylittle\\nabouthowtotrainconvolutionalnetworks.Modelstructureswithparameter\\nsharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels\\nofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976\\nback-propagationalgorithmandgradientdescent.Forexample,theNeocognitron\\n(Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof\\nthemodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering\\nalgorithm.\\n3 6 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a22d0aae-dc80-457c-aff0-e23a660205ae', embedding=None, metadata={'page_label': '383', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nLang\\xa0andHinton\\xa01988()introducedthe\\xa0use\\xa0ofback-propagation\\xa0totrain\\nt i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology,\\nTDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back-\\npropagationappliedtothesemodelswasnotinspiredbyanyneuroscientiﬁcobserva-\\ntionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess\\nofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989\\nthemodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D\\nconvolutionappliedtoimages.\\nSofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor\\ncertainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome\\ntransformationsofthesesimplecellfeatures,andstacksoflayersthatalternate\\nbetweenselectivityandinvariancecanyieldgrandmother cellsforveryspeciﬁc\\nphenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect.\\nInadeep,nonlinearnetwork,itcanbediﬃculttounderstandthefunctionof\\nindividualcells.Simplecellsintheﬁrstlayerareeasiertoanalyze,becausetheir\\nresponsesaredrivenbyalinearfunction.Inanartiﬁcialneuralnetwork,wecan\\njustdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding\\nchannelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we\\ndonothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe\\nneuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal’s\\nretina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan\\nthenﬁtalinearmodeltotheseresponsesinordertoobtainanapproximation of\\ntheneuron’sweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach\\nandShapley2004,).\\nReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed\\nby G ab o r f unc t i o ns.\\xa0TheGaborfunctiondescribestheweightata2-Dpoint\\nintheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates,\\nI( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof\\nlocations,deﬁnedbyasetof xcoordinates Xandasetof ycoordinates, Y,and\\napplyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint\\nofview,theresponseofasimplecelltoanimageisgivenby\\ns I() =\\ue058\\nx ∈ X\\ue058\\ny ∈ Yw x , y I x , y . ()() (9.15)\\nSpeciﬁcally,takestheformofaGaborfunction: w x , y()\\nw x , y α , β (; x , β y , f , φ , x 0 , y 0 , τ α) = exp\\ue000− β x x\\ue030 2− β y y\\ue030 2\\ue001\\ncos( f x\\ue030+) φ ,(9.16)\\nwhere\\nx\\ue030= ( x x − 0)cos()+( τ y y − 0)sin() τ (9.17)\\n3 6 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2e0f6bf-b3f5-4bdd-9558-e59392ce7533', embedding=None, metadata={'page_label': '384', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nand\\ny\\ue030= ( − x x − 0)sin()+( τ y y − 0)cos() τ . (9.18)\\nHere, α, β x, β y, f, φ, x 0, y 0,and τareparametersthatcontroltheproperties\\noftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18\\ndiﬀerentsettingsoftheseparameters.\\nTheparameters x 0, y 0,and τdeﬁneacoordinatesystem.\\xa0Wetranslateand\\nrotate xand ytoform x\\ue030and y\\ue030.Speciﬁcally,thesimplecellwillrespondtoimage\\nfeaturescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness\\naswemovealongalinerotatedradiansfromthehorizontal. τ\\nViewedasafunctionof x\\ue030and y\\ue030,thefunction wthenrespondstochangesin\\nbrightnessaswemovealongthe x\\ue030axis.\\xa0Ithastwoimportantfactors:oneisa\\nGaussianfunctionandtheotherisacosinefunction.\\nTheGaussianfactor αexp\\ue000\\n− β x x\\ue030 2− β y y\\ue030 2\\ue001\\ncanbeseenasagatingtermthat\\nensuresthesimplecellwillonlyrespondtovaluesnearwhere x\\ue030and y\\ue030areboth\\nzero,inotherwords,nearthecenterofthecell’sreceptiveﬁeld.Thescalingfactor\\nαadjuststhetotalmagnitudeofthesimplecell’sresponse,while β xand β ycontrol\\nhowquicklyitsreceptiveﬁeldfallsoﬀ.\\nThecosinefactor cos( f x\\ue030+ φ) controlshowthesimplecellrespondstochanging\\nbrightnessalongthe x\\ue030axis.Theparameter fcontrolsthefrequencyofthecosine\\nandcontrolsitsphaseoﬀset. φ\\nAltogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds\\ntoaspeciﬁcspatialfrequencyofbrightnessinaspeciﬁcdirectionataspeciﬁc\\nlocation.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage\\nhasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe\\nweightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost\\ninhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights—when\\ntheimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare\\nnegative.\\nThecartoonviewofacomplexcellisthatitcomputesthe L2normofthe\\n2-Dvectorcontainingtwosimplecells’responses: c( I)=\\ue070\\ns 0() I2+ s 1() I2.\\xa0An\\nimportantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except\\nfor φ,and φissetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis\\ncase, s 0and s 1forma q uadr at u r e pai r.Acomplexcelldeﬁnedinthisway\\nrespondswhentheGaussianreweightedimage I( x , y)exp( − β x x\\ue030 2− β y y\\ue030 2) contains\\nahighamplitudesinusoidalwavewithfrequency findirection τnear ( x 0 , y 0),\\nregardlessofthephaseoﬀsetofthiswave.Inotherwords,thecomplexcellis\\ninvarianttosmalltranslationsoftheimageindirection τ,ortonegatingtheimage\\n3 6 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6865d05a-f1a7-4610-a901-d95476c84f77', embedding=None, metadata={'page_label': '385', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates\\nlargepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray\\ncorrespondstozeroweight. ( L e f t )Gaborfunctionswithdiﬀerentvaluesoftheparameters\\nthatcontrolthecoordinatesystem: x 0, y 0,and τ.\\xa0EachGaborfunctioninthisgridis\\nassignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and τischosenso\\nthateachGaborﬁlterissensitivetothedirectionradiatingoutfromthecenterofthegrid.\\nFortheothertwoplots, x 0, y 0,and τareﬁxedtozero.\\xa0 Gaborfunctionswith ( C e n t e r )\\ndiﬀerentGaussianscaleparameters β xand β y.Gaborfunctionsarearrangedinincreasing\\nwidth(decreasing β x)aswemovelefttorightthroughthegrid,andincreasingheight\\n(decreasing β y)aswemovetoptobottom.Fortheothertwoplots,the βvaluesareﬁxed\\nto1.5 ×theimagewidth.Gaborfunctionswithdiﬀerentsinusoidparameters ( R i g h t ) f\\nand φ.Aswemovetoptobottom, fincreases,andaswemovelefttoright, φincreases.\\nFortheothertwoplots,isﬁxedto0andisﬁxedto5theimagewidth. φ f ×\\n(replacingblackwithwhiteandviceversa).\\nSomeofthemoststrikingcorrespondencesbetweenneuroscienceandmachine\\nlearningcomefromvisuallycomparingthefeatureslearnedbymachinelearning\\nmodelswiththoseemployedbyV1. ()showedthat OlshausenandField1996\\nasimpleunsupervisedlearningalgorithm,\\xa0sparse coding,learnsfeatureswith\\nreceptiveﬁeldssimilartothoseofsimplecells.Sincethen,wehavefoundthat\\nanextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith\\nGabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep\\nlearningalgorithms,whichlearnthesefeaturesintheirﬁrstlayer.Figure9.19\\nshowssomeexamples.Becausesomanydiﬀerentlearningalgorithmslearnedge\\ndetectors,itisdiﬃculttoconcludethatanyspeciﬁclearningalgorithmisthe\\n“right”modelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan\\ncertainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not\\nwhenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe\\nstatisticalstructureofnaturalimagesandcanberecoveredbymanydiﬀerent\\napproachestostatisticalmodeling.SeeHyvärinen 2009etal.()forareviewofthe\\nﬁeldofnaturalimagestatistics.\\n3 7 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='45c69b89-4832-4aaa-930d-4215fbabbdd1', embedding=None, metadata={'page_label': '386', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nFigure9.19:Manymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciﬁc\\ncolorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof\\ntheGaborfunctionsknowntobepresentinprimaryvisualcortex. ( L e f t )Weightslearned\\nbyanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall\\nimagepatches. ( R i g h t )Convolutionkernelslearnedbytheﬁrstlayerofafullysupervised\\nconvolutionalmaxoutnetwork.Neighboringpairsofﬁltersdrivethesamemaxoutunit.\\n9.11ConvolutionalNetworksandtheHistoryofDeep\\nLearning\\nConvolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep\\nlearning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained\\nbystudyingthebraintomachinelearningapplications.Theywerealsosomeof\\ntheﬁrstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere\\nconsideredviable.Convolutionalnetworkswerealsosomeoftheﬁrstneural\\nnetworkstosolveimportantcommercialapplicationsandremainattheforefront\\nofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the\\nneuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor\\nreadingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b\\nbyNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR\\nandhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby\\nMicrosoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12\\nandmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010\\nforamorein-depthhistoryofconvolutionalnetworksupto2010.\\nConvolutionalnetworkswerealsousedtowinmanycontests.Thecurrent\\nintensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal.\\n()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012\\n3 7 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a6f88266-ab37-41f3-819c-a4ff725cedf6', embedding=None, metadata={'page_label': '387', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER9.CONVOLUTIONALNETWORKS\\nhadbeenusedtowinothermachinelearningandcomputervisioncontestswith\\nlessimpactforyearsearlier.\\nConvolutionalnetsweresomeoftheﬁrstworkingdeepnetworkstrainedwith\\nback-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded\\nwhengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay\\nsimplybethatconvolutionalnetworksweremorecomputationally eﬃcientthan\\nfullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem\\nandtunetheirimplementation andhyperparameters.Largernetworksalsoseem\\ntobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks\\nappeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere\\navailableandactivationfunctionsthatwerepopularduringthetimeswhenfully\\nconnectednetworkswerebelievednottoworkwell.Itmaybethattheprimary\\nbarrierstothesuccessofneuralnetworkswerepsychological(practitioners did\\nnotexpectneuralnetworkstowork,sotheydidnotmakeaseriouseﬀorttouse\\nneuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks\\nperformedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof\\ndeeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral.\\nConvolutionalnetworksprovideawaytospecializeneuralnetworkstowork\\nwithdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto\\nverylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional,\\nimagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto\\nanotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural\\nnetworks.\\n3 7 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f28795da-5de1-4a63-8b1c-0419136aabdb', embedding=None, metadata={'page_label': '388', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 0\\nS e qu e n ce Mo d e l i n g: Recurren t\\nan d Recursiv e N e t s\\nRecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a\\nneuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork\\nisaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas\\nanimage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor\\nprocessingasequenceofvaluesx( 1 ), . . . ,x( ) τ.Justasconvolutionalnetworks\\ncanreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional\\nnetworkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch\\nlongersequencesthanwouldbepracticalfornetworkswithoutsequence-based\\nspecialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable\\nlength.\\nTogofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan-\\ntageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof\\nthe1980s:sharingparametersacrossdiﬀerentpartsofamodel.Parametersharing\\nmakesitpossibletoextendandapplythemodeltoexamplesofdiﬀerentforms\\n(diﬀerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters\\nforeachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot\\nseenduringtraining,norsharestatisticalstrengthacrossdiﬀerentsequencelengths\\nandacrossdiﬀerentpositionsintime.Suchsharingisparticularlyimportantwhen\\naspeciﬁcpieceofinformationcanoccuratmultiplepositionswithinthesequence.\\nForexample,considerthetwosentences“IwenttoNepalin2009”and“In2009,\\nIwenttoNepal.”Ifweaskamachinelearningmodeltoreadeachsentenceand\\nextracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize\\ntheyear2009astherelevantpieceofinformation,whetheritappearsinthesixth\\n373', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2dc37be-cff8-4628-b309-95e26b195a8a', embedding=None, metadata={'page_label': '389', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nwordorthesecondwordofthesentence.Supposethatwetrainedafeedforward\\nnetworkthatprocessessentencesofﬁxedlength.Atraditionalfullyconnected\\nfeedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit\\nwouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin\\nthesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights\\nacrossseveraltimesteps.\\nArelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This\\nconvolutionalapproachisthebasisfortime-delayneuralnetworks(Langand\\nHinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation\\nallowsanetworktoshareparametersacrosstime,butisshallow.Theoutput\\nofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof\\nasmallnumberofneighboringmembersoftheinput.Theideaofparameter\\nsharingmanifestsintheapplicationofthesameconvolutionkernelateachtime\\nstep.Recurrentnetworksshareparametersinadiﬀerentway.Eachmemberofthe\\noutputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe\\noutputisproducedusingthesameupdateruleappliedtothepreviousoutputs.\\nThisrecurrentformulationresultsinthesharingofparametersthroughavery\\ndeepcomputational graph.\\nForthesimplicityofexposition,werefertoRNNsasoperatingonasequence\\nthatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 τ.In\\npractice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences,\\nwithadiﬀerentsequencelength τforeachmemberoftheminibatch.Wehave\\nomittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex\\nneednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers\\nonlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions\\nacrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime,\\nthenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe\\nentiresequenceisobservedbeforeitisprovidedtothenetwork.\\nThischapterextendstheideaofacomputational graphtoincludecycles.These\\ncyclesrepresenttheinﬂuenceofthepresentvalueofavariableonitsownvalue\\natafuturetimestep.Suchcomputational graphsallowustodeﬁnerecurrent\\nneuralnetworks.Wethendescribemanydiﬀerentwaystoconstruct,train,and\\nuserecurrentneuralnetworks.\\nFormoreinformationonrecurrentneuralnetworksthanisavailableinthis\\nchapter,wereferthereadertothetextbookofGraves2012().\\n3 7 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='78292f82-0144-44bd-a240-78373dc4e3e8', embedding=None, metadata={'page_label': '390', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n10.1UnfoldingComputationalGraphs\\nAcomputational graphisawaytoformalizethestructureofasetofcomputations,\\nsuchasthoseinvolvedinmappinginputsandparameterstooutputsandloss.\\nPleaserefertosectionforageneralintroduction. Inthissectionweexplain 6.5.1\\ntheideaofunfoldingarecursiveorrecurrentcomputationintoacomputational\\ngraphthathasarepetitivestructure,typicallycorrespondingtoachainofevents.\\nUnfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork\\nstructure.\\nForexample,considertheclassicalformofadynamicalsystem:\\ns( ) t= ( fs( 1 ) t −;)θ , (10.1)\\nwheres( ) tiscalledthestateofthesystem.\\nEquationisrecurrentbecausethedeﬁnitionof 10.1 sattime trefersbackto\\nthesamedeﬁnitionattime. t−1\\nForaﬁnitenumberoftimesteps τ,thegraphcanbeunfoldedbyapplying\\nthedeﬁnition τ−1times.Forexample,ifweunfoldequationfor10.1 τ= 3time\\nsteps,weobtain\\ns( 3 )=( fs( 2 );)θ (10.2)\\n=(( f fs( 1 ););)θθ (10.3)\\nUnfoldingtheequationbyrepeatedlyapplyingthedeﬁnitioninthiswayhas\\nyieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan\\nnowberepresentedbyatraditionaldirectedacycliccomputational graph.\\xa0The\\nunfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3\\nﬁgure.10.1\\ns( t − 1 )s( t − 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t\\nf fs( ) . . .s( ) . . .s( ) . . .s( ) . . .\\nf f f f f f\\nFigure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1\\nunfoldedcomputationalgraph.\\xa0Eachnoderepresentsthestateatsometime tandthe\\nfunction fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue\\nofusedtoparametrize)areusedforalltimesteps. θ f\\nAsanotherexample,letusconsideradynamicalsystemdrivenbyanexternal\\nsignalx( ) t,\\ns( ) t= ( fs( 1 ) t −,x( ) t;)θ , (10.4)\\n3 7 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8f16aee5-a551-4227-9288-9f0fac0a572c', embedding=None, metadata={'page_label': '391', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nwhereweseethatthestatenowcontainsinformationaboutthewholepastsequence.\\nRecurrentneuralnetworkscanbebuiltinmanydiﬀerentways.Muchas\\nalmostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially\\nanyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork.\\nManyrecurrentneuralnetworksuseequationorasimilarequationto 10.5\\ndeﬁnethevaluesoftheirhiddenunits.\\xa0Toindicatethatthestateisthehidden\\nunitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent\\nthestate:\\nh( ) t= ( fh( 1 ) t −,x( ) t;)θ , (10.5)\\nillustratedinﬁgure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2\\nasoutputlayersthatreadinformationoutofthestatetomakepredictions.h\\nWhentherecurrentnetworkistrainedtoperformataskthatrequirespredicting\\nthefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy\\nsummaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This\\nsummaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence\\n(x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 ))toaﬁxedlengthvectorh( ) t.Dependingonthe\\ntrainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast\\nsequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused\\ninstatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious\\nwords,itmaynotbenecessarytostorealloftheinformationintheinputsequence\\nuptotime t,butratheronlyenoughinformationtopredicttherestofthesentence.\\nThemostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow\\nonetoapproximately recovertheinputsequence,asinautoencoderframeworks\\n(chapter).14\\nf fhh\\nx xh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) . . .h( ) . . .h( ) . . .h( ) . . .\\nf f\\nU nf ol df f f f f\\nFigure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses\\ninformationfromtheinputxbyincorporatingitintothestatehthatispassedforward\\nthroughtime. ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime\\nstep.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t )\\nnodeisnowassociatedwithoneparticulartimeinstance.\\nEquationcanbedrawnintwodiﬀerentways.OnewaytodrawtheRNN 10.5\\niswithadiagramcontainingonenodeforeverycomponentthatmightexistina\\n3 7 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fc571cff-c887-4586-b0a0-b28bb3b7f87c', embedding=None, metadata={'page_label': '392', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nphysicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis\\nview,thenetworkdeﬁnesacircuitthatoperatesinrealtime,withphysicalparts\\nwhosecurrentstatecaninﬂuencetheirfuturestate,asintheleftofﬁgure.10.2\\nThroughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate\\nthataninteractiontakesplacewithadelayofasingletimestep,fromthestate\\nattime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan\\nunfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany\\ndiﬀerentvariables,withonevariablepertimestep,representingthestateofthe\\ncomponentatthatpointintime.Eachvariableforeachtimestepisdrawnasa\\nseparatenodeofthecomputational graph,asintherightofﬁgure.Whatwe10.2\\ncallunfoldingistheoperationthatmapsacircuitasintheleftsideoftheﬁgure\\ntoacomputational graphwithrepeatedpiecesasintherightside.Theunfolded\\ngraphnowhasasizethatdependsonthesequencelength.\\nWecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t:\\nh( ) t= g( ) t(x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 )) (10.6)\\n=( fh( 1 ) t −,x( ) t;)θ (10.7)\\nThefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t −,x( 2 ) t −, . . . ,x( 2 ),x( 1 ))\\nasinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure\\nallowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding\\nprocessthusintroducestwomajoradvantages:\\n1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame\\ninputsize,becauseitisspeciﬁedintermsoftransitionfromonestateto\\nanotherstate,ratherthanspeciﬁedintermsofavariable-length historyof\\nstates.\\n2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters\\nateverytimestep.\\nThesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson\\nalltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate\\nmodel g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows\\ngeneralization tosequencelengthsthatdidnotappearinthetrainingset,and\\nallowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe\\nrequiredwithoutparametersharing.\\nBoththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent\\ngraphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich\\ncomputations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof\\n3 7 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7597b444-8bed-44f5-8982-28afab06ae5e', embedding=None, metadata={'page_label': '393', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ninformationﬂowforwardintime(computingoutputsandlosses)andbackward\\nintime(computinggradients)byexplicitlyshowingthepathalongwhichthis\\ninformationﬂows.\\n10.2RecurrentNeuralNetworks\\nArmedwiththegraphunrollingandparametersharingideasofsection,we10.1\\ncandesignawidevarietyofrecurrentneuralnetworks.\\nUUV V\\nWWo( t − 1 )o( t − 1 )\\nhhooy y\\nLL\\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW\\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V V V V\\nUU UU UUU nf ol d\\nFigure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork\\nthatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues.\\nAloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing\\nsoftmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally\\ncomputesˆy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden\\nconnectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections\\nparametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby\\naweightmatrixV.Equationdeﬁnesforwardpropagationinthismodel. 10.8 ( L e f t )The\\nRNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime-\\nunfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular\\ntimeinstance.\\nSomeexamplesofimportantdesignpatternsforrecurrentneuralnetworks\\nincludethefollowing:\\n3 7 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='64b59197-29ca-4718-a60b-aa4135e6708b', embedding=None, metadata={'page_label': '394', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n•Recurrentnetworksthatproduceanoutputateachtimestepandhave\\nrecurrentconnectionsbetweenhiddenunits,illustratedinﬁgure.10.3\\n•Recurrentnetworksthatproduceanoutputateachtimestepandhave\\nrecurrentconnectionsonlyfromtheoutputatonetimesteptothehidden\\nunitsatthenexttimestep,illustratedinﬁgure10.4\\n•Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that\\nreadanentiresequenceandthenproduceasingleoutput,illustratedin\\nﬁgure.10.5\\nﬁgureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3\\nmostofthechapter.\\nTherecurrentneuralnetworkofﬁgureandequationisuniversalinthe 10.3 10.8\\nsensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch\\narecurrentnetworkofaﬁnitesize.TheoutputcanbereadfromtheRNNafter\\nanumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps\\nusedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput\\n(SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,;\\nHyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete,\\nsotheseresultsregardexactimplementation ofthefunction,notapproximations .\\nTheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits\\noutputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall\\nfunctionsinthissettingusingasinglespeciﬁcRNNofﬁnitesize(Siegelmannand\\nSontag1995()use886units).The“input”oftheTuringmachineisaspeciﬁcation\\nofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring\\nmachineissuﬃcientforallproblems.ThetheoreticalRNNusedfortheproof\\ncansimulateanunboundedstackbyrepresentingitsactivationsandweightswith\\nrationalnumbersofunboundedprecision.\\nWenowdeveloptheforwardpropagationequationsfortheRNNdepictedin\\nﬁgure.Theﬁguredoesnotspecifythechoiceofactivationfunctionforthe 10.3\\nhiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also,\\ntheﬁguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake.\\nHereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords\\norcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput\\noasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete\\nvariable.Wecanthenapplythesoftmaxoperationasapost-processingstepto\\nobtainavectorˆyofnormalizedprobabilitiesovertheoutput.Forwardpropagation\\nbeginswithaspeciﬁcationoftheinitialstateh( 0 ).Then,foreachtimestepfrom\\n3 7 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a3486cda-243a-4fbc-aa90-4dd2a2851a49', embedding=None, metadata={'page_label': '395', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nUV\\nWo( t − 1 )o( t − 1 )\\nhhooy y\\nLL\\nx xo( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) . . .o( ) . . .\\nh( ) . . .h( ) . . .V V V\\nU U UU nf ol d\\nFigure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput\\ntothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare\\nh( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t. ( L e f t )Circuitdiagram.\\n( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa\\nsmallersetoffunctions)thanthoseinthefamilyrepresentedbyﬁgure.TheRNN 10.3\\ninﬁgurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3\\nrepresentationhandtransmithtothefuture.TheRNNinthisﬁgureistrainedto\\nputaspeciﬁcoutputvalueintoo,andoistheonlyinformationitisallowedtosend\\ntothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush\\nisconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce.\\nUnlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation\\nfromthepast.ThismakestheRNNinthisﬁgurelesspowerful,butitmaybeeasierto\\ntrainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater\\nparallelizationduringtraining,asdescribedinsection.10.2.1\\n3 8 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='57c7cea2-501b-404f-a28a-5e51875970a8', embedding=None, metadata={'page_label': '396', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nt t τ = 1to= ,weapplythefollowingupdateequations:\\na( ) t= +bWh( 1 ) t −+Ux( ) t(10.8)\\nh( ) t=tanh(a( ) t) (10.9)\\no( ) t= +cVh( ) t(10.10)\\nˆy( ) t=softmax(o( ) t) (10.11)\\nwheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices\\nU,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to-\\nhiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan\\ninputsequencetoanoutputsequenceofthesamelength.Thetotallossfora\\ngivensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y\\nthesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative\\nlog-likelihoodof y( ) tgivenx( 1 ), . . . ,x( ) t,then\\nL\\ue010\\n{x( 1 ), . . . ,x( ) τ}{ ,y( 1 ), . . . ,y( ) τ}\\ue011\\n(10.12)\\n=\\ue058\\ntL( ) t(10.13)\\n=−\\ue058\\ntlog p m o de l\\ue010\\ny( ) t|{x( 1 ), . . . ,x( ) t}\\ue011\\n, (10.14)\\nwhere p m o de l\\ue000\\ny( ) t|{x( 1 ), . . . ,x( ) t}\\ue001\\nisgivenbyreadingtheentryfor y( ) tfromthe\\nmodel’soutputvectorˆy( ) t.Computingthegradientofthislossfunctionwithrespect\\ntotheparametersisanexpensiveoperation.Thegradientcomputationinvolves\\nperformingaforwardpropagationpassmovinglefttorightthroughourillustration\\noftheunrolledgraphinﬁgure,followedbyabackwardpropagationpass 10.3\\nmovingrighttoleftthroughthegraph.Theruntimeis O( τ) andcannotbereduced\\nbyparallelization becausetheforwardpropagationgraphisinherentlysequential;\\neachtimestepmayonlybecomputedafterthepreviousone.\\xa0Statescomputed\\nintheforwardpassmustbestoreduntiltheyarereusedduringthebackward\\npass,sothememorycostisalso O( τ).Theback-propagation algorithmapplied\\ntotheunrolledgraphwith O( τ)costiscalledback-propagationthroughtime\\norBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2\\nbetweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean\\nalternative?\\n10.2.1TeacherForcingandNetworkswithOutputRecurrence\\nThenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto\\nthehiddenunitsatthenexttimestep(showninﬁgure)isstrictlylesspowerful 10.4\\n3 8 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4b1b300-8db5-4cf7-be26-5ed9bd1e2dc2', embedding=None, metadata={'page_label': '397', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nbecauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot\\nsimulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden\\nrecurrence,itrequiresthattheoutputunitscapturealloftheinformationabout\\nthepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits\\nareexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture\\nthenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser\\nknowshowtodescribethefullstateofthesystemandprovidesitaspartofthe\\ntrainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence\\nisthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe\\ntrainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe\\nparallelized,withthegradientforeachstep tcomputedinisolation.Thereisno\\nneedtocomputetheoutputfortheprevioustimestepﬁrst,becausethetraining\\nsetprovidestheidealvalueofthatoutput.\\nh( t − 1 )h( t − 1 )\\nWh( ) th( ) t . . . . . .\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( ) . . .x( ) . . .W W\\nU U Uh( ) τh( ) τ\\nx( ) τx( ) τW\\nUo( ) τo( ) τy( ) τy( ) τL( ) τL( ) τ\\nV\\n. . . . . .\\nFigure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend\\nofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea\\nﬁxed-sizerepresentationusedasinputforfurtherprocessing.\\xa0Theremightbeatarget\\nrightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby\\nback-propagatingfromfurtherdownstreammodules.\\nModelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto\\nthemodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure\\nthatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe\\nmodelreceivesthegroundtruthoutput y( ) tasinputattime t+1.\\xa0Wecansee\\nthisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum\\n3 8 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c35a2b4-cbe1-4c1b-b311-30b03b147095', embedding=None, metadata={'page_label': '398', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\no( t − 1 )o( t − 1 )o( ) to( ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tW\\nV V\\nU Uo( t − 1 )o( t − 1 )o( ) to( ) tL( t − 1 )L( t − 1 )L( ) tL( ) ty( t − 1 )y( t − 1 )y( ) ty( ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tW\\nV V\\nU U\\nT r ai n\\xa0 t i m e T e s t \\xa0 t i m e\\nFigure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis\\napplicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe\\nnexttimestep. ( L e f t )Attraintime,wefeedthe c o r r e c t o u t p u ty( ) tdrawnfromthetrain\\nsetasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t )\\nnotknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodel’soutput\\no( ) t,andfeedtheoutputbackintothemodel.\\n3 8 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d537e972-d3d6-4ee0-bda6-b97de5907163', embedding=None, metadata={'page_label': '399', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nlikelihoodcriterionis\\nlog p\\ue010\\ny( 1 ),y( 2 )|x( 1 ),x( 2 )\\ue011\\n(10.15)\\n=log p\\ue010\\ny( 2 )|y( 1 ),x( 1 ),x( 2 )\\ue011\\n+log p\\ue010\\ny( 1 )|x( 1 ),x( 2 )\\ue011\\n(10.16)\\nInthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe\\nconditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy\\nvaluefromthetrainingset.Maximumlikelihoodthusspeciﬁesthatduringtraining,\\nratherthanfeedingthemodel’sownoutputbackintoitself,theseconnections\\nshouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe.\\nThisisillustratedinﬁgure.10.6\\nWeoriginallymotivatedteacherforcingasallowingustoavoidback-propagation\\nthroughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing\\nmaystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas\\ntheyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe\\nnexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier\\ntimesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained\\nwithbothteacherforcingandBPTT.\\nThedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe\\nlaterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom\\ntheoutputdistribution)fedbackasinput.\\xa0Inthiscase,thekindofinputsthat\\nthenetworkseesduringtrainingcouldbequitediﬀerentfromthekindofinputs\\nthatitwillseeattesttime.\\xa0Onewaytomitigatethisproblemistotrainwith\\nbothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting\\nthecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent\\noutput-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount\\ninputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not\\nseenduringtrainingandhowtomapthestatebacktowardsonethatwillmake\\nthenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio\\ne t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b\\ninputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata\\nvaluesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually\\nusemoreofthegeneratedvaluesasinput.\\n10.2.2ComputingtheGradientinaRecurrentNeuralNetwork\\nComputingthegradientthrougharecurrentneuralnetworkisstraightforward.\\nOnesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6\\n3 8 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d10a25f2-fa46-4b08-8919-dd129b4de494', embedding=None, metadata={'page_label': '400', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ntotheunrolledcomputational graph.Nospecializedalgorithmsarenecessary.\\nGradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose\\ngradient-basedtechniquestotrainanRNN.\\nTogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean\\nexampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove\\n(equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12\\ntheparametersU,V,W,bandcaswellasthesequenceofnodesindexedby\\ntforx( ) t,h( ) t,o( ) tand L( ) t.\\xa0Foreachnode Nweneedtocomputethegradient\\n∇ N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe\\ngraph.Westarttherecursionwiththenodesimmediatelyprecedingtheﬁnalloss\\n∂ L\\n∂ L( ) t= 1 . (10.17)\\nInthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe\\nsoftmaxfunctiontoobtainthevectorˆyofprobabilitiesovertheoutput.Wealso\\nassumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe\\ninputsofar.Thegradient∇o( ) t Lontheoutputsattimestep t,forall i , t,isas\\nfollows:\\n(∇o( ) t L)i=∂ L\\n∂ o( ) t\\ni=∂ L\\n∂ L( ) t∂ L( ) t\\n∂ o( ) t\\ni=ˆ y( ) t\\ni− 1i , y( ) t .(10.18)\\nWeworkourwaybackwards,startingfromtheendofthesequence.Attheﬁnal\\ntimestep, τh( ) τonlyhaso( ) τasadescendent,soitsgradientissimple:\\n∇h( ) τ L= V\\ue03e∇o( ) τ L. (10.19)\\nWecantheniteratebackwardsintimetoback-propagate gradientsthroughtime,\\nfrom t= τ−1downto t= 1,notingthath( ) t(for t < τ)hasasdescendentsboth\\no( ) tandh( + 1 ) t.Itsgradientisthusgivenby\\n∇h( ) t L=\\ue020\\n∂h( + 1 ) t\\n∂h( ) t\\ue021\\ue03e\\n(∇h( +1) t L)+\\ue020\\n∂o( ) t\\n∂h( ) t\\ue021\\ue03e\\n(∇o( ) t L) (10.20)\\n= W\\ue03e(∇h( +1) t L)diag\\ue012\\n1−\\ue010\\nh( + 1 ) t\\ue0112\\ue013\\n+V\\ue03e(∇o( ) t L)(10.21)\\nwhere diag\\ue010\\n1−\\ue000\\nh( + 1 ) t\\ue0012\\ue011\\nindicatesthediagonalmatrixcontainingtheelements\\n1−( h( + 1 ) t\\ni)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe\\nhiddenunitattime. i t+1\\n3 8 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73271ad9-0f47-4627-8582-c87813a29899', embedding=None, metadata={'page_label': '401', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nOncethegradientsonthe\\xa0internalnodesofthe\\xa0computational graphare\\nobtained,\\xa0wecanobtainthegradientsontheparameternodes.Becausethe\\nparametersaresharedacrossmanytimesteps,wemusttakesomecarewhen\\ndenotingcalculusoperationsinvolvingthesevariables.Theequationswewishto\\nimplementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6\\nofasingleedgeinthecomputational graphtothegradient.However,the∇ W f\\noperatorusedincalculustakesintoaccountthecontributionofWtothevalue\\nof fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l\\nintroducedummyvariablesW( ) tthataredeﬁnedtobecopiesofWbutwitheach\\nW( ) tusedonlyattimestep t.Wemaythenuse∇W( ) ttodenotethecontribution\\noftheweightsattimesteptothegradient. t\\nUsingthisnotation,thegradientontheremainingparametersisgivenby:\\n∇ c L=\\ue058\\nt\\ue020\\n∂o( ) t\\n∂c\\ue021\\ue03e\\n∇o( ) t L=\\ue058\\nt∇o( ) t L (10.22)\\n∇ b L=\\ue058\\nt\\ue020\\n∂h( ) t\\n∂b( ) t\\ue021\\ue03e\\n∇h( ) t L=\\ue058\\ntdiag\\ue012\\n1−\\ue010\\nh( ) t\\ue0112\\ue013\\n∇h( ) t L(10.23)\\n∇ V L=\\ue058\\nt\\ue058\\ni\\ue020\\n∂ L\\n∂ o( ) t\\ni\\ue021\\n∇ V o( ) t\\ni=\\ue058\\nt(∇o( ) t L)h( ) t\\ue03e(10.24)\\n∇ W L=\\ue058\\nt\\ue058\\ni\\ue020\\n∂ L\\n∂ h( ) t\\ni\\ue021\\n∇W( ) t h( ) t\\ni (10.25)\\n=\\ue058\\ntdiag\\ue012\\n1−\\ue010\\nh( ) t\\ue0112\\ue013\\n(∇h( ) t L)h( 1 ) t −\\ue03e(10.26)\\n∇ U L=\\ue058\\nt\\ue058\\ni\\ue020\\n∂ L\\n∂ h( ) t\\ni\\ue021\\n∇U( ) t h( ) t\\ni (10.27)\\n=\\ue058\\ntdiag\\ue012\\n1−\\ue010\\nh( ) t\\ue0112\\ue013\\n(∇h( ) t L)x( ) t\\ue03e(10.28)\\nWedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause\\nitdoesnothaveanyparametersasancestorsinthecomputational graphdeﬁning\\ntheloss.\\n3 8 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='93523bb0-b602-49d3-ab27-28d9553058c7', embedding=None, metadata={'page_label': '402', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n10.2.3RecurrentNetworksasDirectedGraphicalModels\\nIntheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere\\ncross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward\\nnetwork,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork.\\nThelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we\\nusuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and\\nweusuallyusethecross-entropyassociatedwiththatdistributiontodeﬁnetheloss.\\nMeansquarederroristhecross-entropylossassociatedwithanoutputdistribution\\nthatisaunitGaussian,forexample,justaswithafeedforwardnetwork.\\nWhen\\xa0we\\xa0use\\xa0apredictivelog-likelihood\\xa0trainingobjective,such\\xa0asequa-\\ntion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12\\nsequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize\\nthelog-likelihood\\nlog( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29)\\nor,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext\\ntimestep,\\nlog( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t −) . (10.30)\\nDecomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof\\none-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution\\nacrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat\\nconditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges\\nfromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare\\nconditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe\\nactualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues)\\nbackintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i\\nvaluesinthepasttothecurrent y( ) tvalue.\\nAsasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya\\nsequenceofscalarrandomvariables Y={y( 1 ), . . . ,y( ) τ},withnoadditionalinputs\\nx.Theinputattimestep tissimplytheoutputattimestep t−1.TheRNNthen\\ndeﬁnesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint\\ndistributionoftheseobservationsusingthechainrule(equation)forconditional3.6\\nprobabilities:\\nP P () = Y ( y( 1 ), . . . , y( ) τ) =τ\\ue059\\nt = 1P( y( ) t| y( 1 ) t −, y( 2 ) t −, . . . , y( 1 ))(10.31)\\nwheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe\\nnegativelog-likelihoodofasetofvalues { y( 1 ), . . . , y( ) τ}accordingtosuchamodel\\n3 8 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8619a3b0-20c6-4be3-b430-3aee945a4a0c', embedding=None, metadata={'page_label': '403', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .\\nFigure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . . .:every\\npastobservation y( ) imayinﬂuencetheconditionaldistributionofsome y( ) t(for t > i),\\ngiventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis\\ngraph(asinequation)mightbeveryineﬃcient,withanevergrowingnumberof 10.6\\ninputsandparametersforeachelementofthesequence.RNNsobtainthesamefull\\nconnectivitybuteﬃcientparametrization,asillustratedinﬁgure.10.8\\nis\\nL=\\ue058\\ntL( ) t(10.32)\\nwhere\\nL( ) t= log( − Py( ) t= y( ) t| y( 1 ) t −, y( 2 ) t −, . . . , y( 1 )) .(10.33)\\ny( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . .\\nFigure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even\\nthoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery\\neﬃcientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t\\nandy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan\\nsharethesameparameterswiththeotherstages.\\nTheedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother\\nvariables.Manygraphicalmodelsaimtoachievestatisticalandcomputational\\neﬃciencybyomittingedgesthatdonotcorrespondtostronginteractions.For\\n3 8 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='79e23388-ce0c-4783-99da-d6cf43233e53', embedding=None, metadata={'page_label': '404', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nexample,itiscommontomaketheMarkovassumptionthatthegraphicalmodel\\nshouldonlycontainedgesfrom{y( ) t k −, . . . ,y( 1 ) t −}toy( ) t,ratherthancontaining\\nedgesfromtheentirepasthistory.However,insomecases,webelievethatallpast\\ninputsshouldhaveaninﬂuenceonthenextelementofthesequence.RNNsare\\nusefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i\\nfromthedistantpastinawaythatisnotcapturedbytheeﬀectofy( ) iony( 1 ) t −.\\nOnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas\\ndeﬁningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent\\ndirectdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey\\nvalueswiththecompletegraphstructureisshowninﬁgure.Thecomplete10.7\\ngraphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby\\nmarginalizing themoutofthemodel.\\nItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat\\nresultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe\\nhiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeﬃcient\\nparametrization ofthejointdistributionovertheobservations.Supposethatwe\\nrepresentedanarbitraryjointdistributionoverdiscretevalueswithatabular\\nrepresentation—anarraycontainingaseparateentryforeachpossibleassignment\\nofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment\\noccurring.\\xa0If ycantakeon kdiﬀerentvalues,thetabularrepresentationwould\\nhave O( kτ)parameters.Bycomparison,duetoparametersharing,thenumberof\\nparametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof\\nparametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced\\ntoscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5\\nlong-termrelationshipsbetweenvariableseﬃciently,usingrecurrentapplications\\nofthesamefunction fandsameparametersθateachtimestep.Figure10.8\\nillustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin\\nthegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate\\nquantitybetweenthem.Avariable y( ) iinthedistantpastmayinﬂuenceavariable\\ny( ) tviaitseﬀectonh.Thestructureofthisgraphshowsthatthemodelcanbe\\neﬃcientlyparametrized byusingthesameconditionalprobabilitydistributionsat\\neachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe\\njointassignmentofallvariablescanbeevaluatedeﬃciently.\\nEvenwiththeeﬃcientparametrization ofthegraphicalmodel,someoperations\\nremaincomputationally challenging.Forexample,itisdiﬃculttopredictmissing\\n1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c . Th i s i s\\np e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c\\nh i d d e n u n i t s .\\n3 8 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cb483bcf-bd94-4e4e-a823-1e2d76bbbfe0', embedding=None, metadata={'page_label': '405', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nvaluesinthemiddleofthesequence.\\nThepricerecurrentnetworkspayfortheirreducednumberofparametersis\\nthat theparametersmaybediﬃcult. o p t i m i z i ng\\nTheparametersharingusedinrecurrentnetworksreliesontheassumption\\nthatthesameparameterscanbeusedfordiﬀerenttimesteps.Equivalently,the\\nassumptionisthattheconditionalprobabilitydistributionoverthevariablesat\\ntime t+1 giventhevariablesattime tisstationary,meaningthattherelationship\\nbetweentheprevioustimestepandthenexttimestepdoesnotdependon t.In\\nprinciple,itwouldbepossibletouse tasanextrainputateachtimestepandlet\\nthelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween\\ndiﬀerenttimesteps.Thiswouldalreadybemuchbetterthanusingadiﬀerent\\nconditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto\\nextrapolatewhenfacedwithnewvaluesof. t\\nTocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow\\ntodrawsamplesfromthemodel.Themainoperationthatweneedtoperformis\\nsimplytosamplefromtheconditionaldistributionateachtimestep.\\xa0However,\\nthereisoneadditionalcomplication.\\xa0The RNNmusthavesomemechanismfor\\ndeterminingthelengthofthesequence.Thiscanbeachievedinvariousways.\\nInthecasewhentheoutputisasymboltakenfromavocabulary,onecan\\naddaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,).\\nWhenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset,\\nweinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) τ\\nineachtrainingexample.\\nAnotheroptionistointroduceanextraBernoullioutputtothemodelthat\\nrepresentsthedecisiontoeithercontinuegenerationorhaltgenerationateach\\ntimestep.Thisapproachismoregeneralthantheapproachofaddinganextra\\nsymboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan\\nonlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto\\nanRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya\\nsigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis\\ntrainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe\\nsequenceendsorcontinuesateachtimestep.\\nAnotherwaytodeterminethesequencelength τistoaddanextraoutputto\\nthemodelthatpredictstheinteger τitself.Themodelcansampleavalueof τ\\nandthensample τstepsworthofdata.Thisapproachrequiresaddinganextra\\ninputtotherecurrentupdateateachtimestepsothattherecurrentupdateis\\nawareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput\\ncaneitherconsistofthevalueof τorcanconsistof τ t−,thenumberofremaining\\n3 9 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e75d10e5-7223-473f-a826-fde7160b165f', embedding=None, metadata={'page_label': '406', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10. SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ntime steps. Without this extra input, the RNN might generate sequences that\\nendabruptly,suchasasentencethatendsbeforeitiscomplete. Thisapproachis\\nbasedonthedecomposition\\nP(x(1), . . . ,x( ) τ) = ( ) ( P τ Px(1), . . . ,x( ) τ| τ .)(10.34)\\nThe strategy of predicting τdirectly is used for example by Goodfellow et al.\\n( ).2014d\\n10.2.4 Modeling Sequences Conditioned on Context with RNNs\\nIntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected\\ngraphicalmodelover asequence ofrandom variables y( ) twithno inputsx. Of\\ncourse, our development of RNNs as in equation included a sequence of 10.8\\ninputsx(1),x(2), . . . ,x( ) τ. Ingeneral,RNNsallowtheextensionofthegraphical\\nmodel view to represent not only a joint distribution over the yvariables but\\nalso a conditional distribution over ygivenx. As discussed in the context of\\nfeedforwardnetworksinsection ,anymodelrepresentingavariable 6.2.1.1 P(y;θ)\\ncan be reinterpreted as amodel representinga conditional distribution P(yω|)\\nwithω=θ. Wecanextendsuchamodeltorepresentadistribution P(y x|)by\\nusingthesame P(y ω|)asbefore,butmaking ωafunctionofx. Inthecaseof\\nanRNN,thiscanbeachievedindiﬀerentways. Wereviewherethemostcommon\\nandobviouschoices.\\nPreviously, we havediscussed RNNsthattakeasequenceof vectors x( ) tfor\\nt= 1 , . . . , τasinput.\\xa0Anotheroption istotakeonly asinglevector xasinput.\\nWhenxisaﬁxed-sizevector,wecansimplymakeitanextrainputoftheRNN\\nthatgeneratesthe ysequence. Somecommonwaysofprovidinganextrainputto\\nanRNNare:\\n1. asanextrainputateachtimestep,or\\n2. astheinitialstateh(0),or\\n3. both.\\nTheﬁrstandmostcommonapproachisillustratedinﬁgure . Theinteraction 10.9\\nbetweentheinput xandeachhiddenunitvector h( ) tisparametrizedbyanewly\\nintroducedweightmatrix Rthatwasabsentfromthemodelofonlythesequence\\nof yvalues.\\xa0Thesameproduct x\\ue03eRisaddedasadditionalinputtothehidden\\nunitsateverytimestep. Wecanthinkofthechoiceof xasdeterminingthevalue\\n391', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8a2bfe7b-3b83-4085-9a94-13ae9eabe6be', embedding=None, metadata={'page_label': '407', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nofx\\ue03eRthatiseﬀectivelyanewbiasparameterusedforeachofthehiddenunits.\\nTheweightsremainindependentoftheinput.Wecanthinkofthismodelastaking\\ntheparametersθofthenon-conditional modelandturningthemintoω,where\\nthebiasparameterswithinarenowafunctionoftheinput. ω\\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\\ns( ) . . .s( ) . . .h( ) . . .h( ) . . .V V VU U U\\nx xy( ) . . .y( ) . . .\\nR R R R R\\nFigure10.9:AnRNNthatmapsaﬁxed-lengthvectorxintoadistributionoversequences\\nY.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis\\nusedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage.\\nEachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent\\ntimestep)and,duringtraining,astarget(fortheprevioustimestep).\\nRatherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive\\nasequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8\\nspondstoaconditionaldistribution P(y( 1 ), . . . ,y( ) τ|x( 1 ), . . . ,x( ) τ)thatmakesa\\nconditionalindependence assumptionthatthisdistributionfactorizesas\\n\\ue059\\ntP(y( ) t|x( 1 ), . . . ,x( ) t) . (10.35)\\nToremovetheconditionalindependenceassumption,wecanaddconnectionsfrom\\ntheoutputattime ttothehiddenunitattime t+1,asshowninﬁgure.The10.10\\nmodelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence.\\nThiskindofmodelrepresentingadistributionoverasequencegivenanother\\n3 9 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='51cb8c25-8095-4914-b6ab-55895eca96e6', embedding=None, metadata={'page_label': '408', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W\\nh( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V\\nU U U\\nx( t − 1 )x( t − 1 )R\\nx( ) tx( ) tx( + 1 ) tx( + 1 ) tR R\\nFigure10.10:\\xa0Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence\\nofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto\\nﬁgure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate. 10.3\\nTheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy\\ngivensequencesofxofthesamelength.TheRNNofﬁgureisonlyabletorepresent 10.3\\ndistributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven\\nthevalues.x\\n3 9 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d2c3f042-a3a2-460a-8f1c-e5ab4e9e3c91', embedding=None, metadata={'page_label': '409', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nsequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust\\nbethesame.Wedescribehowtoremovethisrestrictioninsection.10.4\\no( t − 1 )o( t − 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t − 1 )L( t − 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t − 1 )y( t − 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) t\\nx( t − 1 )x( t − 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t − 1 )g( t − 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t\\nFigure10.11:\\xa0Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant\\ntolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t.\\nThehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe\\ngrecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach\\npoint t,theoutputunitso( ) tcanbeneﬁtfromarelevantsummaryofthepastinitsh( ) t\\ninputandfromarelevantsummaryofthefutureinitsg( ) tinput.\\n10.3BidirectionalRNNs\\nAlloftherecurrentnetworkswehaveconsidereduptonowhavea“causal”struc-\\nture,meaningthatthestateattime tonlycapturesinformationfromthepast,\\nx( 1 ), . . . ,x( 1 ) t −,andthepresentinputx( ) t.Someofthemodelswehavediscussed\\nalsoallowinformationfrompastyvaluestoaﬀectthecurrentstatewhenthey\\nvaluesareavailable.\\nHowever,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay\\n3 9 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='620cbe67-b527-4bb7-a64e-1fdc91131c0b', embedding=None, metadata={'page_label': '410', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ndependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect\\ninterpretationofthecurrentsoundasaphonememaydependonthenextfew\\nphonemesbecauseofco-articulationandpotentiallymayevendependonthenext\\nfewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere\\naretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we\\nmayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis\\nalsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning\\ntasks,describedinthenextsection.\\nBidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented\\ntoaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc-\\ncessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting\\nrecognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni-\\ntion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics (\\ne t a l .,).1999\\nAsthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward\\nthroughtimebeginningfromthestartofthesequencewithanotherRNNthat\\nmovesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11\\nillustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe\\nsub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe\\nsub-RNNthatmovesbackwardthroughtime.\\xa0Thisallowstheoutputunitso( ) t\\ntocomputearepresentationthatdependson b o t h t h e p a s t a nd t h e f u t u r ebut\\nismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya\\nﬁxed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork,\\naconvolutionalnetwork,oraregularRNNwithaﬁxed-sizelook-aheadbuﬀer).\\nThisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by\\nhavingRNNs,eachonegoinginoneofthefourdirections:\\xa0up, down,left, f o u r\\nright.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea\\nrepresentationthatwouldcapturemostlylocalinformationbutcouldalsodepend\\non\\xa0long-range inputs,ifthe\\xa0RNN\\xa0isable\\xa0tolearn\\xa0tocarry\\xa0that\\xa0information.\\nComparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore\\nexpensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe\\nsamefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the\\nforwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows\\ntheyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior\\ntotherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral\\ninteractions.\\n3 9 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='705ec31d-05a9-44c0-8c51-1d8cd124f4ee', embedding=None, metadata={'page_label': '411', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n10.4Encoder-DecoderSequence-to-SequenceArchitec-\\ntures\\nWehaveseeninﬁgurehowanRNNcanmapaninputsequencetoaﬁxed-size 10.5\\nvector.WehaveseeninﬁgurehowanRNNcanmapaﬁxed-sizevectortoa 10.9\\nsequence.\\xa0Wehaveseeninﬁgures,,andhowanRNNcan 10.310.410.1010.11\\nmapaninputsequencetoanoutputsequenceofthesamelength.\\nE nc ode r\\n…\\nx( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x )\\nD e c ode r\\n…\\ny( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC\\nFigure10.12:\\xa0Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture,\\nforlearningtogenerateanoutputsequence( y( 1 ), . . . , y( n y ))givenaninputsequence\\n( x( 1 ), x( 2 ), . . . , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence\\nandadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa\\ngivenoutputsequence).TheﬁnalhiddenstateoftheencoderRNNisusedtocomputea\\ngenerallyﬁxed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput\\nsequenceandisgivenasinputtothedecoderRNN.\\nHerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan\\noutputsequencewhichisnotnecessarilyofthesamelength.\\xa0This comesupin\\nmanyapplications,suchasspeechrecognition,machinetranslationorquestion\\n3 9 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6507f9ae-2b8b-4865-b2c2-7b3b8e9a30c0', embedding=None, metadata={'page_label': '412', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nanswering,wheretheinputandoutputsequencesinthetrainingsetaregenerally\\nnotofthesamelength(althoughtheirlengthsmightberelated).\\nWeoftencalltheinputtotheRNNthe“context.”Wewanttoproducea\\nrepresentationofthiscontext, C.Thecontext Cmightbeavectororsequenceof\\nvectorsthatsummarizetheinputsequenceXx= (( 1 ), . . . ,x( n x )).\\nThesimplestRNNarchitectureformappingavariable-length sequenceto\\nanothervariable-length sequencewasﬁrstproposedby ()and Cho e t a l .2014a\\nshortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi-\\ntectureandweretheﬁrsttoobtainstate-of-the-art translationusingthisapproach.\\nTheformersystemisbasedonscoringproposalsgeneratedbyanothermachine\\ntranslationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate\\nthetranslations.\\xa0Theseauthorsrespectivelycalledthisarchitecture, illustrated\\ninﬁgure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12\\nideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput\\nsequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits\\nﬁnalhiddenstate.\\xa0(2)adecoderorwriteroroutputRNNisconditionedon\\nthatﬁxed-lengthvector(justlikeinﬁgure)togeneratetheoutputsequence 10.9\\nY=(y( 1 ), . . . ,y( n y )).Theinnovationofthiskindofarchitectureoverthose\\npresentedinearliersectionsofthischapteristhatthelengths n xand n ycan\\nvaryfromeachother,whilepreviousarchitectures constrained n x= n y= τ.Ina\\nsequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize\\ntheaverageoflog P(y( 1 ), . . . ,y( n y )|x( 1 ), . . . ,x( n x ))overallthepairsofxandy\\nsequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically\\nusedasarepresentation Coftheinputsequencethatisprovidedasinputtothe\\ndecoderRNN.\\nIfthecontext Cisavector,thenthedecoderRNNissimplyavector-to-\\nsequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4\\ntwowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided\\nastheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits\\nateachtimestep.Thesetwowayscanalsobecombined.\\nThereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer\\nasthedecoder.\\nOneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe\\nencoderRNNhasadimensionthatistoosmalltoproperlysummarizealong\\nsequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015\\nofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather\\nthanaﬁxed-sizevector.Additionally,theyintroducedanattentionmechanism\\nthatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput\\n3 9 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7bed853b-8cea-4ec2-934e-4d213095cb3a', embedding=None, metadata={'page_label': '413', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10. SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nsequence. Seesection formoredetails. 12.4.5.1\\n10.5 DeepRecurrentNetworks\\nThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters\\nandassociatedtransformations:\\n1. fromtheinputtothehiddenstate,\\n2. fromtheprevioushiddenstatetothenexthiddenstate,and\\n3. fromthehiddenstatetotheoutput.\\nWiththeRNNarchitectureofﬁgure ,eachofthesethreeblocksisassociated 10.3\\nwithasingleweightmatrix. Inotherwords,whenthenetworkisunfolded,each\\nofthese correspondsto ashallowtransformation.\\xa0By ashallowtransformation,\\nwe mean a transformation that would be represented by a single layer within\\na deepMLP. Typically thisis a transformationrepresentedby alearned aﬃnetransformationfollowedbyaﬁxednonlinearity.\\nWould it be advantageous to introduce depth in each of these operations?\\nExperimentalevidence(Graves 2013 Pascanu 2014a et al., ; et al., )stronglysuggests\\nso. Theexperimentalevidenceisinagreementwiththeideathatweneedenough\\ndepthinordertoperformtherequiredmappings. Seealso Schmidhuber 1992 ( ),\\nElHihiandBengio 1996 Jaeger 2007a ( ),or ( )forearlierworkondeepRNNs.\\nGraves 2013 et al.( )weretheﬁrsttoshowasigniﬁcantbeneﬁtofdecomposing\\nthestateofanRNNintomultiplelayers asinﬁgure (left). Wecanthink 10.13\\nof the lower layers in the hierarchy depicted in ﬁgure a as playing a role 10.13\\nintransformingtherawinputintoarepresentationthatismoreappropriate,at\\nthe higher levels of the hidden state. Pascanu 2014a et al.( ) go a step further\\nandproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks\\nenumeratedabove,asillustratedinﬁgure b. Considerationsofrepresentational 10.13\\ncapacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing\\nsobyaddingdepthmayhurtlearningbymakingoptimizationdiﬃcult. Ingeneral,it is easier to optimize shallower architectures, and adding the extra depth of\\nﬁgure bmakestheshortestpathfromavariableintimestep10.13 ttoavariable\\nintimestep t+ 1becomelonger. Forexample,ifanMLPwithasinglehidden\\nlayerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe\\nshortestpathbetweenvariablesinanytwodiﬀerenttimesteps,comparedwiththe\\nordinaryRNNofﬁgure . However,asarguedby 10.3 Pascanu 2014a et al.( ),this\\n398', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5ad9a92-004a-48ef-bb75-891357f72e2f', embedding=None, metadata={'page_label': '414', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nhy\\nxz\\n( a) ( b) ( c )xhy\\nxhy\\nFigure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu\\ne t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a )\\nhierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b )\\nhidden,hidden-to-hiddenandhidden-to-outputparts.\\xa0Thismaylengthentheshortest\\npathlinkingdiﬀerenttimesteps.Thepath-lengtheningeﬀectcanbemitigatedby ( c )\\nintroducingskipconnections.\\n3 9 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e824e8c1-4e1e-4be2-bcda-3ce181d62791', embedding=None, metadata={'page_label': '415', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ncanbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as\\nillustratedinﬁgurec.10.13\\n10.6RecursiveNeuralNetworks\\nx( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L\\nx( 4 )x( 4 )Voo\\nU W U WUW\\nFigure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe\\nrecurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), . . . ,x( ) tcan\\nbemappedtoaﬁxed-sizerepresentation(theoutputo),withaﬁxedsetofparameters\\n(theweightmatricesU,V,W).Theﬁgureillustratesasupervisedlearningcaseinwhich\\nsometargetisprovidedwhichisassociatedwiththewholesequence. y\\nRecursiveneuralnetworks2representyetanothergeneralization ofrecurrent\\nnetworks,withadiﬀerentkindofcomputational graph,whichisstructuredasa\\ndeeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational\\ngraphforarecursivenetworkisillustratedinﬁgure.Recursiveneural 10.14\\n2W e s u g g e s t t o n o t a b b re v i a t e “ re c u rs i v e n e u ra l n e t w o rk ” a s “ R NN” t o a v o i d c o n f u s i o n with\\n“ re c u rre n t n e u ra l n e t w o rk . ”\\n4 0 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7bc58d87-c53d-4976-9ce5-4ca5d2127a6d', embedding=None, metadata={'page_label': '416', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nnetworkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto\\nreasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011\\nappliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,,\\n1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin\\ncomputervision( ,). Socher e t a l .2011b\\nOneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence\\nofthesamelength τ,thedepth(measuredasthenumberofcompositionsof\\nnonlinearoperations)canbedrasticallyreducedfrom τto O(log τ),whichmight\\nhelpdealwithlong-termdependencies.Anopenquestionishowtobeststructure\\nthetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata,\\nsuchasabalancedbinarytree.Insomeapplicationdomains,externalmethods\\ncansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural\\nlanguagesentences,thetreestructurefortherecursivenetworkcanbeﬁxedto\\nthestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage\\nparser( ,,).\\xa0Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a\\ndiscoverandinferthetreestructurethatisappropriateforanygiveninput,as\\nsuggestedby(). Bottou2011\\nManyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi\\ne t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure,\\nandassociatethe\\xa0inputsandtargetswith\\xa0individualnodesofthe\\xa0tree.The\\ncomputationperformedbyeachnodedoesnothavetobethetraditionalartiﬁcial\\nneuroncomputation(aﬃnetransformationofallinputsfollowedbyamonotone\\nnonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a\\nandbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships\\nbetweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare\\nrepresentedbycontinuousvectors(embeddings).\\n10.7TheChallengeofLong-TermDependencies\\nThemathematical challengeoflearninglong-termdependenciesinrecurrentnet-\\nworkswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5\\nagatedovermanystagestendtoeithervanish(mostofthetime)orexplode\\n(rarely,butwithmuchdamagetotheoptimization). Evenifweassumethatthe\\nparametersaresuchthattherecurrentnetworkisstable(canstorememories,\\nwithgradientsnotexploding),thediﬃcultywithlong-termdependenciesarises\\nfromtheexponentiallysmallerweightsgiventolong-terminteractions(involving\\nthemultiplicationofmanyJacobians)comparedtoshort-termones.Manyother\\nsourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l .,\\n4 0 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7712dd1d-3ed0-4aea-be34-27130a751724', embedding=None, metadata={'page_label': '417', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n− − − 6 0 4 0 2 0 0 2 0 4 0 6 0\\nI nput c o o r di na t e− 4− 3− 2− 101234P r o j e c t i o n o f o utput0\\n1\\n2\\n3\\n4\\n5\\nFigure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown\\nhere),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny\\nderivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing\\nanddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate\\ndowntoasingledimension,plottedonthe y-axis.\\xa0The x-axisisthecoordinateofthe\\ninitialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis\\nplotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction\\naftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction\\nhasbeencomposed.\\n1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore\\ndetail.Theremainingsectionsdescribeapproachestoovercomingtheproblem.\\nRecurrentnetworksinvolvethecompositionofthesamefunctionmultiple\\ntimes,oncepertimestep.Thesecompositionscanresultinextremelynonlinear\\nbehavior,asillustratedinﬁgure.10.15\\nInparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks\\nsomewhatresemblesmatrixmultiplication. Wecanthinkoftherecurrencerelation\\nh( ) t= W\\ue03eh( 1 ) t −(10.36)\\nasaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction,\\nandlackinginputsx.As\\xa0described\\xa0insection\\xa0,\\xa0thisrecurrencerelation 8.2.5\\nessentiallydescribesthepowermethod.Itmaybesimpliﬁedto\\nh( ) t=\\ue000\\nWt\\ue001\\ue03eh( 0 ), (10.37)\\nandifadmitsaneigendecompositionoftheform W\\nWQQ = Λ\\ue03e, (10.38)\\n4 0 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='524ff71b-e4e5-47bd-a487-20269dd723b1', embedding=None, metadata={'page_label': '418', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nwithorthogonal ,therecurrencemaybesimpliﬁedfurtherto Q\\nh( ) t= Q\\ue03eΛtQh( 0 ). (10.39)\\nTheeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude\\nlessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\\nexplode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector\\nwilleventuallybediscarded.\\nThisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine\\nmultiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor\\nexplodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent\\nnetworkthathasadiﬀerentweight w( ) tateachtimestep,thesituationisdiﬀerent.\\nIftheinitialstateisgivenby,thenthestateattime 1 tisgivenby\\ue051\\nt w( ) t.Suppose\\nthatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with\\nzeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome\\ndesiredvariance v∗wemaychoosetheindividualweightswithvariance v=n√\\nv∗.\\nVerydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe\\nvanishingandexplodinggradientproblem,asarguedby(). Sussillo2014\\nThevanishingandexplodinggradientproblemforRNNswasindependently\\ndiscoveredbyseparateresearchers(,; ,,). Hochreiter1991Bengio e t a l .19931994\\nOnemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof\\nparameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in\\nordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN\\nmustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993\\n1994).Speciﬁcally,wheneverthemodelisabletorepresentlongtermdependencies,\\nthegradientofalongterminteractionhasexponentiallysmallermagnitudethan\\nthegradientofashortterminteraction.\\xa0It doesnotmeanthatitisimpossible\\ntolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies,\\nbecausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest\\nﬂuctuationsarisingfromshort-termdependencies.Inpractice,theexperiments\\nin ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994\\nneedtobecaptured,gradient-basedoptimization becomesincreasinglydiﬃcult,\\nwiththeprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly\\nreaching0forsequencesofonlylength10or20.\\nForadeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya\\n(), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995\\ninPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious\\napproachesthathavebeenproposedtoreducethediﬃcultyoflearninglong-\\ntermdependencies(insomecasesallowinganRNNtolearndependenciesacross\\n4 0 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b9adbbdc-6174-4525-ba92-5c7edf17b1ac', embedding=None, metadata={'page_label': '419', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nhundredsofsteps),buttheproblemoflearninglong-termdependenciesremains\\noneofthemainchallengesindeeplearning.\\n10.8EchoStateNetworks\\nTherecurrentweightsmappingfromh( 1 ) t −toh( ) tandtheinputweightsmapping\\nfromx( ) ttoh( ) taresomeofthemostdiﬃcultparameterstolearninarecurrent\\nnetwork.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004\\nJaeger2007b,)approachtoavoidingthisdiﬃcultyistosettherecurrentweights\\nsuchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast\\ninputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently\\nproposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b\\nandliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002\\nthatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued\\nhiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed\\nreservoircomputing(LukoševičiusandJaeger2009,)todenotethefactthat\\nthehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediﬀerent\\naspectsofthehistoryofinputs.\\nOnewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat\\ntheyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the\\nhistoryofinputsuptotime t)intoaﬁxed-lengthvector(therecurrentstateh( ) t),\\nonwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve\\ntheproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe\\nconvexasafunctionoftheoutputweights.Forexample,iftheoutputconsists\\noflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining\\ncriterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith\\nsimplelearningalgorithms(,). Jaeger2003\\nTheimportantquestionistherefore:howdowesettheinputandrecurrent\\nweightssothatarichsetofhistoriescanberepresentedintherecurrentneural\\nnetworkstate?\\xa0Theanswerproposedinthereservoircomputingliteratureisto\\nviewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent\\nweightssuchthatthedynamicalsystemisneartheedgeofstability.\\nTheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to-\\nstatetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5\\ncharacteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians\\nJ( ) t=∂ s( ) t\\n∂ s( 1 ) t −.OfparticularimportanceisthespectralradiusofJ( ) t,deﬁnedto\\nbethemaximumoftheabsolutevaluesofitseigenvalues.\\n4 0 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='db58a8db-bf44-4de8-aa18-704434544ae8', embedding=None, metadata={'page_label': '420', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nTounderstandtheeﬀectofthespectralradius,considerthesimplecaseof\\nback-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This\\ncasehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas\\naneigenvectorvwithcorrespondingeigenvalue λ.Considerwhathappensaswe\\npropagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient\\nvectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n\\nstepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate\\naperturbedversionofg.Ifwebeginwithg+ δv,thenafteronestep,wewill\\nhaveJ(g+ δv).After nsteps,wewillhaveJn(g+ δv).Fromthiswecansee\\nthatback-propagationstartingfromgandback-propagationstartingfromg+ δv\\ndivergeby δJnvafter nstepsofback-propagation.Ifvischosentobeaunit\\neigenvectorofJwitheigenvalue λ,thenmultiplicationbytheJacobiansimply\\nscalesthediﬀerenceateachstep.Thetwoexecutionsofback-propagationare\\nseparatedbyadistanceof δ λ||n.Whenvcorrespondstothelargestvalueof|| λ,\\nthisperturbationachievesthewidestpossibleseparationofaninitialperturbation\\nofsize. δ\\nWhen || λ >1,thedeviationsize δ λ||ngrowsexponentiallylarge.When || λ <1,\\nthedeviationsizebecomesexponentiallysmall.\\nOfcourse,thisexampleassumedthattheJacobianwasthesameatevery\\ntimestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena\\nnonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon\\nmanytimesteps,andhelptopreventtheexplosionresultingfromalargespectral\\nradius.\\xa0Indeed,themostrecentworkonechostatenetworksadvocatesusinga\\nspectralradiusmuchlargerthanunity(,;,). Yildiz e t a l .2012Jaeger2012\\nEverythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli-\\ncationappliesequallytoforwardpropagationinanetworkwithnononlinearity,\\nwherethestateh( + 1 ) t= h( ) t \\ue03eW.\\nWhenalinearmapW\\ue03ealwaysshrinkshasmeasuredbythe L2norm,then\\nwesaythatthemapiscontractive.Whenthespectralradiusislessthanone,\\nthemappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller\\naftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout\\nthepastwhenweuseaﬁnitelevelofprecision(suchas32bitintegers)tostore\\nthestatevector.\\nTheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep\\nforward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward,\\nduringback-propagation. NotethatneitherWnorJneedtobesymmetric(al-\\nthoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand\\neigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory\\n4 0 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='116641d4-0c35-4bd3-9aad-5c66ac6edbe5', embedding=None, metadata={'page_label': '421', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nbehavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora\\nsmallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan\\nbeexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto\\nthemagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis\\ncoeﬃcients,\\xa0whenwemultiplythematrixbythevector.Aneigenvaluewith\\nmagnitudegreaterthanonecorrespondstomagniﬁcation (exponentialgrowth,if\\nappliediteratively)orshrinking(exponentialdecay,ifappliediteratively).\\nWithanonlinearmap,\\xa0theJacobianisfreetochangeateachstep.The\\ndynamicsthereforebecomemorecomplicated.However,itremainstruethata\\nsmallinitialvariationcanturnintoalargevariationafterseveralsteps.One\\ndiﬀerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof\\nasquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome\\nbounded.Notethat\\xa0itispossible\\xa0forback-propagation to\\xa0retainunbounded\\ndynamicsevenwhenforwardpropagationhasboundeddynamics,forexample,\\nwhenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare\\nconnectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1\\nrareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint. tanh\\nThestrategyofechostatenetworksissimplytoﬁxtheweightstohavesome\\nspectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3\\ndoesnotexplodeduetothestabilizingeﬀectofsaturatingnonlinearities liketanh.\\nMorerecently,ithasbeenshownthatthetechniquesusedtosettheweights\\ninESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e\\nwork(withthehidden-to-hidden recurrentweightstrainedusingback-propagation\\nthroughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,;\\ne t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013\\nwiththesparseinitialization schemedescribedinsection.8.4\\n10.9LeakyUnitsandOtherStrategiesforMultiple\\nTimeScales\\nOnewaytodealwithlong-termdependencies istodesignamodelthatoperates\\natmultipletimescales,sothatsomepartsofthemodeloperateatﬁne-grained\\ntimescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime\\nscalesandtransferinformationfromthedistantpasttothepresentmoreeﬃciently.\\nVariousstrategiesforbuildingbothﬁneandcoarsetimescalesarepossible.These\\nincludetheadditionofskipconnectionsacrosstime,“leakyunits”thatintegrate\\nsignalswithdiﬀerenttimeconstants,andtheremovalofsomeoftheconnections\\n4 0 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6d6304e9-ce1b-49cd-9caf-7808154ac2d4', embedding=None, metadata={'page_label': '422', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nusedtomodelﬁne-grainedtimescales.\\n10.9.1AddingSkipConnectionsthroughTime\\nOnewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin\\nthedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections\\ndatesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996\\nfeedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988\\nnetwork,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1.\\nItispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991\\nAswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5\\nw i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996\\nconnectionswithatime-delayof dtomitigatethisproblem.Gradientsnow\\ndiminishexponentiallyasafunctionofτ\\ndratherthan τ.Sincethereareboth\\ndelayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin τ.\\nThisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall\\nlong-termdependencies mayberepresentedwellinthisway.\\n10.9.2LeakyUnitsandaSpectrumofDiﬀerentTimeScales\\nAnotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto\\nhaveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections.\\nWhenweaccumulatearunningaverage µ( ) tofsomevalue v( ) tbyapplyingthe\\nupdate µ( ) t← α µ( 1 ) t −+(1− α) v( ) tthe αparameterisanexampleofalinearself-\\nconnectionfrom µ( 1 ) t −to µ( ) t.When αisnearone,therunningaverageremembers\\ninformationaboutthepastforalongtime,andwhen αisnearzero,information\\naboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan\\nbehavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky\\nunits.\\nSkipconnectionsthrough dtimestepsareawayofensuringthataunitcan\\nalwayslearntobeinﬂuencedbyavaluefrom dtimestepsearlier.Theuseofa\\nlinearself-connectionwithaweightnearoneisadiﬀerentwayofensuringthatthe\\nunitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows\\nthiseﬀecttobeadaptedmoresmoothlyandﬂexiblybyadjustingthereal-valued\\nαratherthanbyadjustingtheinteger-valuedskiplength.\\nTheseideaswereproposedby()andby (). Mozer1992 ElHihiandBengio1996\\nLeakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks\\n(,). Jaeger e t a l .2007\\n4 0 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0cff8475-181c-4ed6-96e4-b3903c86eb56', embedding=None, metadata={'page_label': '423', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nTherearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky\\nunits.\\xa0Onestrategyistomanuallyﬁxthemtovaluesthatremainconstant,for\\nexamplebysamplingtheirvaluesfromsomedistributiononceatinitialization time.\\nAnotherstrategyistomakethetimeconstantsfreeparametersandlearnthem.\\nHavingsuchleakyunitsatdiﬀerenttimescalesappearstohelpwithlong-term\\ndependencies(,;Mozer1992Pascanu2013 e t a l .,).\\n10.9.3RemovingConnections\\nAnotherapproachtohandlelong-termdependenciesistheideaoforganizing\\nthestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996\\ninformationﬂowingmoreeasilythroughlongdistancesattheslowertimescales.\\nThisideadiﬀersfromtheskipconnectionsthroughtimediscussedearlier\\nbecauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem\\nwithlongerconnections.Unitsmodiﬁedinsuchawayareforcedtooperateona\\nlongtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d\\nnewconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto\\nfocusontheirothershort-termconnections.\\nTherearediﬀerentwaysinwhichagroupofrecurrentunitscanbeforcedto\\noperateatdiﬀerenttimescales.Oneoptionistomaketherecurrentunitsleaky,\\nbuttohavediﬀerentgroupsofunitsassociatedwithdiﬀerentﬁxedtimescales.\\nThiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu\\ne t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013\\natdiﬀerenttimes,withadiﬀerentfrequencyfordiﬀerentgroupsofunits.Thisis\\ntheapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked\\nwellonanumberofbenchmarkdatasets.\\n10.10TheLongShort-TermMemoryandOtherGated\\nRNNs\\nAsofthiswriting,themosteﬀectivesequencemodelsusedinpracticalapplications\\narecalledgatedRNNs.Theseincludethelongshort-termmemoryand\\nnetworksbasedonthe . gatedrecurrentunit\\nLikeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough\\ntimethathavederivativesthatneithervanishnorexplode.Leakyunits\\xa0did\\nthiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere\\nparameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange\\n4 0 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1928f81d-6d74-4722-bff0-efcb53122936', embedding=None, metadata={'page_label': '424', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nateachtimestep.\\n×\\ni nput i nput\\xa0gate f or ge t \\xa0 gate output\\xa0gateoutput\\ns t at es e l f - l oop×\\n+ ×\\nFigure10.16:BlockdiagramoftheLSTMrecurrentnetwork“cell.”Cellsareconnected\\nrecurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks.\\nAninputfeatureiscomputedwitharegularartiﬁcialneuronunit.Itsvaluecanbe\\naccumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa\\nlinearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan\\nbeshutoﬀbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe\\ninputunitcanhaveanysquashingnonlinearity.\\xa0Thestateunitcanalsobeusedasan\\nextrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep.\\nLeakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence\\nforaparticularfeatureorcategory)overalongduration.However,oncethat\\ninformationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe\\noldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky\\nunittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto\\nforgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento\\nclearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This\\n4 0 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0dd2422b-7fdb-4e3f-a8cd-b6b124f63112', embedding=None, metadata={'page_label': '425', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\niswhatgatedRNNsdo.\\n10.10.1LSTM\\nThecleverideaofintroducingself-loopstoproducepathswherethegradient\\ncanﬂowforlongdurationsisacorecontributionoftheinitiallongshort-term\\nmemory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition\\nhasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan\\nﬁxed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000\\nbyanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically.\\nInthiscase,wemeanthatevenforanLSTMwithﬁxedparameters,thetimescale\\nofintegrationcanchangebasedontheinputsequence,becausethetimeconstants\\nareoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful\\ninmanyapplications,\\xa0suchasunconstrainedhandwriting recognition(Graves\\ne t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,),\\nhandwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,),\\nimagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and\\nparsing(Vinyals2014a e t a l .,).\\nTheLSTMblockdiagramisillustratedinﬁgure.Thecorresponding 10.16\\nforwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent\\nnetworkarchitecture. Deeperarchitectures havealsobeensuccessfullyused(Graves\\ne t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement-\\nwisenonlinearitytotheaﬃnetransformationofinputsandrecurrentunits,LSTM\\nrecurrentnetworkshave“LSTMcells”thathaveaninternalrecurrence(aself-loop),\\ninadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs\\nandoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda\\nsystemofgatingunitsthatcontrolstheﬂowofinformation. Themostimportant\\ncomponentisthestateunit s( ) t\\nithathasalinearself-loopsimilartotheleaky\\nunitsdescribedintheprevioussection.However,here,theself-loopweight(orthe\\nassociatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t\\ni(fortimestep t\\nandcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i\\nf( ) t\\ni= σ\\uf8eb\\n\\uf8ed bf\\ni+\\ue058\\njUf\\ni , j x( ) t\\nj+\\ue058\\njWf\\ni , j h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 ,(10.40)\\nwherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector,\\ncontainingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively\\nbiases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell\\n4 1 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e1b09915-9e39-4921-a51c-59be9bb6416f', embedding=None, metadata={'page_label': '426', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ninternalstateisthusupdatedasfollows,butwithaconditionalself-loopweight\\nf( ) t\\ni:\\ns( ) t\\ni= f( ) t\\ni s( 1 ) t −\\ni + g( ) t\\ni σ\\uf8eb\\n\\uf8ed b i+\\ue058\\njU i , j x( ) t\\nj+\\ue058\\njW i , j h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 ,(10.41)\\nwhereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent\\nweightsintotheLSTMcell.Theexternalinputgateunit g( ) t\\niiscomputed\\nsimilarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween\\n0and1),butwithitsownparameters:\\ng( ) t\\ni= σ\\uf8eb\\n\\uf8ed bg\\ni+\\ue058\\njUg\\ni , j x( ) t\\nj+\\ue058\\njWg\\ni , j h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 .(10.42)\\nTheoutput h( ) t\\nioftheLSTMcellcanalsobeshutoﬀ,viatheoutputgate q( ) t\\ni,\\nwhichalsousesasigmoidunitforgating:\\nh( ) t\\ni= tanh\\ue010\\ns( ) t\\ni\\ue011\\nq( ) t\\ni (10.43)\\nq( ) t\\ni= σ\\uf8eb\\n\\uf8ed bo\\ni+\\ue058\\njUo\\ni , j x( ) t\\nj+\\ue058\\njWo\\ni , j h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 (10.44)\\nwhichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent\\nweights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t\\ni\\nasanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown\\ninﬁgure.Thiswouldrequirethreeadditionalparameters. 10.16\\nLSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily\\nthanthesimplerecurrentarchitectures,ﬁrstonartiﬁcialdatasetsdesignedfor\\ntestingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter\\nandSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence\\nprocessingtaskswherestate-of-the-art performance wasobtained(Graves2012,;\\nGraves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM\\nhavebeenstudiedandusedandarediscussednext.\\n10.10.2OtherGatedRNNs\\nWhichpieces\\xa0ofthe\\xa0LSTMarchitecture are\\xa0actually necessary?Whatother\\nsuccessfularchitecturescouldbedesignedthatallowthenetworktodynamically\\ncontrolthetimescaleandforgettingbehaviorofdiﬀerentunits?\\n4 1 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1edcaeaf-1e3e-479d-8052-e176add91e17', embedding=None, metadata={'page_label': '427', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nSomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs,\\nwhoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b\\nChung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain\\ndiﬀerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe\\nforgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations\\narethefollowing:\\nh( ) t\\ni= u( 1 ) t −\\ni h( 1 ) t −\\ni+(1− u( 1 ) t −\\ni) σ\\uf8eb\\n\\uf8ed b i+\\ue058\\njU i , j x( 1 ) t −\\nj +\\ue058\\njW i , j r( 1 ) t −\\nj h( 1 ) t −\\nj\\uf8f6\\n\\uf8f8 ,\\n(10.45)\\nwhereustandsfor“update”gateandrfor“reset”gate.Theirvalueisdeﬁnedas\\nusual:\\nu( ) t\\ni= σ\\uf8eb\\n\\uf8ed bu\\ni+\\ue058\\njUu\\ni , j x( ) t\\nj+\\ue058\\njWu\\ni , j h( ) t\\nj\\uf8f6\\n\\uf8f8 (10.46)\\nand\\nr( ) t\\ni= σ\\uf8eb\\n\\uf8ed br\\ni+\\ue058\\njUr\\ni , j x( ) t\\nj+\\ue058\\njWr\\ni , j h( ) t\\nj\\uf8f6\\n\\uf8f8 .(10.47)\\nTheresetandupdatesgatescanindividually“ignore”partsofthestatevector.\\nTheupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany\\ndimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely\\nignoreit(attheotherextreme)byreplacingitbythenew“targetstate”value\\n(towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol\\nwhichpartsofthestategetusedtocomputethenexttargetstate,introducingan\\nadditionalnonlineareﬀectintherelationshipbetweenpaststateandfuturestate.\\nManymorevariantsaroundthisthemecanbedesigned.Forexamplethe\\nresetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits.\\nAlternately,theproductofaglobalgate(coveringawholegroupofunits,suchas\\nanentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol\\nandlocalcontrol.However,severalinvestigationsoverarchitectural variations\\noftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese\\nacrossawiderangeoftasks(,; Greﬀ e t a l .2015Jozefowicz2015Greﬀ e t a l .,).\\ne t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz\\ne t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015\\nadvocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000\\nexploredarchitecturalvariants.\\n4 1 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b81bc88f-98af-4850-a5dc-78635356268d', embedding=None, metadata={'page_label': '428', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\n10.11OptimizationforLong-TermDependencies\\nSection andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7\\nproblemsthatoccurwhenoptimizingRNNsovermanytimesteps.\\nAninterestingideaproposedbyMartensandSutskever2011()isthatsecond\\nderivativesmayvanishatthesametimethatﬁrstderivativesvanish.Second-order\\noptimization algorithmsmayroughlybeunderstoodasdividingtheﬁrstderivative\\nbythesecondderivative(inhigherdimension,multiplyingthegradientbythe\\ninverseHessian).Ifthesecondderivativeshrinksatasimilarratetotheﬁrst\\nderivative,thentheratioofﬁrstandsecondderivativesmayremainrelatively\\nconstant.Unfortunately,second-ordermethodshavemanydrawbacks,including\\nhighcomputational cost,theneedforalargeminibatch,andatendencytobe\\nattractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults\\nusingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler\\nmethodssuchasNesterovmomentumwithcarefulinitialization couldachieve\\nsimilarresults.SeeSutskever2012()formoredetail.\\xa0Bothoftheseapproaches\\nhavelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied\\ntoLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften\\nmucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore\\npowerfuloptimization algorithm.\\n10.11.1ClippingGradients\\nAsdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4\\nbyarecurrentnetovermanytimestepstendtohavederivativesthatcanbe\\neitherverylargeorverysmallinmagnitude.Thisisillustratedinﬁgureand8.3\\nﬁgure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17\\nparameters)hasa“landscape”\\xa0inwhichoneﬁnds“cliﬀs”:wideandratherﬂat\\nregionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly,\\nformingakindofcliﬀ.\\nThediﬃcultythatarisesisthatwhentheparametergradientisverylarge,a\\ngradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa\\nregionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad\\nbeendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat\\ncorrespondstothesteepestdescentwithinaninﬁnitesimalregionsurroundingthe\\ncurrentparameters.Outsideofthisinﬁnitesimalregion,thecostfunctionmay\\nbegintocurvebackupwards.Theupdatemustbechosentobesmallenoughto\\navoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat\\n4 1 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6eae4264-fa3f-4ed5-bda4-e35b64417174', embedding=None, metadata={'page_label': '429', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\ndecayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning\\nrate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis\\nofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe\\nlandscapeonthenextstep.\\n\\ue077\\n\\ue062\\ue04a\\ue077\\ue03b \\ue062\\n\\ue028\\ue029\\ue057 \\ue069 \\ue074 \\ue068 \\ue06f \\ue075 \\ue074 \\ue020 \\ue063 \\ue06c \\ue069 \\ue070 \\ue070 \\ue069 \\ue06e \\ue067\\n\\ue077\\n\\ue062\\ue04a\\ue077\\ue03b \\ue062\\n\\ue028\\ue029\\ue057 \\ue069 \\ue074 \\ue068 \\ue020 \\ue063 \\ue06c \\ue069 \\ue070 \\ue070 \\ue069 \\ue06e \\ue067\\nFigure10.17:Exampleoftheeﬀectofgradientclippinginarecurrentnetworkwith\\ntwoparameterswandb.Gradientclippingcanmakegradientdescentperformmore\\nreasonablyinthevicinityofextremelysteepcliﬀs.Thesesteepcliﬀscommonlyoccur\\ninrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly.\\nThecliﬀisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix\\nismultipliedbyitselfonceforeachtimestep. ( L e f t )Gradientdescentwithoutgradient\\nclippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient\\nfromthecliﬀface.Thelargegradientcatastrophicallypropelstheparametersoutsidethe\\naxesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t )\\nreactiontothecliﬀ.Whileitdoesascendthecliﬀface,thestepsizeisrestrictedsothat\\nitcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith\\npermissionfromPascanu2013 e t a l .().\\nAsimpletypeofsolutionhasbeeninusebypractitioners formanyyears:\\nclippingthegradient.Therearediﬀerentinstancesofthisidea(Mikolov2012,;\\nPascanu2013 e t a l .,).Oneoptionistocliptheparametergradientfromaminibatch\\ne l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p\\nt h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter\\nupdate:\\nif||||g > v (10.48)\\ng←g v\\n||||g(10.49)\\n4 1 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3264aa5e-8fe0-4bb8-bd89-f76e2d62939b', embedding=None, metadata={'page_label': '430', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nwhere visthenormthresholdandgisusedtoupdateparameters.Becausethe\\ngradientofalltheparameters(includingdiﬀerentgroupsofparameters,suchas\\nweightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter\\nmethodhastheadvantagethatitguaranteesthateachstepisstillinthegradient\\ndirection,butexperimentssuggestthatbothformsworksimilarly.Although\\ntheparameterupdatehasthesamedirectionasthetruegradient,withgradient\\nnormclipping,theparameterupdatevectornormisnowbounded.Thisbounded\\ngradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In\\nfact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove\\nathresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe\\ngradientisnumerically InforNan(consideredinﬁniteornot-a-number),then\\narandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe\\nnumericallyunstableconﬁguration. Clippingthegradientnormper-minibatchwill\\nnotchangethedirectionofthegradientforanindividualminibatch.However,\\ntakingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot\\nequivalenttoclippingthenormofthetruegradient(thegradientformedfrom\\nusingallexamples).Examplesthathavelargegradientnorm,aswellasexamples\\nthatappearinthesameminibatchassuchexamples,willhavetheircontribution\\ntotheﬁnaldirectiondiminished.Thisstandsincontrasttotraditionalminibatch\\ngradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall\\nminibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses\\nanunbiasedestimateofthegradient,whilegradientdescentwithnormclipping\\nintroducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement-\\nwiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient\\northeminibatchgradient,butitisstilladescentdirection.Ithasalsobeen\\nproposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto\\nhiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we\\nconjecturethatallthesemethodsbehavesimilarly.\\n10.11.2RegularizingtoEncourageInformationFlow\\nGradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith\\nvanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term\\ndependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof\\ntheunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated\\nwitharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself-\\nloopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10\\ntoregularizeorconstraintheparameterssoastoencourage“informationﬂow.”\\nInparticular,wewouldlikethegradientvector∇h( ) t Lbeingback-propagatedto\\n4 1 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35588a0f-f93e-4f29-b74f-bb408e7fdb22', embedding=None, metadata={'page_label': '431', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nmaintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe\\nendofthesequence.Formally,wewant\\n(∇h( ) t L)∂h( ) t\\n∂h( 1 ) t −(10.50)\\ntobeaslargeas\\n∇h( ) t L. (10.51)\\nWiththisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer:\\nΩ =\\ue058\\nt\\uf8eb\\n\\uf8ed\\ue00c\\ue00c\\ue00c|∇(h( ) t L)∂ h( ) t\\n∂ h( 1 ) t −\\ue00c\\ue00c\\ue00c|\\n||∇h( ) t L||−1\\uf8f6\\n\\uf8f82\\n. (10.52)\\nComputingthegradientofthisregularizermayappeardiﬃcult,butPascanu\\ne t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013\\nvectors∇h( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so\\nthatthereisnoneedtoback-propagatethroughthem).Theexperimentswith\\nthisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which\\nhandlesgradientexplosion),theregularizercanconsiderablyincreasethespanof\\nthedependenciesthatanRNNcanlearn.\\xa0BecauseitkeepstheRNNdynamics\\nontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant.\\nWithoutgradientclipping,gradientexplosionpreventslearningfromsucceeding.\\nAkeyweaknessofthisapproachisthatitisnotaseﬀectiveastheLSTMfor\\ntaskswheredataisabundant,suchaslanguagemodeling.\\n10.12ExplicitMemory\\nIntelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning,\\nwhichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However,\\ntherearediﬀerentkindsofknowledge.Someknowledgecanbeimplicit,sub-\\nconscious,anddiﬃculttoverbalize—suchashowtowalk,orhowadoglooks\\ndiﬀerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively\\nstraightforwardtoputintowords—everydaycommonsense knowledge,like“acat\\nisakindofanimal,”orveryspeciﬁcfactsthatyouneedtoknowtoaccomplish\\nyourcurrentgoals,like“themeetingwiththesalesteamisat3:00PMinroom\\n141.”\\nNeuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto\\nmemorizefacts.\\xa0Stochasticgradientdescentrequiresmanypresentationsofthe\\n4 1 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35a19b09-0dff-452c-9a2d-4c6752026e9d', embedding=None, metadata={'page_label': '432', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nT ask \\xa0 ne t w or k ,\\nc ontrol l i ng\\xa0th e \\xa0 m e m o r yMe m or y \\xa0 c e l l s\\nW r i t i ng\\nm e c hani s mR e adi ng\\nm e c hani s m\\nFigure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing\\nsomeofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe\\ndistinguishthe“representation”partofthemodel(the“tasknetwork,”herearecurrent\\nnetinthebottom)fromthe“memory”partofthemodel(thesetofcells),whichcan\\nstorefacts.Thetasknetworklearnsto“control”thememory,decidingwheretoreadfrom\\nandwheretowritetowithinthememory(throughthereadingandwritingmechanisms,\\nindicatedbyboldarrowspointingatthereadingandwritingaddresses).\\n4 1 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d12e986f-ed1e-44b5-ab01-027d4f8236f1', embedding=None, metadata={'page_label': '433', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nsameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen,\\nthatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized\\nthatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory\\nsystemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof\\ninformationthat\\xa0arerelevantto\\xa0achieving\\xa0some goal.Suchexplicit\\xa0memory\\ncomponentswouldallowoursystemsnotonlytorapidlyand“intentionally”store\\nandretrievespeciﬁcfactsbutalsotosequentiallyreasonwiththem.Theneed\\nforneuralnetworksthatcanprocessinformationinasequenceofsteps,changing\\nthewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized\\nasimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive\\nresponsestotheinput(,). Hinton1990\\nToresolvethisdiﬃculty,Weston2014 e t a l .()introducedmemorynetworks\\nthatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha-\\nnism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem\\nhowtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural\\nTuringmachine,whichisabletolearntoreadfromandwritearbitrarycontent\\ntomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake,\\nandallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof\\nacontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015\\ntion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1\\nrelatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows\\ngradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,;\\nKumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,).\\nEachmemorycellcanbethoughtofasanextensionofthememorycellsin\\nLSTMsandGRUs.Thediﬀerenceisthatthenetworkoutputsaninternalstate\\nthatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina\\ndigitalcomputerreadfromorwritetoaspeciﬁcaddress.\\nItisdiﬃculttooptimizefunctionsthatproduceexact,integeraddresses.To\\nalleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells\\nsimultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they\\nmodifymultiplecellsbydiﬀerentamounts.Thecoeﬃcientsfortheseoperations\\narechosentobefocusedonasmallnumberofcells,forexample,byproducing\\nthemviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows\\nthefunctionscontrollingaccesstothememorytobeoptimizedusinggradient\\ndescent.Thegradientonthesecoeﬃcientsindicateswhethereachofthemshould\\nbeincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose\\nmemoryaddressesreceivingalargecoeﬃcient.\\nThesememorycellsaretypicallyaugmentedtocontainavector,ratherthan\\n4 1 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72d6a6ad-f710-477c-8ab6-9b655189ef2c', embedding=None, metadata={'page_label': '434', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nthesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons\\ntoincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe\\ncostofaccessingamemorycell.\\xa0Wepaythecomputational costofproducinga\\ncoeﬃcientformanycells,butweexpectthesecoeﬃcientstoclusteraroundasmall\\nnumberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan\\noﬀsetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat\\ntheyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor\\nwritefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea\\ncompletevector-valuedmemoryifweareabletoproduceapatternthatmatches\\nsomebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan\\nrecallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based\\nreadinstructionassaying,“Retrievethelyricsofthesongthathasthechorus‘We\\nallliveinayellowsubmarine.’”Content-basedaddressingismoreusefulwhenwe\\nmaketheobjectstoberetrievedlarge—ifeveryletterofthesongwasstoredina\\nseparatememorycell,wewouldnotbeabletoﬁndthemthisway.Bycomparison,\\nlocation-basedaddressingisnotallowedtorefertothecontentofthememory.\\nWecanthinkofalocation-basedreadinstructionassaying“Retrievethelyricsof\\nthesonginslot347.”Location-basedaddressingcanoftenbeaperfectlysensible\\nmechanismevenwhenthememorycellsaresmall.\\nIfthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then\\ntheinformationitcontainscanbepropagatedforwardintimeandthegradients\\npropagatedbackwardintimewithouteithervanishingorexploding.\\nTheexplicitmemoryapproachisillustratedinﬁgure,whereweseethat 10.18\\na“taskneuralnetwork”\\xa0iscoupledwithamemory.Althoughthattaskneural\\nnetworkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork.\\nThetasknetworkcanchoosetoreadfromorwritetospeciﬁcmemoryaddresses.\\nExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM\\nRNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand\\ngradientscanbepropagated(forwardintimeorbackwardsintime,respectively)\\nforverylongdurations.\\nAsanalternativetoback-propagationthroughweightedaveragesofmemory\\ncells,wecaninterpretthememoryaddressingcoeﬃcientsasprobabilities and\\nstochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels\\nthatmakediscretedecisionsrequiresspecializedoptimization algorithms,described\\ninsection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1\\ndecisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft\\ndecisions.\\nWhetheritissoft(allowingback-propagation) orstochasticandhard,the\\n4 1 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc63c5e1-896e-4b7c-b69d-76654eb03af4', embedding=None, metadata={'page_label': '435', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\\nmechanism\\xa0forchoosing\\xa0anaddress\\xa0isin\\xa0itsform\\xa0identical\\xa0totheattention\\nmechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine\\ntranslation( ,)anddiscussedinsection.\\xa0Theidea Bahdanau e t a l .2015 12.4.5.1\\nofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe\\ncontextofhandwritinggeneration(Graves2013,),withanattentionmechanism\\nthatwasconstrainedtomoveonlyforwardintimethroughthesequence.In\\nthecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof\\nattentioncanmovetoacompletelydiﬀerentplace,comparedtothepreviousstep.\\nRecurrentneuralnetworksprovideawaytoextenddeeplearningtosequential\\ndata.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow\\nmovestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world\\ntasks.\\n4 2 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ccca74ef-0243-4647-9e0e-c1ff7ca01013', embedding=None, metadata={'page_label': '436', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 1\\nPractical Methodology\\nSuccessfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood\\nknowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey\\nwork.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean\\nalgorithmforaparticularapplicationandhowtomonitorandrespondtofeedback\\nobtainedfromexperimentsinordertoimproveamachinelearningsystem.During\\ndaytodaydevelopmentofmachinelearningsystems,practitioners needtodecide\\nwhethertogathermoredata,increaseordecreasemodelcapacity,addorremove\\nregularizingfeatures,improvetheoptimization ofamodel,improveapproximate\\ninferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allof\\ntheseoperationsareattheveryleasttime-consuming totryout,soitisimportant\\ntobeabletodeterminetherightcourseofactionratherthanblindlyguessing.\\nMostofthisbookisaboutdiﬀerentmachinelearningmodels,trainingalgo-\\nrithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost\\nimportantingredienttobeingamachinelearningexpertisknowingawidevariety\\nofmachinelearningtechniquesandbeinggoodatdiﬀerentkindsofmath.Inprac-\\ntice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace\\nalgorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof\\nanalgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe\\nrecommendations inthischapterareadaptedfrom().Ng2015\\nWerecommendthefollowingpracticaldesignprocess:\\n•Determineyourgoals—whaterrormetrictouse,andyourtargetvaluefor\\nthiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe\\nproblemthattheapplicationisintendedtosolve.\\n•Establishaworkingend-to-endpipelineassoonaspossible,includingthe\\n421', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fe1a0967-8635-4f17-8c3d-a3523ecfd763', embedding=None, metadata={'page_label': '437', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nestimationoftheappropriateperformancemetrics.\\n•Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag-\\nnosewhichcomponentsareperformingworsethanexpectedandwhetherit\\nisduetooverﬁtting,underﬁtting, oradefectinthedataorsoftware.\\n•Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting\\nhyperparameters,orchangingalgorithms,basedonspeciﬁcﬁndingsfrom\\nyourinstrumentation.\\nAsarunningexample,wewilluseStreetViewaddressnumbertranscription\\nsystem( ,).Thepurposeofthisapplicationistoadd Goodfellow etal.2014d\\nbuildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord\\ntheGPScoordinatesassociatedwitheachphotograph. Aconvolutionalnetwork\\nrecognizestheaddressnumberineachphotograph, allowingtheGoogleMaps\\ndatabasetoaddthataddressinthecorrectlocation.Thestoryofhowthis\\ncommercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign\\nmethodologyweadvocate.\\nWenowdescribeeachofthestepsinthisprocess.\\n11.1PerformanceMetrics\\nDeterminingyourgoals,intermsofwhicherrormetrictouse,isanecessaryﬁrst\\nstepbecauseyourerrormetricwillguideallofyourfutureactions.\\xa0Youshould\\nalsohaveanideaofwhatlevelofperformanceyoudesire.\\nKeepinmindthatformostapplications,itisimpossibletoachieveabsolute\\nzeroerror.TheBayeserrordeﬁnestheminimumerrorratethatyoucanhopeto\\nachieve,evenifyouhaveinﬁnitetrainingdataandcanrecoverthetrueprobability\\ndistribution.This\\xa0isbecause\\xa0your\\xa0inputfeatures\\xa0maynot\\xa0contain\\xa0complete\\ninformationabouttheoutputvariable,orbecausethesystemmightbeintrinsically\\nstochastic.Youwillalsobelimitedbyhavingaﬁniteamountoftrainingdata.\\nTheamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour\\ngoalistobuildthebestpossiblereal-worldproductorservice,youcantypically\\ncollectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh\\nthisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime,\\nmoney,orhumansuﬀering(forexample,ifyourdatacollectionprocessinvolves\\nperforminginvasivemedicaltests).Whenyourgoalistoanswerascientiﬁcquestion\\naboutwhichalgorithmperformsbetteronaﬁxedbenchmark,thebenchmark\\n4 2 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef86fa95-d0c6-4098-9569-ddad40dc5768', embedding=None, metadata={'page_label': '438', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nspeciﬁcationusuallydeterminesthetrainingsetandyouarenotallowedtocollect\\nmoredata.\\nHowcanonedetermineareasonablelevelofperformancetoexpect?Typically,\\nintheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable\\nbasedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we\\nhavesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe,\\ncost-eﬀective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic\\ndesirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate.\\nAnotherimportantconsiderationbesidesthetargetvalueoftheperformance\\nmetricisthechoiceofwhichmetrictouse.Severaldiﬀerentperformancemetrics\\nmaybeusedtomeasuretheeﬀectivenessofacompleteapplicationthatincludes\\nmachinelearningcomponents.Theseperformancemetricsareusuallydiﬀerent\\nfromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2\\ncommontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem.\\nHowever,manyapplicationsrequiremoreadvancedmetrics.\\nSometimesitismuchmorecostlytomakeonekindofamistakethananother.\\nForexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes:\\nincorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga\\nspammessagetoappearintheinbox.Itismuchworsetoblockalegitimate\\nmessagethantoallowaquestionablemessagetopassthrough.Ratherthan\\nmeasuringtheerrorrateofaspamclassiﬁer,wemaywishtomeasuresomeform\\noftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost\\nofallowingspammessages.\\nSometimeswewishtotrainabinaryclassiﬁerthatisintendedtodetectsome\\nrareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose\\nthatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve\\n99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassiﬁer\\ntoalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto\\ncharacterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis\\ntoinsteadmeasure pr e c i si o nand r e c al l.Precisionisthefractionofdetections\\nreportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents\\nthatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve\\nperfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease\\nwouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho\\nhavethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleina\\nmillionhave).Whenusingprecisionandrecall,itiscommontoplota P R c ur v e,\\nwithprecisiononthe y-axisandrecallonthe x-axis.Theclassiﬁergeneratesascore\\nthatishigheriftheeventtobedetectedoccurred.\\xa0Forexample,afeedforward\\n4 2 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ac75e7c5-6192-4771-bb72-6150b51bdc50', embedding=None, metadata={'page_label': '439', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nnetworkdesignedtodetectadiseaseoutputs ˆ y= P( y=1| x),estimatingthe\\nprobabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas\\nthedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome\\nthreshold.\\xa0Byvaryingthethreshold,wecantradeprecisionforrecall.\\xa0Inmany\\ncases,wewishtosummarizetheperformanceoftheclassiﬁerwithasinglenumber\\nratherthanacurve.Todoso,wecanconvertprecision pandrecall rintoan\\nF-scor egivenby\\nF=2 pr\\np r+. (11.1)\\nAnotheroptionistoreportthetotalarealyingbeneaththePRcurve.\\nInsomeapplications,itispossibleforthemachinelearningsystemtorefuseto\\nmakeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate\\nhowconﬁdentitshouldbeaboutadecision,especiallyifawrongdecisioncan\\nbeharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet\\nViewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto\\ntranscribetheaddressnumberfromaphotographinordertoassociatethelocation\\nwherethephotowastakenwiththecorrectaddressinamap.Becausethevalue\\nofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoadd\\nanaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystem\\nthinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription,\\nthenthebestcourseofactionistoallowahumantotranscribethephotoinstead.\\nOfcourse,themachinelearningsystemisonlyusefulifitisabletodramatically\\nreducetheamountofphotosthatthehumanoperatorsmustprocess.Anatural\\nperformancemetrictouseinthissituationis c o v e r age.Coverageisthefraction\\nofexamplesforwhichthemachinelearningsystemisabletoproducearesponse.\\nItispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracy\\nbyrefusingtoprocessanyexample,butthisreducesthecoverageto0%.Forthe\\nStreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription\\naccuracywhilemaintaining95%coverage.Human-levelperformanceonthistask\\nis98%accuracy.\\nManyothermetricsarepossible.Wecanforexample,measureclick-through\\nrates,collectusersatisfactionsurveys,andsoon.\\xa0Manyspecializedapplication\\nareashaveapplication-speciﬁccriteriaaswell.\\nWhatisimportantistodeterminewhichperformancemetrictoimproveahead\\noftime,thenconcentrateonimprovingthismetric.Withoutclearlydeﬁnedgoals,\\nitcanbediﬃculttotellwhetherchangestoamachinelearningsystemmake\\nprogressornot.\\n4 2 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d54459f-d179-47eb-acfc-88510f49a2d2', embedding=None, metadata={'page_label': '440', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\n11.2DefaultBaselineModels\\nAfterchoosingperformancemetricsandgoals,\\xa0thenextstepinanypractical\\napplicationistoestablishareasonableend-to-endsystemassoonaspossible.In\\nthissection,weproviderecommendations forwhichalgorithmstouseastheﬁrst\\nbaselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch\\nprogressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon\\nafterthiswriting.\\nDependingonthecomplexityofyourproblem,youmayevenwanttobegin\\nwithoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby\\njustchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple\\nstatisticalmodellikelogisticregression.\\nIfyouknowthatyourproblemfallsintoan“AI-complete”categorylikeobject\\nrecognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely\\ntodowellbybeginningwithanappropriatedeeplearningmodel.\\nFirst,choosethegeneralcategoryofmodelbasedonthestructureofyour\\ndata.Ifyouwanttoperformsupervisedlearningwithﬁxed-sizevectorsasinput,\\nuseafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown\\ntopologicalstructure(forexample,iftheinputisanimage),useaconvolutional\\nnetwork.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear\\nunit(ReLUsortheirgeneralizations likeLeakyReLUs,PreLusandmaxout).If\\nyourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU).\\nAreasonablechoiceofoptimization algorithmisSGDwithmomentumwitha\\ndecayinglearningrate(populardecayschemesthatperformbetterorworseon\\ndiﬀerentproblemsincludedecayinglinearlyuntilreachingaﬁxedminimumlearning\\nrate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10\\neachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam.\\nBatchnormalization canhaveadramaticeﬀectonoptimization performance,\\nespeciallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities.\\nWhileitisreasonabletoomitbatchnormalization fromtheveryﬁrstbaseline,it\\nshouldbeintroducedquicklyifoptimization appearstobeproblematic.\\nUnlessyourtrainingsetcontainstensofmillionsofexamplesormore,you\\nshouldincludesomemildformsofregularizationfromthestart.Earlystopping\\nshouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy\\ntoimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch\\nnormalization alsosometimesreducesgeneralization errorandallowsdropoutto\\nbeomitted,duetothenoiseintheestimateofthestatisticsusedtonormalize\\neachvariable.\\n4 2 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd1324ab-4a21-4391-93e2-c82bcbc1a40a', embedding=None, metadata={'page_label': '441', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nIfyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you\\nwillprobablydowellbyﬁrstcopyingthemodelandalgorithmthatisalready\\nknowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy\\natrainedmodelfromthattask.Forexample,itiscommontousethefeatures\\nfromaconvolutionalnetworktrainedonImageNettosolveothercomputervision\\ntasks( ,). Girshicketal.2015\\nAcommonquestioniswhethertobeginbyusingunsupervisedlearning,de-\\nscribedfurtherinpart.Thisissomewhatdomainspeciﬁc.Somedomains,such III\\nasnaturallanguageprocessing,areknowntobeneﬁttremendouslyfromunsuper-\\nvisedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inother\\ndomains,suchascomputervision,currentunsupervisedlearningtechniquesdo\\nnotbringabeneﬁt,exceptinthesemi-supervisedsetting,whenthenumberof\\nlabeledexamplesisverysmall( ,; Kingma etal.2014Rasmus2015etal.,).Ifyour\\napplicationisinacontextwhereunsupervisedlearningisknowntobeimportant,\\nthenincludeitinyourﬁrstend-to-endbaseline.Otherwise,onlyuseunsupervised\\nlearninginyourﬁrstattemptifthetaskyouwanttosolveisunsupervised.You\\ncanalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial\\nbaselineoverﬁts.\\n11.3DeterminingWhethertoGatherMoreData\\nAftertheﬁrstend-to-endsystemisestablished,itistimetomeasuretheperfor-\\nmanceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning\\nnovicesaretemptedtomakeimprovementsbytryingoutmanydiﬀerentalgorithms.\\nHowever,itisoftenmuchbettertogathermoredatathantoimprovethelearning\\nalgorithm.\\nHowdoesonedecidewhethertogathermoredata?First,determinewhether\\ntheperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining\\nsetispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready\\navailable,sothereisnoreasontogathermoredata.Instead,tryincreasingthe\\nsizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer.\\nAlso,tryimprovingthelearningalgorithm,forexamplebytuningthelearning\\nratehyperparameter. Iflargemodelsandcarefullytunedoptimization algorithms\\ndonotworkwell,thentheproblemmightbetheofthetrainingdata.The quality\\ndatamaybetoonoisyormaynotincludetherightinputsneededtopredictthe\\ndesiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectinga\\nrichersetoffeatures.\\nIftheperformanceonthetrainingsetisacceptable,thenmeasuretheper-\\n4 2 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cec46496-a77d-444d-ad99-602abaf65ef7', embedding=None, metadata={'page_label': '442', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nformanceonatestset.Iftheperformanceonthetestsetisalsoacceptable,\\nthenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan\\ntrainingsetperformance,thengatheringmoredataisoneofthemosteﬀective\\nsolutions.\\xa0Thekeyconsiderationsarethecostandfeasibilityofgatheringmore\\ndata,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe\\namountofdatathatisexpectedtobenecessarytoimprovetestsetperformance\\nsigniﬁcantly.\\xa0Atlargeinternetcompanieswithmillionsorbillionsofusers,itis\\nfeasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably\\nlessthantheotheralternatives,sotheanswerisalmostalwaystogathermore\\ntrainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof\\nthemostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas\\nmedicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple\\nalternativetogatheringmoredataistoreducethesizeofthemodelorimprove\\nregularization, byadjustinghyperparameters suchasweightdecaycoeﬃcients,\\norbyaddingregularizationstrategiessuchasdropout.Ifyouﬁndthatthegap\\nbetweentrainandtestperformanceisstillunacceptable evenaftertuningthe\\nregularizationhyperparameters ,thengatheringmoredataisadvisable.\\nWhendecidingwhethertogathermoredata,itisalsonecessarytodecide\\nhowmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween\\ntrainingsetsizeandgeneralization error,likeinﬁgure.Byextrapolatingsuch 5.4\\ncurves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto\\nachieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal\\nnumberofexampleswillnothaveanoticeableimpactongeneralization error.Itis\\nthereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale,\\nforexampledoublingthenumberofexamplesbetweenconsecutiveexperiments.\\nIfgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove\\ngeneralization erroristoimprovethelearningalgorithmitself.Thisbecomesthe\\ndomainofresearchandnotthedomainofadviceforappliedpractitioners.\\n11.4SelectingHyperparameters\\nMostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmany\\naspectsofthealgorithm’sbehavior.Someofthesehyperparametersaﬀectthetime\\nandmemorycostofrunningthealgorithm.Someofthesehyperparameters aﬀect\\nthequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfer\\ncorrectresultswhendeployedonnewinputs.\\nTherearetwobasicapproachestochoosingthesehyperparameters :choosing\\nthemmanuallyandchoosingthemautomatically .Choosingthehyperparameters\\n4 2 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='26f02cd8-eab9-40e9-ae59-4da9bda2401d', embedding=None, metadata={'page_label': '443', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nmanuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine\\nlearningmodelsachievegoodgeneralization. Automatichyperparameterselection\\nalgorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften\\nmuchmorecomputationally costly.\\n1 1 . 4 . 1 Ma n u a l Hyp erp a ra m et er T u n i n g\\nTosethyperparameters manually,onemustunderstandtherelationshipbetween\\nhyperparameters,trainingerror,generalization errorandcomputational resources\\n(memoryandruntime).Thismeansestablishingasolidfoundationonthefun-\\ndamentalideasconcerningtheeﬀectivecapacityofalearningalgorithmfrom\\nchapter.5\\nThegoalofmanualhyperparametersearchisusuallytoﬁndthelowestgeneral-\\nizationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshow\\ntodeterminetheruntimeandmemoryimpactofvarioushyperparametershere\\nbecausethisishighlyplatform-dependent.\\nTheprimarygoalofmanualhyperparametersearchistoadjusttheeﬀective\\ncapacityofthemodeltomatchthecomplexityofthetask.Eﬀectivecapacity\\nisconstrainedbythreefactors:\\xa0therepresentationalcapacityofthemodel,the\\nabilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto\\ntrainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure\\nregularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas\\nhigherrepresentationalcapacity—itiscapableofrepresentingmorecomplicated\\nfunctions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,if\\nthetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobof\\nminimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbid\\nsomeofthesefunctions.\\nThegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas\\nafunctionofoneofthehyperparameters ,asinﬁgure.\\xa0Atoneextreme,the 5.3\\nhyperparametervaluecorrespondstolowcapacity,andgeneralization errorishigh\\nbecausetrainingerrorishigh.Thisistheunderﬁttingregime.Attheotherextreme,\\nthehyperparameter valuecorrespondstohighcapacity,andthegeneralization\\nerrorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere\\ninthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible\\ngeneralization error,byaddingamediumgeneralization gaptoamediumamount\\noftrainingerror.\\nForsomehyperparameters,overﬁttingoccurswhenthevalueofthehyper-\\nparameterislarge.\\xa0Thenumberofhiddenunitsinalayerisonesuchexample,\\n4 2 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9bb0586a-5ec0-47ba-bbd8-9026b77e8fc9', embedding=None, metadata={'page_label': '444', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nbecauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel.\\nForsomehyperparameters ,overﬁttingoccurswhenthevalueofthehyperparame-\\nterissmall.Forexample,thesmallestallowableweightdecaycoeﬃcientofzero\\ncorrespondstothegreatesteﬀectivecapacityofthelearningalgorithm.\\nNoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve.\\nManyhyperparameters arediscrete,suchasthenumberofunitsinalayerorthe\\nnumberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints\\nalongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters\\nareswitchesthat\\xa0specify\\xa0whetherornotto\\xa0usesomeoptionalcomponentof\\nthelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput\\nfeaturesbysubtractingtheirmeananddividingbytheirstandarddeviation.These\\nhyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparameters\\nhavesomeminimumormaximumvaluethatpreventsthemfromexploringsome\\npartofthecurve.Forexample,theminimumweightdecaycoeﬃcientiszero.This\\nmeansthatifthemodelisunderﬁttingwhenweightdecayiszero,wecannotenter\\ntheoverﬁttingregionbymodifyingtheweightdecaycoeﬃcient.Inotherwords,\\nsomehyperparameters canonlysubtractcapacity.\\nThelearningrateisperhapsthemostimportanthyperparameter. Ifyou\\nhave\\xa0timeto\\xa0tuneonly\\xa0onehyperparameter, tune\\xa0thelearning\\xa0rate. It\\xa0con-\\ntrolstheeﬀectivecapacityofthemodelinamorecomplicatedwaythanother\\nhyperparameters—theeﬀectivecapacityofthemodelishighestwhenthelearning\\nrateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially\\nlargeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror,\\nillustratedinﬁgure.Whenthelearningrateistoolarge,gradientdescent 11.1\\ncaninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized\\nquadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits\\noptimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a\\nisnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror.\\nThiseﬀectispoorlyunderstood(itwouldnothappenforaconvexlossfunction).\\nTuningtheparametersotherthanthelearningraterequiresmonitoringboth\\ntrainingandtesterrortodiagnosewhetheryourmodelisoverﬁttingorunderﬁtting,\\nthenadjustingitscapacityappropriately .\\nIfyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave\\nnochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare\\nconﬁdentthatyouroptimization algorithmisperformingcorrectly,thenyoumust\\naddmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this\\nincreasesthecomputational costsassociatedwiththemodel.\\nIfyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan\\n4 2 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='00d8f874-1859-4243-902f-8af607377c26', embedding=None, metadata={'page_label': '445', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\n1 0− 21 0− 11 00\\nL e a r ni ng r a t e ( l o g a r i t hm i c s c a l e )012345678T r a i ni ng e r r o r\\nFigure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice\\nthesharpriseinerrorwhenthelearningisaboveanoptimalvalue.Thisisforaﬁxed\\ntrainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbya\\nfactorproportionaltothelearningratereduction.\\xa0Generalizationerrorcanfollowthis\\ncurveorbecomplicatedbyregularizationeﬀectsarisingoutofhavingatoolargeor\\ntoosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorprevent\\noverﬁtting,andevenpointswithequivalenttrainingerrorcanhavediﬀerentgeneralization\\nerror.\\nnowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorand\\nthegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading\\noﬀthesequantities.Neuralnetworkstypicallyperformbestwhenthetraining\\nerrorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily\\ndrivenbythegapbetweentrainandtesterror.\\xa0Yourgoalistoreducethisgap\\nwithoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap,\\nchangeregularizationhyperparameters toreduceeﬀectivemodelcapacity,suchas\\nbyaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma\\nlargemodelthatisregularizedwell,forexamplebyusingdropout.\\nMosthyperparameters canbesetbyreasoningaboutwhethertheyincreaseor\\ndecreasemodelcapacity.SomeexamplesareincludedinTable.11.1\\nWhilemanuallytuninghyperparameters,donotlosesightofyourendgoal:\\ngoodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve\\nthisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral-\\nizationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically\\nguaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize\\nuntilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational\\ncostoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In\\n4 3 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6801db0f-3db5-4818-bcaa-d9e4dce9912a', embedding=None, metadata={'page_label': '446', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nHyperparameterIncreases\\ncapacity\\nwhen...Reason Caveats\\nNumberofhid-\\ndenunitsincreasedIncreasingthenumberof\\nhiddenunitsincreasesthe\\nrepresentationalcapacity\\nofthemodel.Increasingthenumber\\nofhiddenunits\\xa0increases\\nboththetimeandmemory\\ncostofessentiallyeveryop-\\nerationonthemodel.\\nLearningratetunedop-\\ntimallyAnimproperlearningrate,\\nwhether\\xa0toohigh\\xa0ortoo\\nlow,resultsinamodel\\nwithloweﬀectivecapacity\\nduetooptimizationfailure\\nConvolutionker-\\nnelwidthincreasedIncreasingthekernelwidth\\nincreasesthenumberofpa-\\nrametersinthemodelAwiderkernelresultsin\\nanarroweroutputdimen-\\nsion,reducingmodelca-\\npacityunlessyouuseim-\\nplicitzeropaddingtore-\\nducethiseﬀect.Wider\\nkernelsrequiremoremem-\\noryforparameterstorage\\nandincreaseruntime,but\\nanarroweroutputreduces\\nmemorycost.\\nImplicitzero\\npaddingincreasedAddingimplicitzerosbe-\\nforeconvolutionkeepsthe\\nrepresentationsizelargeIncreasedtimeandmem-\\norycostofmostopera-\\ntions.\\nWeightdecayco-\\neﬃcientdecreasedDecreasingtheweightde-\\ncaycoeﬃcientfreesthe\\nmodelparameterstobe-\\ncomelarger\\nDropoutratedecreasedDroppingunitslessoften\\ngivestheunitsmoreoppor-\\ntunitiesto“conspire”with\\neachothertoﬁtthetrain-\\ningset\\nTable11.1:Theeﬀectofvarioushyperparametersonmodelcapacity.\\n4 3 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e49e32b0-5482-4760-a4dd-b994ab70cc41', embedding=None, metadata={'page_label': '447', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nprinciple,thisapproachcouldfailduetooptimization diﬃculties,butformany\\nproblemsoptimization doesnotseemtobeasigniﬁcantbarrier,providedthatthe\\nmodelischosenappropriately .\\n1 1 . 4 . 2 A u t o m a t i c Hyp erp a ra m et er O p t i m i za t i o n A l g o ri t h m s\\nTheideallearningalgorithmjusttakesadatasetandoutputsafunction,without\\nrequiringhand-tuning ofhyperparameters .Thepopularityofseverallearning\\nalgorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto\\nperformwellwithonlyoneortwotunedhyperparameters .Neuralnetworkscan\\nsometimesperformwellwithonlyasmallnumberoftunedhyperparameters ,but\\noftenbeneﬁtsigniﬁcantlyfromtuningoffortyormorehyperparameters .Manual\\nhyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint,\\nsuchasonedeterminedbyothershavingworkedonthesametypeofapplication\\nandarchitecture, orwhentheuserhasmonthsoryearsofexperienceinexploring\\nhyperparametervaluesforneuralnetworksappliedtosimilartasks.However,\\nformanyapplications,thesestartingpointsarenotavailable.Inthesecases,\\nautomatedalgorithmscanﬁndusefulvaluesofthehyperparameters .\\nIfwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor\\ngoodvaluesofthehyperparameters ,werealizethatanoptimizationistakingplace:\\nwearetryingtoﬁndavalueofthehyperparametersthatoptimizesanobjective\\nfunction,suchasvalidationerror,sometimesunderconstraints(suchasabudget\\nfortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple,\\nto\\xa0develop h y p e r par am e t e r \\xa0 o p t i m i z a t i o nalgorithms\\xa0thatwrap\\xa0a\\xa0learnin g\\nalgorithmandchooseitshyperparameters ,thushidingthehyperparameters ofthe\\nlearningalgorithmfromtheuser.Unfortunately,hyperparameter optimization\\nalgorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat\\nshouldbeexploredforeachofthelearningalgorithm’shyperparameters .However,\\nthesesecondaryhyperparameters areusuallyeasiertochoose,inthesensethat\\nacceptableperformancemaybeachievedonawiderangeoftasksusingthesame\\nsecondaryhyperparameters foralltasks.\\n1 1 . 4 . 3 G ri d S ea rch\\nWhentherearethreeorfewerhyperparameters ,thecommonpracticeistoperform\\ng r i d se ar c h.Foreachhyperparameter,\\xa0the userselectsasmallﬁnitesetof\\nvaluestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint\\nspeciﬁcationofhyperparametervaluesintheCartesianproductofthesetofvalues\\nforeachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation\\n4 3 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a85f56fd-5be9-420d-b7c2-55378eff5765', embedding=None, metadata={'page_label': '448', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nGrid Random\\nFigure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswe\\ndisplaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore. ( L e f t )To\\nperformgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch\\nalgorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese\\nsets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint ( R i g h t )\\nhyperparameterconﬁgurations.Usuallymostofthesehyperparametersareindependent\\nfromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude\\nuniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofa\\nsamplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint\\nhyperparameterconﬁgurationsandrunstrainingwitheachofthem.Bothgridsearch\\nandrandomsearchevaluatethevalidationseterrorandreturnthebestconﬁguration.\\nTheﬁgureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniﬁcant\\ninﬂuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis\\nhasasigniﬁcanteﬀect.Gridsearchwastesanamountofcomputationthatisexponential\\ninthenumberofnon-inﬂuentialhyperparameters,whilerandomsearchtestsaunique\\nvalueofeveryinﬂuentialhyperparameteronnearlyeverytrial.Figurereproducedwith\\npermissionfrom (). BergstraandBengio2012\\n4 3 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='32c2cfc4-bdf8-4e9d-a4dd-10c7ddc29251', embedding=None, metadata={'page_label': '449', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nseterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof\\nﬁgureforanillustrationofagridofhyperparameter values. 11.2\\nHowshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical\\n(ordered)hyperparameters ,thesmallestandlargestelementofeachlistischosen\\nconservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure\\nthattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agrid\\nsearchinvolvespickingvaluesapproximately onalogarithmicscale,e.g.,alearning\\nratetakenwithintheset{ .1 , .01 ,10−3,10−4,10−5},oranumberofhiddenunits\\ntakenwiththeset . { } 501002005001000 2000 , , , , ,\\nGridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample,\\nsupposethatweranagridsearchoverahyperparameter αusingvaluesof{−1 ,0 ,1}.\\nIfthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1 α\\nliesandweshouldshiftthegridandrunanothersearchwith αin,forexample,\\n{1 ,2 ,3}.Ifweﬁndthatthebestvalueof αis,thenwemaywishtoreﬁneour 0\\nestimatebyzoominginandrunningagridsearchover. {− } . , , .101\\nTheobviousproblemwithgridsearchisthatitscomputational costgrows\\nexponentiallywiththenumberofhyperparameters .Ifthereare mhyperparameters,\\neachtakingatmost nvalues,thenthenumberoftrainingandevaluationtrials\\nrequiredgrowsas O( nm).Thetrialsmayberuninparallelandexploitloose\\nparallelism(withalmostnoneedforcommunication betweendiﬀerentmachines\\ncarryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch,\\nevenparallelization maynotprovideasatisfactorysizeofsearch.\\n1 1 . 4 . 4 Ra n d o m S ea rch\\nFortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more\\nconvenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters :\\nrandomsearch( ,). BergstraandBengio2012\\nArandomsearchproceedsasfollows.Firstwedeﬁneamarginaldistribution\\nforeachhyperparameter, e.g.,aBernoulliormultinoulliforbinaryordiscrete\\nhyperparameters,orauniformdistributiononalog-scaleforpositivereal-valued\\nhyperparameters.Forexample,\\nl o g l e a r n i n g r a t e __ ∼−− u(1 ,5) (11.2)\\nl e a r n i n g r a t e_ = 10loglearningrate _ _. (11.3)\\nwhere u( a , b)indicatesasampleoftheuniformdistributionintheinterval( a , b).\\nSimilarlythe l o g n u m b e r o f h i d d e n u n i t s ____maybesampledfrom u(log(50) ,\\nlog(2000) ).\\n4 3 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8d6227c3-a22f-4320-b773-568758a11f20', embedding=None, metadata={'page_label': '450', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nUnlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevalues\\nofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoes\\nnotincuradditionalcomputational cost.\\xa0Infact,asillustratedinﬁgure,a11.2\\nrandomsearchcanbeexponentiallymoreeﬃcientthanagridsearch,whenthere\\nareseveralhyperparametersthatdonotstronglyaﬀecttheperformancemeasure.\\nThisisstudiedatlengthin (),whofoundthatrandom BergstraandBengio2012\\nsearchreducesthevalidationseterrormuchfasterthangridsearch,intermsof\\nthenumberoftrialsrunbyeachmethod.\\nAswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom\\nsearch,toreﬁnethesearchbasedontheresultsoftheﬁrstrun.\\nThemainreasonwhyrandomsearchﬁndsgoodsolutionsfasterthangridsearch\\nisthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch,\\nwhentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters )\\nwouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters\\nwouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they\\nwouldusuallyhavediﬀerentvalues.Henceifthechangebetweenthesetwovalues\\ndoesnotmarginallymakemuchdiﬀerenceintermsofvalidationseterror,grid\\nsearchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch\\nwillstillgivetwoindependentexplorationsoftheotherhyperparameters .\\n1 1 . 4 . 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n\\nThesearchforgoodhyperparameters canbecastasanoptimization problem.\\nThedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe\\nvalidationseterrorthatresultsfromtrainingusingthesehyperparameters .In\\nsimpliﬁedsettingswhereitisfeasibletocomputethegradientofsomediﬀerentiable\\nerrormeasureonthevalidationsetwithrespecttothehyperparameters ,wecan\\nsimplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurin etal.\\n2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either\\nduetoitshighcomputationandmemorycost,orduetohyperparametershaving\\nintrinsicallynon-diﬀerentiable interactionswiththevalidationseterror,asinthe\\ncaseofdiscrete-valuedhyperparameters .\\nTocompensateforthislackofagradient,wecanbuildamodelofthevalidation\\nseterror,thenproposenewhyperparameterguessesbyperformingoptimization\\nwithinthismodel.Mostmodel-basedalgorithmsforhyperparameter searchusea\\nBayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset\\nerrorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti-\\nmizationthusinvolvesatradeoﬀbetweenexploration(proposinghyperparameters\\n4 3 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1517a6e-e168-412d-9dbf-61161bc4a224', embedding=None, metadata={'page_label': '451', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nforwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmay\\nalsoperformpoorly)andexploitation(proposinghyperparameters whichthemodel\\nisconﬁdentwillperformaswellasanyhyperparameters ithasseensofar—usually\\nhyperparametersthatareverysimilartoonesithasseenbefore).Contemporary\\napproachestohyperparameter optimizationincludeSpearmint(,), Snoeketal.2012\\nTPE( ,)andSMAC( ,). Bergstraetal.2011 Hutteretal.2011\\nCurrently,wecannotunambiguously recommendBayesianhyperparameter\\noptimization asanestablishedtoolforachievingbetterdeeplearningresultsor\\nforobtainingthoseresultswithlesseﬀort.Bayesianhyperparameteroptimization\\nsometimesperformscomparablytohumanexperts,sometimesbetter,butfails\\ncatastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworkson\\naparticularproblembutisnotyetsuﬃcientlymatureorreliable.Thatbeing\\nsaid,hyperparameter optimization isanimportantﬁeldofresearchthat,while\\noftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneﬁt\\nnotonlytheentireﬁeldofmachinelearningbutthedisciplineofengineeringin\\ngeneral.\\nOnedrawbackcommontomosthyperparameter optimization algorithmswith\\nmoresophisticationthanrandomsearchisthattheyrequireforatrainingex-\\nperimenttoruntocompletionbeforetheyareabletoextractanyinformation\\nfromtheexperiment.Thisismuchlesseﬃcient,inthesenseofhowmuchinfor-\\nmationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman\\npractitioner,sinceonecanusuallytellearlyonifsomesetofhyperparameters is\\ncompletelypathological. ()haveintroducedanearlyversion Swerskyetal.2014\\nofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime\\npoints,thehyperparameter optimization algorithmcanchoosetobeginanew\\nexperiment,to“freeze”arunningexperimentthatisnotpromising,orto“thaw”\\nandresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven\\nmoreinformation.\\n11.5DebuggingStrategies\\nWhenamachinelearningsystemperformspoorly,itisusuallydiﬃculttotell\\nwhetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere\\nisabugintheimplementation ofthealgorithm.\\xa0Machine learningsystemsare\\ndiﬃculttodebugforavarietyofreasons.\\nInmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe\\nalgorithmis.Infact,theentirepointofusingmachinelearningisthatitwill\\ndiscoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina\\n4 3 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b499af71-7b76-4adc-a214-de3cb0d07321', embedding=None, metadata={'page_label': '452', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nneuralnetworkonaclassiﬁcationtaskanditachieves5%testerror,wehave new\\nnostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimal\\nbehavior.\\nAfurtherdiﬃcultyisthatmostmachinelearningmodelshavemultipleparts\\nthatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill\\nachieveroughlyacceptableperformance.Forexample,supposethatwearetraining\\naneuralnetwithseverallayersparametrized byweights Wandbiases b.Suppose\\nfurtherthatwehavemanuallyimplemented thegradientdescentruleforeach\\nparameterseparately,andwemadeanerrorintheupdateforthebiases:\\nb b←− α (11.4)\\nwhere αisthelearningrate.Thiserroneousupdatedoesnotusethegradientat\\nall.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which\\nisclearlynotacorrectimplementation ofanyreasonablelearningalgorithm.The\\nbugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough.\\nDependingonthedistributionoftheinput,theweightsmaybeabletoadaptto\\ncompensateforthenegativebiases.\\nMostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor\\nbothofthesetwodiﬃculties.Eitherwedesignacasethatissosimplethatthe\\ncorrectbehavioractuallycanbepredicted,orwedesignatestthatexercisesone\\npartoftheneuralnetimplementationinisolation.\\nSomeimportantdebuggingtestsinclude:\\nVisualizethemodelinaction:Whentrainingamodeltodetectobjectsin\\nimages,viewsomeimageswiththedetectionsproposedbythemodeldisplayed\\nsuperimposedontheimage.Whentrainingagenerativemodelofspeech,listento\\nsomeofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto\\nfallintothepracticeofonlylookingatquantitativeperformancemeasurements\\nlikeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel\\nperformingitstaskwillhelptodeterminewhetherthequantitativeperformance\\nnumbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost\\ndevastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis\\nperformingwellwhenitisnot.\\nVisualizetheworstmistakes:\\xa0Mostmodelsareabletooutputsomesortof\\nconﬁdencemeasureforthetasktheyperform.Forexample,classiﬁersbasedona\\nsoftmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned\\ntothemostlikelyclassthusgivesanestimateoftheconﬁdencethemodelhasin\\nitsclassiﬁcationdecision.Typically,maximumlikelihoodtrainingresultsinthese\\nvaluesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction,\\n4 3 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7b00ea7e-16ca-4470-81fb-6341160aee1f', embedding=None, metadata={'page_label': '453', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nbuttheyaresomewhatusefulinthesensethatexamplesthatareactuallyless\\nlikelytobecorrectlylabeledreceivesmallerprobabilities underthemodel.By\\nviewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecan\\noftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled.\\nForexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwhere\\ntheaddressnumberdetectionsystemwouldcroptheimagetootightlyandomit\\nsomeofthedigits.Thetranscriptionnetworkthenassignedverylowprobability\\ntothecorrectanswerontheseimages.Sortingtheimagestoidentifythemost\\nconﬁdentmistakesshowedthattherewasasystematicproblemwiththecropping.\\nModifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetter\\nperformanceoftheoverallsystem,eventhoughthetranscriptionnetworkneeded\\ntobeabletoprocessgreatervariationinthepositionandscaleoftheaddress\\nnumbers.\\nReasoningaboutsoftwareusingtrainandtesterror:Itisoftendiﬃcultto\\ndeterminewhethertheunderlyingsoftwareiscorrectlyimplemented. Someclues\\ncanbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterror\\nishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthe\\nmodelisoverﬁttingforfundamentalalgorithmicreasons.Analternativepossibility\\nisthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthe\\nmodelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdata\\nwasprepareddiﬀerentlyfromthetrainingdata.Ifbothtrainandtesterrorare\\nhigh,thenitisdiﬃculttodeterminewhetherthereisasoftwaredefectorwhether\\nthemodelisunderﬁttingduetofundamentalalgorithmicreasons.Thisscenario\\nrequiresfurthertests,describednext.\\nFitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether\\nitisduetogenuineunderﬁttingorduetoasoftwaredefect.Usuallyevensmall\\nmodelscanbeguaranteedtobeableﬁtasuﬃcientlysmalldataset.Forexample,\\naclassiﬁcationdatasetwithonlyoneexamplecanbeﬁtjustbysettingthebiases\\noftheoutputlayercorrectly.Usuallyifyoucannottrainaclassiﬁertocorrectly\\nlabelasingleexample,anautoencodertosuccessfullyreproduceasingleexample\\nwithhighﬁdelity,oragenerativemodeltoconsistentlyemitsamplesresemblinga\\nsingleexample,thereisasoftwaredefectpreventingsuccessfuloptimization onthe\\ntrainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples.\\nCompareback-propagatedderivativestonumericalderivatives:Ifyouareusing\\nasoftwareframeworkthatrequiresyoutoimplementyourowngradientcom-\\nputations,orifyouareaddinganewoperationtoadiﬀerentiation libraryand\\nmustdeﬁneitsbpropmethod,thenacommonsourceoferrorisimplementingthis\\ngradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect\\n4 3 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8024de9c-3c8b-4683-8880-981cac1aaf1a', embedding=None, metadata={'page_label': '454', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nistocomparethederivativescomputedbyyourimplementation ofautomatic\\ndiﬀerentiationtothederivativescomputedbya .Because ﬁni t e di ﬀ e r e nc e s\\nf\\ue030() =lim x\\n\\ue00f →0f x \\ue00f f x (+)−()\\n\\ue00f, (11.5)\\nwecanapproximate thederivativebyusingasmall,ﬁnite: \\ue00f\\nf\\ue030() x≈f x \\ue00f f x (+)−()\\n\\ue00f. (11.6)\\nWecanimprovetheaccuracyoftheapproximation byusingthe c e n t e r e d di ﬀ e r -\\ne nc e:\\nf\\ue030() x≈f x(+1\\n2\\ue00f f x )−(−1\\n2 \\ue00f)\\n\\ue00f. (11.7)\\nTheperturbationsize \\ue00fmustchosentobelargeenoughtoensurethatthepertur-\\nbationisnotroundeddowntoomuchbyﬁnite-precisionnumericalcomputations.\\nUsually,wewillwanttotestthegradientorJacobianofavector-valuedfunction\\ng: Rm→ Rn.Unfortunately,ﬁnitediﬀerencingonlyallowsustotakeasingle\\nderivativeatatime.Wecaneitherrunﬁnitediﬀerencing m ntimestoevaluateall\\nofthepartialderivativesof g,orwecanapplythetesttoanewfunctionthatuses\\nrandomprojectionsatboththeinputandoutputof g.Forexample,wecanapply\\nourtestoftheimplementationofthederivativesto f( x)where f( x) = uTg( v x),\\nwhere uand varerandomlychosenvectors.Computing f\\ue030( x)correctlyrequires\\nbeingabletoback-propagatethrough gcorrectly,yetiseﬃcienttodowithﬁnite\\ndiﬀerencesbecause fhasonlyasingleinputandasingleoutput.Itisusually\\nagoodideatorepeatthistestformorethanonevalueof uand vtoreduce\\nthechancethatthetestoverlooksmistakesthatareorthogonaltotherandom\\nprojection.\\nIfonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis\\naveryeﬃcientwaytonumericallyestimatethegradientbyusingcomplexnumbers\\nasinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe\\nobservationthat\\nf x i \\ue00f f x i \\ue00f f (+) = ()+\\ue030()+( x O \\ue00f2) (11.8)\\nreal((+)) = ()+( f x i \\ue00f f x O \\ue00f2)imag( ,f x i \\ue00f (+)\\n\\ue00f) = f\\ue030()+( x O \\ue00f2) ,(11.9)\\nwhere i=√\\n−1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeﬀect\\nduetotakingthediﬀerencebetweenthevalueof fatdiﬀerentpoints.Thisallows\\ntheuseoftinyvaluesof \\ue00flike \\ue00f= 10−150,whichmakethe O( \\ue00f2)errorinsigniﬁcant\\nforallpracticalpurposes.\\n4 3 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fb94489-2a4c-410f-9ef0-675232c7dc51', embedding=None, metadata={'page_label': '455', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nMonitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize\\nstatisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount\\noftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunits\\ncantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiﬁers,\\nhowoftenaretheyoﬀ?Arethereunitsthatarealwaysoﬀ?Fortanhunits,\\ntheaverageoftheabsolutevalueofthepre-activationstellsushowsaturated\\ntheunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor\\nquicklyvanish,optimization maybehampered.Finally,itisusefultocomparethe\\nmagnitudeofparametergradientstothemagnitudeoftheparametersthemselves.\\nAssuggestedby(),wewouldlikethemagnitudeofparameterupdates Bottou2015\\noveraminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter,\\nnot50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmay\\nbethatsomegroupsofparametersaremovingatagoodpacewhileothersare\\nstalled.Whenthedataissparse(likeinnaturallanguage),someparametersmay\\nbeveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheir\\nevolution.\\nFinally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout\\ntheresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III\\nimateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization\\nproblems.\\xa0Typicallythesecanbedebuggedbytestingeachoftheirguarantees.\\nSomeguaranteesthatsomeoptimizationalgorithmsoﬀerincludethattheobjective\\nfunctionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith\\nrespecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm,\\nandthatthegradientwithrespecttoallvariableswillbezeroatconvergence.\\nUsuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital\\ncomputer,sothedebuggingtestshouldincludesometoleranceparameter.\\n11.6Example:Multi-DigitNumberRecognition\\nToprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology\\ninpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem,\\nfromthepointofviewofdesigningthedeeplearningcomponents.Obviously,\\nmanyothercomponentsofthecompletesystem,suchastheStreetViewcars,the\\ndatabaseinfrastructure,andsoon,wereofparamountimportance.\\nFromthepointofviewofthemachinelearningtask,theprocessbeganwith\\ndatacollection.\\xa0The carscollectedtherawdataandhumanoperatorsprovided\\nlabels.Thetranscriptiontaskwasprecededbyasigniﬁcantamountofdataset\\ncuration,includingusingothermachinelearningtechniquestodetectthehouse\\n4 4 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c24f14f1-55e2-4df0-a378-3a9831d788f8', embedding=None, metadata={'page_label': '456', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\nnumberspriortotranscribingthem.\\nThetranscriptionprojectbeganwithachoiceofperformancemetricsand\\ndesiredvaluesforthesemetrics.\\xa0Animportantgeneralprincipleistotailorthe\\nchoiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful\\niftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirement\\nforthisproject.\\xa0Speciﬁcally,thegoalwastoobtainhuman-level,98%accuracy.\\nThislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreach\\nthislevelofaccuracy,theStreetViewtranscriptionsystemsacriﬁcescoverage.\\nCoveragethusbecamethemainperformancemetricoptimizedduringtheproject,\\nwithaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecame\\npossibletoreducetheconﬁdencethresholdbelowwhichthenetworkrefusesto\\ntranscribetheinput,eventuallyexceedingthegoalof95%coverage.\\nAfterchoosingquantitativegoals,thenextstepinourrecommendedmethodol-\\nogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa\\nconvolutionalnetworkwithrectiﬁedlinearunits.Thetranscriptionprojectbegan\\nwithsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork\\ntooutputasequenceofpredictions.Inordertobeginwiththesimplestpossible\\nbaseline,theﬁrstimplementation oftheoutputlayerofthemodelconsistedof n\\ndiﬀerentsoftmaxunitstopredictasequenceof ncharacters.Thesesoftmaxunits\\nweretrainedexactlythesameasifthetaskwereclassiﬁcation,witheachsoftmax\\nunittrainedindependently.\\nOurrecommendedmethodologyistoiterativelyreﬁnethebaselineandtest\\nwhethereachchangemakesanimprovement.TheﬁrstchangetotheStreetView\\ntranscriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage\\nmetricandthestructureofthedata.Speciﬁcally,thenetworkrefusestoclassify\\naninput xwhenevertheprobabilityoftheoutputsequence p( y x|) < tfor\\nsomethreshold t.Initially,thedeﬁnitionof p( y x|)wasad-hoc,basedonsimply\\nmultiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopment\\nofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled\\nlog-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction\\nmuchmoreeﬀectively.\\nAtthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoretical\\nproblemswiththeapproach.Ourmethodologythereforesuggeststoinstrument\\nthetrainandtestsetperformanceinordertodeterminewhethertheproblem\\nisunderﬁttingoroverﬁtting.Inthiscase,trainandtestseterrorwerenearly\\nidentical.Indeed,themainreasonthisprojectproceededsosmoothlywasthe\\navailabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain\\nandtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue\\n4 4 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b32c27a3-0b87-4b0a-b87f-e4197470a314', embedding=None, metadata={'page_label': '457', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER11.PRACTICALMETHODOLOGY\\ntounderﬁttingorduetoaproblemwiththetrainingdata.Oneofthedebugging\\nstrategieswerecommendistovisualizethemodel’sworsterrors.Inthiscase,that\\nmeantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe\\nhighestconﬁdence.Theseprovedtomostlyconsistofexampleswheretheinput\\nimagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing\\nremovedbythecroppingoperation.Forexample,aphotoofanaddress“1849”\\nmightbecroppedtootightly,withonlythe“849”remainingvisible.Thisproblem\\ncouldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress\\nnumberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead,\\ntheteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthe\\ncropregiontobesystematicallywiderthantheaddressnumberdetectionsystem\\npredicted.Thissinglechangeaddedtenpercentagepointstothetranscription\\nsystem’scoverage.\\nFinally,thelastfewpercentagepointsofperformancecamefromadjusting\\nhyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain-\\ntainingsomerestrictionsonitscomputational cost.Becausetrainandtesterror\\nremainedroughlyequal,itwasalwaysclearthatanyperformancedeﬁcitsweredue\\ntounderﬁtting, aswellasduetoafewremainingproblemswiththedatasetitself.\\nOverall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsof\\nmillionsofaddressestobetranscribedbothfasterandatlowercostthanwould\\nhavebeenpossibleviahumaneﬀort.\\nWehopethatthedesignprinciplesdescribedinthischapterwillleadtomany\\nothersimilarsuccesses.\\n4 4 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fa48f97d-aa80-4414-a931-e76602f79a79', embedding=None, metadata={'page_label': '458', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 2\\nA p p l i cat i on s\\nInthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom-\\nputervision,speechrecognition,naturallanguageprocessing,andotherapplication\\nareasofcommercialinterest.Webeginbydiscussingthelargescaleneuralnetwork\\nimplementationsrequiredformostseriousAIapplications.Next,wereviewseveral\\nspeciﬁcapplicationareasthatdeeplearninghasbeenusedtosolve.\\xa0Whileone\\ngoalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad\\nvarietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision\\ntasksrequireprocessingalargenumberofinputfeatures(pixels)perexample.\\nLanguagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe\\nvocabulary)perinputfeature.\\n12. 1 L arge- S c a l e D eep L earni n g\\nDeeplearningisbasedonthephilosophyofconnectionism: whileanindividual\\nbiologicalneuronoranindividualfeatureinamachinelearningmodelisnot\\nintelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan\\nexhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe\\nnumberofneuronsmustbe l a r g e.Oneofthekeyfactorsresponsibleforthe\\nimprovementinneuralnetwork’saccuracyandtheimprovementofthecomplexity\\noftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein\\nthesizeofthenetworksweuse.Aswesawinsection,networksizeshave 1.2.3\\ngrownexponentiallyforthepastthreedecades,yetartiﬁcialneuralnetworksare\\nonlyaslargeasthenervoussystemsofinsects.\\nBecausethesizeofneuralnetworksisofparamountimportance,deeplearning\\n443', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da3387d0-3860-414e-a4dd-3de02db5927f', embedding=None, metadata={'page_label': '459', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nrequireshighperformancehardwareandsoftwareinfrastructure.\\n12.1.1FastCPUImplementations\\nTraditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine.\\nToday,thisapproachisgenerallyconsideredinsuﬃcient.WenowmostlyuseGPU\\ncomputingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto\\ntheseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould\\nnotmanagethehighcomputational workloadrequiredbyneuralnetworks.\\nAdescriptionofhowtoimplementeﬃcientnumericalCPUcodeisbeyond\\nthescopeofthisbook,butweemphasizeherethatcarefulimplementation for\\nspeciﬁcCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest\\nCPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingﬁxed-point\\narithmeticratherthanﬂoating-pointarithmetic.Bycreatingacarefullytunedﬁxed-\\npointimplementation,Vanhoucke2011 e t a l .()obtainedathreefoldspeedupover\\nastrongﬂoating-pointsystem.EachnewmodelofCPUhasdiﬀerentperformance\\ncharacteristics,sosometimesﬂoating-pointimplementations canbefastertoo.\\nTheimportantprincipleisthatcarefulspecializationofnumericalcomputation\\nroutinescanyieldalargepayoﬀ.Otherstrategies,besideschoosingwhethertouse\\nﬁxedorﬂoatingpoint,includeoptimizingdatastructurestoavoidcachemisses\\nandusingvectorinstructions.Manymachinelearningresearchersneglectthese\\nimplementationdetails,butwhentheperformanceofanimplementation restricts\\nthesizeofthemodel,theaccuracyofthemodelsuﬀers.\\n12.1.2GPUImplementations\\nMostmodernneuralnetworkimplementationsarebasedongraphicsprocessing\\nunits.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents\\nthatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor\\nvideogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The\\nperformancecharacteristicsneededforgoodvideogamingsystemsturnouttobe\\nbeneﬁcialforneuralnetworksaswell.\\nVideogamerenderingrequiresperformingmanyoperationsinparallelquickly.\\nModelsof\\xa0characters\\xa0and environments\\xa0arespeciﬁed\\xa0intermsof\\xa0listsof\\xa03-D\\ncoordinatesofvertices.Graphicscardsmustperformmatrixmultiplication and\\ndivisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-D\\non-screencoordinates.Thegraphicscardmustthenperformmanycomputations\\nateachpixelinparalleltodeterminethecolorofeachpixel.\\xa0Inbothcases,the\\n4 4 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4eb5870e-dea5-4728-8eeb-33d3e19f4779', embedding=None, metadata={'page_label': '460', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\ncomputations arefairlysimpleanddonotinvolvemuchbranchingcomparedto\\nthecomputational workloadthataCPUusuallyencounters.Forexample,each\\nvertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisno\\nneedtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiply\\nby.Thecomputations arealsoentirelyindependentofeachother,andthusmay\\nbeparallelizedeasily.Thecomputations alsoinvolveprocessingmassivebuﬀersof\\nmemory,containingbitmapsdescribingthetexture(colorpattern)ofeachobject\\ntoberendered.Together,thisresultsingraphicscardshavingbeendesignedto\\nhaveahighdegreeofparallelismandhighmemorybandwidth,atthecostof\\nhavingalowerclockspeedandlessbranchingcapabilityrelativetotraditional\\nCPUs.\\nNeuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe\\nreal-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve\\nlargeandnumerousbuﬀersofparameters,activationvalues,andgradientvalues,\\neachofwhichmustbecompletelyupdatedduringeverystepoftraining.These\\nbuﬀersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer\\nsothememorybandwidthofthesystemoftenbecomestheratelimitingfactor.\\nGPUsoﬀeracompellingadvantageoverCPUsduetotheirhighmemorybandwidth.\\nNeuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingor\\nsophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneural\\nnetworkscanbedividedintomultipleindividual“neurons”thatcanbeprocessed\\nindependentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasily\\nbeneﬁtfromtheparallelismofGPUcomputing.\\nGPUhardwarewasoriginallysospecializedthatitcouldonlybeusedfor\\ngraphicstasks.Overtime,GPUhardwarebecamemoreﬂexible,allowingcustom\\nsubroutinestobeusedtotransformthecoordinatesofverticesorassigncolorsto\\npixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybe\\nbasedonarenderingtask.TheseGPUscouldbeusedforscientiﬁccomputingby\\nwritingtheoutputofacomputationtoabuﬀerofpixelvalues.Steinkrau e t a l .\\n()implemented atwo-layerfullyconnectedneuralnetworkonaGPUand 2005\\nreportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter,\\nChellapilla 2006 e t a l .()demonstratedthatthesametechniquecouldbeusedto\\nacceleratesupervisedconvolutionalnetworks.\\nThepopularityofgraphicscardsforneuralnetworktrainingexplodedafter\\ntheadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary\\ncode,notjustrenderingsubroutines.\\xa0NVIDIA’sCUDAprogramming language\\nprovidedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir\\nrelativelyconvenientprogramming model,massiveparallelism,andhighmemory\\n4 4 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc028b86-4d3b-40d5-bb33-0e15d0fd0beb', embedding=None, metadata={'page_label': '461', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nbandwidth,GP-GPUsnowoﬀeranidealplatformforneuralnetworkprogramming.\\nThisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame\\navailable(,; ,). Raina e t a l .2009Ciresan e t a l .2010\\nWritingeﬃcientcodeforGP-GPUsremainsadiﬃculttaskbestlefttospe-\\ncialists.\\xa0ThetechniquesrequiredtoobtaingoodperformanceonGPUarevery\\ndiﬀerentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusually\\ndesignedtoreadinformationfromthecacheasmuchaspossible.OnGPU,most\\nwritablememorylocationsarenotcached,soitcanactuallybefastertocompute\\nthesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory.\\nGPUcodeisalsoinherentlymulti-threaded andthediﬀerentthreadsmustbe\\ncoordinatedwitheachothercarefully.Forexample,memoryoperationsarefasterif\\ntheycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscan\\neachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememory\\ntransaction.DiﬀerentmodelsofGPUsareabletocoalescediﬀerentkindsofread\\norwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamong n\\nthreads,thread iaccessesbyte i+ jofmemory,and jisamultipleofsomepower\\nof2.\\xa0TheexactspeciﬁcationsdiﬀerbetweenmodelsofGPU.Anothercommon\\nconsiderationforGPUsismakingsurethateachthreadinagroupexecutesthe\\nsameinstructionsimultaneously.Thismeansthatbranchingcanbediﬃculton\\nGPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarp\\nexecutesthesameinstructionduringeachcycle,soifdiﬀerentthreadswithinthe\\nsamewarpneedtoexecutediﬀerentcodepaths,thesediﬀerentcodepathsmust\\nbetraversedsequentiallyratherthaninparallel.\\nDuetothediﬃcultyofwritinghighperformanceGPUcode,researchersshould\\nstructuretheirworkﬂowtoavoidneedingtowritenewGPUcodeinordertotest\\nnewmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary\\nofhighperformanceoperationslikeconvolutionandmatrixmultiplication, then\\nspecifyingmodelsintermsofcallstothislibraryofoperations.Forexample,the\\nmachinelearninglibraryPylearn2(Goodfellow2013c e t a l .,)speciﬁesallofits\\nmachinelearningalgorithmsintermsofcallstoTheano( ,; Bergstra e t a l .2010\\nBastien2012 e t a l .,)andcuda-convnet(,),whichprovidethese Krizhevsky2010\\nhigh-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor\\nmultiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon\\neitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself.\\nOtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l .\\n2011b)providesimilarfeatures.\\n4 4 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd7c7123-c70f-4d22-9423-ebffaac2d283', embedding=None, metadata={'page_label': '462', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\n12.1.3Large-ScaleDistributedImplementations\\nInmanycases,thecomputational resourcesavailableonasinglemachineare\\ninsuﬃcient.Wethereforewanttodistributetheworkloadoftrainingandinference\\nacrossmanymachines.\\nDistributinginferenceissimple,becauseeachinputexamplewewanttoprocess\\ncanberunbyaseparatemachine.Thisisknownas .dataparallelism\\nItisalsopossibletogetmodelparallelism,wheremultiplemachineswork\\ntogetheronasingledatapoint,witheachmachinerunningadiﬀerentpartofthe\\nmodel.Thisisfeasibleforbothinferenceandtraining.\\nDataparallelismduringtrainingissomewhatharder.Wecanincreasethesize\\noftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear\\nreturnsintermsofoptimization performance.Itwouldbebettertoallowmultiple\\nmachinestocomputemultiplegradientdescentstepsinparallel.Unfortunately,\\nthestandarddeﬁnitionofgradientdescentisasacompletelysequentialalgorithm:\\nthegradientatstepisafunctionoftheparametersproducedbystep. t t−1\\nThiscanbesolvedusingasynchronousstochasticgradientdescent(Ben-\\ngio2001Recht2011 e t a l .,; e t a l .,).Inthisapproach,severalprocessorcoresshare\\nthememoryrepresentingtheparameters.Eachcorereadsparameterswithouta\\nlock,thencomputesagradient,thenincrementstheparameterswithoutalock.\\nThisreducestheaverageamountofimprovementthateachgradientdescentstep\\nyields,becausesomeofthecoresoverwriteeachother’sprogress,buttheincreased\\nrateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean\\ne t a l .()pioneeredthemulti-machineimplementationofthislock-freeapproach 2012\\ntogradientdescent,wheretheparametersaremanagedbyaparameterserver\\nratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent\\nremainstheprimarystrategyfortraininglargedeepnetworksandisusedby\\nmostmajordeeplearninggroupsinindustry( ,; Chilimbi e t a l .2014Wu e t a l .,\\n2015).Academicdeeplearningresearcherstypicallycannotaﬀordthesamescale\\nofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuild\\ndistributednetworkswithrelativelylow-costhardwareavailableintheuniversity\\nsetting( ,). Coates e t a l .2013\\n12.1.4ModelCompression\\nInmanycommercialapplications,itismuchmoreimportantthatthetimeand\\nmemorycostofrunninginferenceinamachinelearningmodelbelowthanthat\\nthetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire\\n4 4 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28a12918-f307-404f-b13e-a04b96bd4ae4', embedding=None, metadata={'page_label': '463', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\npersonalization,itispossibletotrainamodelonce,thendeployittobeusedby\\nbillionsofusers.Inmanycases,theenduserismoreresource-constrainedthan\\nthedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha\\npowerfulcomputercluster,thendeployitonmobilephones.\\nAkeystrategyforreducingthecostofinferenceismodelcompression(Bu-\\nciluˇa2006 e t a l .,).Thebasicideaofmodelcompressionistoreplacetheoriginal,\\nexpensivemodelwithasmallermodelthatrequireslessmemoryandruntimeto\\nstoreandevaluate.\\nModelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven\\nprimarilybyaneedtopreventoverﬁtting.Inmostcases,themodelwiththe\\nlowestgeneralization errorisanensembleofseveralindependentlytrainedmodels.\\nEvaluatingall nensemblemembersisexpensive.Sometimes,evenasinglemodel\\ngeneralizesbetterifitislarge(forexample,ifitisregularizedwithdropout).\\nTheselargemodelslearnsomefunction f(x),butdosousingmanymore\\nparametersthanarenecessaryforthetask.Theirsizeisnecessaryonlydueto\\nthelimitednumberoftrainingexamples.Assoonaswehaveﬁtthisfunction\\nf(x),wecangenerateatrainingsetcontaininginﬁnitelymanyexamples,simply\\nbyapplying ftorandomlysampledpointsx.Wethentrainthenew,smaller,\\nmodeltomatch f(x)onthesepoints.Inordertomosteﬃcientlyusethecapacity\\nofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistribution\\nresemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscan\\nbedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerative\\nmodeltrainedontheoriginaltrainingset.\\nAlternatively,onecantrainthesmallermodelonlyontheoriginaltraining\\npoints,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior\\ndistributionovertheincorrectclasses(Hinton20142015 e t a l .,,).\\n12.1.5DynamicStructure\\nOnestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystems\\nthathavedynamicstructureinthegraphdescribingthecomputationneeded\\ntoprocessaninput.Dataprocessingsystemscandynamicallydeterminewhich\\nsubsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural\\nnetworkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset\\noffeatures(hiddenunits)tocomputegiveninformationfromtheinput.This\\nformofdynamicstructureinsideneuralnetworksissometimescalledconditional\\ncomputation(,; ,).\\xa0Sincemanycomponentsof Bengio2013Bengio e t a l .2013b\\nthearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the\\n4 4 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73ebb982-82f3-46fb-b96c-816fa7e56179', embedding=None, metadata={'page_label': '464', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nsystemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded.\\nDynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied\\ngenerallythroughoutthesoftwareengineeringdiscipline.\\xa0Thesimplestversions\\nofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich\\nsubsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should\\nbeappliedtoaparticularinput.\\nAvenerablestrategyforacceleratinginferenceinaclassiﬁeristouseacascade\\nofclassiﬁers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe\\npresenceofarareobject(orevent).Toknowforsurethattheobjectispresent,\\nwemustuseasophisticatedclassiﬁerwithhighcapacity,thatisexpensivetorun.\\nHowever,becausetheobjectisrare,wecanusuallyusemuchlesscomputation\\ntorejectinputsasnotcontainingtheobject.Inthesesituations,wecantrain\\nasequenceofclassiﬁers.Theﬁrstclassiﬁersinthesequencehavelowcapacity,\\nandaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure\\nwedonotwronglyrejectaninputwhentheobjectispresent.Theﬁnalclassiﬁer\\nistrainedtohavehighprecision.Attesttime,weruninferencebyrunningthe\\nclassiﬁersinasequence,abandoninganyexampleassoonasanyoneelementin\\nthecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith\\nhighconﬁdence,usingahighcapacitymodel,butdoesnotforceustopaythecost\\noffullinferenceforeveryexample.Therearetwodiﬀerentwaysthatthecascade\\ncanachievehighcapacity.Onewayistomakethelatermembersofthecascade\\nindividuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas\\nhighcapacity,becausesomeofitsindividualmembersdo.\\xa0Itisalsopossibleto\\nmakeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem\\nasawholehashighcapacityduetothecombinationofmanysmallmodels.Viola\\nandJones2001()usedacascadeofboosteddecisiontreestoimplementafastand\\nrobustfacedetectorsuitableforuseinhandhelddigitalcameras.Theirclassiﬁer\\nlocalizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindows\\nareexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascades\\nusestheearliermodelstoimplementasortofhardattentionmechanism:the\\nearlymembersofthecascadelocalizeanobjectandlatermembersofthecascade\\nperformfurtherprocessinggiventhelocationoftheobject.Forexample,Google\\ntranscribesaddressnumbersfromStreetViewimageryusingatwo-stepcascade\\nthatﬁrstlocatestheaddressnumberwithonemachinelearningmodelandthen\\ntranscribesitwithanother(Goodfellow2014d e t a l .,).\\nDecisiontreesthemselvesareanexampleofdynamicstructure,becauseeach\\nnodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput.\\nAsimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure\\n4 4 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5f7d3120-6530-4c57-949f-eca57a9e49a1', embedding=None, metadata={'page_label': '465', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nistotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe\\nsplittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992\\ndonewiththeprimarygoalofacceleratinginferencecomputations.\\nInthesamespirit,onecanuseaneuralnetwork,calledthegatertoselect\\nwhichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput,\\ngiventhecurrentinput.Theﬁrstversionofthisideaiscalledthemixtureof\\nexperts(Nowlan1990Jacobs 1991 ,; e t a l .,),inwhichthegateroutputsaset\\nofprobabilities orweights(obtainedviaasoftmaxnonlinearity), oneperexpert,\\nandtheﬁnaloutputisobtainedbytheweightedcombinationoftheoutputof\\ntheexperts.Inthatcase,\\xa0theuseofthegaterdoesnotoﬀerareductionin\\ncomputational cost,butifasingleexpertischosenbythegaterforeachexample,\\nweobtainthehardmixtureofexperts( ,,),which Collobert e t a l .20012002\\ncanconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell\\nwhenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial. But\\nwhenwewanttoselectdiﬀerentsubsetsofunitsorparameters,itisnotpossible\\ntousea“softswitch”becauseitrequiresenumerating(andcomputingoutputsfor)\\nallthegaterconﬁgurations. Todealwiththisproblem,severalapproacheshave\\nbeenexploredtotraincombinatorialgaters. ()experimentwith Bengio e t a l .2013b\\nseveralestimatorsofthegradientonthegatingprobabilities, whileBacon e t a l .\\n()and ()usereinforcementlearningtechniques(policy 2015Bengio e t a l .2015a\\ngradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget\\nanactualreductionincomputational costwithoutimpactingnegativelyonthe\\nqualityoftheapproximation.\\nAnother\\xa0kindof\\xa0dynamicstructure\\xa0isa\\xa0switch,\\xa0where\\xa0ahidden\\xa0unit can\\nreceiveinputfromdiﬀerentunitsdependingonthecontext.Thisdynamicrouting\\napproachcanbeinterpretedasanattentionmechanism( ,). Olshausen e t a l .1993\\nSofar,theuseofahardswitchhasnotproveneﬀectiveonlarge-scaleapplications.\\nContemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs,\\nandthusdonotachieveallofthepossiblecomputational beneﬁtsofdynamic\\nstructure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1\\nOnemajorobstacletousingdynamicallystructuredsystemsisthedecreased\\ndegreeofparallelismthatresultsfromthesystemfollowingdiﬀerentcodebranches\\nfordiﬀerentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed\\nasmatrixmultiplication orbatchconvolutiononaminibatchofexamples.We\\ncanwritemorespecializedsub-routinesthatconvolveeachexamplewithdiﬀerent\\nkernelsormultiplyeachrowofadesignmatrixbyadiﬀerentsetofcolumns\\nofweights.Unfortunately,\\xa0thesemorespecializedsubroutinesarediﬃcultto\\nimplementeﬃciently.CPUimplementations willbeslowduetothelackofcache\\n4 5 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aea5fec1-a3b7-4100-9977-c9e3a555054c', embedding=None, metadata={'page_label': '466', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\ncoherenceandGPUimplementations willbeslowduetothelackofcoalesced\\nmemorytransactionsandtheneedtoserializewarpswhenmembersofawarptake\\ndiﬀerentbranches.Insomecases,theseissuescanbemitigatedbypartitioningthe\\nexamplesintogroupsthatalltakethesamebranch,andprocessingthesegroups\\nofexamplessimultaneously.\\xa0Thiscanbeanacceptablestrategyforminimizing\\nthetimerequiredtoprocessaﬁxedamountofexamplesinanoﬄinesetting.In\\nareal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioning\\ntheworkloadcanresultinload-balancing issues.Forexample,ifweassignone\\nmachinetoprocesstheﬁrststepinacascadeandanothermachinetoprocess\\nthelaststepinacascade,thentheﬁrstwilltendtobeoverloadedandthelast\\nwilltendtobeunderloaded. Similarissuesariseifeachmachineisassignedto\\nimplementdiﬀerentnodesofaneuraldecisiontree.\\n12.1.6SpecializedHardwareImplementationsofDeepNetworks\\nSincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked\\nonspecializedhardwareimplementations thatcouldspeeduptrainingand/or\\ninferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof\\nspecializedhardwarefordeepnetworks( ,;, LindseyandLindblad1994Beiu e t a l .\\n2003MisraandSaha2010 ; ,).\\nDiﬀerentformsofspecializedhardware(GrafandJackel1989Meadand ,;\\nIsmail2012Kim2009Pham2012Chen 2014ab ,; e t a l .,; e t a l .,; e t a l .,,)have\\nbeendevelopedoverthelastdecades,eitherwithASICs(application-speciﬁcinte-\\ngratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers),\\nanalog(GrafandJackel1989MeadandIsmail2012 ,; ,)(basedonphysicalimple-\\nmentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations\\n(combiningdigitalandanalogcomponents).InrecentyearsmoreﬂexibleFPGA\\n(ﬁeldprogrammable gatedarray)implementations(wheretheparticularsofthe\\ncircuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped.\\nThoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs\\nandGPUs)typicallyuse32or64bitsofprecisiontorepresentﬂoatingpoint\\nnumbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at\\nleastatinferencetime(HoltandBaker1991HoliandHwang1993Presley ,; ,;\\nandHaggard1994SimardandGraf1994Wawrzynek 1996Savich ,; ,; e t a l .,; e t a l .,\\n2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning\\nhasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster\\nhardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent\\nresearchonspecializedhardwarefordeepnetworksisthattherateofprogressof\\nasingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin\\n4 5 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='198e742e-7ac8-4015-8b9d-4e259574e4b8', embedding=None, metadata={'page_label': '467', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\ncomputingspeedhavecomefromparallelization acrosscores(eitherinCPUsor\\nGPUs).Thisisverydiﬀerentfromthesituationofthe1990s(thepreviousneural\\nnetworkera)wherethehardwareimplementations ofneuralnetworks(whichmight\\ntaketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith\\ntherapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized\\nhardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware\\ndesignsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor\\ngeneral-public applicationsofdeeplearning(e.g.,withspeech,computervisionor\\nnaturallanguage).\\nRecentworkonlow-precisionimplementationsofbackprop-based neuralnets\\n(Vanhoucke2011Courbariaux 2015Gupta2015 e t a l .,; e t a l .,; e t a l .,)suggests\\nthatbetween8and16bitsofprecisioncansuﬃceforusingortrainingdeep\\nneuralnetworkswithback-propagation.\\xa0Whatisclearisthatmoreprecisionis\\nrequiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic\\nﬁxedpointrepresentationofnumberscanbeusedtoreducehowmanybitsare\\nrequiredpernumber.Traditionalﬁxedpointnumbersarerestrictedtoaﬁxed\\nrange(whichcorrespondstoagivenexponentinaﬂoatingpointrepresentation).\\nDynamicﬁxedpointrepresentationssharethatrangeamongasetofnumbers\\n(suchasalltheweightsinonelayer).Usingﬁxedpointratherthanﬂoatingpoint\\nrepresentationsandusinglessbitspernumberreducesthehardwaresurfacearea,\\npowerrequirementsandcomputingtimeneededforperformingmultiplications,\\nandmultiplications arethemostdemandingoftheoperationsneededtouseor\\ntrainamoderndeepnetworkwithbackprop.\\n12. 2 C om p u t er V i s i on\\nComputervisionhastraditionallybeenoneofthemostactiveresearchareasfor\\ndeeplearningapplications,becausevisionisataskthatiseﬀortlessforhumans\\nandmanyanimalsbutchallengingforcomputers( ,).Manyof Ballard e t a l .1983\\nthemostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms\\nofobjectrecognitionoropticalcharacterrecognition.\\nComputervisionisaverybroadﬁeldencompassingawidevarietyofways\\nofprocessingimages,andanamazingdiversityofapplications.\\xa0Applicationsof\\ncomputervisionrangefromreproducinghumanvisualabilities,suchasrecognizing\\nfaces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleof\\nthelattercategory,onerecentcomputervisionapplicationistorecognizesound\\nwavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davis e t a l .\\n2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch\\n4 5 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e42d1364-be11-4a51-91e4-9c2fb4085a0e', embedding=None, metadata={'page_label': '468', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nexoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut\\nratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep\\nlearningforcomputervisionisusedforobjectrecognitionordetectionofsome\\nform,whetherthismeansreportingwhichobjectispresentinanimage,annotating\\nanimagewithboundingboxesaroundeachobject,transcribingasequenceof\\nsymbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe\\nobjectitbelongsto.Becausegenerativemodelinghasbeenaguidingprinciple\\nofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis\\nusingdeepmodels.Whileimagesynthesisisusuallynotconsidereda e x nihil o\\ncomputervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor\\nimagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor\\nremovingobjectsfromimages.\\n12.2.1Preprocessing\\nManyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal\\ninputcomesinaformthatisdiﬃcultformanydeeplearningarchitecturesto\\nrepresent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre-\\nprocessing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe\\nsame,reasonablerange,like[0,1]or[-1,1].\\xa0Mixingimagesthatliein[0,1]with\\nimagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave\\nthesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many\\ncomputervisionarchitectures requireimagesofastandardsize,soimagesmustbe\\ncroppedorscaledtoﬁtthatsize.Eventhisrescalingisnotalwaysstrictlynecessary.\\nSomeconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjust\\nthesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibel e t a l .,\\n1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomatically\\nscalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan\\nimage( ,). Hadsell e t a l .2007\\nDatasetaugmentation maybeseenasawayofpreprocessingthetrainingset\\nonly.Datasetaugmentationisanexcellentwaytoreducethegeneralization error\\nofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow\\nthemodelmanydiﬀerentversionsofthesameinput(forexample,thesameimage\\ncroppedatslightlydiﬀerentlocations)andhavethediﬀerentinstantiationsofthe\\nmodelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan\\nensembleapproach,andhelpstoreducegeneralization error.\\nOtherkindsofpreprocessingareappliedtoboththetrainandthetestsetwith\\nthegoalofputtingeachexampleintoamorecanonicalforminordertoreducethe\\namountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof\\n4 5 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d32ec0f8-1597-4926-9fac-15d6ccbd1762', embedding=None, metadata={'page_label': '469', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nvariationinthedatacanbothreducegeneralization errorandreducethesizeof\\nthemodelneededtoﬁtthetrainingset.Simplertasksmaybesolvedbysmaller\\nmodels,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessing\\nofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinput\\ndatathatiseasyforahumandesignertodescribeandthatthehumandesigner\\nisconﬁdenthasnorelevancetothetask.Whentrainingwithlargedatasetsand\\nlargemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojust\\nletthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.For\\nexample,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessing\\nstep:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevsky\\ne t a l .,).2012\\n12.2.1.1ContrastNormalization\\nOneofthemostobvioussourcesofvariationthatcanbesafelyremoved\\xa0for\\nmanytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe\\nmagnitudeofthediﬀerencebetweenthebrightandthedarkpixelsinanimage.\\nTherearemanywaysofquantifyingthecontrastofanimage.Inthecontextof\\ndeeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan\\nimageorregionofanimage.Supposewehaveanimagerepresentedbyatensor\\nX∈ Rr c××3,with X i , j ,1beingtheredintensityatrow iandcolumn j, X i , j ,2giving\\nthegreenintensityand X i , j ,3givingtheblueintensity.Thenthecontrastofthe\\nentireimageisgivenby\\n\\ue076\\ue075\\ue075\\ue0741\\n3 r cr \\ue058\\ni=1c \\ue058\\nj=13 \\ue058\\nk=1\\ue000\\nX i , j , k−¯ X\\ue0012(12.1)\\nwhere ¯ Xisthemeanintensityoftheentireimage:\\n¯ X=1\\n3 r cr \\ue058\\ni=1c \\ue058\\nj=13 \\ue058\\nk=1X i , j , k . (12.2)\\nGlobalcontrastnormalization(GCN)aimstopreventimagesfromhaving\\nvaryingamountsofcontrastbysubtractingthemeanfromeachimage,\\xa0then\\nrescalingitsothatthe\\xa0standarddeviation\\xa0across its\\xa0pixelsis\\xa0equaltosome\\nconstant s.Thisapproachiscomplicatedbythefactthatnoscalingfactorcan\\nchangethecontrastofazero-contrastimage(onewhosepixelsallhaveequal\\nintensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformation\\ncontent.Dividingbythetruestandarddeviationusuallyaccomplishesnothing\\n4 5 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9ca2eeea-a8d2-4f7c-8ff6-3f5c25e8e0ec', embedding=None, metadata={'page_label': '470', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nmorethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This\\nmotivatesintroducingasmall,positiveregularizationparameter λtobiasthe\\nestimateofthestandarddeviation.Alternately,onecanconstrainthedenominator\\ntobeatleast \\ue00f.Givenaninputimage X,GCNproducesanoutputimage X\\ue030,\\ndeﬁnedsuchthat\\nX\\ue030\\ni , j , k= sX i , j , k−¯ X\\nmax\\ue01a\\n\\ue00f ,\\ue071\\nλ+1\\n3 r c\\ue050r\\ni=1\\ue050c\\nj=1\\ue0503\\nk=1\\ue000\\nX i , j , k−¯ X\\ue0012\\ue01b .(12.3)\\nDatasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely\\ntocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe\\ntopracticallyignorethesmalldenominator problembysetting λ= 0andavoid\\ndivisionby0inextremelyrarecasesbysetting \\ue00ftoanextremelylowvaluelike\\n10−8.\\xa0Thisistheapproachusedby ()ontheCIFAR-10 Goodfellow e t a l .2013a\\ndataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant\\nintensity,makingaggressiveregularizationmoreuseful. ()used Coates e t a l .2011\\n\\ue00f λ = 0and = 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10.\\nThescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011\\norchosentomakeeachindividualpixelhavestandarddeviationacrossexamples\\ncloseto1,asdoneby (). Goodfellow e t a l .2013a\\nThestandarddeviationinequationisjustarescalingofthe 12.3 L2norm\\noftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis\\npreferabletodeﬁneGCNintermsofstandarddeviationratherthan L2norm\\nbecausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN\\nbasedonstandarddeviationallowsthesame stobeusedregardlessofimage\\nsize.However,theobservationthatthe L2normisproportionaltothestandard\\ndeviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping\\nexamplestoasphericalshell.Seeﬁgureforanillustration.Thiscanbea 12.1\\nusefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections\\ninspaceratherthanexactlocations.Respondingtomultipledistancesinthe\\nsamedirectionrequireshiddenunitswithcollinearweightvectorsbutdiﬀerent\\nbiases.Suchcoordinationcanbediﬃcultforthelearningalgorithmtodiscover.\\nAdditionally,manyshallowgraphicalmodelshaveproblemswithrepresenting\\nmultipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby\\nreducingeachexampletoadirectionratherthanadirectionandadistance.\\nCounterintuitively,thereisapreprocessingoperationknownasspheringand\\nitisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata\\nlieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave\\n4 5 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1052b7c-e563-4d4f-91e9-3b99785a8cf4', embedding=None, metadata={'page_label': '471', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\n− 1 5 0 0 1 5 . . .\\nx 0− 1 5 .0 0 .1 5 .x 1Rawinput\\n− 1 5 0 0 1 5 . . .\\nx 0GCN, = 10 λ− 2\\n− 1 5 0 0 1 5 . . .\\nx 0GCN, = 0 λ\\nFigure12.1:GCNmapsexamplesontoasphere. ( L e f t )Rawinputdatamayhaveanynorm.\\n( C e n t e r )GCNwith λ= 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuse\\ns= 1and \\ue00f= 10− 8.BecauseweuseGCNbasedonnormalizingthestandarddeviation\\nratherthanthe L2norm,theresultingsphereisnottheunitsphere. ( R i g h t )Regularized\\nGCN,with λ >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe\\nvariationintheirnorm.Weleaveandthesameasbefore. s \\ue00f\\nequalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas\\nsphericalcontours.Spheringismorecommonlyknownas .whitening\\nGlobalcontrastnormalization willoftenfailtohighlightimagefeatureswe\\nwouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalarge\\ndarkareaandalargebrightarea(suchasacitysquarewithhalftheimagein\\ntheshadowofabuilding)thenglobalcontrastnormalization willensurethereisa\\nlargediﬀerencebetweenthebrightnessofthedarkareaandthebrightnessofthe\\nlightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout.\\nThismotivateslocalcontrastnormalization.Localcontrastnormalization\\nensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover\\ntheimageasawhole.Seeﬁgureforacomparisonofglobalandlocalcontrast 12.2\\nnormalization.\\nVariousdeﬁnitionsoflocalcontrastnormalization arepossible.Inallcases,\\nonemodiﬁeseachpixelbysubtractingameanofnearbypixelsanddividingby\\nastandarddeviationofnearbypixels.Insomecases,thisisliterallythemean\\nandstandarddeviationofallpixelsinarectangularwindowcenteredonthe\\npixeltobemodiﬁed(,).Inothercases,thisisaweightedmean Pinto e t a l .2008\\nandweightedstandarddeviationusingGaussianweightscenteredonthepixelto\\nbemodiﬁed.\\xa0Inthecaseofcolorimages,somestrategiesprocessdiﬀerentcolor\\n4 5 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90d31375-d480-4d12-ab0c-10d50f89e8da', embedding=None, metadata={'page_label': '472', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nInputimage GCN LCN\\nFigure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeﬀects\\nofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame\\nscale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local\\ncontrastnormalizationmodiﬁestheimagemuchmore,discardingallregionsofconstant\\nintensity.Thisallowsthemodeltofocusonjusttheedges.Regionsofﬁnetexture,\\nsuchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe\\nnormalizationkernelbeingtoohigh.\\nchannelsseparatelywhileotherscombineinformationfromdiﬀerentchannelsto\\nnormalizeeachpixel( ,). Sermanet e t a l .2012\\nLocalcontrastnormalization canusuallybeimplemented eﬃcientlybyusing\\nseparableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8\\nlocalstandarddeviations,thenusingelement-wisesubtractionandelement-wise\\ndivisionondiﬀerentfeaturemaps.\\nLocalcontrastnormalization isadiﬀerentiable operationandcanalsobeusedas\\nanonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing\\noperationappliedtotheinput.\\nAswithglobalcontrastnormalization, wetypicallyneedtoregularizelocal\\ncontrastnormalization toavoiddivisionbyzero.Infact,becauselocalcontrast\\nnormalization typicallyactsonsmallerwindows,itisevenmoreimportantto\\nregularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly\\nthesameaseachother,andthusmorelikelytohavezerostandarddeviation.\\n4 5 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='290e5341-3497-4d45-a97c-eeb137859711', embedding=None, metadata={'page_label': '473', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\n12.2.1.2DatasetAugmentation\\nAsdescribedinsection,itiseasytoimprovethegeneralization ofaclassiﬁer 7.4\\nbyincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining\\nexamplesthathavebeenmodiﬁedwithtransformationsthatdonotchangethe\\nclass.Objectrecognitionisaclassiﬁcationtaskthatisespeciallyamenableto\\nthisform\\xa0ofdataset\\xa0augmentationbecause\\xa0theclass\\xa0isinvariant\\xa0toso\\xa0many\\ntransformationsandtheinputcanbeeasilytransformedwithmanygeometric\\noperations.Asdescribedbefore,classiﬁerscanbeneﬁtfromrandomtranslations,\\nrotations,andinsomecases,ﬂipsoftheinputtoaugmentthedataset.Inspecialized\\ncomputervisionapplications,moreadvancedtransformationsarecommonlyused\\nfordatasetaugmentation. Theseschemesincluderandomperturbationofthe\\ncolorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012\\ntheinput( ,). LeCun e t a l .1998b\\n12. 3 S p eec h R ec ogn i t i o n\\nThetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken\\nnaturallanguageutteranceintothecorrespondingsequenceofwordsintendedby\\nthespeaker.LetX= (x(1),x(2), . . . ,x() T)denotethesequenceofacousticinput\\nvectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most\\nspeechrecognitionsystemspreprocesstheinputusingspecializedhand-designed\\nfeatures,butsome( ,)deeplearningsystemslearnfeatures JaitlyandHinton2011\\nfromrawinput.Lety= ( y1 , y2 , . . . , y N)denotethetargetoutputsequence(usually\\nasequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR)\\ntaskconsistsofcreatingafunction f∗\\nASRthatcomputesthemostprobablelinguistic\\nsequencegiventheacousticsequence: y X\\nf∗\\nASR() = argmaxX\\nyP∗( = ) y X|X (12.4)\\nwhere P∗isthetrueconditionaldistributionrelatingtheinputsXtothetargets\\ny.\\nSincethe1980sanduntilabout2009–2012,state-of-theartspeechrecognition\\nsystemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture\\nmodels(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand\\nphonemes(,),whileHMMsmodeledthesequenceofphonemes. Bahl e t a l .1987\\nTheGMM-HMM\\xa0modelfamilytreats\\xa0acousticwaveformsasbeinggenerated\\nbythefollowingprocess:\\xa0ﬁrstanHMMgeneratesasequenceofphonemesand\\ndiscretesub-phonemicstates(suchasthebeginning,middle,andendofeach\\n4 5 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d855d16-26ff-4a37-aeaf-6672446bec30', embedding=None, metadata={'page_label': '474', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nphoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof\\naudiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently,\\nspeechrecognitionwasactuallyoneoftheﬁrstareaswhereneuralnetworkswere\\napplied,andnumerousASRsystemsfromthelate1980sandearly1990sused\\nneuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; e t a l .,;\\nFallside1991Bengio19911992Konig 1996 ,; e t a l .,,; e t a l .,).Atthetime,the\\nperformanceofASRbasedonneuralnetsapproximately matchedtheperformance\\nofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved\\n26%phonemeerrorrateontheTIMIT( ,)corpus(with39 Garofolo e t a l .1993\\nphonemestodiscriminatebetween),\\xa0whichwasbetterthanorcomparableto\\nHMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme\\nrecognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition.\\nHowever,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor\\nspeechrecognitionandtheeﬀortthathadbeeninvestedinbuildingthesesystems\\nonthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument\\nforswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both\\nacademicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly\\nfocusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems.\\nLater,with m u c h l a r g e r a nd d e e p e r m o d e l sandmuchlargerdatasets,recognition\\naccuracywasdramatically improvedbyusingneuralnetworkstoreplaceGMMs\\nforthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates).\\nStartingin2009,speechresearchersappliedaformofdeeplearningbasedon\\nunsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas\\nbasedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann\\nmachines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.III\\nTosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild\\ndeepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM.\\nThesenetworkstakespectralacousticrepresentationsinaﬁxed-sizeinputwindow\\n(aroundacenterframe)andpredicttheconditionalprobabilities ofHMMstates\\nforthatcenterframe.Trainingsuchdeepnetworkshelpedtosigniﬁcantlyimprove\\ntherecognitionrateonTIMIT( ,,),bringingdownthe Mohamed e t a l .20092012a\\nphonemeerrorratefromabout26%to20.7%.See ()foran Mohamed e t a l .2012b\\nanalysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphone\\nrecognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamed\\ne t a l .,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup 2011\\nbyworktoexpandthearchitecturefromphonemerecognition(whichiswhat\\nTIMITisfocusedon)tolarge-vocabulary speechrecognition(,), Dahl e t a l .2012\\nwhichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesof\\nwordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually\\n4 5 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1fa3437d-7f41-4ba1-b91c-ccdf906a6d56', embedding=None, metadata={'page_label': '475', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nshiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased\\nontechniquessuchasrectiﬁedlinearunitsanddropout(,; Zeiler e t a l .2013Dahl\\ne t a l .,).\\xa0Bythattime,severalofthemajorspeechgroupsinindustryhad 2013\\nstartedexploringdeeplearningincollaborationwithacademicresearchers.Hinton\\ne t a l .()describethebreakthroughs achievedbythesecollaborators,which 2012a\\narenowdeployedinproductssuchasmobilephones.\\nLater,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo-\\nratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture\\nofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither\\nunnecessaryordidnotbringanysigniﬁcantimprovement.\\nThesebreakthroughs inrecognitionperformanceforworderrorrateinspeech\\nrecognitionwereunprecedented (around30%improvement)andwerefollowinga\\nlongperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwith\\nthetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeof\\ntrainingsets(seeﬁgure2.4ofDengandYu2014()).Thiscreatedarapidshiftin\\nthespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughly\\ntwoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeep\\nneuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearning\\nalgorithmsandarchitectures forASR,whichisstillongoingtoday.\\nOneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainath e t a l .\\n2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier\\ntime-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew\\ntwo-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone\\nlongvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto\\nfrequencyofspectralcomponents.\\nAnotherimportantpush,\\xa0stillongoing,hasbeentowardsend-to-enddeep\\nlearningspeechrecognitionsystemsthatcompletelyremovetheHMM.Theﬁrst\\nmajorbreakthrough inthisdirectioncamefromGraves2013 e t a l .()whotrained\\nadeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to- 10.10\\nphonemealignment,asin ()andintheCTCframework( LeCun e t a l .1998b Graves\\ne t a l .,;2006Graves2012 Graves2013 ,).AdeepRNN( e t a l .,)hasstatevariables\\nfromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth:\\nordinarydepthduetoastackoflayers,anddepthduetotimeunfolding.\\xa0This\\nworkbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.See\\nPascanu2014aChung2014 e t a l .()and e t a l .()forothervariantsofdeepRNNs,\\nappliedinothersettings.\\nAnothercontemporarysteptowardend-to-enddeeplearningASRistoletthe\\nsystemlearnhowto“align”theacoustic-levelinformationwiththephonetic-level\\n4 6 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0c820a25-334b-4992-8bc0-9b4bc359a743', embedding=None, metadata={'page_label': '476', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\ninformation( ,;,). Chorowski e t a l .2014Lu e t a l .2015\\n12. 4 Nat u ra l L an gu a g e Pro c es s i n g\\nNaturallanguageprocessing(NLP)istheuseofhumanlanguages,suchas\\nEnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit\\nspecializedlanguagesdesignedtoalloweﬃcientandunambiguousparsingbysimple\\nprograms.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal\\ndescription.\\xa0Naturallanguageprocessingincludesapplicationssuchasmachine\\ntranslation,inwhichthelearnermustreadasentenceinonehumanlanguageand\\nemitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications\\narebasedonlanguagemodelsthatdeﬁneaprobabilitydistributionoversequences\\nofwords,charactersorbytesinanaturallanguage.\\nAswiththeotherapplicationsdiscussedinthischapter,verygenericneural\\nnetworktechniquescanbesuccessfullyappliedtonaturallanguageprocessing.\\nHowever,toachieveexcellentperformanceandtoscalewelltolargeapplications,\\nsomedomain-speciﬁcstrategiesbecomeimportant.Tobuildaneﬃcientmodelof\\nnaturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing\\nsequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence\\nofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal\\nnumberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon\\nanextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave\\nbeendevelopedtomakemodelsofsuchaspaceeﬃcient,bothinacomputational\\nandinastatisticalsense.\\n12.4.1-grams n\\nAlanguagemodeldeﬁnesaprobabilitydistributionoversequencesoftokens\\ninanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay\\nbeaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The\\nearliestsuccessfullanguagemodelswerebasedonmodelsofﬁxed-lengthsequences\\noftokenscalled-grams.An-gramisasequenceoftokens. n n n\\nModelsbasedon n-gramsdeﬁnetheconditionalprobabilityofthe n-thtoken\\ngiventhepreceding n−1tokens.Themodelusesproductsoftheseconditional\\ndistributionstodeﬁnetheprobabilitydistributionoverlongersequences:\\nP x(1 , . . . , x τ) = ( P x1 , . . . , x n−1)τ\\ue059\\nt n=P x( t| x t n−+1 , . . . , x t−1) .(12.5)\\n4 6 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e38196da-9edb-403f-8731-b61e7f0d4355', embedding=None, metadata={'page_label': '477', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nThisdecompositionisjustiﬁedbythechainruleofprobability.Theprobability\\ndistributionovertheinitialsequence P( x1 , . . . , x n−1)maybemodeledbyadiﬀerent\\nmodelwithasmallervalueof. n\\nTraining n-grammodelsisstraightforwardbecausethemaximumlikelihood\\nestimatecanbecomputedsimplybycountinghowmanytimeseachpossible n\\ngramoccursinthetrainingset.Modelsbasedon n-gramshavebeenthecore\\nbuildingblockofstatisticallanguagemodelingformanydecades(Jelinekand\\nMercer1980Katz1987ChenandGoodman1999 ,;,; ,).\\nForsmallvaluesof n,modelshaveparticularnames:unigramfor n=1,bigram\\nfor n=2,andtrigramfor n=3.\\xa0ThesenamesderivefromtheLatinpreﬁxesfor\\nthecorrespondingnumbersandtheGreeksuﬃx“-gram”denotingsomethingthat\\niswritten.\\nUsuallywetrainbothan n-grammodelandan n−1 grammodelsimultaneously.\\nThismakesiteasytocompute\\nP x( t| x t n−+1 , . . . , x t−1) =P n( x t n−+1 , . . . , x t)\\nP n−1( x t n−+1 , . . . , x t−1)(12.6)\\nsimplybylookinguptwostoredprobabilities. Forthistoexactlyreproduce\\ninferencein P n,wemustomittheﬁnalcharacterfromeachsequencewhenwe\\ntrain P n−1.\\nAsanexample,wedemonstratehowatrigrammodelcomputestheprobability\\nofthesentence“THEDOGRANAWAY.”Theﬁrstwordsofthesentencecannotbe\\nhandledbythedefaultformulabasedonconditionalprobabilitybecausethereisno\\ncontextatthebeginningofthesentence.Instead,wemustusethemarginalprob-\\nabilityoverwordsatthestartofthesentence.Wethusevaluate P3( T H E D O G R A N).\\nFinally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi-\\ntionaldistribution P( A W A Y D O G R A N | ).Puttingthistogetherwithequation,12.6\\nweobtain:\\nP P ( ) = T H E D O G R A N A W A Y3( ) T H E D O G R A N P3( ) D O G R A N A W A Y /P2( ) D O G R A N .\\n(12.7)\\nAfundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n\\nasestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even\\nthoughthetuple ( x t n−+1 , . . . , x t)mayappearinthetestset.Thiscancausetwo\\ndiﬀerentkindsofcatastrophicoutcomes.When P n−1iszero,theratioisundeﬁned,\\nsothemodeldoesnotevenproduceasensibleoutput.When P n−1isnon-zerobut\\nP niszero,thetestlog-likelihoodis−∞.\\xa0Toavoidsuchcatastrophicoutcomes,\\nmost n-grammodelsemploysomeformofsmoothing.Smoothingtechniques\\n4 6 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4ac4da5e-b3d8-49de-a0eb-84f6c0c9ae01', embedding=None, metadata={'page_label': '478', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nshiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar.\\nSee ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999\\ntechniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenext\\nsymbolvalues.ThismethodcanbejustiﬁedasBayesianinferencewithauniform\\norDirichletprioroverthecountparameters.Anotherverypopularideaistoform\\namixturemodelcontaininghigher-orderandlower-order n-grammodels,withthe\\nhigher-order modelsprovidingmorecapacityandthelower-ordermodelsbeing\\nmorelikelytoavoidcountsofzero.Back-oﬀmethodslook-upthelower-order\\nn-gramsifthefrequencyofthecontext x t−1 , . . . , x t n−+1istoosmalltousethe\\nhigher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing\\ncontexts x t n k −+ , . . . , x t−1,forincreasing k,untilasuﬃcientlyreliableestimateis\\nfound.\\nClassical n-grammodelsareparticularlyvulnerabletothecurseofdimension-\\nality.Thereare|| Vnpossible n-gramsand|| Visoftenverylarge.Evenwitha\\nmassivetrainingsetandmodest n,most n-gramswillnotoccurinthetrainingset.\\nOnewaytoviewaclassical n-grammodelisthatitisperformingnearest-neighbor\\nlookup.Inotherwords,itcanbeviewedasalocalnon-parametric predictor,\\nsimilarto k-nearestneighbors.Thestatisticalproblemsfacingtheseextremely\\nlocalpredictorsaredescribedinsection.Theproblemforalanguagemodel 5.11.2\\nisevenmoreseverethanusual,becauseanytwodiﬀerentwordshavethesamedis-\\ntancefromeachotherinone-hotvectorspace.Itisthusdiﬃculttoleveragemuch\\ninformationfromany“neighbors”—onlytrainingexamplesthatrepeatliterallythe\\nsamecontextareusefulforlocalgeneralization.\\xa0T oovercometheseproblems,a\\nlanguagemodelmustbeabletoshareknowledgebetweenonewordandother\\nsemanticallysimilarwords.\\nToimprovethestatisticaleﬃciencyof n-grammodels,class-basedlanguage\\nmodels(Brown1992NeyandKneser1993Niesler1998 e t a l .,; ,; e t a l .,)introduce\\nthenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat\\nareinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe\\nsetofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith\\notherwords.ThemodelcanthenusewordclassIDsratherthanindividualword\\nIDstorepresentthecontextontherightsideoftheconditioningbar.Composite\\nmodelscombiningword-basedandclass-basedmodelsviamixingorback-oﬀare\\nalsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences\\ninwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis\\nlostinthisrepresentation.\\n4 6 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='98dbf359-dbab-4659-9e5f-0c98bd9dadfe', embedding=None, metadata={'page_label': '479', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\n12.4.2NeuralLanguageModels\\nNeurallanguagemodelsorNLMsare\\xa0aclassoflanguagemodeldesigned\\ntoovercomethecurseofdimensionalityproblemformodelingnaturallanguage\\nsequencesbyusingadistributedrepresentationofwords( ,). Bengio e t a l .2001\\nUnlikeclass-based n-grammodels,neurallanguagemodelsareabletorecognize\\nthattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct\\nfromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone\\nword(anditscontext)andothersimilarwordsandcontexts.Thedistributed\\nrepresentationthemodellearnsforeachwordenablesthissharingbyallowingthe\\nmodeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe\\nworddogandthewordcatmaptorepresentationsthatsharemanyattributes,then\\nsentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadeby\\nthemodelforsentencesthatcontaintheworddog,andvice-versa.Becausethere\\naremanysuchattributes,therearemanywaysinwhichgeneralization canhappen,\\ntransferringinformationfromeachtrainingsentencetoanexponentiallylarge\\nnumberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe\\nmodeltogeneralizetoanumberofsentencesthatisexponentialinthesentence\\nlength.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan\\nexponentialnumberofsimilarsentences.\\nWesometimescallthesewordrepresentationswordembeddings.Inthis\\ninterpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal\\ntothevocabularysize.Thewordrepresentationsembedthosepointsinafeature\\nspaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby\\naone-hotvector,soeverypairofwordsisatEuclideandistance√\\n2fromeach\\nother.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts\\n(oranypairofwordssharingsome“features”learnedbythemodel)arecloseto\\neachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors.\\nFigurezoomsinonspeciﬁcareasofalearnedwordembeddingspacetoshow 12.3\\nhowsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother.\\nNeuralnetworksinotherdomainsalsodeﬁneembeddings.Forexample,a\\nhiddenlayerofaconvolutionalnetworkprovidesan“imageembedding.”Usually\\nNLPpractitioners aremuchmoreinterestedinthisideaofembeddingsbecause\\nnaturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden\\nlayerhasprovidedamorequalitativelydramaticchangeinthewaythedatais\\nrepresented.\\nThebasicideaofusingdistributedrepresentationstoimprovemodelsfor\\nnaturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe\\nusedwithgraphicalmodelsthathavedistributedrepresentationsintheformof\\n4 6 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d6b2544e-428c-4212-a867-ceec974144de', embedding=None, metadata={'page_label': '480', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nmultiplelatentvariables(MnihandHinton2007,).\\n− − − − − 3432302826−14−13−12−11−10−9−8−7−6\\nCanadaEuropeOntario\\nNorthEnglish\\nCanadianUnionAfricanAfrica\\nBritishFrance\\nRussianChina\\nGermanyFrench\\nAssemblyEU JapanIraq\\nSouthEuropean\\n350355360365370375380 . . . . . . .171819202122\\n1995199619971998199920002001\\n200220032004\\n20052006200720082009\\nFigure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural\\nmachinetranslationmodel( ,),zoominginonspeciﬁcareaswhere Bahdanau e t a l .2015\\nsemanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries\\nappearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D\\nforthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher\\ndimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords.\\n12.4.3High-DimensionalOutputs\\nInmanynaturallanguageapplications,weoftenwantourmodelstoproduce\\nwords(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge\\nvocabularies,itcanbeverycomputationally expensivetorepresentanoutput\\ndistributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany\\napplications, Vcontainshundredsofthousandsofwords.Thenaiveapproachto\\nrepresentingsuchadistributionistoapplyanaﬃnetransformationfromahidden\\nrepresentationtotheoutputspace,thenapplythesoftmaxfunction.Suppose\\nwehaveavocabulary Vwithsize|| V.Theweightmatrixdescribingthelinear\\ncomponentofthisaﬃnetransformationisverylarge,becauseitsoutputdimension\\nis|| V.Thisimposesahighmemorycosttorepresentthematrix,andahigh\\ncomputational costtomultiplybyit.Becausethesoftmaxisnormalizedacrossall\\n|| Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattraining\\ntimeaswellastesttime—wecannotcalculateonlythedotproductwiththeweight\\nvectorforthecorrectoutput.Thehighcomputational costsoftheoutputlayer\\nthusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and\\nattesttime(tocomputeprobabilities forallorselectedwords).Forspecialized\\n4 6 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80570558-7b75-49fa-b980-e3db502d2558', embedding=None, metadata={'page_label': '481', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nlossfunctions,thegradientcanbecomputedeﬃciently( ,),but Vincent e t a l .2015\\nthestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes\\nmanydiﬃculties.\\nSupposethathisthetophiddenlayerusedtopredicttheoutputprobabilities\\nˆy.IfweparametrizethetransformationfromhtoˆywithlearnedweightsW\\nandlearnedbiasesb,thentheaﬃne-softmaxoutputlayerperformsthefollowing\\ncomputations:\\na i= b i+\\ue058\\njW i j h j∀∈{ ||} i1 , . . . , V , (12.8)\\nˆ y i=ea i\\n\\ue050|| V\\ni\\ue030=1 eai \\ue030. (12.9)\\nIfhcontains n helementsthentheaboveoperationis O(|| V n h).With n hinthe\\nthousandsand|| Vinthehundredsofthousands,thisoperationdominatesthe\\ncomputationofmostneurallanguagemodels.\\n12.4.3.1UseofaShortList\\nTheﬁrstneurallanguagemodels( ,,)dealtwiththehighcost Bengio e t a l .20012003\\nofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary\\nsizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007 ()and ()\\nbuiltuponthisapproachbysplittingthevocabulary Vintoashortlist Lofmost\\nfrequentwords(handledbytheneuralnet)andatail T= V L\\\\ofmorerarewords\\n(handledbyan n-grammodel).\\xa0Tobeabletocombinethetwopredictions,the\\nneuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext\\nCbelongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput\\nunittoprovideanestimateof P( i C ∈| T ).Theextraoutputcanthenbeusedto\\nachieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V\\nP y i C (= |) =1 i∈ L P y i C, i P i C (= | ∈ − L)(1 (∈| T ))\\n+1 i∈ T P y i C, i P i C (= | ∈ T)(∈| T )(12.10)\\nwhere P( y= i C, i| ∈ L)isprovidedbytheneurallanguagemodeland P( y= i|\\nC, i∈ T) isprovidedbythe n-grammodel.Withslightmodiﬁcation,thisapproach\\ncanalsoworkusinganextraoutputvalueintheneurallanguagemodel’ssoftmax\\nlayer,ratherthanaseparatesigmoidunit.\\nAnobviousdisadvantageoftheshortlistapproachisthatthepotentialgener-\\nalizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent\\n4 6 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3553c08f-27b2-47b3-b807-214992e12658', embedding=None, metadata={'page_label': '482', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nwords,where,arguably,itistheleastuseful.\\xa0Thisdisadvantagehasstimulated\\ntheexplorationofalternativemethodstodealwithhigh-dimensionaloutputs,\\ndescribedbelow.\\n12.4.3.2HierarchicalSoftmax\\nAclassicalapproach(,)toreducingthecomputational burden Goodman2001\\nofhigh-dimensionaloutputlayersoverlargevocabularysets Vistodecompose\\nprobabilities hierarchically .Insteadofnecessitatinganumberofcomputations\\nproportionalto|| V(andalsoproportionaltothenumberofhiddenunits, n h),\\nthe|| Vfactorcanbereducedtoaslowaslog|| V.()and Bengio2002Morinand\\nBengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage\\nmodels.\\nOnecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories\\nofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc.\\nThesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree,\\nthetreehasdepth O(log|| V).\\xa0Theprobabilityofachoosingawordisgivenby\\ntheproductoftheprobabilities ofchoosingthebranchleadingtothatwordat\\neverynodeonapathfromtherootofthetreetotheleafcontainingtheword.\\nFigureillustratesasimpleexample. ()alsodescribe 12.4 MnihandHinton2009\\nhowtousemultiplepathstoidentifyasinglewordinordertobettermodelwords\\nthathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves\\nsummationoverallofthepathsthatleadtothatword.\\nTopredicttheconditionalprobabilities requiredateachnodeofthetree,we\\ntypicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe\\nsamecontext Casinputtoallofthesemodels.Becausethecorrectoutputis\\nencodedinthetrainingset,wecanusesupervisedlearningtotrainthelogistic\\nregressionmodels.Thisistypicallydoneusingastandardcross-entropyloss,\\ncorrespondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions.\\nBecausetheoutputlog-likelihoodcanbecomputedeﬃciently(aslowaslog|| V\\nratherthan|| V),itsgradientsmayalsobecomputedeﬃciently.Thisincludesnot\\nonlythegradientwithrespecttotheoutputparametersbutalsothegradients\\nwithrespecttothehiddenlayeractivations.\\nItispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize\\ntheexpectednumberofcomputations. Toolsfrominformationtheoryspecifyhow\\ntochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To\\ndoso,wecouldstructurethetreesothatthenumberofbitsassociatedwithaword\\nisapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in\\n4 6 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4e1fb9bf-76a0-4b33-b89e-4fa54665e7bf', embedding=None, metadata={'page_label': '483', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\n( 1) ( 0)\\n( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0)\\nw 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7\\nFigure12.4:Illustrationofasimplehierarchyofwordcategories,with8words w 0 , . . . , w 7\\norganizedintoathreelevelhierarchy.Theleavesofthetreerepresentactualspeciﬁcwords.\\nInternalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequence\\nofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0)\\ncontainstheclasses(0 ,0) (0and ,1),whichrespectivelycontainthesetsofwords{ w 0 , w 1}\\nand{ w 2 , w 3},andsimilarlysuper-classcontainstheclasses (1) (1 ,0) (1and ,1),which\\nrespectivelycontainthewords( w 4 , w 5) (and w 6 , w 7).Ifthetreeissuﬃcientlybalanced,\\nthemaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmof\\nthenumberofwords|| V:\\xa0thechoiceofoneoutof|| Vwordscanbeobtainedbydoing\\nO(log|| V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample,\\ncomputingtheprobabilityofaword ycanbedonebymultiplyingthreeprobabilities,\\nassociatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfrom\\ntheroottoanode y.Let bi( y)bethe i-thbinarydecisionwhentraversingthetree\\ntowardsthevalue y.Theprobabilityofsamplinganoutputydecomposesintoaproduct\\nofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheach\\nnodeindexedbythepreﬁxofthesebits.Forexample,node(1 ,0)correspondstothe\\npreﬁx( b 0( w4) = 1 , b1( w4) = 0),andtheprobabilityof w 4canbedecomposedasfollows:\\nP w (= y 4) = ( Pb 0= 1 ,b 1= 0 ,b 2= 0) (12.11)\\n= ( Pb 0= 1) ( Pb 1= 0 |b 0= 1) ( Pb 2= 0 |b 0= 1 ,b 1= 0) .(12.12)\\n4 6 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='983d355d-b8da-4d21-a977-cae63c1a4758', embedding=None, metadata={'page_label': '484', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\npractice,thecomputational savingsaretypicallynotworththeeﬀortbecausethe\\ncomputationoftheoutputprobabilitiesisonlyonepartofthetotalcomputation\\nintheneurallanguagemodel.Forexample,supposethereare lfullyconnected\\nhiddenlayersofwidth n h.Let n bbetheweightedaverageofthenumberofbits\\nrequiredtoidentifyaword,withtheweightinggivenbythefrequencyofthese\\nwords.Inthisexample,thenumberofoperationsneededtocomputethehidden\\nactivationsgrowsasas O( l n2\\nh)whiletheoutputcomputations growas O( n h n b).\\nAslongas n b≤ l n h,wecanreducecomputationmorebyshrinking n hthanby\\nshrinking n b.Indeed, n bisoftensmall.Becausethesizeofthevocabularyrarely\\nexceedsamillionwordsandlog2(106)≈20,itispossibletoreduce n btoabout,20\\nbut n hisoftenmuchlarger,around 103ormore.Ratherthancarefullyoptimizing\\natreewithabranchingfactorof,onecaninsteaddeﬁneatreewithdepthtwo 2\\nandabranchingfactorof\\ue070\\n|| V.Suchatreecorrespondstosimplydeﬁningaset\\nofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepth\\ntwocapturesmostofthecomputational beneﬁtofthehierarchicalstrategy.\\nOnequestionthatremainssomewhatopenishowtobestdeﬁnetheseword\\nclasses,orhowtodeﬁnethewordhierarchyingeneral.Earlyworkusedexisting\\nhierarchies( ,)butthehierarchycanalsobelearned,ideally MorinandBengio2005\\njointlywiththeneurallanguagemodel.Learningthehierarchyisdiﬃcult.Anexact\\noptimization ofthelog-likelihoodappearsintractablebecausethechoiceofaword\\nhierarchyisadiscreteone,notamenabletogradient-basedoptimization. However,\\nonecouldusediscreteoptimization toapproximately optimizethepartitionof\\nwordsintowordclasses.\\nAnimportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa-\\ntionalbeneﬁtsbothattrainingtimeandattesttime,ifattesttimewewantto\\ncomputetheprobabilityofspeciﬁcwords.\\nOfcourse,computingtheprobabilityofall|| Vwordswillremainexpensive\\nevenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthe\\nmostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot\\nprovideaneﬃcientandexactsolutiontothisproblem.\\nAdisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse\\ntestresultsthansampling-basedmethodswewilldescribenext.Thismaybedue\\ntoapoorchoiceofwordclasses.\\n12.4.3.3ImportanceSampling\\nOnewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly\\ncomputingthecontributionofthegradientfromallofthewordsthatdonotappear\\n4 6 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74c17c28-4032-4872-a164-01980d8fedea', embedding=None, metadata={'page_label': '485', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\ninthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe\\nmodel.Itcanbecomputationally costlytoenumerateallofthesewords.Instead,\\nitispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced\\ninequation,thegradientcanbewrittenasfollows: 12.8\\n∂ P y C log(|)\\n∂ θ=∂logsoftmax y()a\\n∂ θ(12.13)\\n=∂\\n∂ θlogea y\\n\\ue050\\ni ea i(12.14)\\n=∂\\n∂ θ( a y−log\\ue058\\niea i) (12.15)\\n=∂ a y\\n∂ θ−\\ue058\\niP y i C (= |)∂ a i\\n∂ θ(12.16)\\nwhereaisthevectorofpre-softmaxactivations(orscores),withoneelement\\nperword.Theﬁrsttermisthepositivephaseterm(pushing a yup)whilethe\\nsecondtermisthenegativephaseterm(pushing a idownforall i,withweight\\nP( i C|).Sincethenegativephasetermisanexpectation,wecanestimateitwith\\naMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself.\\nSamplingfromthemodelrequirescomputing P( i C|)forall iinthevocabulary,\\nwhichispreciselywhatwearetryingtoavoid.\\nInsteadofsamplingfromthemodel,onecansamplefromanotherdistribution,\\ncalledtheproposaldistribution(denoted q),anduseappropriateweightstocorrect\\nforthebiasintroducedbysamplingfromthewrongdistribution(Bengioand\\nSénécal2003BengioandSénécal2008 ,; ,).Thisisanapplicationofamoregeneral\\ntechniquecalledimportancesampling,whichwillbedescribedinmoredetail\\ninsection.Unfortunately,evenexactimportancesamplingisnoteﬃcient 17.2\\nbecauseitrequirescomputingweights p i /q i,where p i= P( i C|),whichcan\\nonlybecomputedifallthescores a iarecomputed.Thesolutionadoptedfor\\nthisapplicationiscalledbiasedimportancesampling,wheretheimportance\\nweightsarenormalizedtosumto1.Whennegativeword n iissampled,the\\nassociatedgradientisweightedby\\nw i=p n i /q n i\\ue050N\\nj=1 p n j /q n j. (12.17)\\nTheseweightsareusedtogivetheappropriateimportancetothe mnegative\\nsamplesfrom qusedtoformtheestimatednegativephasecontributiontothe\\n4 7 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54d18b8f-3a13-42c1-811d-446bdadba3f0', embedding=None, metadata={'page_label': '486', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\ngradient:\\n|| V\\ue058\\ni=1P i C(|)∂ a i\\n∂ θ≈1\\nmm \\ue058\\ni=1w i∂ a n i\\n∂ θ. (12.18)\\nAunigramorabigramdistributionworkswellastheproposaldistribution q.Itis\\neasytoestimatetheparametersofsuchadistributionfromdata.Afterestimating\\ntheparameters,itisalsopossibletosamplefromsuchadistributionveryeﬃciently.\\nImportancesamplingisnotonlyusefulforspeedingupmodelswithlarge\\nsoftmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge\\nsparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1 n\\nchoice.Anexampleisabagofwords.Abagofwordsisasparsevectorv\\nwhere v iindicatesthepresenceorabsenceofword ifromthevocabularyinthe\\ndocument.Alternately, v icanindicatethenumberoftimesthatword iappears.\\nMachinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain\\nforavarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto\\nmaketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight\\nmostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto\\neveryelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa\\ncomputational beneﬁttousingsparseoutputs,becausethemodelmaychooseto\\nmakethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedto\\nbecomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero.\\nDauphin 2011 e t a l .()demonstratedthatsuchmodelscanbeacceleratedusing\\nimportancesampling.Theeﬃcientalgorithmminimizesthelossreconstructionfor\\nthe“positivewords”(thosethatarenon-zerointhetarget)andanequalnumber\\nof“negativewords.”Thenegativewordsarechosenrandomly,usingaheuristicto\\nsamplewordsthataremorelikelytobemistaken.\\xa0Thebiasintroducedbythis\\nheuristicoversamplingcanthenbecorrectedusingimportanceweights.\\nInallofthesecases,thecomputational complexityofgradientestimationfor\\ntheoutputlayerisreducedtobeproportionaltothenumberofnegativesamples\\nratherthanproportionaltothesizeoftheoutputvector.\\n12.4.3.4Noise-ContrastiveEstimationandRankingLoss\\nOtherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa-\\ntionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly\\nexampleistherankinglossproposedbyCollobertandWeston2008a(),which\\nviewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto\\nmakethescoreofthecorrectword a yberankedhighincomparisontotheother\\n4 7 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed563b27-529e-4752-87fb-66cbe6b1d078', embedding=None, metadata={'page_label': '487', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nscores a i.Therankinglossproposedthenis\\nL=\\ue058\\nimax(01 ,− a y+ a i) . (12.19)\\nThegradientiszeroforthe i-thtermifthescoreoftheobservedword, a y,is\\ngreaterthanthescoreofthenegativeword a ibyamarginof1.Oneissuewith\\nthiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities, which\\nareusefulinsomeapplications,includingspeechrecognitionandtextgeneration\\n(includingconditionaltextgenerationtaskssuchastranslation).\\nAmorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise-\\ncontrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6\\nbeensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,;\\nandKavukcuoglu2013,).\\n12.4.4CombiningNeuralLanguageModelswith-grams n\\nAmajoradvantageof n-grammodelsoverneuralnetworksisthat n-grammodels\\nachievehighmodelcapacity(bystoringthefrequenciesofverymanytuples)\\nwhilerequiringverylittlecomputationtoprocessanexample(bylookingup\\nonlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees\\ntoaccessthecounts,thecomputationusedfor n-gramsisalmostindependent\\nofcapacity.Incomparison,doublinganeuralnetwork’snumberofparameters\\ntypicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels\\nthatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle\\nembeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing\\nthecomputationtimeperexample.Someothermodels,suchastiledconvolutional\\nnetworks,canaddparameterswhilereducingthedegreeofparametersharing\\ninordertomaintainthesameamountofcomputation. However,typicalneural\\nnetworklayersbasedonmatrixmultiplication useanamountofcomputation\\nproportionaltothenumberofparameters.\\nOneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble\\nconsistingofaneurallanguagemodelandan n-gramlanguagemodel(Bengio\\ne t a l .,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003\\ntheensemblemembersmakeindependentmistakes.Theﬁeldofensemblelearning\\nprovidesmanywaysofcombiningtheensemblemembers’predictions,including\\nuniformweightingandweightschosenonavalidationset.Mikolov2011a e t a l .()\\nextendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels.\\nItisalsopossibletopairaneuralnetworkwithamaximumentropymodeland\\ntrainbothjointly(Mikolov2011b e t a l .,).Thisapproachcanbeviewedastraining\\n4 7 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d415ec22-db81-4558-9d95-b19a249309a1', embedding=None, metadata={'page_label': '488', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\naneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe\\noutput,andnotconnectedtoanyotherpartofthemodel.Theextrainputsare\\nindicatorsforthepresenceofparticular n-gramsintheinputcontext,sothese\\nvariablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacity\\nishuge—thenewportionofthearchitecturecontainsupto|| s Vnparameters—but\\ntheamountofaddedcomputationneededtoprocessaninputisminimalbecause\\ntheextrainputsareverysparse.\\n12.4.5NeuralMachineTranslation\\nMachinetranslationisthetaskofreadingasentenceinonenaturallanguageand\\nemittingasentencewiththeequivalentmeaninginanotherlanguage.\\xa0Mac hine\\ntranslationsystemsofteninvolvemanycomponents.Atahighlevel,thereis\\noftenonecomponentthatproposesmanycandidatetranslations.Manyofthese\\ntranslationswillnotbegrammaticalduetodiﬀerencesbetweenthelanguages.For\\nexample,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish\\ndirectlytheyyieldphrasessuchas“applered.”Theproposalmechanismsuggests\\nmanyvariantsofthesuggestedtranslation,ideallyincluding“redapple.”Asecond\\ncomponentofthetranslationsystem,alanguagemodel,evaluatestheproposed\\ntranslations,andcanscore“redapple”asbetterthan“applered.”\\nTheearliestuseofneuralnetworksformachinetranslationwastoupgradethe\\nlanguagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenk\\ne t a l .,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshad\\nusedan n-grammodelforthiscomponent.The n-grambasedmodelsusedfor\\nmachinetranslationincludenotjusttraditionalback-oﬀ n-grammodels(Jelinek\\nandMercer1980Katz1987ChenandGoodman1999 ,;,; ,)butalsomaximum\\nentropylanguagemodels(,),inwhichanaﬃne-softmaxlayer Berger e t a l .1996\\npredictsthenextwordgiventhepresenceoffrequent-gramsinthecontext. n\\nTraditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage\\nsentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven\\naninputsentence,itmakessensetoextendthenaturallanguagemodeltobe\\nconditional.Asdescribedinsection,itisstraightforwardtoextendamodel 6.2.1.1\\nthatdeﬁnesamarginaldistributionoversomevariabletodeﬁneaconditional\\ndistributionoverthatvariablegivenacontext C,where Cmightbeasinglevariable\\noralistofvariables. ()beatthestate-of-the-art insomestatistical Devlin e t a l .2014\\nmachinetranslationbenchmarksbyusinganMLPtoscoreaphraset1 ,t2 , . . . ,t k\\ninthetargetlanguagegivenaphrases1 ,s2 , . . . ,s ninthesourcelanguage.The\\nMLPestimates P(t1 ,t2 , . . . ,t k|s1 ,s2 , . . . ,s n).TheestimateformedbythisMLP\\nreplacestheestimateprovidedbyconditional-grammodels. n\\n4 7 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a467dc6b-0788-4928-895d-8ed5c9a015ac', embedding=None, metadata={'page_label': '489', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nD e c ode rO ut put\\xa0ob j e c t \\xa0 ( E ngl i s h\\xa0\\ns e nt e nc e )\\nI nt e r m e di at e , \\xa0 s e m a n t i c \\xa0 r e pr e s e nt a t i o n\\nSourc e \\xa0 ob j e c t \\xa0 ( F r e nc h\\xa0 s e n t e nc e \\xa0 or \\xa0 i m a g e )E nc ode r\\nFigure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface\\nrepresentation(suchasasequenceofwordsoranimage)andasemanticrepresentation.\\nByusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping\\nfromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as\\ntheinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden\\nrepresentationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat\\ntranslatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust\\ntomachinetranslationbutalsotocaptiongenerationfromimages.\\nAdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe\\npreprocessedtobeofﬁxedlength.Tomakethetranslationmoreﬂexible,wewould\\nliketouseamodelthatcanaccommodatevariablelengthinputsandvariable\\nlengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4\\nofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence\\ngivensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4\\nwhentheinputisasequence.Inallcases,onemodelﬁrstreadstheinputsequence\\nandemitsadatastructurethatsummarizestheinputsequence.Wecallthis\\nsummarythe“context” C.Thecontext Cmaybealistofvectors,oritmaybea\\nvectorortensor.Themodelthatreadstheinputtoproduce CmaybeanRNN\\n(,; Cho e t a l .2014aSutskever2014Jean2014 e t a l .,; e t a l .,)oraconvolutional\\nnetwork(KalchbrennerandBlunsom2013,).\\xa0Asecondmodel,usuallyanRNN,\\nthenreadsthecontext Candgeneratesasentenceinthetargetlanguage.This\\ngeneralideaofanencoder-decoderframeworkformachinetranslationisillustrated\\ninﬁgure.12.5\\nInordertogenerateanentiresentenceconditionedonthesourcesentence,the\\nmodelmusthaveawaytorepresenttheentiresourcesentence.\\xa0Earliermodels\\nwereonlyabletorepresentindividualwordsorphrases.\\xa0Fromarepresentation\\n4 7 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99cdad7d-ddcb-4395-94aa-f14a866498c6', embedding=None, metadata={'page_label': '490', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nlearningpointofview,itcanbeusefultolearnarepresentationinwhichsentences\\nthathavethesamemeaninghavesimilarrepresentationsregardlessofwhether\\ntheywerewritteninthesourcelanguageorthetargetlanguage.Thisstrategywas\\nexploredﬁrstusingacombinationofconvolutionsandRNNs(Kalchbrennerand\\nBlunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposed\\ntranslations(,)andforgeneratingtranslatedsentences( Cho e t a l .2014a Sutskever\\ne t a l . e t a l . ,).2014Jean()scaledthesemodelstolargervocabularies. 2014\\n12.4.5.1UsinganAttentionMechanismandAligningPiecesofData\\nα( t − 1 )α( t − 1 )α( ) tα( ) tα( + 1 ) tα( + 1 ) t\\nh( t − 1 )h( t − 1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c\\n× × × × × ×+\\nFigure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanau e t a l .2015\\nessentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverage\\noffeaturevectorsh( ) twithweights α( ) t.Insomeapplications,thefeaturevectorshare\\nhiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The\\nweights α( ) tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval\\n[0 ,1]andareintendedtoconcentratearoundjustoneh( ) tsothattheweightedaverage\\napproximatesreadingthatonespeciﬁctimestepprecisely.Theweights α( ) tareusually\\nproducedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion\\nofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly\\nindexingthedesiredh( ) t,butdirectindexingcannotbetrainedwithgradientdescent.The\\nattentionmechanismbasedonweightedaveragesisasmooth,diﬀerentiableapproximation\\nthatcanbetrainedwithexistingoptimizationalgorithms.\\nUsingaﬁxed-sizerepresentationtocaptureallthesemanticdetailsofavery\\nlongsentenceofsay60wordsisverydiﬃcult.\\xa0Itcanbeachievedbytraininga\\nsuﬃcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho\\ne t a l .()and2014aSutskever2014 e t a l .().However,amoreeﬃcientapproachis\\ntoreadthewholesentenceorparagraph(togetthecontextandthegistofwhat\\n4 7 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4a479d77-7950-454b-85f2-d561a3f28588', embedding=None, metadata={'page_label': '491', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nisbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime\\nfocusingonadiﬀerentpartoftheinputsentenceinordertogatherthesemantic\\ndetailsthatarerequiredtoproducethenextoutputword.\\xa0Thatisexactlythe\\nideathat ()ﬁrstintroduced.Theattentionmechanismused Bahdanau e t a l .2015\\ntofocusonspeciﬁcpartsoftheinputsequenceateachtimestepisillustratedin\\nﬁgure.12.6\\nWecanthinkofanattention-basedsystemashavingthreecomponents:\\n1.Aprocessthat“ r e a d s”rawdata(suchassourcewordsinasourcesentence),\\nandconvertsthemintodistributedrepresentations,withonefeaturevector\\nassociatedwitheachwordposition.\\n2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbe\\nunderstoodasa“”\\xa0containingasequenceoffacts,whichcanbe m e m o r y\\nretrievedlater,notnecessarilyinthesameorder,withouthavingtovisitall\\nofthem.\\n3.Aprocessthat“”thecontentofthememorytosequentiallyperform e x p l o i t s\\natask,ateachtimestephavingtheabilityputattentiononthecontentof\\nonememoryelement(orafew,withadiﬀerentweight).\\nThethirdcomponentgeneratesthetranslatedsentence.\\nWhenwordsinasentencewritteninonelanguagearealignedwithcorrespond-\\ningwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate\\nthecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna\\nkindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe\\nwordembeddingsinanother(Kočiský2014 e t a l .,),yieldingloweralignmenterror\\nratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable.\\nThereisevenearlierworkonlearningcross-lingualwordvectors(Klementiev e t a l .,\\n2012).Manyextensionstothisapproacharepossible.Forexample,moreeﬃcient\\ncross-lingualalignment( ,)allowstrainingonlargerdatasets. Gouws e t a l .2014\\n12.4.6HistoricalPerspective\\nTheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart\\ne t a l .()inoneoftheﬁrstexplorationsofback-propagation, withsymbols 1986a\\ncorrespondingtotheidentityoffamilymembersandtheneuralnetworkcapturing\\ntherelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets\\nsuchas(Colin,Mother,Victoria).\\xa0The ﬁrstlayeroftheneuralnetworklearned\\narepresentationofeachfamilymember.Forexample,\\xa0thefeaturesforColin\\n4 7 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='04f11bf4-6cbf-4f48-b19a-1cb24001c151', embedding=None, metadata={'page_label': '492', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nmightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewas\\nin,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkas\\ncomputinglearnedrulesrelatingtheseattributestogetherinordertoobtainthe\\ndesiredpredictions.Themodelcanthenmakepredictionssuchasinferringwhois\\nthemotherofColin.\\nTheideaofforminganembeddingforasymbolwasextendedtotheideaofan\\nembeddingforawordbyDeerwester1990 e t a l .().Theseembeddingswerelearned\\nusingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks.\\nThehistoryofnaturallanguageprocessingismarkedbytransitionsinthe\\npopularityofdiﬀerentwaysofrepresentingtheinputtothemodel.Following\\nthisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneural\\nnetworkstoNLP( ,; Miikkulainen andDyer1991Schmidhuber1996,)represented\\ntheinputasasequenceofcharacters.\\nBengio2001 e t a l .()returnedthefocustomodelingwordsandintroduced\\nneurallanguagemodels,whichproduceinterpretable wordembeddings.These\\nneuralmodelshavescaledupfromdeﬁningrepresentationsofasmallsetofsymbols\\ninthe1980stomillionsofwords(includingpropernounsandmisspellings)in\\nmodernapplications.Thiscomputational scalingeﬀortledtotheinventionofthe\\ntechniquesdescribedaboveinsection.12.4.3\\nInitially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded\\nimprovedlanguage\\xa0modeling performance( ,).Tothisday, Bengio e t a l .2001\\nnewtechniquescontinuallypushbothcharacter-based models(Sutskever e t a l .,\\n2011)andword-basedmodelsforward,withrecentwork( ,)even Gillick e t a l .2015\\nmodelingindividualbytesofUnicodecharacters.\\nTheideasbehindneurallanguagemodelshavebeenextendedintoseveral\\nnaturallanguageprocessingapplications,suchasparsing(,,; Henderson20032004\\nCollobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc,\\nsometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston,\\n2008aCollobert2011a ; e t a l .,)inwhichthewordembeddingsaresharedacross\\ntasks.\\nTwo-dimensionalvisualizationsofembeddingsbecameapopulartoolforan-\\nalyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality\\nreductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proﬁleappli-\\ncationtovisualizationwordembeddingsbyJosephTurianin2009.\\n4 7 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9594a5d8-22eb-4695-a0c1-9cd7abfeeeb6', embedding=None, metadata={'page_label': '493', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\n12. 5 O t h er A p p l i c a t i o n s\\nInthissectionwecoverafewothertypesofapplicationsofdeeplearningthat\\narediﬀerentfromthestandardobjectrecognition,speechrecognitionandnatural\\nlanguageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III\\nscopeevenfurthertotasksthatremainprimarilyresearchareas.\\n12.5.1RecommenderSystems\\nOneofthemajorfamiliesofapplicationsofmachinelearningintheinformation\\ntechnologysectoristheabilitytomakerecommendations ofitemstopotential\\nusersorcustomers.Twomajortypesofapplicationscanbedistinguished:online\\nadvertisinganditemrecommendations (oftentheserecommendations arestillfor\\nthepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween\\nauserandanitem,eithertopredicttheprobabilityofsomeaction(theuser\\nbuyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which\\nmaydependonthevalueoftheproduct)ifanadisshownorarecommendation is\\nmaderegardingthatproducttothatuser.Theinternetiscurrentlyﬁnancedin\\ngreatpartbyvariousformsofonlineadvertising.\\xa0Therearemajorpartsofthe\\neconomythatrelyononlineshopping.\\xa0CompaniesincludingAmazonandeBay\\nusemachinelearning,includingdeeplearning,fortheirproductrecommendations .\\nSometimes,theitemsarenotproductsthatareactuallyforsale.Examplesinclude\\nselectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviesto\\nwatch,recommendingjokes,recommendingadvicefromexperts,matchingplayers\\nforvideogames,ormatchingpeopleindatingservices.\\nOften,thisassociationproblemishandledlikeasupervisedlearningproblem:\\ngivensomeinformationabouttheitemandabouttheuser,predicttheproxyof\\ninterest(userclicksonad,userentersarating,userclicksona“like”button,user\\nbuysproduct,userspendssomeamountofmoneyontheproduct,userspends\\ntimevisitingapagefortheproduct,etc).Thisoftenendsupbeingeithera\\nregressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilistic\\nclassiﬁcationproblem(predictingtheconditionalprobabilityofsomediscrete\\nevent).\\nTheearlyworkonrecommendersystemsreliedonminimalinformationas\\ninputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the\\nonlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof\\nthetargetvariablefordiﬀerentusersorfordiﬀerentitems.Supposethatuser1\\nanduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and\\n4 7 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61854bbb-87c8-434a-9f0f-58f5a09c9137', embedding=None, metadata={'page_label': '494', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nuser2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong\\ncuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder\\nthenameofcollaborativeﬁltering.Bothnon-parametric approaches(suchas\\nnearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsof\\npreferences)andparametricmethodsarepossible.Parametricmethodsoftenrely\\nonlearningadistributedrepresentation(alsocalledanembedding)foreachuser\\nandforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa\\nsimpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent\\nofstate-of-the-art systems.Thepredictionisobtainedbythedotproductbetween\\ntheuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat\\ndependonlyoneithertheuserIDortheitemID).LetˆRbethematrixcontaining\\nourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwith\\nitemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectively\\nakindofbiasforeachuser(representinghowgrumpyorpositivethatuseris\\ningeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear\\npredictionisthusobtainedasfollows:\\nˆ R u , i= b u+ c i+\\ue058\\njA u , j B j , i . (12.20)\\nTypicallyonewantstominimizethesquarederrorbetweenpredictedratings\\nˆ R u , iandactualratings R u , i.Userembeddingsanditemembeddingscanthenbe\\nconvenientlyvisualizedwhentheyareﬁrstreducedtoalowdimension(twoor\\nthree),ortheycanbeusedtocompareusersoritemsagainsteachother,just\\nlikewordembeddings.\\xa0One waytoobtaintheseembeddingsisbyperforminga\\nsingularvaluedecompositionofthematrixRofactualtargets(suchasratings).\\nThiscorrespondstofactorizingR=UDV\\ue030(oranormalizedvariant)intothe\\nproductoftwofactors,thelowerrankmatricesA=UDandB=V\\ue030.One\\nproblemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway,\\nasiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid\\npayinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum\\nofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient-\\nbasedoptimization. TheSVDandthebilinearpredictionofequation both12.20\\nperformedverywellinthecompetitionfortheNetﬂixprize( , BennettandLanning\\n2007),aimingatpredictingratingsforﬁlms,basedonlyonpreviousratingsby\\nalargesetofanonymoususers.\\xa0Manymachinelearningexpertsparticipatedin\\nthiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof\\nresearchinrecommendersystemsusingadvancedmachinelearningandyielded\\nimprovementsinrecommendersystems.Eventhoughitdidnotwinbyitself,\\nthesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels\\n4 7 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='830e8f7f-c99b-4f2c-8c9c-ca1508865917', embedding=None, metadata={'page_label': '495', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\npresentedbymostofthecompetitors,includingthewinners( ,; Töscher e t a l .2009\\nKoren2009,).\\nBeyondthesebilinearmodelswithdistributedrepresentations,oneoftheﬁrst\\nusesofneuralnetworksforcollaborativeﬁlteringisbasedontheRBMundirected\\nprobabilisticmodel(Salakhutdinov2007 e t a l .,).RBMswereanimportantelement\\noftheensembleofmethodsthatwontheNetﬂixcompetition(Töscher2009 e t a l .,;\\nKoren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix\\nhavealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand\\nMnih2008,).\\nHowever,thereisabasiclimitationofcollaborativeﬁlteringsystems:whena\\nnewitemoranewuserisintroduced,itslackofratinghistorymeansthatthere\\nisnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),or\\nthedegreeofassociationbetween,say,thatnewuserandexistingitems.This\\niscalledtheproblemofcold-startrecommendations .Ageneralwayofsolving\\nthecold-startrecommendation problemistointroduceextrainformationabout\\ntheindividualusersanditems.Forexample,thisextrainformationcouldbeuser\\nproﬁleinformationorfeaturesofeachitem.\\xa0Systems thatusesuchinformation\\narecalledcontent-basedrecommendersystems.Themappingfromarich\\nsetofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthrougha\\ndeeplearningarchitecture( ,; Huang e t a l .2013Elkahky2015 e t a l .,).\\nSpecializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealso\\nbeenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusical\\naudiotracks,formusicrecommendation (vandenOörd2013 e t a l .,).Inthatwork,\\ntheconvolutionalnettakesacousticfeaturesasinputandcomputesanembedding\\nfortheassociatedsong.Thedotproductbetweenthissongembeddingandthe\\nembeddingforauseristhenusedtopredictwhetherauserwilllistentothesong.\\n12.5.1.1ExplorationVersusExploitation\\nWhenmakingrecommendations tousers,anissuearisesthatgoesbeyondordinary\\nsupervisedlearningandintotherealmofreinforcementlearning.Manyrecom-\\nmendationproblemsaremostaccuratelydescribedtheoreticallyascontextual\\nbandits( ,;,).Theissueisthatwhenwe LangfordandZhang2008Lu e t a l .2010\\nusetherecommendation systemtocollectdata,wegetabiasedandincomplete\\nviewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitems\\ntheywererecommendedandnottotheotheritems.\\xa0Inaddition,insomecases\\nwemaynotgetanyinformationonusersforwhomnorecommendation hasbeen\\nmade(forexample,withadauctions,itmaybethatthepriceproposedforan\\n4 8 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9dbdf70-07d7-484d-a2bd-2ec5038546ee', embedding=None, metadata={'page_label': '496', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nadwasbelowaminimumpricethreshold,ordoesnotwintheauction,sothe\\nadisnotshownatall).Moreimportantly,wegetnoinformationaboutwhat\\noutcomewouldhaveresultedfromrecommendinganyoftheotheritems.This\\nwouldbeliketrainingaclassiﬁerbypickingoneclassˆ yforeachtrainingexample\\nx(typicallytheclasswiththehighestprobabilityaccordingtothemodel)and\\nthenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly,\\neachexampleconveyslessinformationthaninthesupervisedcasewherethetrue\\nlabel yisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenot\\ncareful,wecouldendupwithasystemthatcontinuespickingthewrongdecisions\\nevenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhada\\nverylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearn\\naboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearning\\nwhereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement\\nlearningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits\\nscenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly\\nasingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe\\nsensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In\\nthegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight\\nhavebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm\\ncontextualbanditsreferstothecasewheretheactionistakeninthecontextof\\nsomeinputvariablethatcaninformthedecision.Forexample,weatleastknow\\ntheuseridentity,andwewanttopickanitem.Themappingfromcontextto\\nactionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedata\\ndistribution(whichnowdependsontheactionsofthelearner)isacentralresearch\\nissueinthereinforcementlearningandbanditsliterature.\\nReinforcementlearningrequireschoosingatradeoﬀbetweenexplorationand\\nexploitation.Exploitationreferstotakingactionsthatcomefromthecurrent,\\nbestversionofthelearnedpolicy—actionsthatweknowwillachieveahighreward.\\nExplorationreferstotakingactionsspeciﬁcallyinordertoobtainmoretraining\\ndata.Ifweknowthatgivencontextx,action agivesusarewardof1,wedonot\\nknowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent\\npolicyandcontinuetakingaction ainordertoberelativelysureofobtaininga\\nrewardof1.However,wemayalsowanttoexplorebytryingaction a\\ue030.Wedonot\\nknowwhatwillhappenifwetryaction a\\ue030.Wehopetogetarewardof,butwe 2\\nruntheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge. 0\\nExplorationcanbeimplementedinmanyways,rangingfromoccasionally\\ntakingrandomactionsintendedtocovertheentirespaceofpossibleactions,to\\nmodel-basedapproachesthatcomputeachoiceofactionbasedonitsexpected\\nrewardandthemodel’samountofuncertaintyaboutthatreward.\\n4 8 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d399e2b-238d-4f4f-a70e-d550b4e090ff', embedding=None, metadata={'page_label': '497', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nManyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation.\\nOneofthemostprominentfactorsisthetimescaleweareinterestedin.\\xa0Ifthe\\nagenthasonlyashortamountoftimetoaccruereward,thenweprefermore\\nexploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith\\nmoreexplorationsothatfutureactionscanbeplannedmoreeﬀectivelywithmore\\nknowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward\\nmoreexploitation.\\nSupervised\\xa0learninghas\\xa0notradeoﬀ\\xa0between\\xa0explorationand\\xa0exploitation\\nbecausethesupervisionsignalalwaysspeciﬁeswhichoutputiscorrectforeach\\ninput.Thereisnoneedtotryoutdiﬀerentoutputstodetermineifoneisbetter\\nthanthemodel’scurrentoutput—wealwaysknowthatthelabelisthebestoutput.\\nAnotherdiﬃcultyarisinginthecontextofreinforcementlearning,besidesthe\\nexploration-exploitationtrade-oﬀ,isthediﬃcultyofevaluatingandcomparing\\ndiﬀerentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner\\nandtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto\\nevaluatethelearner’sperformanceusingaﬁxedsetoftestsetinputvalues.The\\npolicyitselfdetermineswhichinputswillbeseen. ()present Dudik e t a l .2011\\ntechniquesforevaluatingcontextualbandits.\\n12.5.2KnowledgeRepresentation,ReasoningandQuestionAn-\\nswering\\nDeeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine\\ntranslationandnaturallanguageprocessingduetotheuseofembeddingsfor\\nsymbols( ,)andwords( Rumelhart e t a l .1986a Deerwester1990Bengio e t a l .,; e t a l .,\\n2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords\\nandconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor\\nrelationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor\\nthispurposebutmuchmoreremainstobedonetoimprovethesemoreadvanced\\nrepresentations.\\n12.5.2.1Knowledge,RelationsandQuestionAnswering\\nOneinterestingresearchdirectionisdetermininghowdistributedrepresentations\\ncanbetrainedtocapturetherelationsbetweentwoentities.Theserelations\\nallowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother.\\nInmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairs\\nthatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset\\n4 8 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3575a5f9-d7cd-47a2-871f-29cd471a48b3', embedding=None, metadata={'page_label': '498', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\ndonot.Forexample,wecandeﬁnetherelation“islessthan”onthesetofentities\\n{1 ,2 ,3}bydeﬁningthesetoforderedpairs S={(1 ,2) ,(1 ,3) ,(2 ,3)}.Oncethis\\nrelationisdeﬁned,wecanuseitlikeaverb.Because(1 ,2)∈ S,wesaythat1is\\nlessthan2.Because(2 ,1)\\ue036∈ S,wecannotsaythat2islessthan1.Ofcourse,the\\nentitiesthatarerelatedtooneanotherneednotbenumbers.Wecoulddeﬁnea\\nrelation containingtupleslike(,). is_a_type_of dogmammal\\nInthecontextofAI,wethinkofarelationasasentenceinasyntactically\\nsimpleandhighlystructuredlanguage.Therelationplaystheroleofaverb,\\nwhiletwoargumentstotherelationplaytheroleofitssubjectandobject.These\\nsentencestaketheformofatripletoftokens\\n(subjectverbobject) , , (12.21)\\nwithvalues\\n(entityi ,relation j ,entityk) . (12.22)\\nWecanalsodeﬁneanattribute,aconceptanalogoustoarelation,buttaking\\nonlyoneargument:\\n(entity i ,attribute j) . (12.23)\\nForexample,wecoulddeﬁnethehas_furattribute,andapplyittoentitieslike\\ndog.\\nManyapplicationsrequirerepresentingrelationsandreasoningaboutthem.\\nHowshouldwebestdothiswithinthecontextofneuralnetworks?\\nMachinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations\\nbetweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage.\\nTherearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon\\nstructureforthesedatabasesistherelationaldatabase,whichstoresthissame\\nkindofinformation,\\xa0alb eit\\xa0notformattedasthreetokensentences.Whena\\ndatabaseisintendedtoconveycommonsense knowledgeabouteverydaylifeor\\nexpertknowledgeaboutanapplicationareatoanartiﬁcialintelligencesystem,\\nwecallthedatabaseaknowledgebase.Knowledgebasesrangefromgeneral\\noneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecialized\\nknowledgebases,likeGeneOntology.2Representationsforentitiesandrelations\\ncanbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample\\nandmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes\\ne t a l .,).2013a\\n1R e s p e c t i v e l y a v a i l a b l e \\xa0 f ro m t h e s e \\xa0 w e b \\xa0 s i t e s : f r e e b a s e . c o m , c y c . c o m / o p e n c y c , w o r d n e t .\\np r i n c e t o n . e d u w i k i b a . s e ,\\n2g e n e o n t o l o g y . o r g\\n4 8 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3197132b-68ab-411e-801f-877c7260f6a8', embedding=None, metadata={'page_label': '499', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nInadditiontotrainingdata,wealsoneedtodeﬁneamodelfamilytotrain.\\nAcommonapproachistoextendneurallanguagemodelstomodelentitiesand\\nrelations.Neurallanguagemodelslearnavectorthatprovidesadistributed\\nrepresentationofeachword.Theyalsolearnaboutinteractionsbetweenwords,\\nsuchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions\\nofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning\\nanembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling\\nlanguageandmodelingknowledgeencodedasrelationsissoclosethatresearchers\\nhavetrainedrepresentationsofsuchentitiesbyusing b o t h a nd knowledgebases\\nnaturallanguagesentences( ,,; Bordes e t a l .20112012Wang2014a e t a l .,)or\\ncombiningdatafrommultiplerelationaldatabases( ,).Many Bordes e t a l .2013b\\npossibilitiesexistfortheparticularparametrization associatedwithsuchamodel.\\nEarlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton\\n2000)positedhighlyconstrainedparametricforms(“linearrelationalembeddings”),\\noftenusingadiﬀerentformofrepresentationfortherelationthanfortheentities.\\nForexample,PaccanaroandHinton2000Bordes2011 ()and e t a l .()usedvectorsfor\\nentitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator\\nonentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes\\ne t a l .,),allowingustomakestatementsaboutrelations,butmoreﬂexibilityis 2012\\nputinthemachinerythatcombinestheminordertomodeltheirjointdistribution.\\nApracticalshort-termapplicationofsuchmodelsislinkprediction:predict-\\ningmissingarcsintheknowledgegraph.Thisisaformofgeneralization tonew\\nfacts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthave\\nbeenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably\\nthemajorityoftruerelationsabsentfromtheknowledgebase.SeeWang e t a l .\\n(),()and ()forexamplesofsuchan 2014bLin e t a l .2015Garcia-Duran e t a l .2015\\napplication.\\nEvaluatingtheperformanceofamodelonalinkpredictiontaskisdiﬃcult\\nbecausewehaveonlyadatasetofpositiveexamples(factsthatareknownto\\nbetrue).\\xa0Ifthemodelproposesafactthatisnotinthedataset,weareunsure\\nwhetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown\\nfact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe\\nmodelranksaheld-outofsetofknowntruepositivefactscomparedtootherfacts\\nthatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples\\nthatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue\\nfactandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentity\\nintherelationwithadiﬀerententityselectedatrandom.Thepopularprecisionat\\n10%metriccountshowmanytimesthemodelranksa“correct”factamongthe\\ntop10%ofallcorruptedversionsofthatfact.\\n4 8 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2deeea0d-821b-4d75-ad59-34f6ea3a87bf', embedding=None, metadata={'page_label': '500', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER12.APPLICATIONS\\nAnotherapplicationofknowledgebasesanddistributedrepresentationsfor\\nthemisword-sensedisambiguation(NavigliandVelardi2005Bordes,; e t a l .,\\n2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriate\\none,insomecontext.\\nEventually,knowledgeofrelationscombinedwithareasoningprocessand\\nunderstandingofnaturallanguagecouldallowustobuildageneralquestion\\nansweringsystem.Ageneralquestionansweringsystemmustbeabletoprocess\\ninputinformationandrememberimportantfacts,organizedinawaythatenables\\nittoretrieveandreasonaboutthemlater.Thisremainsadiﬃcultopenproblem\\nwhichcanonlybesolvedinrestricted“toy”environments.Currently,thebest\\napproachtorememberingandretrievingspeciﬁcdeclarativefactsistousean\\nexplicitmemorymechanism,asdescribedinsection.Memorynetworkswere 10.12\\nﬁrstproposedtosolveatoyquestionansweringtask(Weston2014Kumar e t a l .,).\\ne t a l .()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015\\ntheinputintothememoryandtoproducetheanswergiventhecontentsofthe\\nmemory.\\nDeeplearninghasbeenappliedtomanyotherapplicationsbesidestheones\\ndescribedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould\\nbeimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage\\nofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible\\nasofthiswriting.\\nThisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II\\nnetworks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,these\\nmethodsinvolveusingthegradientofacostfunctiontoﬁndtheparametersofa\\nmodelthatapproximates somedesiredfunction.Withenoughtrainingdata,this\\napproachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III\\nterritoryofresearch—methodsthataredesignedtoworkwithlesstrainingdata\\nortoperformagreatervarietyoftasks,wherethechallengesaremorediﬃcult\\nandnotasclosetobeingsolvedasthesituationswehavedescribedsofar.\\n4 8 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3d3a1522-3a7c-404b-80b0-eebb1d625fe7', embedding=None, metadata={'page_label': '501', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='P a rt I I I\\nDeepLearningResearch\\n486', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c095a98e-7330-428e-817e-1416927ac978', embedding=None, metadata={'page_label': '502', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='This part of t he b o ok des c r ib e s t he more am bitious and adv anced approac hes\\nt o deep learning, c urren t ly purs ued b y t he r e s e arc h c omm unit y .\\nIn t he previous parts of t he b o ok, we ha v e s ho wn how t o s olv e s up e r v is e d\\nlearning problems — how t o learn t o map one v e c t or t o another, given e nough\\ne x amples of t he mapping.\\nN ot all problems w e might w ant t o s olve f all in t o t his c ategory . W e ma y\\nwis h t o generate new e x amples , or determine how likely s ome p oin t is , or handle\\nmis s ing v alues and t ake adv an t age of a large s e t of unlab e led e x amples or e x amples\\nf r om r e lated t as k s . A s hortcom ing of t he c urren t s t ate of t he art f or indus t r ial\\napplications is t hat our learning algorithms r e q uire large amounts of s up e r v is e d\\ndata t o ac hieve go o d accuracy . In t his part of t he b o ok, w e dis c us s s ome of\\nt he s p e c ulative approac hes t o r e ducing t he amoun t of lab e led data neces s ary\\nf or e x is t ing mo dels t o work w e ll and b e applicable acros s a broader r ange of\\nt as k s . A c c omplis hing t hes e goals us ually r e q uires s ome f orm of uns up e r v is e d or\\ns e mi-s up e r v is e d learning.\\nMan y deep learning algorithms ha v e b e e n des igned t o t ackle uns upervis e d\\nlearning problems , but none hav e t r uly s olved t he problem in t he s ame w a y t hat\\ndeep learning has largely s olv e d t he s up e r v is e d learning problem f or a wide v ariet y of\\nt as k s . In t his part of t he b o ok, we des c r ibe t he e x is t ing approaches t o uns upervis e d\\nlearning and s ome of t he p opular t hought ab out how w e c an make progres s in t his\\nﬁ e ld.\\nA c e ntral c aus e of t he diﬃculties with uns upervis e d learning is t he high di-\\nmens iona lit y of t he r andom v ariables being mo deled. This brings t wo dis t inct\\nc hallenges : a s t atis t ical c hallenge and a c omputational c hallenge. The s t a t i s t i c a l\\nc h a l l e ng e r e gards generalization: t he num b e r of c onﬁgurations we may wan t t o\\ndis t inguis h c an grow e x p onentially with t he num b e r of dimens ion s of in t e r e s t , and\\nt his q uickly b e c omes muc h larger t han t he num b e r of e x amples one c an p os s ibly\\nha v e ( or us e with b ounded c omputational r e s ources ) . The c o m p u t a t i o na l c h a l l e ng e\\nas s o c iated with high-dimens ional dis t r ibuti ons aris e s b e c aus e man y algorithms f or\\nlearning or us ing a t r ained mo del ( e s p e c ially t hos e bas e d on e s t imatin g an e x plicit\\nprobabilit y f unction) in v olv e in t r actable c omputations t hat gro w e x ponent ially\\nwith t he n um b e r of dimens ion s .\\nWith probabilis t ic mo dels , t his c omputational c hallenge aris e s f r om t he need t o\\np e r f orm intractable inference or s imply f r om t he need t o normalize t he dis t r ibuti on.\\n• I nt r a c t a b l e i nfe r e nc e : inference is dis c us s e d mos t ly in c hapter . It r e gards 19\\nt he q ues t ion of gues s ing t he probable v alues of s ome v ariables a , given other\\nv ariables b , with r e s p e c t t o a mo del t hat c aptures t he j oin t dis t r ibuti on ov e r\\n4 8 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c79c26fb-c295-49d2-b5e7-3c33105537fb', embedding=None, metadata={'page_label': '503', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='a , b and c . In order t o e v e n c ompute s uch c onditional probabilities one needs\\nt o s um ov e r t he v alues of t he v ariables c , as well as c ompute a normalization\\nc ons t an t whic h s ums o v e r t he v alues of a and c .\\n• I nt r a c t a b l e norm a l i z a t i o n c o ns t a nt s ( t h e p a r t i t i o n f u nc t i o n) : t he partition\\nf unction is dis c us s e d mos t ly in c hapter . N ormalizing c ons t ants of proba- 18\\nbilit y f unctions c ome up in inference ( ab o v e ) as well as in learning.\\xa0Man y\\nprobabilis t ic mo dels in v olve s uc h a normalizing c ons t ant. U nfortun ately ,\\nlearning s uc h a mo del often r e q uires c omputing t he gradient of t he loga-\\nr ithm of t he partition f unction with r e s p e c t t o t he mo del parameters . That\\nc omputation is generally as intractable as c omputing t he partition f unction\\nits e lf. Mon t e Carlo Mark ov c hain ( MCMC) metho ds ( c hapter ) are of- 17\\nt e n us e d t o deal with t he partition f unction ( c omputing it or its gradient).\\nU nfortun ately , MCMC metho ds s uﬀer when t he mo des of t he mo del dis t r ibu-\\nt ion are n umerous and well-s e par ated, e s p e c ially in high-dimens ional s paces\\n( s e c t ion ) . 17.5\\nOne wa y t o c onfront t hes e intractable c omputations is t o approximate t hem,\\nand many approaches hav e b e e n prop os e d as dis c us s e d in t his t hird part of t he\\nb o ok. A nother interes t in g w ay ,\\xa0als o dis c us s e d here,\\xa0w ould b e t o av oid t hes e\\nin t r actable c omputations altogether by des ign, and metho ds t hat do not r e q uire\\ns uc h c omputations are t hus v e r y app e aling. Several generativ e mo dels ha v e b e e n\\nprop os e d in r e c e nt y e ars , with t hat motiv ation. A wide v ariety of c ontemporary\\napproac hes t o generativ e mo deling are dis c us s e d in c hapter . 20\\nP art is t he mos t imp ortant f or a r e s e arc her—s om e one who wan t s t o un- I I I\\nders t and t he breadth of p e r s p e c t iv e s t hat hav e b e e n brought t o t he ﬁ e ld of deep\\nlearning, and pus h t he ﬁ e ld f orward t ow ards t r ue artiﬁcial intelligence.\\n4 8 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d1c72e9-8b9e-4711-b086-5faefcc4c170', embedding=None, metadata={'page_label': '504', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 3\\nL i n e ar F act or Mo d e l s\\nManyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodel\\noftheinput, p m o de l( x).Suchamodelcan,inprinciple,useprobabilisticinferenceto\\npredictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Many\\nofthesemodelsalsohavelatentvariables h,with p m o de l() = x E h p m o de l( ) x h|.\\nTheselatentvariablesprovideanothermeansofrepresentingthedata.Distributed\\nrepresentationsbased\\xa0onlatent\\xa0variablescanobtain\\xa0alloftheadvantagesof\\nrepresentationlearningthatwehaveseenwithdeepfeedforwardandrecurrent\\nnetworks.\\nInthischapter,wedescribesomeofthesimplestprobabilisticmodelswith\\nlatentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuilding\\nblocksofmixturemodels(Hinton1995aGhahramaniandHinton1996 e t a l .,; ,;\\nRoweis2002 Tang2012 e t a l .,)orlarger,deepprobabilisticmodels( e t a l .,).They\\nalsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthat\\nthemoreadvanceddeepmodelswillextendfurther.\\nAlinearfactormodelisdeﬁnedbytheuseofastochastic,lineardecoder\\nfunctionthatgeneratesbyaddingnoisetoalineartransformationof. x h\\nThesemodelsareinterestingbecausetheyallowustodiscoverexplanatory\\nfactorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecoder\\nmadethesemodelssomeoftheﬁrstlatentvariablemodelstobeextensivelystudied.\\nAlinearfactormodeldescribesthedatagenerationprocessasfollows.First,\\nwesampletheexplanatoryfactorsfromadistribution h\\nh∼ p ,() h (13.1)\\nwhere p( h)isafactorialdistribution,with p( h) =\\ue051\\ni p( h i),sothatitiseasyto\\n489', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6cc3044-8607-444c-9bf2-4ea0e8a698c7', embedding=None, metadata={'page_label': '505', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nsamplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors:\\nx W h b = ++noise (13.2)\\nwherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions).\\nThisisillustratedinﬁgure.13.1\\nh 1 h 1 h 2 h 2 h 3 h 3\\nx 1 x 1 x 2 x 2 x 3 x 3\\nx h n  o i s  e x h n  o i s  e = W + + b = W + + b\\nFigure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,in\\nwhichweassumethatanobserveddatavector xisobtainedbyalinearcombinationof\\nindependentlatentfactors h,plussomenoise.Diﬀerentmodels,suchasprobabilistic\\nPCA,factoranalysisorICA,makediﬀerentchoicesabouttheformofthenoiseandof\\ntheprior. p() h\\n13.1ProbabilisticPCAandFactorAnalysis\\nProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinear\\nfactormodelsarespecialcasesoftheaboveequations(and)andonly 13.113.2\\ndiﬀerinthechoicesmadeforthenoisedistributionandthemodel’spriorover\\nlatentvariablesbeforeobserving. h x\\nIn f ac t o r analysis( ,;,),thelatentvariable Bartholomew1987Basilevsky1994\\npriorisjusttheunitvarianceGaussian\\nh 0 ∼N(; h , I) (13.3)\\nwhiletheobservedvariables x iareassumedtobe c o ndi t i o n a l l y i ndep e ndent,\\ngiven h.Speciﬁcally,\\xa0the\\xa0noiseisassumed\\xa0tobedrawnfroma\\xa0diagonalco-\\nvariance\\xa0Gaussian distribution,with\\xa0covariancematrix ψ=diag( σ2),with\\nσ2= [ σ2\\n1 , σ2\\n2 , . . . , σ2\\nn]\\ue03eavectorofper-variablevariances.\\nTheroleofthelatentvariablesisthusto c a p t u r e t h e d e p e nde nc i e sbetween\\nthediﬀerentobservedvariables x i.Indeed,itcaneasilybeshownthat xisjusta\\nmultivariatenormalrandomvariable,with\\nx∼N(; x b W W ,\\ue03e+) ψ . (13.4)\\n490', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='66550fca-8199-4186-89bc-894401fc0cec', embedding=None, metadata={'page_label': '506', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nInordertocastPCAinaprobabilisticframework,\\xa0wecanmakeaslight\\nmodiﬁcationtothefactoranalysismodel,makingtheconditionalvariances σ2\\ni\\nequaltoeachother.Inthatcasethecovarianceof xisjust W W\\ue03e+ σ2I,where\\nσ2isnowascalar.Thisyieldstheconditionaldistribution\\nx∼N(; x b W W ,\\ue03e+ σ2I) (13.5)\\norequivalently\\nx h z = W ++ b σ (13.6)\\nwhere z∼N( z; 0 , I)isGaussiannoise. ()thenshowan TippingandBishop1999\\niterativeEMalgorithmforestimatingtheparameters and W σ2.\\nThis pr o babili s t i c P CAmodeltakesadvantageoftheobservationthatmost\\nvariationsinthedatacanbecapturedbythelatentvariables h,uptosomesmall\\nresidual r e c o nst r u c t i o n e r r o r σ2.Asshownby (), TippingandBishop1999\\nprobabilisticPCAbecomesPCAas σ→0.Inthatcase,theconditionalexpected\\nvalueof hgiven xbecomesanorthogonalprojectionof x b−ontothespace\\nspannedbythecolumnsof,likeinPCA. d W\\nAs σ→0,thedensitymodeldeﬁnedbyprobabilisticPCAbecomesverysharp\\naroundthese ddimensionsspannedbythecolumnsof W.Thiscanmakethe\\nmodelassignverylowlikelihoodtothedataifthedatadoesnotactuallycluster\\nnearahyperplane.\\n13.2IndependentComponentAnalysis(ICA)\\nIndependentcomponentanalysis(ICA)isamongtheoldestrepresentationlearning\\nalgorithms( ,; ,;\\xa0,; Herault\\xa0andAns1984Jutten\\xa0andHerault1991Comon1994\\nHyvärinen1999Hyvärinen 2001aHinton2001Teh2003 ,; e t a l .,; e t a l .,; e t a l .,).\\nItisanapproachtomodelinglinearfactorsthatseekstoseparateanobserved\\nsignalintomanyunderlyingsignalsthatarescaledandaddedtogethertoform\\ntheobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthan\\nmerelydecorrelatedfromeachother.1\\nManydiﬀerentspeciﬁcmethodologiesarereferredtoasICA.Thevariant\\nthatismostsimilartotheothergenerativemodelswehavedescribedhereisa\\nvariant(,)thattrainsafullyparametricgenerativemodel.The Pham e t a l .1992\\npriordistributionovertheunderlyingfactors, p( h),mustbeﬁxedaheadoftimeby\\ntheuser.Themodelthendeterministicallygenerates x= W h.Wecanperforma\\n1Seesectionforadiscussionofthediﬀerencebetweenuncorrelatedvariablesandindepen- 3.8\\ndentvariables.\\n491', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='53937c8d-2145-4e78-a3cc-2d6651adecfe', embedding=None, metadata={'page_label': '507', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nnonlinearchangeofvariables(usingequation)todetermine3.47 p( x) .Learning\\nthemodelthenproceedsasusual,usingmaximumlikelihood.\\nThemotivationforthisapproachisthatbychoosing p( h)tobeindependent,\\nwecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent.\\nThisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,butto\\nrecoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,each\\ntrainingexampleisonemomentintime,each x iisonesensor’sobservationof\\nthemixedsignals,andeach h iisoneestimateofoneoftheoriginalsignals.For\\nexample,wemighthave npeoplespeakingsimultaneously.Ifwehave ndiﬀerent\\nmicrophonesplacedindiﬀerentlocations,ICAcandetectthechangesinthevolume\\nbetweeneachspeakerasheardbyeachmicrophone, andseparatethesignalsso\\nthateach h icontainsonlyonepersonspeakingclearly.Thisiscommonlyused\\ninneuroscienceforelectroencephalograph y,atechnologyforrecordingelectrical\\nsignalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubject’s\\nheadareusedtomeasuremanyelectricalsignalscomingfromthebody.The\\nexperimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfrom\\nthesubject’sheartandeyesarestrongenoughtoconfoundmeasurementstaken\\natthesubject’sscalp.Thesignalsarriveattheelectrodesmixedtogether,so\\nICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignals\\noriginatinginthebrain,andtoseparatesignalsindiﬀerentbrainregionsfrom\\neachother.\\nAsmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoise\\ninthegenerationof xratherthanusingadeterministicdecoder.Mostdonot\\nusethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsof\\nh= W− 1xindependentfromeachother.Manycriteriathataccomplishthisgoal\\narepossible.Equationrequirestakingthedeterminantof 3.47 W,whichcanbe\\nanexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthis\\nproblematicoperationbyconstrainingtobeorthogonal. W\\nAllvariantsofICArequirethat p( h)benon-Gaussian.Thisisbecauseif p( h)\\nisanindependentpriorwithGaussiancomponents,then Wisnotidentiﬁable.\\nWecanobtainthesamedistributionover p( x)formanyvaluesof W.Thisisvery\\ndiﬀerentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis,\\nthatoftenrequire p( h)tobeGaussianinordertomakemanyoperationsonthe\\nmodelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwherethe\\nuserexplicitlyspeciﬁesthedistribution,atypicalchoiceistouse p( h i) =d\\nd h iσ( h i).\\nTypicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0than\\ndoestheGaussiandistribution,sowecanalsoseemostimplementations ofICA\\naslearningsparsefeatures.\\n492', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3fa2f08a-7445-45ee-a65e-e0da14989714', embedding=None, metadata={'page_label': '508', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nManyvariantsofICAarenotgenerativemodelsinthesensethatweusethe\\nphrase.Inthisbook,agenerativemodeleitherrepresents p( x) orcandrawsamples\\nfromit.ManyvariantsofICAonlyknowhowtotransformbetween xand h,but\\ndonothaveanywayofrepresenting p( h),andthusdonotimposeadistribution\\nover p( x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisof\\nh= W− 1x,becausehighkurtosisindicatesthat p( h)isnon-Gaussian,butthisis\\naccomplishedwithoutexplicitlyrepresenting p( h).ThisisbecauseICAismore\\noftenusedasananalysistoolforseparatingsignals,ratherthanforgenerating\\ndataorestimatingitsdensity.\\nJustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedin\\nchapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich 14\\nweuseanonlinearfunction ftogeneratetheobserveddata.SeeHyvärinenand\\nPajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewith\\nensemblelearningby ()and (). RobertsandEverson2001Lappalainen e t a l .2000\\nAnothernonlinearextensionofICAistheapproachof nonlinear i ndep e ndent\\nc o m p o nen t s e st i m at i o n,orNICE(,),whichstacksaseries Dinh e t a l .2014\\nofinvertibletransformations(encoderstages)thathavethepropertythatthe\\ndeterminantoftheJacobianofeachtransformationcanbecomputedeﬃciently.\\nThismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attempts\\ntotransformthedataintoaspacewhereithasafactorizedmarginaldistribution,\\nbutismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoder\\nisassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardto\\ngeneratesamplesfromthemodel(byﬁrstsamplingfrom p( h)andthenapplying\\nthedecoder).\\nAnothergeneralization ofICAistolearngroupsoffeatures,withstatistical\\ndependenceallowedwithinagroupbutdiscouragedbetweengroups(Hyvärinenand\\nHoyer1999Hyvärinen 2001b ,; e t a l .,).Whenthegroupsofrelatedunitsarechosen\\ntobenon-overlapping,thisiscalled i ndep e nden t subspac e analysis.Itisalso\\npossibletoassignspatialcoordinatestoeachhiddenunitandformoverlapping\\ngroupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilar\\nfeatures.Whenappliedtonaturalimages,this t o p o g r aphic I CAapproachlearns\\nGaborﬁlters,suchthatneighboringfeatureshavesimilarorientation,locationor\\nfrequency.ManydiﬀerentphaseoﬀsetsofsimilarGaborfunctionsoccurwithin\\neachregion,sothatpoolingoversmallregionsyieldstranslationinvariance.\\n13.3SlowFeatureAnalysis\\nSl o w f e at ur e analysis(SFA)isalinearfactormodelthatusesinformationfrom\\n493', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1305e010-f7ea-4f14-be90-ca5029d2b433', embedding=None, metadata={'page_label': '509', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\ntimesignalstolearninvariantfeatures( ,). WiskottandSejnowski2002\\nSlowfeatureanalysisismotivatedbyageneralprinciplecalledtheslowness\\nprinciple.Theideaisthattheimportantcharacteristicsofsceneschangevery\\nslowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofa\\nscene.Forexample,incomputervision,individualpixelvaluescanchangevery\\nrapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixel\\nwillrapidlychangefromblacktowhiteandbackagainasthezebra’sstripespass\\noverthepixel.Bycomparison,thefeatureindicatingwhetherazebraisinthe\\nimagewillnotchangeatall,andthefeaturedescribingthezebra’spositionwill\\nchangeslowly.\\xa0Wethereforemaywishtoregularizeourmodeltolearnfeatures\\nthatchangeslowlyovertime.\\nTheslownessprinciplepredatesslowfeatureanalysisandhasbeenapplied\\ntoawidevarietyofmodels(,;,; ,; Hinton1989Földiák1989Mobahi e t a l .2009\\nBergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoany\\ndiﬀerentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybe\\nintroducedbyaddingatermtothecostfunctionoftheform\\nλ\\ue058\\ntL f(( x( + 1 ) t)( , f x( ) t)) (13.7)\\nwhere λisahyperparameter determiningthestrengthoftheslownessregularization\\nterm, tistheindexintoatimesequenceofexamples, fisthefeatureextractor\\ntoberegularized,and Lisalossfunctionmeasuringthedistancebetween f( x( ) t)\\nand f( x( + 1 ) t).Acommonchoiceforisthemeansquareddiﬀerence. L\\nSlowfeatureanalysisisaparticularlyeﬃcientapplicationoftheslowness\\nprinciple.Itiseﬃcientbecauseitisappliedtoalinearfeatureextractor,andcan\\nthusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquitea\\ngenerativemodelperse,inthesensethatitdeﬁnesalinearmapbetweeninput\\nspaceandfeaturespacebutdoesnotdeﬁneaprioroverfeaturespaceandthus\\ndoesnotimposeadistributiononinputspace. p() x\\nTheSFAalgorithm(WiskottandSejnowski2002,)consistsofdeﬁning f( x; θ)\\ntobealineartransformation,andsolvingtheoptimization problem\\nmin\\nθE t(( f x( + 1 ) t) i− f( x( ) t) i)2(13.8)\\nsubjecttotheconstraints\\nE t f( x( ) t) i= 0 (13.9)\\nand\\nE t[( f x( ) t)2\\ni] = 1 . (13.10)\\n494', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47951c8d-c10b-4185-9e32-f1592f3f49b2', embedding=None, metadata={'page_label': '510', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nTheconstraintthatthelearnedfeaturehavezeromeanisnecessarytomakethe\\nproblemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeature\\nvaluesandobtainadiﬀerentsolutionwithequalvalueoftheslownessobjective.\\nTheconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthe\\npathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures 0\\nareordered,withtheﬁrstfeaturebeingtheslowest.Tolearnmultiplefeatures,we\\nmustalsoaddtheconstraint\\n∀ i < j , E t[( f x( ) t) i f( x( ) t) j] = 0 . (13.11)\\nThisspeciﬁesthatthelearnedfeaturesmustbelinearlydecorrelated fromeach\\nother.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturethe\\noneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizing\\nreconstructionerror,\\xa0to\\xa0forcethe\\xa0featurestodiversify,\\xa0but\\xa0thisdecorrelation\\nmechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFA\\nproblemmaybesolvedinclosedformbyalinearalgebrapackage.\\nSFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasis\\nexpansionto xbeforerunningSFA.Forexample,itiscommontoreplace xbythe\\nquadraticbasisexpansion,avectorcontainingelements x i x jforall iand j.Linear\\nSFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractors\\nbyrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasis\\nexpansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractoron\\ntopofthatexpansion.\\nWhentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwith\\nquadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswith\\nthoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrained\\nonvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deep\\nSFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresented\\nbyneuronsinratbrainsthatareusedfornavigation(Franzius 2007 e t a l .,).SFA\\nthusseemstobeareasonablybiologicallyplausiblemodel.\\nAmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhich\\nfeaturesSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoretical\\npredictions,onemustknowaboutthedynamicsoftheenvironmentintermsof\\nconﬁguration space\\xa0(e.g.,\\xa0inthe\\xa0caseofrandom\\xa0motion inthe\\xa03-Drendered\\nenvironment,thetheoreticalanalysisproceedsfromknowledgeoftheprobability\\ndistributionoverpositionandvelocityofthecamera).Giventheknowledgeofhow\\ntheunderlyingfactorsactuallychange,itispossibletoanalyticallysolveforthe\\noptimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFA\\nappliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions.\\n495', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9a687da1-2305-4296-8b6b-ef3d691f577a', embedding=None, metadata={'page_label': '511', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nThisisincomparisontootherlearningalgorithmswherethecostfunctiondepends\\nhighlyonspeciﬁcpixelvalues,makingitmuchmorediﬃculttodeterminewhat\\nfeaturesthemodelwilllearn.\\nDeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandpose\\nestimation(Franzius 2008 e t a l .,).Sofar,theslownessprinciplehasnotbecome\\nthebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimited\\nitsperformance.Wespeculatethatperhapstheslownessprioristoostrong,and\\nthat,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant,\\nitwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfrom\\nonetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessof\\nwhethertheobject’svelocityishighorlow,buttheslownessprincipleencourages\\nthemodeltoignorethepositionofobjectsthathavehighvelocity.\\n13.4SparseCoding\\nSpar se c o di ng( ,)isalinearfactormodelthathas OlshausenandField1996\\nbeenheavilystudiedasanunsupervisedfeaturelearningandfeatureextraction\\nmechanism.\\xa0Strictlyspeaking,theterm“sparsecoding”referstotheprocessof\\ninferringthevalueof hinthismodel,while“sparsemodeling”referstotheprocess\\nofdesigningandlearningthemodel,buttheterm“sparsecoding”isoftenusedto\\nrefertoboth.\\nLikemostotherlinearfactormodels,itusesalineardecoderplusnoiseto\\nobtainreconstructionsof x,asspeciﬁedinequation.Morespeciﬁcally,sparse 13.2\\ncodingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewith\\nisotropicprecision: β\\np , ( ) = (; + x h| N x W h b1\\nβI) . (13.12)\\nThedistribution p( h)ischosentobeonewithsharppeaksnear0(Olshausen\\nandField1996,).CommonchoicesincludefactorizedLaplace,Cauchyorfactorized\\nStudent- tdistributions.Forexample,theLaplacepriorparametrized intermsof\\nthesparsitypenaltycoeﬃcientisgivenby λ\\np h( i) = Laplace( h i;0 ,2\\nλ) =λ\\n4e−1\\n2λ h| i|(13.13)\\nandtheStudent-priorby t\\np h( i) ∝1\\n(1+h2\\ni\\nν)ν +1\\n2. (13.14)\\n496', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68ddc55d-aacb-4779-9972-82e297b832fa', embedding=None, metadata={'page_label': '512', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nTrainingsparsecodingwithmaximumlikelihoodisintractable.Instead,the\\ntrainingalternatesbetweenencodingthedataandtrainingthedecodertobetter\\nreconstructthedatagiventheencoding.Thisapproachwillbejustiﬁedfurtheras\\naprincipledapproximation tomaximumlikelihoodlater,insection.19.3\\nFormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunction\\nthatpredicts handconsistsonlyofmultiplication byaweightmatrix.Theencoder\\nthatweusewithsparsecodingisnotaparametricencoder.Instead,theencoder\\nisanoptimization algorithm,thatsolvesanoptimization probleminwhichweseek\\nthesinglemostlikelycodevalue:\\nh∗= () = argmax f x\\nhp . ( ) h x| (13.15)\\nWhencombinedwithequationandequation,thisyieldsthefollowing 13.13 13.12\\noptimization problem:\\nargmax\\nhp( ) h x| (13.16)\\n=argmax\\nhlog( ) p h x| (13.17)\\n=argmin\\nhλ|||| h 1+ β||− || x W h2\\n2 , (13.18)\\nwherewehavedroppedtermsnotdependingon handdividedbypositivescaling\\nfactorstosimplifytheequation.\\nDuetotheimpositionofan L1normon h,thisprocedurewillyieldasparse\\nh∗(Seesection).7.1.2\\nTotrainthemodelratherthanjustperforminference,wealternatebetween\\nminimization withrespectto handminimization withrespectto W.Inthis\\npresentation,wetreat βasahyperparameter.Typicallyitissetto1becauseits\\nroleinthisoptimization problemissharedwith λandthereisnoneedforboth\\nhyperparameters.Inprinciple,wecouldalsotreat βasaparameterofthemodel\\nandlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdepend\\non hbutdodependon β.Tolearn β,thesetermsmustbeincluded,or βwill\\ncollapseto.0\\nNotallapproachestosparsecodingexplicitlybuilda p( h)anda p( x h|).\\nOftenwearejustinterestedinlearningadictionaryoffeatureswithactivation\\nvaluesthatwilloftenbezerowhenextractedusingthisinferenceprocedure.\\nIfwesample hfromaLaplaceprior,itisinfactazeroprobabilityeventfor\\nanelementof htoactuallybezero.Thegenerativemodelitselfisnotespecially\\nsparse,onlythefeatureextractoris. ()describeapproximate Goodfellow e t a l .2013d\\n497', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='daa6fc40-80ff-4f1f-8752-20ae05821bca', embedding=None, metadata={'page_label': '513', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\ninferenceinadiﬀerentmodelfamily,thespikeandslabsparsecodingmodel,for\\nwhichsamplesfromthepriorusuallycontaintruezeros.\\nThesparsecodingapproachcombinedwiththeuseofthenon-parametric\\nencodercaninprincipleminimizethecombinationofreconstructionerrorand\\nlog-priorbetterthananyspeciﬁcparametricencoder.Anotheradvantageisthat\\nthereisnogeneralization errortotheencoder.Aparametricencodermustlearn\\nhowtomap xto hinawaythatgeneralizes.Forunusual xthatdonotresemble\\nthetrainingdata,alearned,parametricencodermayfailtoﬁndan hthatresults\\ninaccuratereconstructionorasparsecode.Forthevastmajorityofformulations\\nofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimization\\nprocedurewillalwaysﬁndtheoptimalcode(unlessdegeneratecasessuchas\\nreplicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncosts\\ncanstillriseonunfamiliarpoints,butthisisduetogeneralization errorinthe\\ndecoderweights,ratherthangeneralization errorintheencoder.Thelackof\\ngeneralization errorinsparsecoding’soptimization-based encodingprocessmay\\nresultinbettergeneralization whensparsecodingisusedasafeatureextractorfor\\naclassiﬁerthanwhenaparametricfunctionisusedtopredictthecode.Coates\\nandNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterfor\\nobjectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametric\\nencoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellow e t a l .\\n()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature 2013d\\nextractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewer\\nlabelsperclass).\\nTheprimarydisadvantageofthenon-parametric encoderisthatitrequires\\ngreatertimetocompute hgiven xbecausethenon-parametric approachrequires\\nrunninganiterativealgorithm.Theparametricautoencoderapproach,developed\\nin\\xa0chapter\\xa0,usesonly\\xa0a\\xa0ﬁxed\\xa0n umber\\xa0of\\xa0layers,\\xa0often\\xa0only\\xa0one.Another 14\\ndisadvantageisthatitisnotstraight-forwardtoback-propagatethroughthe\\nnon-parametric encoder,whichmakesitdiﬃculttopretrainasparsecodingmodel\\nwithanunsupervisedcriterionandthenﬁne-tuneitusingasupervisedcriterion.\\nModiﬁedversionsofsparsecodingthatpermitapproximate derivativesdoexist\\nbutarenotwidelyused( ,). BagnellandBradley2009\\nSparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,as\\nshowninﬁgure.Thishappensevenwhenthemodelisabletoreconstruct 13.2\\nthedatawellandprovideusefulfeaturesforaclassiﬁer.Thereasonisthateach\\nindividualfeaturemaybelearnedwell,butthefactorialprioronthehiddencode\\nresultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgenerated\\nsample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon-\\n498', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='29a41910-1916-40a9-a974-eedb64e7e064', embedding=None, metadata={'page_label': '514', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nFigure13.2:\\xa0Example samplesandweightsfromaspikeandslabsparsecodingmodel\\ntrainedontheMNISTdataset. ( L e f t )Thesamplesfromthemodeldonotresemblethe\\ntrainingexamples.Atﬁrstglance,onemightassumethemodelispoorlyﬁt.The ( R i g h t )\\nweightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescomplete\\ndigits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprior\\noverfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.Fewsuchsubsets\\nareappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentof\\ngenerativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.Figure\\nreproducedwithpermissionfromGoodfellow2013d e t a l .().\\nfactorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmore\\nsophisticatedshallowmodels.\\n13.5ManifoldInterpretationofPCA\\nLinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedas\\nlearningamanifold( ,).WecanviewprobabilisticPCAas Hinton e t a l .1997\\ndeﬁningathinpancake-shapedregionofhighprobability—aGaussiandistribution\\nthatisverynarrowalongsomeaxes,justasapancakeisveryﬂatalongitsvertical\\naxis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontal\\naxes.\\xa0Thisisillustratedinﬁgure.\\xa0PCAcanbeinterpretedasaligningthis 13.3\\npancakewithalinearmanifoldinahigher-dimens ionalspace.Thisinterpretation\\nappliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearns\\nmatrices Wand Vwiththegoalofmakingthereconstructionof xlieascloseto\\nxaspossible,\\nLettheencoderbe\\nh x W = ( f) = \\ue03e( ) x µ− . (13.19)\\n499', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ec9c296c-fc69-4138-8ee6-292890bffeb2', embedding=None, metadata={'page_label': '515', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\nTheencodercomputesalow-dimensional representationof h.Withtheautoencoder\\nview,wehaveadecodercomputingthereconstruction\\nˆ x h b V h = ( g) = + . (13.20)\\nFigure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensional\\nmanifold.Theﬁgureshowstheupperhalfofthe“pancake”abovethe“manifoldplane”\\nwhichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldis\\nverysmall(arrowpointingoutofplane)andcanbeconsideredlike“noise,”whiletheother\\nvariancesarelarge(arrowsintheplane)andcorrespondto“signal,”andacoordinate\\nsystemforthereduced-dimensiondata.\\nThechoicesoflinearencoderanddecoderthatminimizereconstructionerror\\nE[||− xˆ x||2] (13.21)\\ncorrespondto V= W, µ= b= E[ x]andthecolumnsof Wformanorthonormal\\nbasiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariance\\nmatrix\\nC x µ x µ = [( E −)(−)\\ue03e] . (13.22)\\nInthecaseofPCA,thecolumnsof Waretheseeigenvectors,orderedbythe\\nmagnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative).\\nOnecanalsoshowthateigenvalue λ iof Ccorrespondstothevarianceof x\\ninthedirectionofeigenvector v( ) i.If x∈ RDand h∈ Rdwith d < D,thenthe\\n500', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7c1e2cde-b3ae-4924-a2d7-f89f1c69cc58', embedding=None, metadata={'page_label': '516', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER13.LINEARFACTORMODELS\\noptimalreconstructionerror(choosing,,andasabove)is µ b V W\\nmin[ E||− xˆ x||2] =D \\ue058\\ni d = + 1λ i . (13.23)\\nHence,ifthecovariancehasrank d,theeigenvalues λ d + 1to λ Dare0andrecon-\\nstructionerroris0.\\nFurthermore,onecanalsoshowthattheabovesolutioncanbeobtainedby\\nmaximizingthevariancesoftheelementsof h,underorthogonal W,insteadof\\nminimizingreconstructionerror.\\nLinearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthe\\nsimplestmodelsthatlearnarepresentationofdata.Muchaslinearclassiﬁersand\\nlinearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinear\\nfactormodelsmaybeextendedtoautoencodernetworksanddeepprobabilistic\\nmodelsthatperformthesametasksbutwithamuchmorepowerfulandﬂexible\\nmodelfamily.\\n501', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e07fc7e-1bf5-4dc4-8c7e-570aa14b7d80', embedding=None, metadata={'page_label': '517', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 4\\nA u t o e n co d e rs\\nAn aut o e nc o derisaneuralnetworkthatistrainedtoattempttocopyitsinput\\ntoitsoutput.\\xa0Internally ,ithasahiddenlayer hthatdescribesa c o deusedto\\nrepresenttheinput.Thenetworkmaybeviewedasconsistingoftwoparts:an\\nencoderfunction h= f( x)andadecoderthatproducesareconstruction r= g( h).\\nThisarchitectureispresentedinﬁgure.Ifanautoencodersucceedsinsimply 14.1\\nlearningtoset g( f( x)) = xeverywhere,thenitisnotespeciallyuseful.Instead,\\nautoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyare\\nrestrictedinwaysthatallowthemtocopyonlyapproximately ,andtocopyonly\\ninputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritize\\nwhichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthe\\ndata.\\nModern\\xa0autoencoders\\xa0havegeneralized\\xa0the\\xa0idea of\\xa0anencoder\\xa0and\\xa0ade-\\ncoderbeyonddeterministicfunctionstostochasticmappings pencoder( h x|)and\\npdecoder( ) x h|.\\nTheideaofautoencodershasbeenpartofthehistoricallandscapeofneural\\nnetworksfordecades(,; ,; , LeCun1987BourlardandKamp1988HintonandZemel\\n1994).Traditionally,\\xa0autoencoderswereused\\xa0fordimensionalityreductionor\\nfeaturelearning.Recently,theoreticalconnectionsbetweenautoencodersand\\nlatentvariablemodelshavebroughtautoencoderstotheforefrontofgenerative\\nmodeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing 20\\naspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesame\\ntechniques,typicallyminibatchgradientdescentfollowinggradientscomputed\\nbyback-propagation. Unlikegeneralfeedforwardnetworks,autoencodersmay\\nalsobetrainedusing r e c i r c ul at i o n(HintonandMcClelland1988,),alearning\\nalgorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput\\n502', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b362704-0690-4ff8-9716-814869ea8cfd', embedding=None, metadata={'page_label': '518', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\ntotheactivationsonthereconstructedinput.Recirculationisregardedasmore\\nbiologicallyplausiblethanback-propagation, butisrarelyusedformachinelearning\\napplications.\\nxx rrh h\\nf g\\nFigure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutput x\\n(calledreconstruction) rthroughaninternalrepresentationorcode h.Theautoencoder\\nhastwocomponents:theencoder f(mapping xto h)andthedecoder g(mapping hto\\nr).\\n14.1UndercompleteAutoencoders\\nCopyingtheinputtotheoutputmaysounduseless,butwearetypicallynot\\ninterestedintheoutputofthe\\xa0decoder. Instead,\\xa0wehope\\xa0thattrainingthe\\nautoencodertoperformtheinputcopyingtaskwillresultin htakingonuseful\\nproperties.\\nOnewaytoobtainusefulfeaturesfromtheautoencoderistoconstrain hto\\nhavesmallerdimensionthan x.Anautoencoderwhosecodedimensionisless\\nthantheinputdimensioniscalled under c o m p l e t e.Learninganundercomplete\\nrepresentationforcestheautoencodertocapturethemostsalientfeaturesofthe\\ntrainingdata.\\nThelearningprocessisdescribedsimplyasminimizingalossfunction\\nL , g f ( x(())) x (14.1)\\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\\nthemeansquarederror.\\nWhenthedecoderislinearand Listhemeansquarederror,anundercomplete\\nautoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencoder\\ntrainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthe\\ntrainingdataasaside-eﬀect.\\nAutoencoderswithnonlinearencoderfunctions fandnonlineardecoderfunc-\\ntions gcanthuslearnamorepowerfulnonlineargeneralization ofPCA.Unfortu-\\n5 0 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aba1ede7-3eb7-4488-9aed-b3810e8c000a', embedding=None, metadata={'page_label': '519', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nnately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencoder\\ncanlearntoperformthecopyingtaskwithoutextractingusefulinformationabout\\nthedistributionofthedata.Theoretically,onecouldimaginethatanautoencoder\\nwithaone-dimensional codebutaverypowerfulnonlinearencodercouldlearnto\\nrepresenteachtrainingexample x() iwiththecode i.Thedecodercouldlearnto\\nmaptheseintegerindicesbacktothevaluesofspeciﬁctrainingexamples.This\\nspeciﬁcscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen-\\ncodertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulabout\\nthedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat.\\n14.2RegularizedAutoencoders\\nUndercomplete autoencoders,withcodedimensionlessthantheinputdimension,\\ncanlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthat\\ntheseautoencodersfailtolearnanythingusefuliftheencoderanddecoderare\\ngiventoomuchcapacity.\\nAsimilarproblemoccursifthehiddencodeisallowedtohavedimension\\nequaltotheinput,andinthe o v e r c o m pl e t ecaseinwhichthehiddencodehas\\ndimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlinear\\ndecodercanlearntocopytheinputtotheoutputwithoutlearninganythinguseful\\naboutthedatadistribution.\\nIdeally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosing\\nthecodedimensionandthecapacityoftheencoderanddecoderbasedonthe\\ncomplexityofdistributiontobemodeled.Regularizedautoencodersprovidethe\\nabilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoder\\nanddecodershallowandthecodesizesmall,regularizedautoencodersusealoss\\nfunctionthatencouragesthemodeltohaveotherpropertiesbesidestheability\\ntocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityofthe\\nrepresentation,smallnessofthederivativeoftherepresentation,androbustness\\ntonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearand\\novercompletebutstilllearnsomethingusefulaboutthedatadistributionevenif\\nthemodelcapacityisgreatenoughtolearnatrivialidentityfunction.\\nInadditiontothemethodsdescribedherewhicharemostnaturallyinterpreted\\nasregularizedautoencoders,nearlyanygenerativemodelwithlatentvariables\\nandequippedwithaninferenceprocedure(forcomputinglatentrepresentations\\ngiveninput)maybeviewedasaparticularformofautoencoder.Twogenerative\\nmodelingapproachesthatemphasizethisconnectionwithautoencodersarethe\\ndescendantsoftheHelmholtzmachine( ,),suchasthevariational Hinton e t a l .1995b\\n5 0 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='64b24f04-2dc7-4568-974a-61b13de5815d', embedding=None, metadata={'page_label': '520', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nautoencoder(section)andthegenerativestochasticnetworks(section). 20.10.3 20.12\\nThesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinput\\nanddonotrequireregularizationfortheseencodingstobeuseful.Theirencodings\\narenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximize\\ntheprobabilityofthetrainingdataratherthantocopytheinputtotheoutput.\\n1 4 . 2 . 1 S p a rse A u t o en co d ers\\nAsparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesa\\nsparsitypenaltyΩ( h)onthecodelayer h,inadditiontothereconstructionerror:\\nL , g f ( x(()))+Ω() x h (14.2)\\nwhere g( h)isthedecoderoutputandtypicallywehave h= f( x),theencoder\\noutput.\\nSparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuch\\nasclassiﬁcation.Anautoencoderthathasbeenregularizedtobesparsemust\\nrespondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,rather\\nthansimplyactingasanidentityfunction.Inthisway,trainingtoperformthe\\ncopyingtaskwithasparsitypenaltycanyieldamodelthathaslearneduseful\\nfeaturesasabyproduct.\\nWecanthink\\xa0ofthepenalty Ω( h)simplyasaregularizertermaddedto\\nafeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput\\n(unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask\\n(with\\xa0asupervised\\xa0learning\\xa0ob jective)\\xa0thatdepends\\xa0on\\xa0thesesparsefeatures.\\nUnlikeotherregularizerssuchasweightdecay,thereisnotastraightforward\\nBayesianinterpretationtothisregularizer.Asdescribedinsection,training5.6.1\\nwithweightdecayandotherregularizationpenaltiescanbeinterpretedasa\\nMAPapproximationtoBayesianinference,withtheaddedregularizingpenalty\\ncorrespondingtoapriorprobabilitydistributionoverthemodelparameters.In\\nthisview,regularizedmaximumlikelihoodcorrespondstomaximizing p( θ x|),\\nwhichisequivalenttomaximizing log p( x θ|)+log p( θ).\\xa0The log p( x θ|)term\\nistheusualdatalog-likelihoodtermandthelog p( θ)term,thelog-priorover\\nparameters,incorporatesthepreferenceoverparticularvaluesof θ.Thisviewwas\\ndescribedinsection.Regularizedautoencodersdefysuchaninterpretation 5.6\\nbecausetheregularizerdependsonthedataandisthereforebydeﬁnitionnota\\npriorintheformalsenseoftheword.Wecanstillthinkoftheseregularization\\ntermsasimplicitlyexpressingapreferenceoverfunctions.\\nRatherthanthinkingofthesparsitypenaltyasaregularizerforthecopying\\ntask,wecanthinkoftheentiresparseautoencoderframeworkasapproximating\\n5 0 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85308fa4-974f-4c9d-80fb-3defc33337c9', embedding=None, metadata={'page_label': '521', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nmaximumlikelihood\\xa0trainingofagenerativemodel\\xa0thathaslatentvariables.\\nSupposewehaveamodelwithvisiblevariables xandlatentvariables h,with\\nanexplicitjointdistribution pmodel( x h ,)= pmodel( h) pmodel( x h|).Wereferto\\npmodel( h)asthemodel’spriordistributionoverthelatentvariables,representing\\nthemodel’sbeliefspriortoseeing x.Thisisdiﬀerentfromthewaywehave\\npreviouslyusedtheword“prior,”torefertothedistribution p( θ)encodingour\\nbeliefsaboutthemodel’sparametersbeforewehaveseenthetrainingdata.The\\nlog-likelihoodcanbedecomposedas\\nlog pmodel() = log x\\ue058\\nhpmodel( ) h x , . (14.3)\\nWecanthinkoftheautoencoderasapproximatingthissumwithapointestimate\\nforjustonehighlylikelyvaluefor h.Thisissimilartothesparsecodinggenerative\\nmodel(section),butwith13.4 hbeingtheoutputoftheparametricencoderrather\\nthantheresultofanoptimization thatinfersthemostlikely h.Fromthispointof\\nview,withthischosen,wearemaximizing h\\nlog pmodel( ) = log h x , pmodel()+log h pmodel( ) x h| .(14.4)\\nThelog pmodel() htermcanbesparsity-inducing.Forexample,theLaplaceprior,\\npmodel( h i) =λ\\n2e−| λ h i|, (14.5)\\ncorrespondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasan\\nabsolutevaluepenalty,weobtain\\nΩ() = h λ\\ue058\\ni| h i| (14.6)\\n−log pmodel() = h\\ue058\\ni\\ue012\\nλ h| i|−logλ\\n2\\ue013\\n= Ω()+const h (14.7)\\nwheretheconstanttermdependsonlyon λandnot h.Wetypicallytreat λasa\\nhyperparameteranddiscardtheconstanttermsinceitdoesnotaﬀecttheparameter\\nlearning.OtherpriorssuchastheStudent- tpriorcanalsoinducesparsity.From\\nthispointofviewofsparsityasresultingfromtheeﬀectof pmodel( h)onapproximate\\nmaximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermat\\nall.\\xa0Itisjustaconsequenceofthemodel’sdistributionoveritslatentvariables.\\nThisviewprovidesadiﬀerentmotivationfortraininganautoencoder:itisaway\\nofapproximately trainingagenerativemodel.Italsoprovidesadiﬀerentreasonfor\\n5 0 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6f5a8556-f045-47d5-88e0-5939d0e4fef7', embedding=None, metadata={'page_label': '522', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nwhythefeatureslearnedbytheautoencoderareuseful:theydescribethelatent\\nvariablesthatexplaintheinput.\\nEarlyworkonsparseautoencoders( ,,)explored Ranzato e t a l .2007a2008\\nvariousformsofsparsityandproposedaconnectionbetweenthesparsitypenalty\\nandthelog Ztermthatariseswhenapplyingmaximumlikelihoodtoanundirected\\nprobabilisticmodel p( x) =1\\nZ˜ p( x).Theideaisthatminimizing log Zpreventsa\\nprobabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsity\\non\\xa0anautoencoder\\xa0preventstheautoencoderfrom\\xa0having\\xa0lowreconstruction\\nerroreverywhere.Inthiscase,\\xa0theconnectionisonthelevelofanintuitive\\nunderstandingofageneralmechanismratherthanamathematical correspondence.\\nTheinterpretation ofthesparsitypenaltyascorrespondingtolog pmodel( h)ina\\ndirectedmodel pmodel() h pmodel( ) x h|ismoremathematically straightforward.\\nOnewaytoachieve a c t u a l z e r o sin hforsparse(anddenoising)autoencoders\\nwasintroducedin ().Theideaistouserectiﬁedlinearunitsto Glorot e t a l .2011b\\nproducethecodelayer.Withapriorthatactuallypushestherepresentationsto\\nzero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaverage\\nnumberofzerosintherepresentation.\\n1 4 . 2 . 2 D en o i s i n g A u t o en co d ers\\nRatherthanaddingapenaltytothecostfunction,wecanobtainanautoencoder Ω \\nthatlearnssomethingusefulbychangingthereconstructionerrortermofthecost\\nfunction.\\nTraditionally,autoencodersminimizesomefunction\\nL , g f ( x(())) x (14.8)\\nwhere Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas\\nthe L2normoftheirdiﬀerence.\\xa0This encourages g f◦tolearntobemerelyan\\nidentityfunctioniftheyhavethecapacitytodoso.\\nA orDAEinsteadminimizes denoising aut o e nc o der\\nL , g f ( x((˜ x))) , (14.9)\\nwhere ˜ xisacopyof xthathasbeencorruptedbysomeformofnoise.Denoising\\nautoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheir\\ninput.\\nDenoisingtrainingforces fand gtoimplicitlylearnthestructureof pdata( x),\\nasshown\\xa0by\\xa0 ()\\xa0and ().Denoising AlainandBengio2013Bengio\\xa0 e t a l .2013c\\n5 0 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bcf1da2e-dbd0-4397-8c93-588895a1aba3', embedding=None, metadata={'page_label': '523', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nautoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemerge\\nasabyproductofminimizingreconstructionerror.Theyarealsoanexampleof\\nhowovercomplete,high-capacity modelsmaybeusedasautoencoderssolong\\nascareistakentopreventthemfromlearningtheidentityfunction.\\xa0Denoising\\nautoencodersarepresentedinmoredetailinsection.14.5\\n1 4 . 2 . 3 Regu l a ri z i n g b y P en a l i zi n g D eri v a t i v es\\nAnotherstrategyforregularizinganautoencoderistouseapenaltyasinsparse Ω\\nautoencoders,\\nL , g f , , ( x(()))+Ω( x h x) (14.10)\\nbutwithadiﬀerentformof:Ω\\nΩ( ) = h x , λ\\ue058\\ni||∇ x h i||2. (14.11)\\nThisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhen x\\nchangesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforces\\ntheautoencodertolearnfeaturesthatcaptureinformationaboutthetraining\\ndistribution.\\nAnautoencoderregularizedinthiswayiscalleda c o n t r ac t i v e aut o e nc o der\\norCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders,\\nmanifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetail\\ninsection.14.7\\n14.3RepresentationalPower,LayerSizeandDepth\\nAutoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayer\\ndecoder.However,thisisnotarequirement.Infact,usingdeepencodersand\\ndecodersoﬀersmanyadvantages.\\nRecallfromsectionthattherearemanyadvantagestodepthinafeedfor- 6.4.1\\nwardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantages\\nalsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetwork\\nasisthedecoder,soeachofthesecomponentsoftheautoencodercanindividually\\nbeneﬁtfromdepth.\\nOnemajoradvantageofnon-trivialdepthisthattheuniversalapproximator\\ntheoremguaranteesthatafeedforwardneuralnetworkwithatleastonehidden\\nlayercanrepresentanapproximationofanyfunction(withinabroadclass)toan\\n5 0 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4803d9fa-3108-406a-b43f-819dd903cd44', embedding=None, metadata={'page_label': '524', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\narbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeans\\nthatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentity\\nfunctionalongthedomainofthedataarbitrarilywell.However,themappingfrom\\ninputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitrary\\nconstraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withat\\nleastoneadditionalhiddenlayerinsidetheencoderitself,canapproximate any\\nmappingfrominputtocodearbitrarilywell,givenenoughhiddenunits.\\nDepthcanexponentiallyreducethecomputational costofrepresentingsome\\nfunctions.Depthcanalsoexponentiallydecreasetheamountoftrainingdata\\nneededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof 6.4.1\\ndepthinfeedforwardnetworks.\\nExperimentally,deepautoencodersyieldmuchbettercompressionthancorre-\\nspondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,).\\nAcommonstrategyfortrainingadeepautoencoderistogreedilypretrain\\nthedeeparchitecturebytrainingastackofshallowautoencoders,soweoften\\nencountershallowautoencoders,evenwhentheultimategoalistotrainadeep\\nautoencoder.\\n14.4StochasticEncodersandDecoders\\nAutoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutput\\nunittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedfor\\nautoencoders.\\nAsdescribedinsection,ageneralstrategyfordesigningtheoutputunits 6.2.2.4\\nandthelossfunctionofafeedforwardnetworkistodeﬁneanoutputdistribution\\np( y x|)andminimizethenegativelog-likelihood−log p( y x|).Inthatsetting, y\\nwasavectoroftargets,suchasclasslabels.\\nInthecaseofanautoencoder, xisnowthetargetaswellastheinput.However,\\nwecanstillapplythesamemachineryasbefore.Givenahiddencode h,wemay\\nthinkofthedecoderasprovidingaconditionaldistribution pdecoder( x h|).\\xa0We\\nmaythentraintheautoencoderbyminimizing −log pdecoder( ) x h|.Theexact\\nformofthislossfunctionwillchangedependingontheformof pdecoder.Aswith\\ntraditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrize\\nthemeanofaGaussiandistributionif xisreal-valued.Inthatcase,thenegative\\nlog-likelihoodyieldsameansquarederrorcriterion.Similarly,binary xvalues\\ncorrespondtoaBernoullidistributionwhoseparametersaregivenbyasigmoid\\noutputunit,discrete xvaluescorrespondtoasoftmaxdistribution,andsoon.\\n5 0 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4dbd4d0f-58f6-42d9-9136-e1b49becab34', embedding=None, metadata={'page_label': '525', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nTypically,theoutputvariablesaretreatedasbeingconditionallyindependent\\ngiven hsothatthisprobabilitydistributionisinexpensivetoevaluate,butsome\\ntechniquessuchasmixturedensityoutputsallowtractablemodelingofoutputs\\nwithcorrelations.\\nxx rrh h\\np e n c o d e r ( ) h x| p d e c o d e r ( ) x h|\\nFigure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthe\\ndecoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthat\\ntheiroutputcanbeseenassampledfromadistribution, pencoder( h x|)fortheencoder\\nand pdecoder( ) x h|forthedecoder.\\nTomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseen\\npreviously,wecanalsogeneralizethenotionofan e nc o di ng f unc t i o n f( x)to\\nan e nc o di ng di st r i but i o n pencoder( ) h x|,asillustratedinﬁgure.14.2\\nAnylatentvariablemodel pmodel( ) h x ,deﬁnesastochasticencoder\\npencoder( ) = h x| pmodel( ) h x| (14.12)\\nandastochasticdecoder\\npdecoder( ) = x h| pmodel( ) x h| . (14.13)\\nIngeneral,theencoderanddecoderdistributionsarenotnecessarilyconditional\\ndistributionscompatiblewithauniquejointdistribution pmodel( x h ,).Alain e t a l .\\n()showedthattrainingtheencoderanddecoderasadenoisingautoencoder 2015\\nwilltendtomakethemcompatibleasymptotically(withenoughcapacityand\\nexamples).\\n14.5DenoisingAutoencoders\\nThe denoising aut o e nc o der(DAE)isanautoencoderthatreceivesacorrupted\\ndatapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapoint\\nasitsoutput.\\nTheDAEtrainingprocedureisillustratedinﬁgure.Weintroducea 14.3\\ncorruptionprocess C(˜x x|)whichrepresentsaconditional\\xa0distrib utionover\\n5 1 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c2ea201f-9d42-414c-99c1-3d79ac79f3b7', embedding=None, metadata={'page_label': '526', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14. AUTOENCODERS\\n˜ x ˜ x L Lh h\\nfg\\nx xC ( ˜ x x| )\\nFigure14.3: Thecomputational graph of the cost function for a denoising autoencoder,\\nwhich is trained to reconstruct the clean data point xfrom its corrupted version ˜ x.\\nThis is accomplished by minimizing the loss L=−log pdecoder ( x h|= f(˜ x)), where\\n˜ xis a corrupted version of the data example x, obtained through a given corruption\\nprocess C(˜ x x|). Typicallythedistribution pdecoderisafactorialdistributionwhosemean\\nparameters are emitted bya feedforwardnetwork . g\\ncorrupted samples ˜ x, given a data sample x. The autoencoder then learns a\\nreconstruction distribution preconstruct ( x|˜ x)estimated from training pairs\\n(x,˜ x),asfollows:\\n1. Sampleatrainingexample fromthetrainingdata. x\\n2. Sampleacorruptedversion ˜ xfrom C(˜ x x|= ) x.\\n3.Use( x,˜ x)asatrainingexampleforestimatingtheautoencoderreconstruction\\ndistribution preconstruct ( x|˜x) = pdecoder ( x h|)with htheoutputofencoder\\nf(˜ x)and pdecodertypicallydeﬁnedbyadecoder . g( ) h\\nTypicallywecansimplyperformgradient-basedapproximateminimization(such\\nasminibatchgradientdescent)onthenegativelog-likelihood −log pdecoder ( x h|).\\nSolongastheencoderisdeterministic,thedenoisingautoencoderisafeedforward\\nnetwork\\xa0and may\\xa0be\\xa0trained with\\xa0exactly the same\\xa0techniques\\xa0as\\xa0any other\\nfeedforwardnetwork.\\nWecanthereforeviewtheDAEasperformingstochasticgradientdescenton\\nthefollowingexpectation:\\n− E x∼ˆ p data( ) x E˜ x∼C(˜x|x)log pdecoder ( = ( x h| f˜ x))(14.14)\\nwhere ˆ pdata( ) xisthetrainingdistribution.\\n511', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e9903174-3229-43b8-b397-ea0849343f6f', embedding=None, metadata={'page_label': '527', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nx˜ x\\ng f◦\\n˜ x\\nC ( ˜ x x| )\\nx\\nFigure14.4:Adenoisingautoencoderistrainedtomapacorrupteddatapoint˜xbackto\\ntheoriginaldatapoint x.Weillustratetrainingexamples xasredcrosseslyingneara\\nlow-dimensionalmanifoldillustratedwiththeboldblackline.Weillustratethecorruption\\nprocess C(˜x x|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrates\\nhowonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess.\\nWhenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors\\n|| g( f(˜ x))−|| x2,thereconstruction g( f(˜ x)) estimates E x ,˜ x∼ p dat a()( x C˜x x|)[ x|˜ x].Thevector\\ng( f(˜x))−˜ xpointsapproximatelytowardsthenearestpointonthemanifold,since g( f(˜x))\\nestimatesthecenterofmassofthecleanpoints xwhichcouldhavegivenriseto˜ x.The\\nautoencoderthuslearnsavectorﬁeld g( f( x))− xindicatedbythegreenarrows.This\\nvectorﬁeldestimatesthescore∇ xlog pdata( x)uptoamultiplicativefactorthatisthe\\naveragerootmeansquarereconstructionerror.\\n5 1 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='115d1e26-c70f-4ab7-93b0-5cad5b6b090b', embedding=None, metadata={'page_label': '528', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\n1 4 . 5 . 1 E s t i m a t i n g t h e S co re\\nScorematching(,)isanalternativetomaximumlikelihood.It Hyvärinen2005\\nprovidesaconsistentestimatorofprobabilitydistributionsbasedonencouraging\\nthemodeltohavethesame sc o r easthedatadistributionateverytrainingpoint\\nx.Inthiscontext,thescoreisaparticulargradientﬁeld:\\n∇ xlog() p x . (14.15)\\nScorematchingisdiscussedfurtherinsection.Forthepresentdiscussion 18.4\\nregardingautoencoders,itissuﬃcienttounderstandthatlearningthegradient\\nﬁeldoflog pdataisonewaytolearnthestructureof pdataitself.\\nAveryimportantpropertyofDAEsisthat\\xa0theirtrainingcriterion(with\\nconditionallyGaussian p( x h|))makes\\xa0theautoencoder\\xa0learnavectorﬁeld\\n( g( f( x))− x)thatestimatesthescoreofthedatadistribution.Thisisillustrated\\ninﬁgure.14.4\\nDenoisingtrainingofaspeciﬁckindofautoencoder(sigmoidalhiddenunits,\\nlinear\\xa0reconstr uction\\xa0units) usingGaussiannoiseand\\xa0meansquared\\xa0erroras\\nthereconstructioncostisequivalent(,)totrainingaspeciﬁckind Vincent2011\\nofundirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits.\\nThiskindofmodelwillbedescribedindetailinsection;forthepresent 20.5.1\\ndiscussionitsuﬃcestoknowthatitisamodelthatprovidesanexplicit pmodel( x; θ).\\nWhentheRBMistrainedusing denoising sc o r e m at c hi n g( , KingmaandLeCun\\n2010),itslearningalgorithmisequivalenttodenoisingtraininginthecorresponding\\nautoencoder.Withaﬁxednoiselevel,regularizedscorematchingisnotaconsistent\\nestimator;itinsteadrecoversablurredversionofthedistribution.However,if\\nthenoiselevelischosentoapproach0whenthenumberofexamplesapproaches\\ninﬁnity,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedin\\nmoredetailinsection.18.5\\nOtherconnectionsbetweenautoencodersandRBMsexist.Scorematching\\nappliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerror\\ncombinedwitharegularizationtermsimilartothecontractivepenaltyofthe\\nCAE(Swersky2011BengioandDelalleau2009 e t a l .,). ()showedthatanautoen-\\ncodergradientprovidesanapproximationtocontrastivedivergencetrainingof\\nRBMs.\\nForcontinuous-valued x,thedenoisingcriterionwithGaussiancorruptionand\\nreconstructiondistributionyieldsanestimatorofthescorethatisapplicableto\\ngeneralencoderanddecoderparametrizations ( ,).This AlainandBengio2013\\nmeansagenericencoder-decoderarchitecturemaybemadetoestimatethescore\\n5 1 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='649d012c-ae2f-48a1-8b2d-0b72f583a91b', embedding=None, metadata={'page_label': '529', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nbytrainingwiththesquarederrorcriterion\\n|| g f((˜ x x))−||2(14.16)\\nandcorruption\\nC(˜ x=˜x x|) = (N˜ x x ;= µ , σΣ = 2I) (14.17)\\nwithnoisevariance σ2.Seeﬁgureforanillustrationofhowthisworks. 14.5\\nFigure14.5:Vectorﬁeldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifold\\nnearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothe\\nreconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobability\\naccordingtotheimplicitlyestimatedprobabilitydistribution.Thevectorﬁeldhaszeros\\natbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminima\\nofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldof\\nlocalmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleof\\nthegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelength\\nofthearrows)islarge,itmeansthatprobabilitycanbesigniﬁcantlyincreasedbymoving\\ninthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability.\\nTheautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions.\\nWhereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmore\\naccurate.Figurereproducedwithpermissionfrom (). AlainandBengio2013\\nIngeneral,thereisnoguaranteethatthereconstruction g( f( x))minusthe\\ninput xcorrespondstothegradientofanyfunction,letalonetothescore.Thatis\\n5 1 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a70f4654-4422-46e3-b396-816d8988f8f6', embedding=None, metadata={'page_label': '530', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nwhytheearlyresults(,)arespecializedtoparticularparametrizations Vincent2011\\nwhere g( f( x))− xmaybeobtainedbytakingthederivativeofanotherfunction.\\nKamyshanskaandMemisevic2015 Vincent2011 ()generalizedtheresultsof()by\\nidentifyingafamilyofshallowautoencoderssuchthat g( f( x))− xcorrespondsto\\nascoreforallmembersofthefamily.\\nSofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresent\\naprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderas\\nagenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribed\\nlater,insection.20.11\\n1 4 . 5 . 1 . 1 Hi st o r i c a l P e r spec t i v e\\nTheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987\\nand ().()alsousedrecurrentnetworkstodenoise Gallinari e t a l .1987Behnke2001\\nimages.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise.\\nHowever,thename“denoisingautoencoder”referstoamodelthatisintendednot\\nmerelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentation\\nas\\xa0asideeﬀect\\xa0oflearningto\\xa0denoise.This\\xa0ideacame\\xa0muchlater\\xa0(Vincent\\ne t a l .,,).Thelearnedrepresentationmaythenbeusedtopretraina 20082010\\ndeeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders,\\nsparsecoding,contractiveautoencodersandotherregularizedautoencoders,the\\nmotivationforDAEswastoallowthelearningofaveryhigh-capacity encoder\\nwhilepreventingtheencoderanddecoderfromlearningauselessidentityfunction.\\nPriortotheintroduction ofthemodernDAE,InayoshiandKurita2005()\\nexploredsomeofthesamegoalswithsomeofthesamemethods.Theirapproach\\nminimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjecting\\nnoiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprove\\ngeneralization byintroducing\\xa0the reconstructionerror\\xa0andtheinjectednoise.\\nHowever,theirmethodwasbasedonalinearencoderandcouldnotlearnfunction\\nfamiliesaspowerfulascanthemodernDAE.\\n14.6LearningManifoldswithAutoencoders\\nLike\\xa0many\\xa0other\\xa0machine\\xa0learning\\xa0algorithms,\\xa0auto encoders\\xa0exploittheidea\\nthatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuch\\nmanifolds,asdescribedinsection.Somemachinelearningalgorithmsexploit 5.11.3\\nthisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthe\\nmanifoldbutmayhaveunusualbehaviorifgivenaninputthatisoﬀthemanifold.\\n5 1 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b957c93-61a9-4c2a-9e97-c9da0fc73bde', embedding=None, metadata={'page_label': '531', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nAutoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold.\\nTounderstandhowautoencodersdothis,wemustpresentsomeimportant\\ncharacteristicsofmanifolds.\\nAnimportantcharacterization ofamanifoldisthesetofits t angen t pl anes.\\nAtapoint xona d-dimensionalmanifold,thetangentplaneisgivenby dbasis\\nvectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.As\\nillustratedinﬁgure,theselocaldirectionsspecifyhowonecanchange 14.6 x\\ninﬁnitesimallywhilestayingonthemanifold.\\nAllautoencodertrainingproceduresinvolveacompromisebetweentwoforces:\\n1.Learningarepresentation hofatrainingexample xsuchthat xcanbe\\napproximatelyrecoveredfrom hthroughadecoder.Thefactthat xisdrawn\\nfromthetrainingdataiscrucial,becauseitmeanstheautoencoderneed\\nnotsuccessfullyreconstructinputsthatarenotprobableunderthedata\\ngeneratingdistribution.\\n2.\\xa0Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec-\\nturalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbe\\naregularizationtermaddedtothereconstructioncost.Thesetechniques\\ngenerallyprefersolutionsthatarelesssensitivetotheinput.\\nClearly,neitherforcealonewouldbeuseful—copyingtheinputtotheoutput\\nisnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogether\\nareusefulbecausetheyforcethehiddenrepresentationtocaptureinformation\\naboutthestructureofthedatageneratingdistribution.Theimportantprinciple\\nisthattheautoencodercanaﬀordtorepresent o nl y t h e v a r i a t i o ns t h a t a r e ne e d e d\\nt o r e c o ns t r u c t t r a i ning e x a m p l e s.Ifthedatageneratingdistributionconcentrates\\nnearalow-dimensional manifold,thisyieldsrepresentationsthatimplicitlycapture\\nalocalcoordinatesystemforthismanifold:onlythevariationstangenttothe\\nmanifoldaround xneedtocorrespondtochangesin h= f( x).Hencetheencoder\\nlearnsamappingfromtheinputspace xtoarepresentationspace,amappingthat\\nisonlysensitivetochangesalongthemanifolddirections,butthatisinsensitiveto\\nchangesorthogonaltothemanifold.\\nAone-dimensional exampleisillustratedinﬁgure,showingthat,bymaking 14.7\\nthereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthe\\ndatapoints,wecausetheautoencodertorecoverthemanifoldstructure.\\nTounderstandwhyautoencodersareusefulformanifoldlearning,itisin-\\nstructivetocomparethemtootherapproaches.Whatismostcommonlylearned\\ntocharacterizeamanifoldisa r e pr e se n t at i o nofthedatapointson(ornear)\\n5 1 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b23837d6-b4c9-434b-bd3a-44e13441e024', embedding=None, metadata={'page_label': '532', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nFigure14.6:\\xa0Anillustrationoftheconceptofatangenthyperplane.Herewecreatea\\none-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784\\npixelsandtransformitbytranslatingitvertically.\\xa0Theamountofverticaltranslation\\ndeﬁnesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpath\\nthroughimagespace.Thisplotshowsafewpointsalongthismanifold.\\xa0Forvisualization,\\nwehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.An n-dimensional\\nmanifoldhasan n-dimensionaltangentplaneateverypoint.Thistangentplanetouches\\nthemanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint.\\nItdeﬁnesthespaceofdirectionsinwhichitispossibletomovewhileremainingon\\nthemanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicatean\\nexampletangentlineatonepoint,withanimageshowinghowthistangentdirection\\nappearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealong\\nthetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixels\\nthatdarken.\\n5 1 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22e98a01-25a8-4043-8666-2b4e3b0f0c7c', embedding=None, metadata={'page_label': '533', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nx 0 x 1 x 2\\nx0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .r x ( )Id e n t i t y\\nO p t i m a l r e c o n s t r u c t i o n\\nFigure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmall\\nperturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Here\\nthemanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal 0\\nlineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstruction\\nfunctioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontal\\narrowsatthebottomoftheplotindicatethe r( x)− xreconstructiondirectionvector\\natthebaseofthearrow,ininputspace,alwayspointingtowardsthenearest“manifold”\\n(asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomake\\nthederivativeofthereconstructionfunction r( x)smallaroundthedatapoints.The\\ncontractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeof r( x)is\\naskedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.The\\nspacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,where\\nthereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpoints\\nbackontothemanifold.\\nthemanifold.Sucharepresentationforaparticularexampleisalsocalledits\\nembedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensions\\nthanthe“ambient”spaceofwhichthemanifoldisalow-dimensionalsubset.Some\\nalgorithms(non-parametric manifoldlearningalgorithms,discussedbelow)directly\\nlearnanembeddingforeachtrainingexample,whileotherslearnamoregeneral\\nmapping,sometimescalledanencoder,orrepresentationfunction,thatmapsany\\npointintheambientspace(theinputspace)toitsembedding.\\nManifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthat\\nattempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearch\\nonlearningnonlinearmanifoldshasfocusedon non-par a m e t r i cmethodsbased\\nonthe near e st - n e i g h b o r g r aph.Thisgraphhasonenodepertrainingexample\\nandedgesconnectingnearneighborstoeachother.Thesemethods(Schölkopf\\ne t a l .,;1998RoweisandSaul2000Tenenbaum2000Brand2003Belkin ,; e t a l .,;,;\\n5 1 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='71fc8ff2-e47b-46a2-bea6-7e3c8709ed41', embedding=None, metadata={'page_label': '534', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nFigure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraph\\ninwhichnodesrepresenttrainingexamplesadirectededgesindicatenearestneighbor\\nrelationships.\\xa0Variousprocedurescanthusobtainthetangentplaneassociatedwitha\\nneighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtraining\\nexamplewithareal-valuedvectorposition,or e m b e d d in g.Itispossibletogeneralize\\nsucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumber\\nofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,these\\napproachesworkwell.ImagesfromtheQMULMultiviewFaceDataset( , Gong e t a l .\\n2000).\\nandNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton ,; ,; ,;\\nandRoweis2003vanderMaatenandHinton2008 ,; ,)associateeachofnodeswitha\\ntangentplanethatspansthedirectionsofvariationsassociatedwiththediﬀerence\\nvectorsbetweentheexampleanditsneighbors,asillustratedinﬁgure.14.8\\nAglobalcoordinatesystemcanthenbeobtainedthroughanoptimization or\\nsolvingalinearsystem.Figureillustrateshowamanifoldcanbetiledbya 14.9\\nlargenumberoflocallylinearGaussian-likepatches(or“pancakes,”becausethe\\nGaussiansareﬂatinthetangentdirections).\\nHowever,thereisafundamentaldiﬃcultywithsuchlocalnon-parametric\\napproachestomanifoldlearning,raisedin ():ifthe BengioandMonperrus2005\\nmanifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists),\\nonemayneedaverylargenumberoftrainingexamplestocovereachoneof\\n5 1 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='393d6bb4-0e06-40ad-91cc-d6ec71ed4ddb', embedding=None, metadata={'page_label': '535', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nFigure14.9:Ifthetangentplanes(seeﬁgure)ateachlocationareknown,thenthey 14.6\\ncanbetiledtoformaglobalcoordinatesystemoradensityfunction.Eachlocalpatch\\ncanbethoughtofasalocalEuclideancoordinatesystemorasalocallyﬂatGaussian,or\\n“pancake,”withaverysmallvarianceinthedirectionsorthogonaltothepancakeanda\\nverylargevarianceinthedirectionsdeﬁningthecoordinatesystemonthepancake.A\\nmixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifold\\nParzenwindowalgorithm( ,)oritsnon-localneural-netbased VincentandBengio2003\\nvariant( ,). Bengio e t a l .2006c\\nthesevariations,withnochancetogeneralizetounseenvariations.Indeed,these\\nmethodscanonlygeneralizetheshapeofthemanifoldbyinterpolating between\\nneighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscan\\nhaveverycomplicatedstructurethatcanbediﬃculttocapturefromonlylocal\\ninterpolation.Considerforexamplethemanifoldresultingfromtranslationshown\\ninﬁgure.Ifwewatchjustonecoordinatewithintheinputvector, 14.6 x i,asthe\\nimageistranslated,wewillobservethatonecoordinateencountersapeakora\\ntroughinitsvalueonceforeverypeakortroughinbrightnessintheimage.\\xa0In\\notherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimage\\ntemplatedrivesthecomplexityofthemanifoldsthataregeneratedbyperforming\\nsimpleimagetransformations.Thismotivatestheuseofdistributedrepresentations\\nanddeeplearningforcapturingmanifoldstructure.\\n5 2 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d5a4f48d-d6d7-4ad2-a413-7a3bf581d9d8', embedding=None, metadata={'page_label': '536', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\n14.7ContractiveAutoencoders\\nThecontractiveautoencoder(,,)introducesanexplicitregularizer Rifai e t a l .2011ab\\nonthecode h= f( x),encouragingthederivativesof ftobeassmallaspossible:\\nΩ() = h λ\\ue00d\\ue00d\\ue00d\\ue00d∂ f() x\\n∂ x\\ue00d\\ue00d\\ue00d\\ue00d2\\nF. (14.18)\\nThepenaltyΩ( h)isthesquaredFrobeniusnorm(sumofsquaredelements)ofthe\\nJacobianmatrixofpartialderivativesassociatedwiththeencoderfunction.\\nThereisaconnectionbetweenthedenoisingautoencoderandthecontractive\\nautoencoder: ()showedthatinthelimitofsmallGaussian AlainandBengio2013\\ninput\\xa0noise,\\xa0the\\xa0denoising\\xa0reconstruction erroris\\xa0equivalent\\xa0toacontractive\\npenaltyonthereconstructionfunctionthatmaps xto r= g( f( x)).Inother\\nwords,denoisingautoencodersmakethereconstructionfunctionresistsmallbut\\nﬁnite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethe\\nfeatureextractionfunctionresistinﬁnitesimalperturbationsoftheinput.When\\nusingtheJacobian-basedcontractivepenaltytopretrainfeatures f( x)foruse\\nwithaclassiﬁer,thebestclassiﬁcationaccuracyusuallyresultsfromapplyingthe\\ncontractivepenaltyto f( x)ratherthanto g( f( x)).Acontractivepenaltyon f( x)\\nalsohascloseconnectionstoscorematching,asdiscussedinsection.14.5.1\\nThename c o n t r ac t i v earisesfromthewaythattheCAEwarpsspace.Speciﬁ-\\ncally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouraged\\ntomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints.\\nWecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutput\\nneighborhood.\\nToclarify,theCAEiscontractiveonlylocally—allperturbationsofatraining\\npoint xaremappednearto f( x).Globally,twodiﬀerentpoints xand x\\ue030maybe\\nmappedto f( x)and f( x\\ue030)pointsthatarefartherapartthantheoriginalpoints.\\nItisplausiblethat fbeexpandingin-betweenorfarfromthedatamanifolds(see\\nforexamplewhathappensinthe1-Dtoyexampleofﬁgure).Whenthe 14.7 Ω( h)\\npenaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianisto\\nmakethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode 01\\ninputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasa\\nbinarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughout\\nmostofthehypercubethatitssigmoidalhiddenunitscanspan.\\nWecanthinkoftheJacobianmatrix Jatapoint xasapproximating the\\nnonlinearencoder f( x)asbeingalinearoperator.Thisallowsustousetheword\\n“contractive”moreformally.\\xa0Inthetheoryoflinearoperators,alinearoperator\\n5 2 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e33abb41-3766-4312-981f-6cd7f56f8a47', embedding=None, metadata={'page_label': '537', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nissaidtobecontractiveifthenormof J xremainslessthanorequaltofor1\\nallunit-norm x.Inotherwords, Jiscontractiveifitshrinkstheunitsphere.\\nWecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinear\\napproximationof f( x)ateverytrainingpoint xinordertoencourageeachof\\ntheselocallinearoperatortobecomeacontraction.\\nAsdescribed\\xa0insection,\\xa0regularized autoencoderslearnmanifoldsby 14.6\\nbalancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesare\\nreconstructionerrorandthecontractivepenaltyΩ( h).Reconstructionerroralone\\nwouldencouragetheCAEtolearnanidentityfunction.Thecontractivepenalty\\nalonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespectto x.\\nThecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives\\n∂ f() x\\n∂ xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoa\\nsmallnumberofdirectionsintheinput,mayhavesigniﬁcantderivatives.\\nThegoaloftheCAEistolearnthemanifoldstructureofthedata.Directions\\nxwithlarge J xrapidlychange h,sothesearelikelytobedirectionswhich\\napproximatethetangentplanesofthemanifold.Experimentsby () Rifai e t a l .2011a\\nand ()showthattrainingtheCAEresultsinmostsingularvalues Rifai e t a l .2011b\\nof Jdroppingbelowinmagnitudeandthereforebecomingcontractive.However, 1\\nsomesingularvaluesremainabove,becausethereconstructionerrorpenalty 1\\nencouragestheCAEtoencodethedirectionswiththemostlocalvariance.The\\ndirectionscorrespondingtothelargestsingularvaluesareinterpretedasthetangent\\ndirectionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangent\\ndirectionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAE\\nappliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesas\\nobjectsintheimagegraduallychangepose,asshowninﬁgure.Visualizations 14.6\\noftheexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningful\\ntransformationsoftheinputimage,asshowninﬁgure.14.10\\nOnepracticalissuewiththeCAEregularizationcriterionisthatalthoughit\\nischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomes\\nmuchmoreexpensiveinthecaseofdeeperautoencoders.Thestrategyfollowedby\\nRifai2011a e t a l .()istoseparatelytrainaseriesofsingle-layerautoencoders,each\\ntrainedtoreconstructthepreviousautoencoder’shiddenlayer.Thecomposition\\noftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwas\\nseparatelytrainedtobelocallycontractive,thedeepautoencoderiscontractive\\naswell.Theresultisnotthesameaswhatwouldbeobtainedbyjointlytraining\\ntheentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butit\\ncapturesmanyofthedesirablequalitativecharacteristics.\\nAnotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults\\n5 2 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f66e9bc2-5391-4b5b-ad27-4fd97d4bfb66', embedding=None, metadata={'page_label': '538', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nInput\\npointTangentvectors\\nLocalPCA(nosharingacrossregions)\\nContractiveautoencoder\\nFigure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCA\\nandbyacontractiveautoencoder.Thelocationonthemanifoldisdeﬁnedbytheinput\\nimageofadogdrawnfromtheCIFAR-10dataset.\\xa0Thetangentvectorsareestimated\\nbytheleadingsingularvectorsoftheJacobianmatrix∂ h\\n∂ xoftheinput-to-codemapping.\\nAlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisableto\\nformmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparameter\\nsharingacrossdiﬀerentlocationsthatshareasubsetofactivehiddenunits.\\xa0TheCAE\\ntangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchas\\ntheheadorlegs).Imagesreproducedwithpermissionfrom (). Rifai e t a l .2011c\\nifwedonotimposesomesortofscaleonthedecoder.Forexample,theencoder\\ncouldconsistofmultiplyingtheinputbyasmallconstant \\ue00fandthedecoder\\ncouldconsistofdividingthecodeby \\ue00f.As \\ue00fapproaches,theencoderdrivesthe 0\\ncontractivepenaltyΩ( h)toapproachwithouthavinglearnedanythingaboutthe 0\\ndistribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifai\\ne t a l .(),thisispreventedbytyingtheweightsof 2011a fand g.Both fand gare\\nstandardneuralnetworklayersconsistingofanaﬃnetransformationfollowedby\\nanelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixof g\\ntobethetransposeoftheweightmatrixof. f\\n14.8PredictiveSparseDecomposition\\nP r e di c t i v e spar se dec o m p o si t i o n(PSD)isamodelthatisahybridofsparse\\ncodingandparametricautoencoders(Kavukcuoglu2008 e t a l .,).Aparametric\\nencoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeen\\nappliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo\\n(Kavukcuoglu20092010Jarrett2009Farabet2011 e t a l .,,; e t a l .,; e t a l .,),aswell\\nasforaudio( ,).Themodelconsistsofanencoder Henaﬀ e t a l .2011 f( x)anda\\ndecoder g( h)thatarebothparametric.Duringtraining, hiscontrolledbythe\\n5 2 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='18620c88-f027-4888-96f0-78ef50b5447f', embedding=None, metadata={'page_label': '539', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\noptimization algorithm.Trainingproceedsbyminimizing\\n||− || x g() h2+ λ|| h1+ () γ f ||− h x||2. (14.19)\\nLikeinsparsecoding,thetrainingalgorithmalternatesbetweenminimization with\\nrespectto handminimization withrespecttothemodelparameters.Minimization\\nwithrespectto hisfastbecause f( x)providesagoodinitialvalueof handthe\\ncostfunctionconstrains htoremainnear f( x)anyway.Simplegradientdescent\\ncanobtainreasonablevaluesofinasfewastensteps. h\\nThetrainingprocedureusedbyPSDisdiﬀerentfromﬁrsttrainingasparse\\ncodingmodelandthentraining f( x)topredictthevaluesofthesparsecoding\\nfeatures.ThePSDtrainingprocedureregularizesthedecodertouseparameters\\nforwhichcaninfergoodcodevalues. f() x\\nPredictivesparsecodingisanexampleof l e ar ned appr o x i m a t e i nf e r e nc e.\\nInsection,thistopicisdevelopedfurther.Thetoolspresentedinchapter 19.5 19\\nmakeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecoding\\nprobabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthe\\nmodel.\\nInpracticalapplicationsofPSD,theiterativeoptimization isonlyusedduring\\ntraining.Theparametricencoder fisusedtocomputethelearnedfeatureswhen\\nthemodelisdeployed.Evaluating fiscomputationally inexpensivecomparedto\\ninferring hviagradientdescent.Because fisadiﬀerentiableparametricfunction,\\nPSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrained\\nwithanothercriterion.\\n14.9ApplicationsofAutoencoders\\nAutoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor-\\nmationretrievaltasks.Dimensionalityreductionwasoneoftheﬁrstapplications\\nofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivations\\nforstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trained\\nastackofRBMsandthenusedtheirweightstoinitializeadeepautoencoder\\nwithgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.The\\nresultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsand\\nthelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetothe\\nunderlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters.\\nLower-dimensionalrepresentationscanimproveperformanceonmanytasks,\\nsuchasclassiﬁcation.Modelsofsmallerspacesconsumelessmemoryandruntime.\\n5 2 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a45ec15d-220c-4463-9da8-df759bb158a6', embedding=None, metadata={'page_label': '540', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER14.AUTOENCODERS\\nManyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear\\neachother,asobservedbySalakhutdinovandHinton2007bTorralba ()and e t a l .\\n().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid 2008\\ngeneralization.\\nOnetaskthatbeneﬁtsevenmorethanusualfromdimensionalityreductionis\\ni nf o r m at i o n r e t r i e v al,thetaskofﬁndingentriesinadatabasethatresemblea\\nqueryentry.\\xa0Thistaskderivestheusualbeneﬁtsfromdimensionalityreduction\\nthatothertasksdo,butalsoderivestheadditionalbeneﬁtthatsearchcanbecome\\nextremelyeﬃcientincertainkindsoflowdimensionalspaces.Speciﬁcally,\\xa0if\\nwetrainthedimensionalityreductionalgorithmtoproduceacodethatislow-\\ndimensionaland,thenwecanstorealldatabaseentriesinahashtable b i nary\\nmappingbinarycodevectorstoentries.Thishashtableallowsustoperform\\ninformationretrievalbyreturningalldatabaseentriesthathavethesamebinary\\ncodeasthe\\xa0query.Wecanalso\\xa0search\\xa0overslightlylesssimilar\\xa0entries\\xa0very\\neﬃciently,justbyﬂippingindividualbitsfromtheencodingofthequery.\\xa0This\\napproachtoinformationretrievalviadimensionalityreductionandbinarization\\niscalled se m an t i c hashing(SalakhutdinovandHinton2007b2009b,,),andhas\\nbeenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)and\\nimages(Torralba 2008Weiss2008KrizhevskyandHinton2011 e t a l .,; e t a l .,; ,).\\nToproducebinarycodesforsemantichashing,onetypicallyusesanencoding\\nfunctionwithsigmoidsontheﬁnallayer.Thesigmoidunitsmustbetrainedtobe\\nsaturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplish\\nthisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduring\\ntraining.Themagnitudeofthenoiseshouldincreaseovertime.Toﬁghtthat\\nnoiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethe\\nmagnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs.\\nTheideaoflearningahashingfunctionhasbeenfurtherexploredinseveral\\ndirections,includingtheideaoftrainingtherepresentationssoastooptimize\\nalossmoredirectlylinkedtothetaskofﬁndingnearbyexamplesinthehash\\ntable( ,). NorouziandFleet2011\\n5 2 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6582b72e-c7d9-4a47-892c-f9623daa31e4', embedding=None, metadata={'page_label': '541', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 5\\nRepresen t at i on L e ar n i n g\\nInthischapter,weﬁrstdiscusswhatitmeanstolearnrepresentationsandhow\\nthenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscuss\\nhowlearningalgorithmssharestatisticalstrengthacrossdiﬀerenttasks,including\\nusinginformationfromunsupervisedtaskstoperformsupervisedtasks.Shared\\nrepresentationsareusefultohandlemultiplemodalitiesordomains,ortotransfer\\nlearnedknowledgetotasksforwhichfewornoexamplesaregivenbutatask\\nrepresentationexists.Finally,westepbackandargueaboutthereasonsforthe\\nsuccessofrepresentationlearning,startingwiththetheoreticaladvantagesof\\ndistributedrepresentations(Hinton1986etal.,)anddeeprepresentationsand\\nendingwiththemoregeneralideaofunderlyingassumptionsaboutthedata\\ngeneratingprocess,inparticularaboutunderlyingcausesoftheobserveddata.\\nManyinformationprocessingtaskscanbeveryeasyorverydiﬃcultdepending\\nonhowtheinformationisrepresented.Thisisageneralprincipleapplicableto\\ndailylife,computerscienceingeneral,andtomachinelearning.Forexample,it\\nisstraightforwardforapersontodivide210by6usinglongdivision.\\xa0Thetask\\nbecomesconsiderablylessstraightforwardifitisinsteadposedusingtheRoman\\nnumeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCX\\nbyVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation,\\npermittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.More\\nconcretely,wecanquantifytheasymptoticruntimeofvariousoperationsusing\\nappropriateorinappropriate representations.Forexample,insertinganumber\\nintothecorrectpositioninasortedlistofnumbersisanO(n)operationifthe\\nlistisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasa\\nred-blacktree.\\nInthecontextofmachinelearning,whatmakesonerepresentationbetterthan\\n526', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3bf07ecd-bc5c-410e-9c05-5c96315e6d5e', embedding=None, metadata={'page_label': '542', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nanother?Generallyspeaking,agoodrepresentationisonethatmakesasubsequent\\nlearningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoice\\nofthesubsequentlearningtask.\\nWecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper-\\nformingakindofrepresentationlearning.Speciﬁcally,thelastlayerofthenetwork\\nistypicallyalinearclassiﬁer,suchasasoftmaxregressionclassiﬁer.Therestof\\nthenetworklearnstoprovidearepresentationtothisclassiﬁer.Trainingwitha\\nsupervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(but\\nmoresonearthetophiddenlayer)takingonpropertiesthatmaketheclassiﬁcation\\ntaskeasier.Forexample,classesthatwerenotlinearlyseparableintheinput\\nfeaturesmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,the\\nlastlayercouldbeanotherkindofmodel,suchasanearestneighborclassiﬁer\\n(SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershould\\nlearndiﬀerentpropertiesdependingonthetypeofthelastlayer.\\nSupervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposing\\nanyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentation\\nlearningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationin\\nsomeparticularway.Forexample,supposewewanttolearnarepresentationthat\\nmakesdensityestimationeasier.Distributionswithmoreindependencesareeasier\\ntomodel,sowecoulddesignanobjectivefunctionthatencouragestheelements\\noftherepresentationvectorhtobeindependent.Justlikesupervisednetworks,\\nunsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalso\\nlearnarepresentationasasideeﬀect.Regardlessofhowarepresentationwas\\nobtained,itcanbeusedforanothertask.Alternatively,multipletasks(some\\nsupervised,someunsupervised)canbelearnedtogetherwithsomesharedinternal\\nrepresentation.\\nMostrepresentationlearningproblemsfaceatradeoﬀbetweenpreservingas\\nmuchinformationabouttheinputaspossibleandattainingniceproperties(such\\nasindependence).\\nRepresentationlearningisparticularlyinterestingbecauseitprovidesone\\nwaytoperformunsupervisedandsemi-supervisedlearning.Weoftenhavevery\\nlargeamountsofunlabeledtrainingdataandrelativelylittlelabeledtraining\\ndata.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoften\\nresultsinsevereoverﬁtting.Semi-supervisedlearningoﬀersthechancetoresolve\\nthisoverﬁttingproblembyalsolearningfromtheunlabeleddata.Speciﬁcally,\\nwecanlearngoodrepresentationsfortheunlabeleddata,andthenusethese\\nrepresentationstosolvethesupervisedlearningtask.\\nHumansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo\\n5 2 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a9b6a0fb-d85e-4893-9bf3-ad0ef3944e80', embedding=None, metadata={'page_label': '543', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nnotyetknowhowthisispossible.Manyfactorscouldexplainimprovedhuman\\nperformance—forexample,thebrainmayuseverylargeensemblesofclassiﬁers\\norBayesianinferencetechniques.Onepopularhypothesisisthatthebrainis\\nabletoleverageunsupervisedorsemi-supervisedlearning.Therearemanyways\\ntoleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthatthe\\nunlabeleddatacanbeusedtolearnagoodrepresentation.\\n15. 1 Greed y L a y er-Wi s e Un s u p ervi s ed Pret ra i n i n g\\nUnsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneural\\nnetworks,enablingresearchersfortheﬁrsttimetotrainadeepsupervisednetwork\\nwithoutrequiringarchitectural specializationslikeconvolutionorrecurrence.We\\ncallthisprocedure unsup e r v i se d pr e t r ai ni n g,ormoreprecisely, g r e e dy l a y e r -\\nwi se unsup e r v i se d pr e t r ai ni n g.Thisprocedureisacanonicalexampleofhow\\narepresentationlearnedforonetask(unsupervisedlearning,tryingtocapture\\ntheshapeoftheinputdistribution)cansometimesbeusefulforanothertask\\n(supervisedlearningwiththesameinputdomain).\\nGreedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen-\\ntationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparse\\ncodingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayeris\\npretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayer\\nandproducingasoutputanewrepresentationofthedata,whosedistribution(or\\nitsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler.\\nSeealgorithm foraformaldescription. 15.1\\nGreedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelong\\nbeenusedtosidestepthediﬃcultyofjointlytrainingthelayersofadeepneuralnet\\nforasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron\\n(Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscovery\\nthatthisgreedylearningprocedurecouldbeusedtoﬁndagoodinitialization for\\najointlearningprocedureoverallthelayers,andthatthisapproachcouldbeused\\ntosuccessfullytrainevenfullyconnectedarchitectures (Hinton2006Hinton etal.,;\\nandSalakhutdinov2006Hinton2006Bengio2007Ranzato 2007a ,;,; etal.,; etal.,).\\nPriortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepth\\nresultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknow\\nthatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeep\\narchitectures,buttheunsupervisedpretrainingapproachwastheﬁrstmethodto\\nsucceed.\\nGreedylayer-wisepretrainingiscalled g r e e dybecauseitisa g r e e dy al g o -\\n5 2 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='52d7177d-62a5-444f-937e-df085d2a99c4', embedding=None, metadata={'page_label': '544', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nr i t hm,meaningthatitoptimizeseachpieceofthesolutionindependently,one\\npieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalled l a y e r - wi se\\nbecausetheseindependentpiecesarethelayersofthenetwork.Speciﬁcally,greedy\\nlayer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhile\\nkeepingthepreviousonesﬁxed.Inparticular,thelowerlayers(whicharetrained\\nﬁrst)arenotadaptedaftertheupperlayersareintroduced.Itiscalled unsup e r -\\nv i se dbecauseeachlayeristrainedwithanunsupervisedrepresentationlearning\\nalgorithm.Howeveritisalsocalled pr e t r ai ni n g,becauseitissupposedtobe\\nonlyaﬁrststepbeforeajointtrainingalgorithmisappliedto ﬁne-t uneallthe\\nlayerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewed\\nasaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithout\\ndecreasingtrainingerror)andaformofparameterinitialization.\\nItiscommontousetheword“pretraining”torefernotonlytothepretraining\\nstageitselfbuttotheentiretwophaseprotocolthatcombinesthepretraining\\nphaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolve\\ntrainingasimpleclassiﬁerontopofthefeatureslearnedinthepretrainingphase,\\noritmayinvolvesupervisedﬁne-tuningoftheentirenetworklearnedinthe\\npretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmor\\nwhatmodeltypeisemployed,inthevastmajorityofcases,theoveralltraining\\nschemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithm\\nwillobviouslyimpactthedetails,mostapplicationsofunsupervisedpretraining\\nfollowthisbasicprotocol.\\nGreedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitialization\\nforotherunsupervisedlearningalgorithms,suchasdeepautoencoders(Hinton\\nandSalakhutdino v2006,)andprobabilisticmodelswithmanylayersoflatent\\nvariables.Suchmodelsincludedeepbeliefnetworks( ,)anddeep Hintonetal.2006\\nBoltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerative\\nmodelswillbedescribedinchapter.20\\nAsdiscussedinsection,\\xa0itisalsopossibletohavegreedylayer-wise 8.7.4\\nsupervisedpretraining.Thisbuildsonthepremisethattrainingashallownetwork\\niseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveral\\ncontexts(,). Erhanetal.2010\\n1 5 . 1 . 1 Wh en a n d Wh y D o es Un s u p ervi s ed P ret ra i n i n g W o rk?\\nOnmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantial\\nimprovementsintesterrorforclassiﬁcationtasks.Thisobservationwasresponsible\\nfortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal.,\\n5 2 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f7bfd5c3-ecf9-41cb-bd57-f3bcca45a185', embedding=None, metadata={'page_label': '545', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nAl g o r i t hm 1 5 . 1Greedylayer-wiseunsupervisedpretrainingprotocol.\\nGiventhefollowing:\\xa0Unsupervisedfeaturelearningalgorithm L,whichtakesa\\ntrainingsetofexamplesandreturnsanencoderorfeaturefunctionf.Theraw\\ninputdataisX,withonerowperexampleandf( 1 )(X)istheoutputoftheﬁrst\\nstageencoderonX.Inthecasewhereﬁne-tuningisperformed,weusealearner\\nTwhichtakesaninitialfunctionf,inputexamplesX(andinthesupervised\\nﬁne-tuningcase,associatedtargetsY),andreturnsatunedfunction.Thenumber\\nofstagesis.m\\nf←Identityfunction\\n˜XX= \\nf o r dok,...,m = 1\\nf( ) k= (L˜X)\\nff←( ) k◦f\\n˜X←f( ) k(˜X)\\ne nd f o r\\ni fﬁne-tuning t he n\\nff,, ←T(XY)\\ne nd i f\\nRet ur nf\\n2006Bengio2007Ranzato 2007a ; etal.,; etal.,).Onmanyothertasks,however,\\nunsupervisedpretrainingeitherdoesnotconferabeneﬁtorevencausesnoticeable\\nharm. ()studiedtheeﬀectofpretrainingonmachinelearning Maetal.2015\\nmodelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwas\\nslightlyharmful,butformanytaskswassigniﬁcantlyhelpful.Becauseunsupervised\\npretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstand\\nwhenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticular\\ntask.\\nAttheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestricted\\ntogreedyunsupervisedpretraininginparticular.Thereareother,completely\\ndiﬀerentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks,\\nsuchasvirtualadversarialtrainingdescribedinsection.Itisalsopossibleto 7.13\\ntrainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel.\\nExamplesofthissingle-stageapproachincludethediscriminativeRBM(Larochelle\\nandBengio2008,)andtheladdernetwork( ,),inwhichthetotal Rasmusetal.2015\\nobjectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonly\\nusingtheinput).\\nUnsupervisedpretrainingcombinestwodiﬀerentideas.First,itmakesuseof\\n5 3 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='483f2faf-6efa-457a-b214-3f7ad9ef7ffc', embedding=None, metadata={'page_label': '546', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\ntheideathatthechoiceofinitialparametersforadeepneuralnetworkcanhave\\nasigniﬁcantregularizingeﬀectonthemodel(and,toalesserextent,thatitcan\\nimproveoptimization). Second,itmakesuseofthemoregeneralideathatlearning\\nabouttheinputdistributioncanhelptolearnaboutthemappingfrominputsto\\noutputs.\\nBothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveral\\npartsofthemachinelearningalgorithmthatarenotentirelyunderstood.\\nTheﬁrstidea,thatthechoiceofinitialparametersforadeepneuralnetwork\\ncanhaveastrongregularizingeﬀectonitsperformance, istheleastwellunderstood.\\nAtthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthe\\nmodelinalocationthatwouldcauseittoapproachonelocalminimumratherthan\\nanother.\\xa0Today,localminimaarenolongerconsideredtobeaseriousproblem\\nforneuralnetworkoptimization. Wenowknowthatourstandardneuralnetwork\\ntrainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremains\\npossiblethatpretraininginitializesthemodelinalocationthatwouldotherwise\\nbeinaccessible—forexample,aregionthatissurroundedbyareaswherethecost\\nfunctionvariessomuchfromoneexampletoanotherthatminibatchesgiveonly\\naverynoisyestimateofthegradient,oraregionsurroundedbyareaswherethe\\nHessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuse\\nverysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthe\\npretrainedparametersareretainedduringthesupervisedtrainingstageislimited.\\nThisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervised\\nlearningandsupervisedlearningratherthantwosequentialstages.Onemay\\nalsoavoidstrugglingwiththesecomplicatedideasabouthowoptimization inthe\\nsupervisedlearningstagepreservesinformationfromtheunsupervisedlearning\\nstagebysimplyfreezingthe\\xa0parameters for\\xa0thefeature\\xa0extractorsand\\xa0using\\nsupervisedlearningonlytoaddaclassiﬁerontopofthelearnedfeatures.\\nTheotheridea,thatalearningalgorithmcanuseinformationlearnedinthe\\nunsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetter\\nunderstood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised\\ntaskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrain\\nagenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowabout\\nwheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate,\\ntherepresentationofthewheelswilltakeonaformthatiseasyforthesupervised\\nlearnertoaccess.Thisisnotyetunderstoodatamathematical, theoreticallevel,\\nsoitisnotalwayspossibletopredictwhichtaskswillbeneﬁtfromunsupervised\\nlearninginthisway.Manyaspectsofthisapproacharehighlydependenton\\nthespeciﬁcmodelsused.Forexample,ifwewishtoaddalinearclassiﬁeron\\n5 3 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6c7a4fa3-fbd7-42dd-bc64-a4c00784fc9f', embedding=None, metadata={'page_label': '547', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\ntopofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearly\\nseparable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.This\\nisanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbe\\npreferable—theconstraintsimposedbytheoutputlayerarenaturallyincluded\\nfromthestart.\\nFromthepointofviewofunsupervisedpretrainingaslearningarepresentation,\\nwecanexpectunsupervisedpretrainingtobemoreeﬀectivewhentheinitial\\nrepresentationispoor.\\xa0Onekeyexampleofthisistheuseofwordembeddings.\\nWordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwo\\ndistinctone-hotvectorsarethesamedistancefromeachother(squaredL2distance\\nof).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir 2\\ndistancefromeachother.Becauseofthis,unsupervisedpretrainingisespecially\\nusefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhaps\\nbecauseimagesalreadylieinarichvectorspacewheredistancesprovidealow\\nqualitysimilaritymetric.\\nFromthepointofviewofunsupervisedpretrainingasaregularizer,wecan\\nexpectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeled\\nexamplesisverysmall.Becausethesourceofinformationaddedbyunsupervised\\npretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretraining\\ntoperformbest\\xa0whenthe\\xa0number\\xa0ofunlabeled\\xa0examples is\\xa0very\\xa0large.The\\nadvantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmany\\nunlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin\\n2011withunsupervisedpretrainingwinningtwointernationaltransferlearning\\ncompetitions( ,; ,),insettingswherethe Mesniletal.2011Goodfellowetal.2011\\nnumberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozens\\nofexamplesperclass).Theseeﬀectswerealsodocumentedincarefullycontrolled\\nexperimentsbyPaine2014etal.().\\nOtherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretraining\\nislikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated.\\nUnsupervisedlearningdiﬀersfromregularizerslikeweightdecaybecauseitdoesnot\\nbiasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscovering\\nfeaturefunctionsthatareusefulfortheunsupervisedlearningtask.\\xa0Ifthetrue\\nunderlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinput\\ndistribution,unsupervisedlearningcanbeamoreappropriateregularizer.\\nThesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervised\\npretrainingisknowntocauseanimprovement,andexplainwhatisknownabout\\nwhythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenused\\ntoimproveclassiﬁers,andisusuallymostinterestingfromthepointofviewof\\n5 3 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d32d5a3-e58f-4e37-9248-232e06ef3553', embedding=None, metadata={'page_label': '548', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\n\\x00\\ue034 \\ue030 \\ue030 \\ue030 \\x00\\ue033 \\ue030 \\ue030 \\ue030 \\x00\\ue032 \\ue030 \\ue030 \\ue030 \\x00\\ue031 \\ue030 \\ue030 \\ue030 \\ue030 \\ue031 \\ue030 \\ue030 \\ue030 \\ue032 \\ue030 \\ue030 \\ue030 \\ue033 \\ue030 \\ue030 \\ue030 \\ue034 \\ue030 \\ue030 \\ue030\\x00\\ue031 \\ue035 \\ue030 \\ue030\\x00\\ue031 \\ue030 \\ue030 \\ue030\\x00\\ue035 \\ue030 \\ue030\\ue030\\ue035 \\ue030 \\ue030\\ue031 \\ue030 \\ue030 \\ue030\\ue031 \\ue035 \\ue030 \\ue030\\n\\ue057 \\ue069 \\ue074 \\ue068 \\ue020 \\ue070 \\ue072 \\ue065 \\ue074 \\ue072 \\ue061 \\ue069 \\ue06e \\ue069 \\ue06e \\ue067\\n\\ue057 \\ue069 \\ue074 \\ue068 \\ue06f \\ue075 \\ue074 \\ue020 \\ue070 \\ue072 \\ue065 \\ue074 \\ue072 \\ue061 \\ue069 \\ue06e \\ue069 \\ue06e \\ue067\\nFigure15.1:Visualizationvianonlinearprojectionofthelearningtrajectoriesofdiﬀerent\\nneuralnetworksin f u n c t i o n s p a c e(notparameterspace,toavoidtheissueofmany-to-one\\nmappingsfromparametervectorstofunctions),withdiﬀerentrandominitializations\\nandwithorwithoutunsupervisedpretraining.Eachpointcorrespondstoadiﬀerent\\nneuralnetwork,ataparticulartimeduringitstrainingprocess.Thisﬁgureisadapted\\nwithpermissionfrom ().Acoordinateinfunctionspaceisaninﬁnite- Erhan e t a l .2010\\ndimensionalvectorassociatingeveryinputxwithanoutputy. ()made Erhan e t a l .2010\\nalinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspeciﬁcx\\npoints.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaum\\ne t a l .,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot 2000\\n(correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributions\\novertheclassyformostinputs).Overtime,learningmovesthefunctionoutward,to\\npointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhen\\nusingpretrainingandinanother,non-overlappingregionwhennotusingpretraining.\\nIsomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregion\\ncorrespondingtopretrainedmodelsmayindicatethatthepretraining-basedestimator\\nhasreducedvariance.\\n5 3 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f9952d12-000b-4028-aa78-c6466aa24f3c', embedding=None, metadata={'page_label': '549', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nreducingtestseterror.However,unsupervisedpretrainingcanhelptasksother\\nthanclassiﬁcation,andcanacttoimproveoptimization ratherthanbeingmerely\\naregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerror\\nfordeepautoencoders(HintonandSalakhutdinov2006,).\\nErhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesof\\nunsupervisedpretraining.Bothimprovementstotrainingerrorandimprovements\\ntotesterrormaybeexplainedintermsofunsupervisedpretrainingtakingthe\\nparametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetwork\\ntrainingisnon-determinis tic,andconvergestoadiﬀerentfunctioneverytimeit\\nisrun.\\xa0Trainingmayhaltatapointwherethegradientbecomessmall,apoint\\nwhereearlystoppingendstrainingtopreventoverﬁtting,oratapointwherethe\\ngradientislargebutitisdiﬃculttoﬁndadownhillstepduetoproblemssuchas\\nstochasticityorpoorconditioningoftheHessian.\\xa0Neuralnetworksthatreceive\\nunsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace,\\nwhileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.See\\nﬁgureforavisualizationofthisphenomenon. Theregionwherepretrained 15.1\\nnetworksarriveissmaller,suggestingthatpretrainingreducesthevarianceofthe\\nestimationprocess,whichcaninturnreducetheriskofsevereover-ﬁtting.In\\notherwords,unsupervisedpretraininginitializesneuralnetworkparametersinto\\naregionthattheydonotescape,andtheresultsfollowingthisinitialization are\\nmoreconsistentandlesslikelytobeverybadthanwithoutthisinitialization.\\nErhan2010etal.()alsoprovidesomeanswersastopretrainingworks when\\nbest—themeanandvarianceofthetesterrorweremostreducedbypretrainingfor\\ndeepernetworks.Keepinmindthattheseexperimentswereperformedbeforethe\\ninventionandpopularization ofmoderntechniquesfortrainingverydeepnetworks\\n(rectiﬁedlinearunits,dropoutandbatchnormalization) solessisknownaboutthe\\neﬀectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches.\\nAnimportantquestionishowunsupervisedpretrainingcanactasaregularizer.\\nOnehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscover\\nfeaturesthatrelatetotheunderlyingcausesthatgeneratetheobserveddata.\\nThisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervised\\npretraining,andisdescribedfurtherinsection.15.3\\nComparedtootherformsofunsupervisedlearning,unsupervisedpretraining\\nhasthedisadvantagethatitoperateswithtwoseparatetrainingphases.\\xa0Many\\nregularizationstrategieshavetheadvantageofallowingtheusertocontrolthe\\nstrengthoftheregularizationbyadjustingthevalueofasinglehyperparameter.\\nUnsupervisedpretrainingdoesnotoﬀeraclearwaytoadjustthethestrength\\noftheregularization\\xa0arisi ngfromtheunsupervised\\xa0stage.Instead,\\xa0thereare\\n5 3 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f9b322af-fb99-4edc-9d40-017159ae432d', embedding=None, metadata={'page_label': '550', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nverymanyhyperparameters ,whoseeﬀectmaybemeasuredafterthefactbut\\nisoftendiﬃculttopredictaheadoftime.Whenweperformunsupervisedand\\nsupervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,there\\nisasinglehyperparameter,usuallyacoeﬃcientattachedtotheunsupervised\\ncost,\\xa0thatdetermineshowstronglytheunsupervisedobjectivewillregularize\\nthesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationby\\ndecreasingthiscoeﬃcient.Inthecaseofunsupervisedpretraining,thereisnota\\nwayofﬂexiblyadaptingthestrengthoftheregularization—either thesupervised\\nmodelisinitializedtopretrainedparameters,oritisnot.\\nAnotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphase\\nhasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannot\\nbepredictedduringtheﬁrstphase,sothereisalongdelaybetweenproposing\\nhyperparametersfortheﬁrstphaseandbeingabletoupdatethemusingfeedback\\nfromthesecondphase.Themostprincipledapproachistousevalidationseterror\\ninthesupervisedphaseinordertoselectthehyperparameters ofthepretraining\\nphase,asdiscussedin ().Inpractice,somehyperparameters, Larochelleetal.2009\\nlikethenumberofpretrainingiterations,aremoreconvenientlysetduringthe\\npretrainingphase,usingearlystoppingontheunsupervisedobjective,whichis\\nnotidealbutcomputationally muchcheaperthanusingthesupervisedobjective.\\nToday,unsupervisedpretraininghasbeenlargelyabandoned,exceptinthe\\nﬁeldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsas\\none-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeled\\nsetsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrain\\nonceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsof\\nwords),learnagoodrepresentation(typicallyofwords,butalsoofsentences),and\\nthenusethisrepresentationorﬁne-tuneitforasupervisedtaskforwhichthe\\ntrainingsetcontainssubstantiallyfewerexamples.Thisapproachwaspioneered\\nbybyCollobertandWeston2008bTurian2010Collobert (), etal.(),and etal.\\n()andremainsincommonusetoday. 2011a\\nDeeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropout\\norbatchnormalization, areabletoachievehuman-levelperformanceonverymany\\ntasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesout-\\nperformunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10and\\nMNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmall\\ndatasets,suchasthealternativesplicingdataset,Bayesianmethodsoutperform\\nmethodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons,\\nthepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervised\\npretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch\\n5 3 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='88ba7b30-3feb-4478-a10c-02a2385ee9c4', embedding=None, metadata={'page_label': '551', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nandcontinuestoinﬂuencecontemporaryapproaches.Theideaofpretraininghas\\nbeengeneralizedto sup e r v i se d pr e t r ai ni n gdiscussedinsection,asavery 8.7.4\\ncommonapproachfortransferlearning.Supervisedpretrainingfortransferlearning\\nispopular( ,; Oquabetal.2014Yosinski2014etal.,)forusewithconvolutional\\nnetworkspretrainedontheImageNetdataset.Practitionerspublishtheparameters\\nofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsare\\npublishedfornaturallanguagetasks( ,; Collobertetal.2011aMikolov2013aetal.,).\\n15. 2 T ransfer L earni n g an d D om ai n A d ap t at i o n\\nTransferlearninganddomainadaptationrefertothesituationwherewhathasbeen\\nlearnedinonesetting(i.e.,distributionP 1)isexploitedtoimprovegeneralization\\ninanothersetting(saydistributionP 2).Thisgeneralizestheideapresentedinthe\\nprevioussection,wherewetransferredrepresentationsbetweenanunsupervised\\nlearningtaskandasupervisedlearningtask.\\nIn t r ansf e r l e ar ni ng,thelearnermustperformtwoormorediﬀerenttasks,\\nbutweassumethatmanyofthefactorsthatexplainthevariationsinP 1are\\nrelevanttothevariationsthatneedtobecapturedforlearningP 2.Thisistypically\\nunderstoodinasupervisedlearningcontext,wheretheinputisthesamebutthe\\ntargetmaybeofadiﬀerentnature.Forexample,wemaylearnaboutonesetof\\nvisualcategories,suchascatsanddogs,intheﬁrstsetting,thenlearnabouta\\ndiﬀerentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.If\\nthereissigniﬁcantlymoredataintheﬁrstsetting(sampledfromP 1),thenthat\\nmayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonly\\nveryfewexamplesdrawnfromP 2.Manyvisualcategories sharelow-levelnotions\\nofedgesandvisualshapes,theeﬀectsofgeometricchanges,changesinlighting,\\netc.\\xa0Ingeneral,transferlearning,multi-tasklearning(section),anddomain7.7\\nadaptationcanbeachievedviarepresentationlearningwhenthereexistfeatures\\nthatareusefulforthediﬀerentsettingsortasks,correspondingtounderlying\\nfactorsthatappearinmorethanonesetting.Thisisillustratedinﬁgure,with7.2\\nsharedlowerlayersandtask-dependentupperlayers.\\nHowever,\\xa0sometimes,\\xa0whatisshared\\xa0amongthe\\xa0diﬀerent\\xa0tasksisnotthe\\nsemanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeech\\nrecognitionsystemneedstoproducevalidsentencesattheoutputlayer,but\\ntheearlierlayersneartheinputmayneedtorecognizeverydiﬀerentversionsof\\nthesamephonemesorsub-phonemicvocalizationsdependingonwhichperson\\nisspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers\\n(neartheoutput)oftheneuralnetwork,andhaveatask-speciﬁcpreprocessing,as\\n5 3 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7ae555b1-221b-4bfa-8694-dee1889fd080', embedding=None, metadata={'page_label': '552', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nillustratedinﬁgure.15.2\\nSe l e c t i on\\xa0sw i t c h\\nh(1)h(1)h(2)h(2)h(3)h(3)yy\\nh(shared)h(shared)\\nx(1)x(1)x( 2 )x( 2 )x( 3 )x( 3 )\\nFigure15.2:\\xa0Example architectureformulti-taskortransferlearningwhentheoutput\\nvariablehasthesamesemanticsforalltaskswhiletheinputvariablehasadiﬀerent y x \\nmeaning(andpossiblyevenadiﬀerentdimension)foreachtask(or,forexample,each\\nuser),called x( 1 ),x( 2 )andx( 3 )forthreetasks.Thelowerlevels(uptotheselection\\nswitch)aretask-speciﬁc,whiletheupperlevelsareshared.Thelowerlevelslearnto\\ntranslatetheirtask-speciﬁcinputintoagenericsetoffeatures.\\nIntherelatedcaseof domain adapt at i o n,thetask(andtheoptimalinput-to-\\noutputmapping)remainsthesamebetweeneachsetting,buttheinputdistribution\\nisslightlydiﬀerent.Forexample,considerthetaskofsentimentanalysis,which\\nconsistsofdeterminingwhetheracommentexpressespositiveornegativesentiment.\\nCommentspostedonthewebcomefrommanycategories.Adomainadaptation\\nscenariocanarisewhenasentimentpredictortrainedoncustomerreviewsof\\nmediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecomments\\naboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimagine\\nthatthereisanunderlyingfunctionthattellswhetheranystatementispositive,\\nneutralornegative,butofcoursethevocabularyandstylemayvaryfromone\\ndomaintoanother,makingitmorediﬃculttogeneralizeacrossdomains.Simple\\nunsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobevery\\nsuccessfulforsentimentanalysiswithdomainadaptation( ,). Glorotetal.2011b\\nArelatedproblemisthatof c o nc e pt dr i f t,whichwecanviewasaform\\noftransferlearningduetogradualchangesinthedatadistributionovertime.\\nBothconceptdriftandtransferlearningcanbeviewedasparticularformsof\\n5 3 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='607163be-1020-4e1d-866c-6b732e7b1f33', embedding=None, metadata={'page_label': '553', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nmulti-tasklearning.Whilethephrase“multi-tasklearning”\\xa0typicallyrefersto\\nsupervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicable\\ntounsupervisedlearningandreinforcementlearningaswell.\\nInallofthesecases,theobjectiveistotakeadvantageofdatafromtheﬁrst\\nsettingtoextractinformationthatmaybeusefulwhenlearningorevenwhen\\ndirectlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentation\\nlearningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthe\\nsamerepresentationinbothsettingsallowstherepresentationtobeneﬁtfromthe\\ntrainingdatathatisavailableforbothtasks.\\nAsmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfound\\nsuccessinsomemachinelearningcompetitions( ,; Mesniletal.2011Goodfellow\\netal.,).Intheﬁrstofthesecompetitions,theexperimentalsetupisthe 2011\\nfollowing.Eachparticipantisﬁrstgivenadatasetfromtheﬁrstsetting(from\\ndistributionP 1),illustratingexamplesofsomesetofcategories.Theparticipants\\nmustusethistolearnagoodfeaturespace(mappingtherawinputtosome\\nrepresentation),suchthatwhenweapplythislearnedtransformationtoinputs\\nfromthetransfersetting(distributionP 2),alinearclassiﬁercanbetrainedand\\ngeneralizewellfromveryfewlabeledexamples.Oneofthemoststrikingresults\\nfoundinthiscompetitionisthatasanarchitecturemakesuseofdeeperand\\ndeeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollected\\nintheﬁrstsetting,P 1),thelearningcurveonthenewcategoriesofthesecond\\n(transfer)settingP 2becomesmuchbetter.Fordeeprepresentations,fewerlabeled\\nexamplesofthetransfertasksarenecessarytoachievetheapparentlyasymptotic\\ngeneralization performance.\\nTwoextremeformsoftransferlearningare o ne-shot l e ar ni ngand z e r o - sho t\\nl e ar ni ng,sometimesalsocalled z e r o - dat a l e ar ni ng.Onlyonelabeledexample\\nofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesare\\ngivenatallforthezero-shotlearningtask.\\nOne-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentation\\nlearnstocleanlyseparatetheunderlyingclassesduringtheﬁrststage.Duringthe\\ntransferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmany\\npossibletestexamplesthatallclusteraroundthesamepointinrepresentation\\nspace.Thisworkstotheextentthatthefactorsofvariationcorrespondingto\\ntheseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearned\\nrepresentationspace,andwehavesomehowlearnedwhichfactorsdoanddonot\\nmatterwhendiscriminatingobjectsofcertaincategories.\\nAsanexampleofazero-shotlearningsetting,considertheproblemofhaving\\nalearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems.\\n5 3 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a45106c5-4d23-44d1-836b-ba378f4beff8', embedding=None, metadata={'page_label': '554', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nItmaybepossibletorecognizeaspeciﬁcobjectclassevenwithouthavingseenan\\nimageofthatobject,ifthetextdescribestheobjectwellenough.\\xa0Forexample,\\nhavingreadthatacathasfourlegsandpointyears,thelearnermightbeableto\\nguessthatanimageisacat,withouthavingseenacatbefore.\\nZero-datalearning(Larochelle2008 Palatucci etal.,)andzero-shotlearning(\\netal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformation\\nhasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenario\\nasincludingthreerandomvariables:thetraditionalinputsx,thetraditional\\noutputsortargetsy,andanadditionalrandomvariabledescribingthetask,T.\\nThemodelistrainedtoestimatetheconditionaldistributionp(yx|,T)where\\nTisadescriptionofthetaskwewishthemodeltoperform.\\xa0Inourexampleof\\nrecognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariabley\\nwithy= 1indicating“yes”andy= 0indicating“no.”ThetaskvariableTthen\\nrepresentsquestionstobeansweredsuchas“Isthereacatinthisimage?”Ifwe\\nhaveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthe\\nsamespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT.\\nInourexampleofrecognizingcatswithouthavingseenanimageofthecat,itis\\nimportantthatwehavehadunlabeledtextdatacontainingsentencessuchas“cats\\nhavefourlegs”or“catshavepointyears.”\\nZero-shotlearningrequiresTtoberepresentedinawaythatallowssomesort\\nofgeneralization. Forexample,Tcannotbejustaone-hotcodeindicatingan\\nobjectcategory. ()provideinsteadadistributedrepresentation Socheretal.2013b\\nofobjectcategoriesbyusingalearnedwordembeddingforthewordassociated\\nwitheachcategory.\\nAsimilarphenomenon happensinmachinetranslation(Klementiev2012etal.,;\\nMikolov2013bGouws2014 etal.,; etal.,):wehavewordsinonelanguage,and\\ntherelationshipsbetweenwordscanbelearnedfromunilingualcorpora;onthe\\notherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewith\\nwordsintheother.Eventhoughwemaynothavelabeledexamplestranslating\\nwordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessa\\ntranslationforwordAbecausewehavelearnedadistributedrepresentationfor\\nwordsinlanguageX,adistributedrepresentationforwordsinlanguageY,and\\ncreatedalink(possiblytwo-way)relatingthetwospaces,viatrainingexamples\\nconsistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbe\\nmostsuccessfulifallthreeingredients(thetworepresentationsandtherelations\\nbetweenthem)arelearnedjointly.\\nZero-shotlearningisaparticularformoftransferlearning.Thesameprinciple\\nexplainshowonecanperform m ul t i - m o dal l e ar ni ng,capturingarepresentation\\n5 3 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='10ce6a0c-083c-4c1b-b0f9-355072bba3bf', embedding=None, metadata={'page_label': '555', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nh x = f x ( ) x\\nx t e s t\\ny t e s th y = f y ( ) y\\ny − s pa ce\\nR e l at i onshi p\\xa0 b e t w e e n \\xa0 e m be dde d\\xa0 p oi n t s \\xa0 w i t hi n\\xa0 one \\xa0 o f \\xa0 t h e \\xa0 d o m a i n s\\nMaps\\xa0be t w e e n \\xa0 r e p r e s e n t at i on\\xa0spac e s \\xa0f x\\nf y\\nx − s pa ce\\n( ) pa i r s i n t he t r a i ni ng s et x y ,\\nf x : enco der f unctio n f o r x\\nf y : enco der f unctio n f o r y\\nFigure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning.\\nLabeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionf xand\\nsimilarlywithexamplesofytolearnf y.Eachapplicationofthef xandf yfunctions\\nappearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionis\\napplied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpoints\\ninxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distance\\ninh yspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Both\\nofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeled\\nexamples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-way\\nortwo-waymap(solidbidirectionalarrow)betweentherepresentationsf x(x)andthe\\nrepresentationsf y(y)andanchortheserepresentationstoeachother.Zero-datalearning\\nisthenenabledasfollows.Onecanassociateanimagex t e s ttoawordy t e s t,evenifno\\nimageofthatwordwaseverpresented,simplybecauseword-representationsfy(yt e s t)\\nandimage-representationsf x(x t e s t)canberelatedtoeachotherviathemapsbetween\\nrepresentationspaces.Itworksbecause,althoughthatimageandthatwordwerenever\\npaired,theirrespectivefeaturevectorsf x(x t e s t)andf y(y t e s t)havebeenrelatedtoeach\\nother.FigureinspiredfromsuggestionbyHrantKhachatrian.\\n5 4 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5972f593-7621-4f33-9f3d-4844557e8e9f', embedding=None, metadata={'page_label': '556', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15. REPRESENTATIONLEARNING\\ninonemodality,arepresentationintheother,andtherelationship(ingeneralajoint\\ndistribution)betweenpairs (xy,)consistingofoneobservation xinonemodality\\nandanotherobservation yintheothermodality(SrivastavaandSalakhutdinov,\\n2012). Bylearningallthreesetsofparameters(from xtoitsrepresentation,from\\nyto its representation, and the relationship between thetwo representations),\\nconceptsinonerepresentationareanchoredintheother,andvice-versa,allowing\\none to meaningfully\\xa0generalize to\\xa0new pairs. The procedure is\\xa0illustrated in\\nﬁgure .15.3\\n15.3 Semi-Sup ervised Disentangling of Causal F actors\\nAnimportantquestionaboutrepresentationlearningis“whatmakesonerepre-\\nsentationbetterthananother?” Onehypothesisisthatanidealrepresentation\\nisoneinwhichthefeatures withintherepresentationcorrespondtotheunder-\\nlyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeature\\nspacecorrespondingtodiﬀerentcauses,sothattherepresentationdisentanglesthe\\ncausesfromoneanother. Thishypothesismotivatesapproachesinwhichweﬁrst\\nseekagoodrepresentationfor p(x).\\xa0Sucharepresentationmayalsobeagood\\nrepresentation forcomputing p(y x|)ifyis amongthe most salientcauses of\\nx. Thisideahasguidedalargeamountofdeeplearningresearchsinceatleast\\nthe1990s(BeckerandHinton 1992 HintonandSejnowski 1999 , ; , ),inmoredetail.\\nForotherargumentsaboutwhensemi-supervisedlearningcanoutperformpure\\nsupervisedlearning,wereferthereadertosection1.2of ( ). Chapelle et al.2006\\nInotherapproachestorepresentationlearning,wehaveoftenbeenconcerned\\nwitharepresentationthatiseasytomodel—forexample,onewhoseentriesare\\nsparse,orindependentfromeachother. Arepresentationthatcleanlyseparates\\ntheunderlyingcausalfactorsmaynotnecessarilybeonethat iseasytomodel.\\nHowever, a furtherpart ofthe hypothesis motivating semi-supervised learning\\nvia unsupervised representation learning is that for many AI tasks, these two\\npropertiescoincide:\\xa0onceweareabletoobtaintheunderlyingexplanationsforwhatweobserve,itgenerallybecomeseasytoisolateindividualattributesfrom\\ntheothers. Speciﬁcally,ifarepresentation hrepresentsmanyoftheunderlying\\ncausesoftheobserved x,andtheoutputs yareamongthemostsalientcauses,\\nthenitiseasytopredict from . y h\\nFirst,letusseehowsemi-supervisedlearningcanfailbecauseunsupervised\\nlearning ofp(x)isof no help tolearn p(y x|). Consider for example thecase\\nwherep(x)isuniformlydistributedandwewanttolearn f(x) = E[y|x]. Clearly,\\nobservingatrainingsetof valuesalonegivesusnoinformationabout . x p( )y x|\\n541', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8d8f878f-4c97-4ac8-a788-8008ca7ed8c1', embedding=None, metadata={'page_label': '557', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nxp x ( )y = 1 y = 2 y = 3\\nFigure15.4:Exampleofadensityoverxthatisamixtureoverthree\\xa0components.\\nThecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixture\\ncomponents(e.g.,\\xa0naturalobjectclassesinimagedata)arestatisticallysalient,just\\nmodelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactor\\ny.\\nNext,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed.\\nConsiderthesituationwhere xarisesfromamixture,withonemixturecomponent\\npervalueofy,asillustratedinﬁgure.\\xa0Ifthemixturecomponentsarewell- 15.4\\nseparated,thenmodelingp(x)revealspreciselywhereeachcomponentis,anda\\nsinglelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|).\\nButmoregenerally,whatcouldmakeandbetiedtogether? p( )y x|p()x\\nIfyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)and\\np(yx|)will\\xa0bestronglytied,\\xa0andunsupervisedrepresentationlearningthat\\ntriestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasa\\nsemi-supervisedlearningstrategy.\\nConsidertheassumptionthatyisoneofthecausalfactorsofx,andlet\\nhrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedas\\nstructuredaccordingtothisdirectedgraphicalmodel,withastheparentof: h x\\np,pp. (hx) = ( )xh|()h (15.1)\\nAsaconsequence,thedatahasmarginalprobability\\np() = x E hp. ( )xh| (15.2)\\nFromthisstraightforwardobservation,weconcludethatthebestpossiblemodel\\nofx(fromageneralization pointofview)istheonethatuncoverstheabove“true”\\n5 4 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2788176e-551e-4fee-a411-e634beda40cf', embedding=None, metadata={'page_label': '558', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nstructure,withhasalatentvariablethatexplainstheobservedvariationsinx.\\nThe“ideal”representationlearningdiscussedaboveshouldthusrecovertheselatent\\nfactors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbe\\nveryeasytolearntopredict yfromsucharepresentation.Wealsoseethatthe\\nconditionaldistributionofygivenxistiedbyBayes’ruletothecomponentsin\\ntheaboveequation:\\np( ) = yx|pp ( )xy|()y\\np()x. (15.3)\\nThusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledge\\nofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,in\\nsituationsrespectingtheseassumptions,semi-supervisedlearningshouldimprove\\nperformance.\\nAnimportantresearchproblemregardsthefactthatmostobservationsare\\nformedbyanextremelylargenumberofunderlyingcauses.Supposey=h i,but\\ntheunsupervisedlearnerdoesnotknowwhichh i.Thebruteforcesolutionisfor\\nanunsupervisedlearnertolearnarepresentationthatcapturesthereasonably all\\nsalientgenerativefactorsh janddisentanglesthemfromeachother,thusmaking\\niteasytopredictfrom,regardlessofwhichh y h iisassociatedwith.y\\nInpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossible\\ntocaptureallormostofthefactorsofvariationthatinﬂuenceanobservation.\\nForexample,inavisualscene,shouldtherepresentationalwaysencodeallof\\nthesmallestobjectsinthebackground? Itisawell-documented psychological\\nphenomenon thathumanbeingsfailtoperceivechangesintheirenvironmentthat\\narenotimmediately relevanttothetasktheyareperforming—see,e.g.,Simons\\nandLevin1998().Animportantresearchfrontierinsemi-supervisedlearningis\\ndetermining toencodeineachsituation.Currently,twoofthemainstrategies what\\nfordealingwithalargenumberofunderlyingcausesaretouseasupervised\\nlearningsignalatthesametimeastheunsupervisedlearningsignalsothatthe\\nmodelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuch\\nlargerrepresentationsifusingpurelyunsupervisedlearning.\\nAnemergingstrategyforunsupervisedlearningistomodifythedeﬁnitionof\\nwhichunderlyingcausesaremostsalient.Historically,autoencodersandgenerative\\nmodelshavebeentrainedtooptimizeaﬁxedcriterion,oftensimilartomean\\nsquarederror.Theseﬁxedcriteriadeterminewhichcausesareconsideredsalient.\\nForexample,meansquarederrorappliedtothepixelsofanimageimplicitly\\nspeciﬁesthatanunderlyingcauseisonlysalientifitsigniﬁcantlychangesthe\\nbrightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewish\\ntosolveinvolvesinteractingwithsmallobjects.Seeﬁgureforanexample 15.5\\n5 4 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5348a567-c321-4792-9b16-88d2a2108ac4', embedding=None, metadata={'page_label': '559', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nInput Reconstruction\\nFigure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhas\\nfailedtoreconstructapingpongball.Theexistenceofthepingpongballandallofits\\nspatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageand\\narerelevanttotheroboticstask.\\xa0Unfortunately,theautoencoderhaslimitedcapacity,\\nandthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeing\\nsalientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn.\\nofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmall\\npingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlarger\\nobjects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror.\\nOtherdeﬁnitionsofsaliencearepossible.Forexample,ifagroupofpixels\\nfollowahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextreme\\nbrightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient.\\nOnewaytoimplementsuchadeﬁnitionofsalienceistousearecentlydeveloped\\napproachcalled g e ner at i v e adv e r sar i al net w o r k s( ,). Goodfellow etal.2014c\\nInthisapproach,agenerativemodelistrainedtofoolafeedforwardclassiﬁer.\\nThefeedforwardclassiﬁerattemptstorecognizeallsamplesfromthegenerative\\nmodelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthis\\nframework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeis\\nhighlysalient.Thegenerativeadversarialnetworkwillbedescribedinmoredetail\\ninsection.Forthepurposesofthepresentdiscussion,itissuﬃcientto 20.10.4\\nunderstandthattheylearnhowtodeterminewhatissalient. () Lotteretal.2015\\nshowedthatmodelstrainedtogenerateimagesofhumanheadswilloftenneglect\\ntogeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfully\\ngeneratetheearswhentrainedwiththeadversarialframework.Becausethe\\nearsarenotextremelybrightordarkcomparedtothesurroundingskin,they\\narenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighly\\n5 4 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bcaa6f46-e915-49a9-939c-ed43dfd8cd45', embedding=None, metadata={'page_label': '560', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nGroundTruth MSE Adversarial\\nFigure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceof\\nlearningwhichfeaturesaresalient.\\xa0Inthisexample,thepredictivegenerativenetwork\\nhasbeentrainedtopredicttheappearanceofa3-Dmodelofahumanheadataspeciﬁc\\nviewingangle. ( L e f t )Groundtruth.Thisisthecorrectimage,thatthenetworkshould\\nemit.Imageproducedbyapredictivegenerativenetworktrainedwithmean ( C e n t e r )\\nsquarederroralone.Becausetheearsdonotcauseanextremediﬀerenceinbrightness\\ncomparedtotheneighboringskin,theywerenotsuﬃcientlysalientforthemodeltolearn\\ntorepresentthem. ( R i g h t )Imageproducedbyamodeltrainedwithacombinationof\\nmeansquarederrorandadversarialloss.\\xa0Usingthislearnedcostfunction,theearsare\\nsalientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesare\\nimportantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figures\\ngraciouslyprovidedby (). Lotter e t a l .2015\\nrecognizableshapeandconsistentpositionmeansthatafeedforwardnetwork\\ncaneasilylearntodetectthem,makingthemhighlysalientunderthegenerative\\nadversarialframework.Seeﬁgureforexampleimages.Generativeadversarial 15.6\\nnetworksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented.\\nWeexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhich\\nfactorstorepresent,anddevelopmechanismsforrepresentingdiﬀerentfactors\\ndependingonthetask.\\nAbeneﬁtoflearningtheunderlyingcausalfactors,aspointedoutbySchölkopf\\netal.(),isthatifthetruegenerativeprocesshas 2012 xasaneﬀectandyas\\nacause,thenmodelingp(x y|)isrobusttochangesinp(y).\\xa0Ifthecause-eﬀect\\nrelationshipwasreversed,thiswouldnotbetrue,sincebyBayes’rule,p(x y|)\\nwouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesin\\ndistributionduetodiﬀerentdomains,temporalnon-stationarity,orchangesin\\nthenatureofthetask,thecausalmechanismsremaininvariant(thelawsofthe\\nuniverseareconstant)whilethemarginaldistributionovertheunderlyingcauses\\ncanchange.Hence,bettergeneralization androbustnesstoallkindsofchangescan\\n5 4 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23016bf4-35a2-48ce-9c50-2edb214e3568', embedding=None, metadata={'page_label': '561', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nbeexpectedvialearningagenerativemodelthatattemptstorecoverthecausal\\nfactorsand. h p( )xh|\\n15. 4 D i s t ri b u t ed R ep res en t at i on\\nDistributedrepresentationsofconcepts—representationscomposedofmanyele-\\nmentsthatcanbesetseparatelyfromeachother—areoneofthemostimportant\\ntoolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecause\\ntheycanusenfeatureswithkvaluestodescribekndiﬀerentconcepts.Aswe\\nhaveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunits\\nandprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyof\\ndistributedrepresentation.\\xa0Wenowintroduceanadditionalobservation.\\xa0Many\\ndeeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunits\\ncanlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,as\\ndiscussedinsection.Distributedrepresentationsarenaturalforthisapproach, 15.3\\nbecauseeachdirectioninrepresentationspacecancorrespondtothevalueofa\\ndiﬀerentunderlyingconﬁgurationvariable.\\nAnexampleofadistributedrepresentationisavectorofnbinaryfeatures,\\nwhichcantake2nconﬁgurations, eachpotentiallycorrespondingtoadiﬀerent\\nregionininputspace,asillustratedinﬁgure.Thiscanbecomparedwith 15.7\\nasymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolor\\ncategory.Iftherearensymbolsinthedictionary,onecanimaginenfeature\\ndetectors,eachcorrespondingtothedetectionofthepresenceoftheassociated\\ncategory.Inthatcaseonlyndiﬀerentconﬁgurations oftherepresentationspace\\narepossible,carvingndiﬀerentregionsininputspace,asillustratedinﬁgure.15.8\\nSuchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcan\\nbecapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyone\\nofthemcanbeactive).Asymbolicrepresentationisaspeciﬁcexampleofthe\\nbroaderclassofnon-distributedrepresentations,whicharerepresentationsthat\\nmaycontainmanyentriesbutwithoutsigniﬁcantmeaningfulseparatecontrolover\\neachentry.\\nExamplesoflearningalgorithms\\xa0basedonnon-distributedrepresentations\\ninclude:\\n•Clusteringmethods,includingthek-meansalgorithm:eachinputpointis\\nassignedtoexactlyonecluster.\\n•k-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamples\\nareassociatedwithagiveninput.Inthecaseofk>1,therearemultiple\\n5 4 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89bb38cd-1c09-4157-bb23-f41b1d5022cd', embedding=None, metadata={'page_label': '562', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nh 1h 2 h 3\\nh = [ 1 , , 1 1 ]\\ue021\\nh = [ 0 , , 1 1 ]\\ue021h = [ 1 , , 0 1 ]\\ue021h = [ 1 , , 1 0 ]\\ue021\\nh = [ 0 , , 1 0 ]\\ue021h = [ 0 , , 0 1 ]\\ue021h = [ 1 , , 0 0 ]\\ue021\\nFigure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentation\\nbreaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeatures\\nh 1,h 2,andh 3.\\xa0Eachfeatureisdeﬁnedbythresholdingtheoutputofalearned,linear\\ntransformation.Eachfeaturedivides R2intotwohalf-planes.Leth+\\nibethesetofinput\\npointsforwhichh i=1andh−\\nibethesetofinputpointsforwhichh i=0.Inthis\\nillustration,eachlinerepresentsthedecisionboundaryforoneh i,withthecorresponding\\narrowpointingtotheh+\\nisideoftheboundary.Therepresentationasawholetakes\\nonauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,the\\nrepresentationvalue[1,1,1]\\ue03ecorrespondstotheregionh+\\n1∩h+\\n2∩h+\\n3.Comparethistothe\\nnon-distributedrepresentationsinﬁgure.Inthegeneralcaseof 15.8 dinputdimensions,\\nadistributedrepresentationdivides Rdbyintersectinghalf-spacesratherthanhalf-planes.\\nThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)diﬀerent\\nregions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonly\\nnregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymany\\nmoreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible\\n(thereisnoh=0inthisexample)andthatalinearclassiﬁerontopofthedistributed\\nrepresentationisnotabletoassigndiﬀerentclassidentitiestoeveryneighboringregion;\\nevenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog )wherew\\nisthenumberofweights(,).Thecombinationofapowerfulrepresentation Sontag1998\\nlayerandaweakclassiﬁerlayercanbeastrongregularizer;aclassiﬁertryingtolearn\\ntheconceptof“person”versus“notaperson”doesnotneedtoassignadiﬀerentclassto\\naninputrepresentedas“womanwithglasses”thanitassignstoaninputrepresentedas\\n“manwithoutglasses.”Thiscapacityconstraintencourageseachclassiﬁertofocusonfew\\nh iandencouragestolearntorepresenttheclassesinalinearlyseparableway. h\\n5 4 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1995b4f9-cc0e-4fe5-9eb9-bbadba886b1d', embedding=None, metadata={'page_label': '563', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nvaluesdescribingeachinput,buttheycannotbecontrolledseparatelyfrom\\neachother,sothisdoesnotqualifyasatruedistributedrepresentation.\\n•Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)is\\nactivatedwhenaninputisgiven.\\n•Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)or\\nexpertsarenowassociatedwithadegreeofactivation.Aswiththek-nearest\\nneighborsalgorithm,eachinputisrepresentedwithmultiplevalues,but\\nthosevaluescannotreadilybecontrolledseparatelyfromeachother.\\n•KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel):\\nalthoughthedegreeofactivationofeach“supportvector”ortemplateexample\\nisnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures.\\n•Languageortranslationmodelsbasedonn-grams.Thesetofcontexts\\n(sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuﬃxes.\\nAleafmaycorrespondtothelasttwowordsbeingw 1andw 2,forexample.\\nSeparateparametersareestimatedforeachleafofthetree(withsomesharing\\nbeingpossible).\\nForsomeofthesenon-distributedalgorithms,theoutputisnotconstantby\\npartsbutinsteadinterpolatesbetweenneighboringregions.Therelationship\\nbetweenthenumberofparameters(orexamples)andthenumberofregionsthey\\ncandeﬁneremainslinear.\\nAnimportantrelatedconceptthatdistinguishesadistributedrepresentation\\nfromasymboliconeisthatgeneralizationarisesduetosharedattributesbetween\\ndiﬀerentconcepts.Aspuresymbols,“cat”and“dog”areasfarfromeachother\\nasanyothertwosymbols.However,ifoneassociatesthemwithameaningful\\ndistributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcats\\ncangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentation\\nmaycontainentriessuchas“has_fur”or“number_of_legs”thathavethesame\\nvaluefortheembeddingofboth“cat”and“dog.”Neurallanguagemodelsthat\\noperateondistributedrepresentationsofwordsgeneralizemuchbetterthanother\\nmodelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedin\\nsection.Distributedrepresentationsinducearich 12.4 similarityspace,inwhich\\nsemanticallycloseconcepts(orinputs)arecloseindistance,apropertythatis\\nabsentfrompurelysymbolicrepresentations.\\nWhenandwhycantherebeastatisticaladvantagefromusingadistributed\\nrepresentationaspartofalearningalgorithm?\\xa0D istributedrepresentationscan\\n5 4 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d3988e8-2f34-440c-8cd0-6208816b0342', embedding=None, metadata={'page_label': '564', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nFigure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace\\nintodiﬀerentregions.Thenearestneighboralgorithmprovidesanexampleofalearning\\nalgorithmbasedonanon-distributedrepresentation.Diﬀerentnon-distributedalgorithms\\nmayhavediﬀerentgeometry,\\xa0but\\xa0theytypicallybreaktheinput\\xa0spaceintoregions,\\nw i t h a s e p a r a t e s e t o f p a r a m e t e r s f o r e a c h r e g i o n.Theadvantageofanon-distributed\\napproachisthat,givenenoughparameters,itcanﬁtthetrainingsetwithoutsolvinga\\ndiﬃcultoptimizationalgorithm,becauseitisstraightforwardtochooseadiﬀerentoutput\\ni n d e p e n d e n t l yforeachregion.Thedisadvantageisthatsuchnon-distributedmodels\\ngeneralizeonlylocallyviathesmoothnessprior,makingitdiﬃculttolearnacomplicated\\nfunctionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrast\\nthiswithadistributedrepresentation,ﬁgure.15.7\\n5 4 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='50fe524b-9bc4-4da3-b9f8-353c449c54fe', embedding=None, metadata={'page_label': '565', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nhaveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbe\\ncompactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon-\\ndistributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption,\\nwhichstatesthatifuv≈,thenthetargetfunctionftobelearnedhasthe\\npropertythatf(u)≈f(v),ingeneral.Therearemanywaysofformalizingsuchan\\nassumption,buttheendresultisthatifwehaveanexample (x,y)forwhichwe\\nknowthatf(x)≈y,thenwechooseanestimator ˆfthatapproximatelysatisﬁes\\ntheseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearby\\ninputx+\\ue00f.Thisassumptionisclearlyveryuseful,butitsuﬀersfromthecurseof\\ndimensionality:\\xa0inordertolearnatargetfunctionthatincreasesanddecreases\\nmanytimesinmanydiﬀerentregions,1wemayneedanumberofexamplesthatis\\natleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachof\\ntheseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomfor\\neachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymbol\\ntovalue.\\xa0However,thisdoesnotallowustogeneralizetonewsymbolsfornew\\nregions.\\nIfwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeing\\nsmooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizean\\nobjectregardlessofitslocationintheimage,eventhoughspatialtranslationof\\ntheobjectmaynotcorrespondtosmoothtransformationsintheinputspace.\\nLetusexamineaspecialcaseofadistributedrepresentationlearningalgorithm,\\nthatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.Each\\nbinaryfeatureinthisrepresentationdivides Rdintoapairofhalf-spaces,\\xa0as\\nillustratedinﬁgure.Theexponentiallylargenumberofintersectionsof 15.7 n\\nofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributed\\nrepresentationlearnercandistinguish.Howmanyregionsaregeneratedbyan\\narrangementofnhyperplanesin Rd?Byapplyingageneralresultconcerningthe\\nintersectionofhyperplanes(,),onecanshow( Zaslavsky1975 Pascanu2014betal.,)\\nthatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishis\\nd\\ue058\\nj = 0\\ue012n\\nj\\ue013\\n= (Ond). (15.4)\\nTherefore,weseeagrowththatisexponentialintheinputsizeandpolynomialin\\nthenumberofhiddenunits.\\n1P o t e n t i a l l y , we m a y w a n t t o l e a rn a f u n c t i o n wh o s e b e h a v i o r i s d i s t i n c t i n e x p o n e n t i a l l y m a n y\\nre g i o n s : i n a d - d i m e n s i o n a l s p a c e with a t l e a s t 2 d i ﬀ e re n t v a l u e s t o d i s t i n g u i s h p e r d i m e n s i o n , w e\\nm i g h t wa n t t o d i ﬀ e r i n f 2dd i ﬀ e re n t re g i o n s , re q u i rin g O ( 2d) t ra i n i n g e x a m p l e s .\\n5 5 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='371f46b2-1ba3-4579-b940-f84b1590c846', embedding=None, metadata={'page_label': '566', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nThisprovidesageometricargumenttoexplainthegeneralization powerof\\ndistributedrepresentation:withO(nd)parameters(fornlinear-threshold features\\nin Rd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemade\\nnoassumptionatallaboutthedata,andusedarepresentationwithoneunique\\nsymbolforeachregion,andseparateparametersforeachsymboltorecognizeits\\ncorrespondingportionof Rd,thenspecifyingO(nd)regionswouldrequireO(nd)\\nexamples.Moregenerally,theargumentinfavorofthedistributedrepresentation\\ncouldbeextendedtothecasewhereinsteadofusinglinearthresholdunitswe\\nusenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesin\\nthedistributedrepresentation.Theargumentinthiscaseisthatifaparametric\\ntransformationwithkparameterscanlearnaboutrregionsininputspace,with\\nkr\\ue01c,andifobtainingsucharepresentationwasusefultothetaskofinterest,then\\nwecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributed\\nsettingwherewewouldneedO(r)examplestoobtainthesamefeaturesand\\nassociatedpartitioningoftheinputspaceintorregions.Usingfewerparametersto\\nrepresentthemodelmeansthatwehavefewerparameterstoﬁt,andthusrequire\\nfarfewertrainingexamplestogeneralizewell.\\nAfurtherpartoftheargumentforwhymodelsbasedondistributedrepresen-\\ntationsgeneralizewellisthattheircapacityremainslimiteddespitebeingableto\\ndistinctlyencodesomanydiﬀerentregions.Forexample,theVCdimensionofa\\nneuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumber\\nofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignvery\\nmanyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecode\\nspace,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspace\\nhtotheoutputyusingalinearclassiﬁer.Theuseofadistributedrepresentation\\ncombinedwithalinearclassiﬁerthusexpressesapriorbeliefthattheclassesto\\nberecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactors\\ncapturedbyh.\\xa0Wewilltypicallywanttolearncategoriessuchasthesetofall\\nimagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthat\\nrequirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartition\\nthedataintothesetofallredcarsandgreentrucksasoneclassandthesetofall\\ngreencarsandredtrucksasanotherclass.\\nTheideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentally\\nvalidated. ()ﬁndthathiddenunitsinadeepconvolutionalnetwork Zhouetal.2015\\ntrainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatarevery\\nofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign.\\nInpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomething\\nthathasasimplelinguisticname,butitisinterestingtoseethisemergenearthe\\ntoplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein\\n5 5 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='120c8a9e-8534-4ba9-ae15-5fd1a7d5ccad', embedding=None, metadata={'page_label': '567', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\n-+ =\\nFigure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentangles\\ntheconceptofgenderfromtheconceptofwearingglasses.\\xa0Ifwebeginwiththerepre-\\nsentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingthe\\nconceptofamanwithoutglasses,andﬁnallyaddthevectorrepresentingtheconcept\\nofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawoman\\nwithglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorsto\\nimagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwith\\npermissionfrom (). Radford e t a l .2015\\ncommonisthatonecouldimagine learningabouteachofthemwithouthavingto\\nseealltheconﬁgurationsofalltheothers. ()demonstratedthat Radfordetal.2015\\nagenerativemodelcanlearnarepresentationofimagesoffaces,withseparate\\ndirectionsinrepresentationspacecapturingdiﬀerentunderlyingfactorsofvariation.\\nFiguredemonstratesthatonedirectioninrepresentationspacecorresponds 15.9\\ntowhetherthepersonismaleorfemale,whileanothercorrespondstowhether\\nthepersoniswearingglasses.Thesefeatureswerediscoveredautomatically ,not\\nﬁxedapriori.Thereisnoneedtohavelabelsforthehiddenunitclassiﬁers:\\ngradientdescentonanobjectivefunctionofinterestnaturallylearnssemantically\\ninterestingfeatures,solongasthetaskrequiressuchfeatures.Wecanlearnabout\\nthedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceof\\nglasses,withouthavingtocharacterizealloftheconﬁgurations ofthen−1other\\nfeaturesbyexamplescoveringallofthesecombinationsofvalues.\\xa0Thisformof\\nstatisticalseparabilityiswhatallowsonetogeneralizetonewconﬁgurations ofa\\nperson’sfeaturesthathaveneverbeenseenduringtraining.\\n5 5 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9aecd3bd-e8c0-4cdc-8c28-9dd0e935b4b5', embedding=None, metadata={'page_label': '568', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\n15. 5 E x p on en t i al Gai n s f rom D ep t h\\nWehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima- 6.4.1\\ntors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeep\\nnetworkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadsto\\nimprovedstatisticaleﬃciency.Inthissection,wedescribehowsimilarresultsapply\\nmoregenerallytootherkindsofmodelswithdistributedhiddenrepresentations.\\nInsection,wesawanexampleofagenerativemodelthatlearnedabout 15.4\\ntheexplanatoryfactorsunderlyingimagesoffaces,includingtheperson’sgender\\nandwhethertheyarewearingglasses.Thegenerativemodelthataccomplished\\nthistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpect\\nashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationship\\nbetweentheseabstractexplanatoryfactorsandthepixelsintheimage.\\xa0Inthis\\nandotherAItasks,thefactorsthatcanbechosenalmostindependentlyfrom\\neachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobevery\\nhigh-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthis\\ndemands deepdistributedrepresentations,wherethehigherlevelfeatures(seenas\\nfunctionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthrough\\nthecompositionofmanynonlinearities.\\nIthasbeenproveninmanydiﬀerentsettingsthatorganizingcomputation\\nthroughthecompositionofmanynonlinearities andahierarchyofreusedfeatures\\ncangiveanexponentialboosttostatisticaleﬃciency,ontopoftheexponential\\nboostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g.,\\nwithsaturatingnonlinearities, Booleangates,sum/products,orRBFunits)with\\nasinglehiddenlayercanbeshowntobeuniversalapproximators.Amodel\\nfamilythatisauniversalapproximator canapproximatealargeclassoffunctions\\n(includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenough\\nhiddenunits.\\xa0However,therequirednumberofhiddenunitsmaybeverylarge.\\nTheoreticalresultsconcerningtheexpressivepowerofdeeparchitectures statethat\\ntherearefamiliesoffunctionsthatcanberepresentedeﬃcientlybyanarchitecture\\nofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespect\\ntotheinputsize)withinsuﬃcientdepth(depth2ordepth).k−1\\nInsection,wesawthatdeterministicfeedforwardnetworksareuniversal 6.4.1\\napproximatorsoffunctions.Manystructuredprobabilisticmodelswithasingle\\nhiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeep\\nbeliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRoux\\nandBengio20082010MontúfarandAy2011Montúfar2014Krause ,,; ,;,; etal.,\\n2013).\\n5 5 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9b499ca-4490-4d5f-a89f-476cd9025db3', embedding=None, metadata={'page_label': '569', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nInsection,wesawthatasuﬃcientlydeepfeedforwardnetworkcanhave 6.4.1\\nanexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalso\\nbeobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilistic\\nmodelisthe sum-pr o duc t net w o r korSPN(PoonandDomingos2011,).These\\nmodelsusepolynomialcircuitstocomputetheprobabilitydistributionovera\\nsetofrandomvariables. ()showedthatthereexist DelalleauandBengio2011\\nprobabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoid\\nneedinganexponentiallylargemodel.Later, () MartensandMedabalimi 2014\\nshowedthattherearesigniﬁcantdiﬀerencesbetweeneverytwoﬁnitedepthsof\\nSPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimit\\ntheirrepresentationalpower.\\nAnotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressive\\npoweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightingan\\nexponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowed\\ntoonlyapproximatethefunctioncomputedbythedeepcircuit( ,Cohenetal.\\n2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythe\\ncasewheretheshallowcircuitmustexactlyreplicateparticularfunctions.\\n15. 6 Pro v i d i n g C l u es t o D i s c o v er Un d erl y i n g C au s es\\nToclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesone\\nrepresentationbetterthananother?Oneanswer,ﬁrstintroducedinsection,is15.3\\nthatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsof\\nvariationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoour\\napplications.Moststrategiesforrepresentationlearningarebasedonintroducing\\ncluesthathelpthelearningtoﬁndtheseunderlyingfactorsofvariations.Theclues\\ncanhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervised\\nlearningprovidesaverystrongclue:alabely,presentedwitheachx,thatusually\\nspeciﬁesthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally,\\ntomakeuseofabundantunlabeleddata,representationlearningmakesuseof\\nother,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformof\\nimplicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposein\\nordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthat\\nregularizationstrategiesarenecessarytoobtaingoodgeneralization. Whileitis\\nimpossibletoﬁndauniversallysuperiorregularizationstrategy,onegoalofdeep\\nlearningistoﬁndasetoffairlygenericregularizationstrategiesthatareapplicable\\ntoawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareable\\ntosolve.\\n5 5 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a2aba24-597c-4b75-b0af-569792620daf', embedding=None, metadata={'page_label': '570', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nWeprovideherealistofthesegenericregularizationstrategies.Thelistis\\nclearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearning\\nalgorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlying\\nfactors.Thislistwasintroducedinsection3.1of ()andhas Bengioetal.2013d\\nbeenpartiallyexpandedhere.\\n•Smoothness:Thisistheassumptionthatf(x+\\ue00fd)≈f(x)forunitdand\\nsmall\\ue00f.Thisassumptionallowsthelearnertogeneralizefromtraining\\nexamplestonearbypointsininputspace.Manymachinelearningalgorithms\\nleveragethisidea,butitisinsuﬃcienttoovercomethecurseofdimensionality.\\n•Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensome\\nvariablesarelinear.Thisallowsthealgorithmtomakepredictionseven\\nveryfarfromtheobserveddata,butcansometimesleadtooverlyextreme\\npredictions.Mostsimplemachinelearningalgorithmsthatdonotmakethe\\nsmoothnessassumptioninsteadmakethelinearityassumption.Theseare\\ninfactdiﬀerentassumptions—linearfunctionswithlargeweightsapplied\\ntohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal.\\n()forafurtherdiscussionofthelimitationsofthelinearityassumption. 2014b\\n•Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsare\\nmotivatedbytheassumptionthatthedataisgeneratedbymultipleunderlying\\nexplanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestate\\nofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi- 15.3\\nsupervisedlearningviarepresentationlearning.Learningthestructureofp(x)\\nrequireslearningsomeofthesamefeaturesthatareusefulformodelingp(y|\\nx)becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4\\ndescribeshowthisviewmotivatestheuseofdistributedrepresentations,with\\nseparatedirectionsinrepresentationspacecorrespondingtoseparatefactors\\nofvariation.\\n•Causalfactors:themodelisconstructedinsuchawaythatittreatsthe\\nfactorsofvariationdescribedbythelearnedrepresentationhasthecauses\\noftheobserveddatax,andnotvice-versa.Asdiscussedinsection,this15.3\\nisadvantageousforsemi-supervisedlearningandmakesthelearnedmodel\\nmorerobustwhenthedistributionovertheunderlyingcauseschangesor\\nwhenweusethemodelforanewtask.\\n•Depthahierarchical\\xa0organization\\xa0ofexplanatory\\xa0factors ,\\xa0or\\xa0 :High-level,\\nabstractconceptscanbedeﬁnedintermsofsimpleconcepts,forminga\\nhierarchy.From\\xa0another\\xa0point of\\xa0view,\\xa0the\\xa0us e\\xa0ofa\\xa0deeparchitecture\\n5 5 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b3b20d57-c373-4158-8dfc-8f0e2baebc0d', embedding=None, metadata={'page_label': '571', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\nexpressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-step\\nprogram,\\xa0with eachstep\\xa0referringbacktothe\\xa0outputoftheprocessing\\naccomplishedviaprevioussteps.\\n•Sharedfactors\\xa0across\\xa0tasks:In\\xa0thecontextwherewehavemanytasks,\\ncorrespondingtodiﬀerentyivariablessharingthesameinput xorwhere\\neachtaskisassociatedwithasubsetorafunctionf( ) i(x)ofaglobalinput\\nx,theassumptionisthateachyiisassociatedwithadiﬀerentsubsetfroma\\ncommonpoolofrelevantfactors h.Becausethesesubsetsoverlap,learning\\nalltheP(yi|x)viaasharedintermediate representationP(h x|)allows\\nsharingofstatisticalstrengthbetweenthetasks.\\n•Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon-\\ncentratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuous\\ncase,theseregionscanbeapproximatedbylow-dimensional manifoldswith\\namuchsmallerdimensionalitythantheoriginalspacewherethedatalives.\\nManymachinelearningalgorithmsbehavesensiblyonlyonthismanifold\\n( ,).Somemachinelearningalgorithms,especially Goodfellow etal.2014b\\nautoencoders,attempttoexplicitlylearnthestructureofthemanifold.\\n•Naturalclustering:Manymachinelearningalgorithmsassumethateach\\nconnectedmanifoldintheinputspacemaybeassignedtoasingleclass.The\\ndatamaylieonmanydisconnectedmanifolds,buttheclassremainsconstant\\nwithineachoneofthese.\\xa0Thisassumptionmotivatesavarietyoflearning\\nalgorithms,includingtangentpropagation, doublebackprop,themanifold\\ntangentclassiﬁerandadversarialtraining.\\n•Temporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithms\\nmaketheassumptionthatthemostimportantexplanatoryfactorschange\\nslowlyovertime,oratleastthatitiseasiertopredictthetrueunderlying\\nexplanatoryfactorsthantopredictrawobservationssuchaspixelvalues.\\nSeesectionforfurtherdescriptionofthisapproach. 13.3\\n•Sparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmost\\ninputs—thereisnoneedtouseafeaturethatdetectselephanttrunkswhen\\nrepresentinganimageofacat.Itisthereforereasonabletoimposeaprior\\nthatanyfeaturethatcanbeinterpretedas“present”or“absent”shouldbe\\nabsentmostofthetime.\\n•SimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,the\\nfactorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest\\n5 5 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7693cef3-a5bb-4e63-bf27-801dd716f5b3', embedding=None, metadata={'page_label': '572', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER15.REPRESENTATIONLEARNING\\npossibleismarginalindependence,P(h) =\\ue051\\niP(h i),butlineardependencies\\northosecapturedbyashallowautoencoderarealsoreasonableassumptions.\\nThiscanbeseeninmanylawsofphysics,andisassumedwhenplugginga\\nlinearpredictororafactorizedpriorontopofalearnedrepresentation.\\nTheconceptofrepresentationlearningtiestogetherallofthemanyforms\\nofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeep\\nprobabilisticmodelsalllearnandexploitrepresentations.Learning\\xa0thebest\\npossiblerepresentationremainsanexcitingavenueofresearch.\\n5 5 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5eb52e53-4604-4727-98f6-d7d4c36e1535', embedding=None, metadata={'page_label': '573', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 6\\nS t ru ct u r e d Probabilis t i c Mo d e l s\\nf or D e e p L e ar n i n g\\nDeeplearningdrawsuponmanymodelingformalismsthatresearcherscanuseto\\nguidetheirdesigneﬀortsanddescribetheiralgorithms.Oneoftheseformalisms\\nistheideaofstructuredprobabilisticmodels.Wehavealreadydiscussed\\nstructuredprobabilisticmodelsbrieﬂyinsection.Thatbriefpresentationwas 3.14\\nsuﬃcienttounderstandhowtousestructuredprobabilisticmodelsasalanguageto\\ndescribesomeofthealgorithmsinpart.Now,inpart,structuredprobabilistic II III\\nmodelsareakeyingredientofmanyofthemostimportantresearchtopicsindeep\\nlearning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribes\\nstructuredprobabilisticmodelsinmuchgreaterdetail.Thischapterisintended\\ntobeself-contained;thereaderdoesnotneedtoreviewtheearlierintroduction\\nbeforecontinuingwiththischapter.\\nAstructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution,\\nusingagraphtodescribewhichrandomvariablesintheprobabilitydistribution\\ninteractwitheachotherdirectly.Hereweuse“graph”inthegraphtheorysense—a\\nsetofverticesconnectedtooneanotherbyasetofedges.Becausethestructure\\nofthemodelisdeﬁnedbyagraph,thesemodelsareoftenalsoreferredtoas\\ngraphicalmodels.\\nThegraphicalmodelsresearchcommunityislargeandhasdevelopedmany\\ndiﬀerentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,we\\nprovidebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels,\\nwithanemphasisontheconceptsthathaveprovenmostusefultothedeeplearning\\nresearchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels,\\nyoumaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert\\n558', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc182f03-d101-44f2-8c04-9d71d5cfcc94', embedding=None, metadata={'page_label': '574', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nmaybeneﬁtfromreadingtheﬁnalsectionofthischapter,section,inwhichwe 16.7\\nhighlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearning\\nalgorithms.Deeplearningpractitioners tendtouseverydiﬀerentmodelstructures,\\nlearningalgorithmsandinferenceproceduresthanarecommonlyusedbytherest\\nofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythese\\ndiﬀerencesinpreferencesandexplainthereasonsforthem.\\nInthischapterweﬁrstdescribethechallengesofbuildinglarge-scaleproba-\\nbilisticmodels.\\xa0Next,wedescribehowtouseagraphtodescribethestructure\\nofaprobabilitydistribution.Whilethisapproachallowsustoovercomemany\\nchallenges,itisnotwithoutitsowncomplications. Oneofthemajordiﬃcultiesin\\ngraphicalmodelingisunderstandingwhichvariablesneedtobeabletointeract\\ndirectly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem.\\xa0We\\noutlinetwoapproachestoresolvingthisdiﬃcultybylearningaboutthedependen-\\nciesinsection.Finally,weclosewithadiscussionoftheuniqueemphasisthat 16.5\\ndeeplearningpractitioners placeonspeciﬁcapproachestographicalmodelingin\\nsection.16.7\\n16.1TheChallengeofUnstructuredModeling\\nThegoalofdeeplearningistoscalemachinelearningtothekindsofchallenges\\nneededtosolveartiﬁcialintelligence.Thismeansbeingabletounderstandhigh-\\ndimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmsto\\nbeabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,and\\ndocumentscontainingmultiplewordsandpunctuationcharacters.\\nClassiﬁcationalgorithmscantakeaninputfromsucharichhigh-dimensional\\ndistributionandsummarizeitwithacategoricallabel—whatobjectisinaphoto,\\nwhatwordisspokeninarecording,whattopicadocumentisabout.Theprocess\\nofclassiﬁcationdiscardsmostoftheinformationintheinputandproducesa\\nsingleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).The\\nclassiﬁerisalsooftenabletoignoremanypartsoftheinput.Forexample,when\\nrecognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundof\\nthephoto.\\nItispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksare\\noftenmoreexpensivethanclassiﬁcation.Someofthemrequireproducingmultiple\\noutputvalues.Mostrequireacompleteunderstandingoftheentirestructureof\\n1A n a t u ra l im a ge i s a n i m a g e t h a t m i g h t b e c a p t u re d b y a c a m e ra i n a re a s o n a b l y o rd i n a ry\\ne n v i ro n m e n t , a s o p p o s e d t o a s y n t h e t i c a l l y re n d e re d i m a g e , a s c re e n s h o t o f a we b p a g e , e t c .\\n5 5 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5e836ca-341e-4511-9615-45b0b31e8800', embedding=None, metadata={'page_label': '575', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\ntheinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing:\\n•Densityestimation:givenaninput x,themachinelearningsystemreturns\\nanestimateofthetruedensity p( x)underthedatageneratingdistribution.\\nThisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstand-\\ningoftheentireinput.Ifevenoneelementofthevectorisunusual,the\\nsystemmustassignitalowprobability.\\n•Denoising:givenadamagedorincorrectlyobservedinput ˜ x,themachine\\nlearningsystemreturnsanestimateoftheoriginalorcorrect x.Forexample,\\nthemachinelearningsystemmightbeaskedtoremovedustorscratches\\nfromanoldphotograph.Thisrequiresmultipleoutputs(everyelementofthe\\nestimatedcleanexample x)andanunderstandingoftheentireinput(since\\nevenonedamagedareawillstillrevealtheﬁnalestimateasbeingdamaged).\\n•Missingvalueimputation:giventheobservationsofsomeelementsof x,\\nthemodelisaskedtoreturnestimatesoforaprobabilitydistributionover\\nsomeoralloftheunobservedelementsof x.Thisrequiresmultipleoutputs.\\nBecausethemodelcouldbeaskedtorestoreanyoftheelementsof x,it\\nmustunderstandtheentireinput.\\n•Sampling:themodelgeneratesnewsamplesfromthedistribution p( x).\\nApplicationsincludespeechsynthesis,i.e.producingnewwaveformsthat\\nsoundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesanda\\ngoodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawn\\nfromthewrongdistribution,thenthesamplingprocessiswrong.\\nForanexampleofasamplingtaskusingsmallnaturalimages,seeﬁgure.16.1\\nModelingarichdistributionoverthousandsormillionsofrandomvariablesisa\\nchallengingtask,bothcomputationally andstatistically.Supposeweonlywanted\\ntomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyit\\nseemsoverwhelming.Forasmall, 32×32 2 pixelcolor(RGB)image,thereare3 0 7 2\\npossiblebinaryimagesofthisform.Thisnumberisover108 0 0timeslargerthan\\ntheestimatednumberofatomsintheuniverse.\\nIngeneral,ifwewishtomodeladistributionoverarandomvectorxcontaining\\nndiscretevariablescapableoftakingon kvalueseach,thenthenaiveapproachof\\nrepresenting P(x)bystoringalookuptablewithoneprobabilityvalueperpossible\\noutcomerequires knparameters!\\nThisisnotfeasibleforseveralreasons:\\n5 6 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c7db851e-3920-40d0-a361-3f7997961c1f', embedding=None, metadata={'page_label': '576', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nFigure16.1:Probabilisticmodelingofnaturalimages. ( T o p )Example32×32pixelcolor\\nimagesfromtheCIFAR-10dataset( ,).Samples KrizhevskyandHinton2009 ( Bottom )\\ndrawnfromastructuredprobabilisticmodeltrainedonthisdataset.Eachsampleappears\\natthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclidean\\nspace.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages,\\nratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeen\\nadjustedfordisplay.Figurereproducedwithpermissionfrom (). Courville e t a l .2011\\n5 6 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8fcc60af-66c1-41a6-a4f9-e656ef3a77ba', embedding=None, metadata={'page_label': '577', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\n• M e m o r y : t h e c o s t o f s t o r i ng t h e r e p r e s e nt a t i o n:Forallbutverysmallvalues\\nof nand k,representingthedistributionasatablewillrequiretoomany\\nvaluestostore.\\n• St a t i s t i c a l e ﬃ c i e nc y:Asthenumberofparametersinamodelincreases,\\nsodoestheamountoftrainingdataneededtochoosethevaluesofthose\\nparametersusingastatisticalestimator.Becausethetable-basedmodel\\nhasanastronomicalnumberofparameters,itwillrequireanastronomically\\nlargetrainingsettoﬁtaccurately.Anysuchmodelwilloverﬁtthetraining\\nsetverybadlyunlessadditionalassumptionsaremadelinkingthediﬀerent\\nentriesinthetable(forexample,likeinback-oﬀorsmoothed n-grammodels,\\nsection).12.4.1\\n• R u nt i m e : \\xa0 t h e c o s t o f i nfe r e nc e:\\xa0Supposewewanttoperformaninference\\ntaskwhereweuseourmodelofthejointdistribution P(x)tocomputesome\\notherdistribution,suchasthemarginaldistribution P(x 1)ortheconditional\\ndistribution P(x 2|x 1).Computingthesedistributionswillrequiresumming\\nacrosstheentiretable,sotheruntimeoftheseoperationsisashighasthe\\nintractablememorycostofstoringthemodel.\\n• R u nt i m e : t h e c o s t o f s a m p l i ng:Likewise,supposewewanttodrawasample\\nfromthemodel.Thenaivewaytodothisistosamplesomevalueu∼ U(0 ,1),\\ntheniteratethroughthetable,addinguptheprobabilityvaluesuntilthey\\nexceed uandreturntheoutcomecorrespondingtothatpositioninthetable.\\nThisrequiresreadingthroughthewholetableintheworstcase,soithas\\nthesameexponentialcostastheotheroperations.\\nTheproblemwiththetable-basedapproachisthatweareexplicitlymodeling\\neverypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.The\\nprobabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis.\\nUsually,mostvariablesinﬂuenceeachotheronlyindirectly.\\nForexample,considermodelingtheﬁnishingtimesofateaminarelayrace.\\nSupposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartof\\ntherace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompleting\\nherlaparoundthetrack,shehandsthebatontoBob.Bobthenrunshisown\\nlapandhandsthebatontoCarol,whorunstheﬁnallap.Wecanmodeleachof\\ntheirﬁnishingtimesasacontinuousrandomvariable.Alice’sﬁnishingtimedoes\\nnotdependonanyoneelse’s,sinceshegoesﬁrst.Bob’sﬁnishingtimedepends\\nonAlice’s,becauseBobdoesnothavetheopportunitytostarthislapuntilAlice\\nhascompletedhers.\\xa0IfAliceﬁnishesfaster,Bobwillﬁnishfaster,allelsebeing\\n5 6 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4f6a144a-44c0-4804-9330-288bc832ed67', embedding=None, metadata={'page_label': '578', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nequal.Finally,Carol’sﬁnishingtimedependsonbothherteammates.IfAliceis\\nslow,Bobwillprobablyﬁnishlatetoo.Asaconsequence,Carolwillhavequitea\\nlatestartingtimeandthusislikelytohavealateﬁnishingtimeaswell.However,\\nCarol’sﬁnishingtimedependsonly i ndir e c t l yonAlice’sﬁnishingtimeviaBob’s.\\nIfwealreadyknowBob’sﬁnishingtime,wewillnotbeabletoestimateCarol’s\\nﬁnishingtimebetterbyﬁndingoutwhatAlice’sﬁnishingtimewas.Thismeans\\nwecanmodeltherelayraceusingonlytwointeractions: Alice’seﬀectonBoband\\nBob’seﬀectonCarol.Wecanomitthethird,indirectinteractionbetweenAlice\\nandCarolfromourmodel.\\nStructuredprobabilisticmodelsprovideaformalframeworkformodelingonly\\ndirectinteractionsbetweenrandomvariables.Thisallowsthemodelstohave\\nsigniﬁcantlyfewerparametersandthereforebeestimatedreliablyfromlessdata.\\nThesesmallermodelsalsohavedramatically reducedcomputational costinterms\\nofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfrom\\nthemodel.\\n16.2UsingGraphstoDescribeModelStructure\\nStructuredprobabilisticmodelsusegraphs(inthegraphtheorysenseof“nodes”or\\n“vertices”connectedbyedges)torepresentinteractionsbetweenrandomvariables.\\nEachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction.\\nThesedirectinteractionsimplyother,indirectinteractions,butonlythedirect\\ninteractionsneedtobeexplicitlymodeled.\\nThereismore\\xa0thanone\\xa0wayto\\xa0describe\\xa0theinteractionsin\\xa0aprobability\\ndistributionusingagraph.Inthefollowingsectionswedescribesomeofthemost\\npopularandusefulapproaches.Graphicalmodelscanbelargelydividedinto\\ntwocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedon\\nundirectedgraphs.\\n1 6 . 2 . 1 D i rect ed Mo d el s\\nOnekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel,\\notherwiseknownasthebeliefnetworkBayesiannetwork or2(Pearl1985,).\\nDirectedgraphicalmodelsarecalled“directed”becausetheiredgesaredirected,\\n2Ju d e a P e a rl s u g g e s t e d u s i n g t h e t e rm “ B a y e s i a n n e t wo rk ” wh e n o n e wis h e s t o “ e m p h a s i z e\\nt h e j u d g m e n t a l ” n a t u re o f t h e v a l u e s c o m p u t e d b y t h e n e t wo rk , i . e . t o h i g h l i g h t t h a t t h e y u s u a l l y\\nre p re s e n t d e g re e s o f b e l i e f ra t h e r t h a n f re q u e n c i e s o f e v e n t s .\\n5 6 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ace0597a-321e-402d-b8cb-d91a57ab5a60', embedding=None, metadata={'page_label': '579', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nt 0 t 0 t 1 t 1 t 2 t 2A l i c e B ob C ar ol\\nFigure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alice’sﬁnishing\\ntimet 0inﬂuencesBob’sﬁnishingtimet 1,becauseBobdoesnotgettostartrunninguntil\\nAliceﬁnishes.Likewise,CarolonlygetstostartrunningafterBobﬁnishes,soBob’s\\nﬁnishingtimet 1directlyinﬂuencesCarol’sﬁnishingtimet 2.\\nthatis,theypointfromonevertextoanother.Thisdirectionisrepresentedin\\nthedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariable’s\\nprobabilitydistributionisdeﬁnedintermsoftheother’s.Drawinganarrowfrom\\natobmeansthatwedeﬁnetheprobabilitydistributionoverbviaaconditional\\ndistribution,withaasoneofthevariablesontherightsideoftheconditioning\\nbar.Inotherwords,thedistributionoverbdependsonthevalueofa.\\nContinuingwiththerelayraceexamplefromsection,supposewename 16.1\\nAlice’sﬁnishingtimet 0,Bob’sﬁnishingtimet 1,andCarol’sﬁnishingtimet 2.\\nAswesawearlier,ourestimateoft 1dependsont 0.Ourestimateoft 2depends\\ndirectlyont 1butonlyindirectlyont 0.Wecandrawthisrelationshipinadirected\\ngraphicalmodel,illustratedinﬁgure.16.2\\nFormally,adirectedgraphicalmodeldeﬁnedonvariables xisdeﬁnedbya\\ndirectedacyclicgraph Gwhoseverticesaretherandomvariablesinthemodel,\\nandasetoflocalconditionalprobabilitydistributions p(x i| P aG(x i)) where\\nP aG(x i)givestheparentsofx iinG.Theprobabilitydistributionoverxisgiven\\nby\\np() = Πx i p(x i| P aG(x i)) . (16.1)\\nInourrelayraceexample,thismeansthat,usingthegraphdrawninﬁgure,16.2\\np(t 0 ,t 1 ,t 2) = ( pt 0)( pt 1|t 0)( pt 2|t 1) . (16.2)\\nThisisourﬁrsttimeseeingastructuredprobabilisticmodelinaction.We\\ncanexaminethecostofusingit,inordertoobservehowstructuredmodelinghas\\nmanyadvantagesrelativetounstructuredmodeling.\\nSupposewerepresentedtimebydiscretizingtimerangingfromminute0to\\nminute10into6secondchunks.Thiswouldmaket 0,t 1andt 2eachbeadiscrete\\nvariablewith100possiblevalues.Ifweattemptedtorepresent p(t 0 ,t 1 ,t 2)witha\\ntable,itwouldneedtostore999,999values(100valuesoft 0×100valuesoft 1×\\n100valuesoft 2,minus1,sincetheprobabilityofoneoftheconﬁgurations ismade\\n5 6 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1b4f473c-a16f-4072-8fcf-1ab6c2e28548', embedding=None, metadata={'page_label': '580', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nredundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,we\\nonlymakeatableforeachoftheconditionalprobabilitydistributions,thenthe\\ndistributionovert 0requires99values,thetabledeﬁningt 1givent 0requires9900\\nvalues,andsodoesthetabledeﬁningt 2givent 1.Thiscomestoatotalof19,899\\nvalues.Thismeansthatusingthedirectedgraphicalmodelreducedournumberof\\nparametersbyafactorofmorethan50!\\nIngeneral,tomodel ndiscretevariableseachhaving kvalues,thecostofthe\\nsingletableapproachscaleslike O( kn),aswehaveobservedbefore.Nowsuppose\\nwebuildadirectedgraphicalmodeloverthesevariables.\\xa0If misthemaximum\\nnumberofvariablesappearing(oneithersideoftheconditioningbar)inasingle\\nconditionalprobabilitydistribution,thenthecostofthetablesforthedirected\\nmodelscaleslike O( km).Aslongaswecandesignamodelsuchthat m < < n,we\\ngetverydramaticsavings.\\nInotherwords,solongaseachvariablehasfewparentsinthegraph,the\\ndistributioncanberepresentedwithveryfewparameters.\\xa0Somerestrictionson\\nthegraphstructure,suchasrequiringittobeatree,canalsoguaranteethat\\noperationslikecomputingmarginalorconditionaldistributionsoversubsetsof\\nvariablesareeﬃcient.\\nItisimportanttorealizewhatkindsofinformationcanandcannotbeencodedin\\nthegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariables\\nareconditionallyindependentfromeachother.Itisalsopossibletomakeother\\nkindsofsimplifyingassumptions.\\xa0Forexample,supposeweassumeBobalways\\nrunsthesameregardlessofhowAliceperformed.(Inreality,Alice’sperformance\\nprobablyinﬂuencesBob’sperformance—dependingonBob’spersonality,ifAlice\\nrunsespeciallyfastinagivenrace,thismightencourageBobtopushhardand\\nmatchherexceptionalperformance,oritmightmakehimoverconﬁdentandlazy).\\nThentheonlyeﬀectAlicehasonBob’sﬁnishingtimeisthatwemustaddAlice’s\\nﬁnishingtimetothetotalamountoftimewethinkBobneedstorun.This\\nobservationallowsustodeﬁneamodelwith O( k)parametersinsteadof O( k2).\\nHowever,notethatt 0andt 1arestilldirectlydependentwiththisassumption,\\nbecauset 1representstheabsolutetimeatwhichBobﬁnishes,notthetotaltime\\nhehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfrom\\nt 0tot 1.TheassumptionthatBob’spersonalrunningtimeisindependentfrom\\nallotherfactorscannotbeencodedinagraphovert 0,t 1,andt 2.Instead,we\\nencodethisinformationinthedeﬁnitionoftheconditionaldistributionitself.The\\nconditionaldistributionisnolongera k k×−1elementtableindexedbyt 0andt 1\\nbutisnowaslightlymorecomplicatedformulausingonly k−1parameters.The\\ndirectedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedeﬁne\\n5 6 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='55bbd343-3a93-4c6f-86c8-30f17970e340', embedding=None, metadata={'page_label': '581', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nourconditionaldistributions.Itonlydeﬁneswhichvariablestheyareallowedto\\ntakeinasarguments.\\n1 6 . 2 . 2 Un d i rec t ed Mo d el s\\nDirectedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis-\\nticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwise\\nknownasMarkovrandomﬁelds(MRFs)orMarkovnetworks(Kinder-\\nmann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedges\\nareundirected.\\nDirectedmodelsaremostnaturallyapplicabletosituationswherethereis\\naclearreasontodraweacharrowinoneparticulardirection.Oftentheseare\\nsituationswhereweunderstandthecausalityandthecausalityonlyﬂowsinone\\ndirection.Onesuchsituationistherelayraceexample.Earlierrunnersaﬀectthe\\nﬁnishingtimesoflaterrunners;laterrunnersdonotaﬀecttheﬁnishingtimesof\\nearlierrunners.\\nNotallsituationswemightwanttomodelhavesuchacleardirectiontotheir\\ninteractions.Whentheinteractionsseemtohavenointrinsicdirection,orto\\noperateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel.\\nAsanexampleofsuchasituation,supposewewanttomodeladistribution\\noverthreebinaryvariables:whetherornotyouaresick,whetherornotyour\\ncoworkerissick,andwhetherornotyourroommateissick.Asintherelayrace\\nexample,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthat\\ntakeplace.Assumingthatyourcoworkerandyourroommatedonotknoweach\\nother,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasa\\ncolddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodel\\nit.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,and\\nthatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionof\\nacoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthe\\ncoldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyour\\nroommate.\\nInthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickas\\nitisforyourroommatetomakeyousick,sothereisnotaclean,uni-directional\\nnarrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel.\\nAswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyan\\nedge,thentherandomvariablescorrespondingtothosenodesinteractwitheach\\notherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasno\\narrow,andisnotassociatedwithaconditionalprobabilitydistribution.\\n5 6 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4dd3d2c7-dd80-4af7-9ae7-693540c61f6f', embedding=None, metadata={'page_label': '582', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nh r h r h y h y h c h c\\nFigure16.3:Anundirectedgraphrepresentinghowyourroommate’shealthh r,your\\nhealthh y,andyourworkcolleague’s healthh caﬀecteachother.Youandyourroommate\\nmightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame,\\nbutassumingthatyourroommateandyourcolleaguedonotknoweachother,theycan\\nonlyinfecteachotherindirectlyviayou.\\nWedenotetherandomvariablerepresentingyourhealthash y,therandom\\nvariablerepresentingyourroommate’shealthash r,andtherandomvariable\\nrepresentingyourcolleague’shealthash c.Seeﬁgureforadrawingofthe 16.3\\ngraphrepresentingthisscenario.\\nFormally,anundirectedgraphicalmodelisastructuredprobabilisticmodel\\ndeﬁnedonanundirectedgraph G.Foreachclique Cinthegraph,3afactor φ(C)\\n(alsocalledacliquepotential)\\xa0measurestheaﬃnityofthevariablesinthatclique\\nforbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobe\\nnon-negative.Togethertheydeﬁneanunnormalizedprobabilitydistribution\\n˜ p() = Πx C∈G φ .()C (16.3)\\nTheunnormalized probabilitydistributioniseﬃcienttoworkwithsolongas\\nallthecliquesaresmall.Itencodestheideathatstateswithhigheraﬃnityare\\nmorelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothe\\ndeﬁnitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthem\\ntogetherwillyieldavalidprobabilitydistribution.Seeﬁgureforanexample 16.4\\nofreadingfactorizationinformationfromanundirectedgraph.\\nOurexampleofthecoldspreadingbetweenyou,yourroommate,andyour\\ncolleaguecontainstwocliques.Onecliquecontainsh yandh c.Thefactorforthis\\ncliquecanbedeﬁnedbyatable,andmighthavevaluesresemblingthese:\\nh y= 0h y= 1\\nh c= 021\\nh c= 1110\\n3A c l i q u e o f t h e g ra p h i s a s u b s e t o f n o d e s t h a t a re a l l c o n n e c t e d t o e a c h o t h e r b y a n e d g e o f\\nt h e g ra p h .\\n5 6 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c90fd592-85c0-4a77-b9b5-26b767bc04d5', embedding=None, metadata={'page_label': '583', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nAstateof1indicatesgoodhealth,whileastateof0indicatespoorhealth\\n(havingbeen\\xa0infectedwith\\xa0acold).Both\\xa0ofyou\\xa0areusuallyhealthy,\\xa0sothe\\ncorrespondingstatehasthehighestaﬃnity.Thestatewhereonlyoneofyouis\\nsickhasthelowestaﬃnity,becausethisisararestate.Thestatewherebothof\\nyouaresick(becauseoneofyouhasinfectedtheother)isahigheraﬃnitystate,\\nthoughstillnotascommonasthestatewherebotharehealthy.\\nTocompletethemodel,wewouldneedtoalsodeﬁneasimilarfactorforthe\\ncliquecontainingh yandh r.\\n1 6 . 2 . 3 T h e P a rt i t i o n F u n ct i o n\\nWhiletheunnormalized probabilitydistributionisguaranteedtobenon-negative\\neverywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalid\\nprobabilitydistribution,wemustusethecorrespondingnormalizedprobability\\ndistribution:4\\np() =x1\\nZ˜ p()x (16.4)\\nwhere Zisthevalue\\xa0thatresultsintheprobability\\xa0distributionsummingor\\nintegratingto1:\\nZ=\\ue05a\\n˜ p d . ()xx (16.5)\\nYoucanthinkof Zasaconstantwhenthe φfunctionsareheldconstant.Note\\nthatifthe φfunctionshaveparameters,then Zisafunctionofthoseparameters.\\nItiscommonintheliteraturetowrite Zwithitsargumentsomittedtosavespace.\\nThenormalizingconstant Zisknownasthepartitionfunction,atermborrowed\\nfromstatisticalphysics.\\nSince Zisanintegralorsumoverallpossiblejointassignmentsofthestatex\\nitisoftenintractabletocompute.\\xa0Inordertobeabletoobtainthenormalized\\nprobabilitydistributionofanundirectedmodel,\\xa0themodelstructureandthe\\ndeﬁnitionsofthe φfunctionsmustbeconducivetocomputing Zeﬃciently.In\\nthecontextofdeeplearning, Zisusuallyintractable.\\xa0Due totheintractability\\nofcomputing Zexactly,wemustresorttoapproximations .Suchapproximate\\nalgorithmsarethetopicofchapter.18\\nOneimportantconsiderationtokeepinmindwhendesigningundirectedmodels\\nisthatitispossibletospecifythefactorsinsuchawaythat Zdoesnotexist.\\nThishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral\\n4A d i s t rib u t i o n d e ﬁ n e d b y n o rm a l i z i n g a p ro d u c t o f c l i q u e p o t e n t i a l s i s a l s o c a l l e d a Gib b s\\nd is t rib u t i on .\\n5 6 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60f0cfde-6702-4830-983d-5a3477b605b4', embedding=None, metadata={'page_label': '584', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nof˜ povertheirdomaindiverges.Forexample,supposewewanttomodelasingle\\nscalarvariablexwithasinglecliquepotential ∈ R φ x x () = 2.Inthiscase,\\nZ=\\ue05a\\nx2d x . (16.6)\\nSincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingto\\nthischoiceof φ( x).\\xa0Sometimes thechoiceofsomeparameterofthe φfunctions\\ndetermineswhetherthe\\xa0probabilit ydistribution\\xa0isdeﬁned.For\\xa0example,\\xa0for\\nφ( x; β) =exp\\ue000− β x2\\ue001\\n,the βparameterdetermineswhether Zexists.Positive β\\nresultsinaGaussiandistributionoverxbutallothervaluesof βmake φimpossible\\ntonormalize.\\nOnekeydiﬀerencebetweendirectedmodelingandundirectedmodelingisthat\\ndirectedmodelsaredeﬁneddirectlyintermsofprobabilitydistributionsfrom\\nthestart,whileundirectedmodelsaredeﬁnedmorelooselyby φfunctionsthat\\narethenconvertedintoprobabilitydistributions.Thischangestheintuitionsone\\nmustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmind\\nwhileworkingwithundirectedmodelsisthatthedomainofeachofthevariables\\nhasdramaticeﬀectonthekindofprobabilitydistributionthatagivensetof φ\\nfunctionscorrespondsto.Forexample,consideran n-dimensionalvector-valued\\nrandomvariable xandanundirectedmodelparametrized byavectorofbiases\\nb.Supposewehaveonecliqueforeachelementofx, φ( ) i(x i) =exp( b ix i).What\\nkindofprobabilitydistributiondoesthisresultin?Theansweristhatwedo\\nnothaveenoughinformation,becausewehavenotyetspeciﬁedthedomainofx.\\nIfx ∈ Rn,thentheintegraldeﬁning Zdivergesandnoprobabilitydistribution\\nexists.Ifx∈{0 ,1}n,then p(x)factorizesinto nindependentdistributions,with\\np(x i= 1) =sigmoid ( b i).Ifthedomainofxisthesetofelementarybasisvectors\\n({[1 ,0 , . . . ,0] ,[0 ,1 , . . . ,0] , . . . ,[0 ,0 , . . . ,1]})then p(x)=softmax ( b),soalarge\\nvalueof b iactuallyreduces p(x j=1)for j\\ue036= i.\\xa0Often,itispossibletoleverage\\ntheeﬀectofacarefullychosendomainofavariableinordertoobtaincomplicated\\nbehaviorfromarelativelysimplesetof φfunctions.Wewillexploreapractical\\napplicationofthisidealater,insection.20.6\\n1 6 . 2 . 4 E n erg y-B a s ed Mo d el s\\nManyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas-\\nsumptionthat∀x ,˜ p(x) >0.Aconvenientwaytoenforcethisconditionistouse\\nan (EBM)where energy-basedmodel\\n˜ p E () = exp( x −())x (16.7)\\n5 6 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2cf16309-5c12-40ef-befc-fb6bea712433', embedding=None, metadata={'page_label': '585', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\na b c\\nd e f\\nFigure16.4:Thisgraphimpliesthat p(abcdef , , , , ,)canbewrittenas\\n1\\nZφ a b ,(ab ,) φ b c ,(bc ,) φ a d ,(ad ,) φ b e ,(be ,) φ e f ,(ef ,)foranappropriatechoiceofthe φfunc-\\ntions.\\nand E(x)isknownastheenergyfunction.Becauseexp( z)ispositiveforall\\nz,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzero\\nforanystatex.Beingcompletely\\xa0free to\\xa0choose\\xa0theenergyfunction\\xa0makes\\nlearningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouse\\nconstrainedoptimization toarbitrarilyimposesomespeciﬁcminimalprobability\\nvalue.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5\\nTheprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozero\\nbutneverreachit.\\nAnydistributionoftheformgivenbyequationisanexampleofa 16.7 Boltz-\\nmann\\xa0distribution.For\\xa0this\\xa0reason,\\xa0manyenergy-based\\xa0models\\xa0are\\xa0called\\nBoltzmannmachines(Fahlman 1983Ackley1985Hinton e t a l .,; e t a l .,; e t a l .,\\n1984HintonandSejnowski1986 ; ,).Thereisnoacceptedguidelineforwhentocall\\namodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.The\\ntermBoltzmannmachinewasﬁrstintroducedtodescribeamodelwithexclusively\\nbinaryvariables,buttodaymanymodelssuchasthemean-covariancerestricted\\nBoltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmann\\nmachineswereoriginallydeﬁnedtoencompassbothmodelswithandwithoutla-\\ntentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignate\\nmodelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariables\\naremoreoftencalledMarkovrandomﬁeldsorlog-linearmodels.\\nCliquesinanundirectedgraphcorrespondtofactorsoftheunnormalized\\nprobabilityfunction.Becauseexp( a)exp( b) =exp( a+ b),thismeansthatdiﬀerent\\ncliquesintheundirectedgraphcorrespondtothediﬀerenttermsoftheenergy\\nfunction.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkov\\nnetwork:theexponentiationmakeseachtermintheenergyfunctioncorrespond\\ntoafactorforadiﬀerentclique.Seeﬁgureforanexampleofhowtoreadthe 16.5\\n5F o r s o m e m o d e l s , we m a y s t i l l n e e d t o u s e c o n s t ra i n e d o p t i m i z a t i o n t o m a k e s u re e x i s t s . Z\\n5 7 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='90552385-f5a7-46d2-a971-f24b9bea046f', embedding=None, metadata={'page_label': '586', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\na b c\\nd e f\\nFigure\\xa016.5:Thisgraph\\xa0impliesthat E(abcdef , , , , ,)can\\xa0be\\xa0writtenas E a b ,(ab ,)+\\nE b c ,(bc ,)+ E a d ,(ad ,)+ E b e ,(be ,)+ E e f ,(ef ,)foranappropriatechoiceoftheper-clique\\nenergyfunctions.Notethatwecanobtainthe φfunctionsinﬁgurebysettingeach 16.4 φ\\ntotheexponentialofthecorrespondingnegativeenergy,e.g., φ a b ,(ab ,) =exp(()) − Eab ,.\\nformoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewan\\nenergy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproduct\\nofexperts(Hinton1999,).Eachtermintheenergyfunctioncorrespondsto\\nanotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncan\\nbethoughtofasan“expert”thatdetermineswhetheraparticularsoftconstraint\\nissatisﬁed.Eachexpertmayenforceonlyoneconstraintthatconcernsonly\\nalow-dimensionalprojectionoftherandomvariables,butwhencombinedby\\nmultiplicationofprobabilities, theexpertstogetherenforceacomplicatedhigh-\\ndimensionalconstraint.\\nOnepartofthedeﬁnitionofanenergy-basedmodelservesnofunctionalpurpose\\nfromamachinelearningpointofview:the−signinequation.This16.7 −sign\\ncouldbeincorporatedintothedeﬁnitionof E.Formanychoicesofthefunction\\nE,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The\\n−signispresentprimarilytopreservecompatibilitybetweenthemachinelearning\\nliteratureandthephysicsliterature.Manyadvancesinprobabilisticmodeling\\nwereoriginallydevelopedbystatisticalphysicists,forwhom Ereferstoactual,\\nphysicalenergyanddoesnothavearbitrarysign.\\xa0Terminologysuchas“energy”\\nand“partitionfunction”remainsassociatedwiththesetechniques,eventhough\\ntheirmathematical applicabilityisbroaderthanthephysicscontextinwhichthey\\nweredeveloped.Somemachinelearningresearchers(e.g., (),who Smolensky1986\\nreferredtonegativeenergyasharmony)havechosentoemitthenegation,but\\nthisisnotthestandardconvention.\\nManyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocompute\\np m o de l( x)butonly log ˜ p m o de l( x).Forenergy-basedmodelswithlatentvariables h,\\nthesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity,\\n5 7 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4e3ab6b2-9963-4fac-afcf-48a1566cc6ee', embedding=None, metadata={'page_label': '587', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\na s b a s b\\n(a) (b)\\nFigure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsis\\nactive,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heres\\nisshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbis\\nthroughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens.\\ncalledthe :freeenergy\\nF − () = x log\\ue058\\nhexp(( )) − E x h , . (16.8)\\nInthisbook,weusuallypreferthemoregeneral log ˜ p m o de l() xformulation.\\n1 6 . 2 . 5 S ep a ra t i o n a n d D - S ep a r a t i o n\\nTheedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoften\\nneedtoknowwhichvariables i ndir e c t l yinteract.Someoftheseindirectinteractions\\ncanbeenabledordisabledbyobservingothervariables.Moreformally,wewould\\nliketoknowwhichsubsetsofvariablesareconditionallyindependentfromeach\\nother,giventhevaluesofothersubsetsofvariables.\\nIdentifyingtheconditionalindependencesinagraphisverysimpleinthecase\\nofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraph\\niscalledseparation.Wesaythatasetofvariables Aisseparatedfromanother\\nsetofvariables Bgivenathirdsetofvariables Sifthegraphstructureimpliesthat\\nAisindependentfrom Bgiven S.Iftwovariablesaandbareconnectedbyapath\\ninvolvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifno\\npathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyare\\nseparated.Werefertopathsinvolvingonlyunobservedvariablesas“active”and\\npathsincludinganobservedvariableas“inactive.”\\nWhenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin.\\nSeeﬁgureforadepictionofhowactiveandinactivepathsinanundirected 16.6\\nmodellookwhendrawninthisway.Seeﬁgureforanexampleofreading 16.7\\nseparationfromanundirectedgraph.\\nSimilar\\xa0concepts apply\\xa0todirected\\xa0models ,except\\xa0that\\xa0inthe\\xa0context\\xa0of\\ndirectedmodels,theseconceptsarereferredtoasd-separation.The“d”stands\\nfor“dependence.”\\xa0D-separati onfordirectedgraphsisdeﬁnedthesameasseparation\\n5 7 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c4833f55-5ddd-43b8-9e05-57b567de415c', embedding=None, metadata={'page_label': '588', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\na\\nb c\\nd\\nFigure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Here\\nbisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfrom\\natoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofb\\nalsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem.\\nTherefore,aanddarenotseparatedgivenb.\\nforundirectedgraphs:Wesaythatasetofvariables Aisd-separatedfromanother\\nsetofvariables Bgivenathirdsetofvariables Sifthegraphstructureimplies\\nthatisindependentfromgiven. A B S\\nAswithundirectedmodels,wecanexaminetheindependencesimpliedbythe\\ngraphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariables\\naredependentifthereisanactivepathbetweenthem,andd-separatedifnosuch\\npathexists.Indirectednets,determiningwhetherapathisactiveissomewhat\\nmorecomplicated. Seeﬁgureforaguidetoidentifyingactivepathsina 16.8\\ndirectedmodel.Seeﬁgureforanexampleofreadingsomepropertiesfroma 16.9\\ngraph.\\nItisimportanttorememberthatseparationandd-separationtellusonly\\naboutthoseconditionalindependences t h a t a r e i m p l i e d b y t h e g r a p h .Thereisno\\nrequirementthatthegraphimplyallindependencesthatarepresent.Inparticular,\\nitisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges)\\ntorepresentanydistribution.Infact,somedistributionscontainindependences\\nthatarenotpossibletorepresentwithexistinggraphicalnotation.Context-\\nspeciﬁcindependencesareindependencesthatarepresentdependentonthe\\nvalueofsomevariablesinthenetwork.\\xa0Forexample,consideramodelofthree\\nbinaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent,\\nbutwhenais1,bisdeterministicallyequaltoc.\\xa0Encodingthebehaviorwhen\\na= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatb\\nandcareindependentwhena.= 0\\nIngeneral,agraphwillneverimplythatanindependenceexistswhenitdoes\\nnot.However,agraphmayfailtoencodeanindependence.\\n5 7 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76d0f7ae-da05-4b88-a9e6-5c7a3adcd1d9', embedding=None, metadata={'page_label': '589', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\na s b\\na s b\\na\\nsb a s ba s b\\nc( a ) ( b )\\n( c ) ( d )\\nFigure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandom\\nvariablesaandb.Anypathwitharrowsproceedingdirectlyfrom ( a ) atoborviceversa.\\nThiskindofpathbecomesblockedifsisobserved.\\xa0Wehavealreadyseenthiskindof\\npathintherelayraceexample. ( b )aandbareconnectedbya c o m m o n c a u s es.For\\nexample,supposesisavariableindicatingwhetherornotthereisahurricaneandaand\\nbmeasurethewindspeedattwodiﬀerentnearbyweathermonitoringoutposts.Ifwe\\nobserveveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.This\\nkindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,we\\nexpecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpected\\nwindata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowing\\nthereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,the\\npathisactive. ( c )aandbarebothparentsofs.ThisiscalledaV-structureorthe\\ncollidercase.\\xa0TheV-structurecausesaandbtoberelatedbytheexplainingaway\\neﬀect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,suppose\\nsisavariableindicatingthatyourcolleagueisnotatwork.\\xa0Thevariablearepresents\\nherbeingsick,whilebrepresentsherbeingonvacation.\\xa0Ifyouobservethatsheisnot\\natwork,youcanpresumesheisprobablysickoronvacation,butitisnotespecially\\nlikelythatbothhavehappenedatthesametime.Ifyouﬁndoutthatsheisonvacation,\\nthisfactissuﬃcienttoherabsence.Youcaninferthatsheisprobablynotalso e x p l a i n\\nsick.Theexplainingawayeﬀecthappensevenifanydescendantof ( d ) sisobserved!For\\nexample,supposethatcisavariablerepresentingwhetheryouhavereceivedareport\\nfromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreases\\nyourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesit\\nmorelikelythatsheiseithersickoronvacation.Theonlywaytoblockapaththrougha\\nV-structureistoobservenoneofthedescendantsofthesharedchild.\\n5 7 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2fc5888c-642a-43d4-9c65-99c7c0fb9b93', embedding=None, metadata={'page_label': '590', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\na b\\nc\\nd e\\nFigure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examples\\ninclude:\\n•aandbared-separatedgiventheemptyset.\\n•aandeared-separatedgivenc.\\n•dandeared-separatedgivenc.\\nWecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesome\\nvariables:\\n•aandbarenotd-separatedgivenc.\\n•aandbarenotd-separatedgivend.\\n5 7 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e27b4d2-d09b-48f6-89d0-74b544f65bee', embedding=None, metadata={'page_label': '591', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\n1 6 . 2 . 6 Co n vert i n g b et ween Un d i rec t ed a n d D i rect ed G ra p h s\\nWeoftenrefertoaspeciﬁcmachinelearningmodelasbeingundirectedordirected.\\nForexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected.\\nThischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodel\\nisinherentlydirectedorundirected.Instead,somemodelsaremosteasily d e s c r i b e d\\nusingadirectedgraph,ormosteasilydescribedusinganundirectedgraph.\\nDirectedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad-\\nvantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead,\\nweshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartially\\ndependonwhichprobabilitydistributionwewishtodescribe.Wemaychooseto\\nuseeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcan\\ncapturethemostindependencesintheprobabilitydistributionorwhichapproach\\nusesthefewestedgestodescribethedistribution.Thereareotherfactorsthat\\ncanaﬀectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingle\\nprobabilitydistribution,wemaysometimesswitchbetweendiﬀerentmodeling\\nlanguages.Sometimesadiﬀerentlanguagebecomesmoreappropriateifweobserve\\nacertainsubsetofvariables,orifwewishtoperformadiﬀerentcomputational\\ntask.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforward\\napproachtoeﬃcientlydrawsamplesfromthemodel(describedinsection)16.3\\nwhiletheundirectedmodelformulationisoftenusefulforderivingapproximate\\ninferenceprocedures(aswewillseeinchapter,wheretheroleofundirected 19\\nmodelsishighlightedinequation).19.56\\nEveryprobabilitydistributioncanberepresentedbyeitheradirectedmodel\\norbyanundirectedmodel.Intheworstcase,onecanalwaysrepresentany\\ndistributionbyusinga“completegraph.”Inthecaseofadirectedmodel,the\\ncompletegraphisanydirectedacyclicgraphwhereweimposesomeorderingon\\ntherandomvariables,andeachvariablehasallothervariablesthatprecedeitin\\ntheorderingasitsancestorsinthegraph.Foranundirectedmodel,thecomplete\\ngraphissimplyagraphcontainingasinglecliqueencompassingallofthevariables.\\nSeeﬁgureforanexample. 16.10\\nOfcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsome\\nvariablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseit\\ndoesnotimplyanyindependences.\\nWhenwerepresentaprobabilitydistributionwithagraph,wewanttochoose\\nagraphthatimpliesasmanyindependencesaspossible,withoutimplyingany\\nindependencesthatdonotactuallyexist.\\nFromthispointofview,somedistributionscanberepresentedmoreeﬃciently\\n5 7 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1231bc46-6706-4094-bd30-d544f5ed799f', embedding=None, metadata={'page_label': '592', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nFigure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution.\\nHereweshowexampleswithfourrandomvariables. ( L e f t )Thecompleteundirectedgraph.\\nIntheundirectedcase,thecompletegraphisunique.Acompletedirectedgraph. ( R i g h t )\\nInthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthe\\nvariablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritinthe\\nordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandom\\nvariables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom.\\nusingdirectedmodels,whileotherdistributionscanberepresentedmoreeﬃciently\\nusing\\xa0undirectedmodels.In\\xa0other\\xa0words,directed\\xa0models\\xa0canencode\\xa0some\\nindependencesthatundirectedmodelscannotencode,andviceversa.\\nDirectedmodelsareabletouseonespeciﬁckindofsubstructurethatundirected\\nmodelscannotrepresentperfectly.Thissubstructureiscalledanimmorality.\\nThestructureoccurswhentworandomvariablesaandbarebothparentsofa\\nthirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineither\\ndirection.(Thename“immorality”mayseemstrange;itwascoinedinthegraphical\\nmodelsliteratureasajokeaboutunmarriedparents.)Toconvertadirectedmodel\\nwithgraph Dintoanundirectedmodel,weneedtocreateanewgraph U.\\xa0For\\neverypairofvariablesxandy,weaddanundirectededgeconnectingxandyto\\nUifthereisadirectededge(ineitherdirection)connectingxandyinDorifx\\nandyarebothparentsinDofathirdvariablez.Theresulting Uisknownasa\\nmoralizedgraph.Seeﬁgureforexamplesofconvertingdirectedmodelsto 16.11\\nundirectedmodelsviamoralization.\\nLikewise,undirectedmodelscanincludesubstructuresthatnodirectedmodel\\ncanrepresentperfectly.Speciﬁcally,adirectedgraphcannotcaptureallofthe D\\nconditionalindependencesimpliedbyanundirectedgraph UifUcontainsaloop\\noflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopis\\nasequenceofvariablesconnectedbyundirectededges,withthelastvariablein\\nthesequenceconnectedbacktotheﬁrstvariableinthesequence.\\xa0Achordisa\\nconnectionbetweenanytwonon-consecutivevariablesinthesequencedeﬁninga\\nloop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsforthese\\nloops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding\\n5 7 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cc9f6cab-5789-495c-8455-53f7eedc5396', embedding=None, metadata={'page_label': '593', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nh 1 h 1 h 2 h 2 h 3 h 3\\nv 1 v 1 v 2 v 2 v 3 v 3a b\\nca\\ncb\\nh 1 h 1 h 2 h 2 h 3 h 3\\nv 1 v 1 v 2 v 2 v 3 v 3a b\\nca\\ncb\\nFigure16.11:\\xa0Exam plesofconvertingdirectedmodels(toprow)toundirectedmodels\\n(bottomrow)byconstructingmoralizedgraphs. ( L e f t )Thissimplechaincanbeconverted\\ntoamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.The\\nresultingundirectedmodelimpliesexactlythesamesetofindependencesandconditional\\nindependences.Thisgraphisthesimplestdirectedmodelthatcannotbeconverted ( C e n t e r )\\ntoanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirely\\nofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactive\\npathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustinclude\\nacliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthatab⊥.\\n( R i g h t )Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany\\nimpliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizing\\nedgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnew\\ndirectdependences.\\n5 7 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ea7beca0-cc72-47e7-835d-6dc27f2b0e0a', embedding=None, metadata={'page_label': '594', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\na b\\nd ca b\\nd ca b\\nd c\\nFigure16.12:Convertinganundirectedmodeltoadirectedmodel. ( L e f t )Thisundirected\\nmodelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfour\\nwithnochords.Speciﬁcally,theundirectedmodelencodestwodiﬀerentindependencesthat\\nnodirectedmodelcancapturesimultaneously:acbd ⊥|{ ,}andbdac ⊥|{ ,}.To ( C e n t e r )\\nconverttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,by\\nensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecaneither\\naddanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthis\\nexample,wechoosetoaddtheedgeconnectingaandc.Toﬁnishtheconversion ( R i g h t )\\nprocess,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateany\\ndirectedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes,\\nandalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenode\\nthatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimpose\\nalphabeticalorder.\\nthesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU.\\nThegraphformedbyaddingchordstoUisknownasachordalortriangulated\\ngraph,becausealltheloopscannowbedescribedintermsofsmaller,triangular\\nloops.Tobuildadirectedgraph Dfromthechordalgraph,weneedtoalsoassign\\ndirectionstotheedges.Whendoingso,wemustnotcreateadirectedcyclein\\nD,ortheresultdoesnotdeﬁneavaliddirectedprobabilisticmodel.Oneway\\ntoassigndirectionstotheedgesinDistoimposeanorderingontherandom\\nvariables,thenpointeachedgefromthenodethatcomesearlierintheorderingto\\nthenodethatcomeslaterintheordering.Seeﬁgureforademonstration. 16.12\\n1 6 . 2 . 7 F a ct o r G ra p h s\\nFactorgraphsareanotherwayofdrawingundirectedmodelsthatresolvean\\nambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.In\\nanundirectedmodel,thescopeofevery φfunctionmustbeaofsomeclique s u b s e t\\ninthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhas\\nacorrespondingfactorwhosescopeencompassestheentireclique—forexample,\\nacliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes,\\normaycorrespondtothreefactorsthateachcontainonlyapairofthenodes.\\n5 7 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e1b03bd5-4916-4d85-9d76-57cad0769707', embedding=None, metadata={'page_label': '595', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nFactorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeach φ\\nfunction.Speciﬁcally,afactorgraphisagraphicalrepresentationofanundirected\\nmodelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawn\\nascircles.Thesenodescorrespondtorandomvariablesasinastandardundirected\\nmodel.\\xa0Therestofthenodesaredrawnassquares.\\xa0Thesenodescorrespondto\\nthefactors φoftheunnormalized probabilitydistribution.Variablesandfactors\\nmaybeconnectedwithundirectededges.Avariableandafactorareconnected\\ninthegraphifandonlyifthevariableisoneoftheargumentstothefactorin\\ntheunnormalized probabilitydistribution.Nofactormaybeconnectedtoanother\\nfactorinthegraph,norcanavariablebeconnectedtoavariable.Seeﬁgure16.13\\nforanexampleofhowfactorgraphscanresolveambiguityintheinterpretation of\\nundirectednetworks.\\na b\\nca b\\ncf 1 f 1a b\\ncf 1 f 1f 2 f 2\\nf 3 f 3\\nFigure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretation\\nofundirectednetworks. ( L e f t )Anundirectednetworkwithacliqueinvolvingthreevariables:\\na,bandc.Afactorgraphcorrespondingtothesameundirectedmodel.This ( C e n t e r )\\nfactorgraphhasonefactoroverallthreevariables.\\xa0Anothervalidfactorgraph ( R i g h t )\\nforthesameundirectedmodel.Thisfactorgraphhasthreefactors,eachoveronlytwo\\nvariables.Representation,inference,andlearningareallasymptoticallycheaperinthis\\nfactorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethe\\nsameundirectedgraphtorepresent.\\n16.3SamplingfromGraphicalModels\\nGraphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel.\\nOneadvantageofdirectedgraphicalmodelsisthatasimpleandeﬃcientproce-\\ndurecalledancestralsamplingcanproduceasamplefromthejointdistribution\\nrepresentedbythemodel.\\nThebasicideaistosortthevariablesx iinthegraphintoatopologicalordering,\\nsothatforall iand j, jisgreaterthan iifx iisaparentofx j.Thevariables\\n5 8 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c61330f5-cbaf-4aa1-aba4-db91cc1271ed', embedding=None, metadata={'page_label': '596', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\ncanthenbesampledinthisorder.Inotherwords,weﬁrstsamplex 1∼ P(x 1),\\nthensample P(x 2| P aG(x 2)),andsoon,untilﬁnallywesample P(x n| P aG(x n)).\\nSolongaseachconditionaldistribution p(x i| P aG(x i))iseasytosamplefrom,\\nthenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperation\\nguaranteesthatwecanreadtheconditionaldistributionsinequationand16.1\\nsamplefromtheminorder.Withoutthetopologicalsorting,wemightattemptto\\nsampleavariablebeforeitsparentsareavailable.\\nForsomegraphs,morethanonetopologicalorderingispossible.Ancestral\\nsamplingmaybeusedwithanyofthesetopologicalorderings.\\nAncestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi-\\ntionaliseasy)andconvenient.\\nOnedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphical\\nmodels.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsampling\\noperation.Whenwewishtosamplefromasubsetofthevariablesinadirected\\ngraphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition-\\ningvariablescomeearlierthanthevariablestobesampledintheorderedgraph.\\nInthiscase,wecansamplefromthelocalconditionalprobabilitydistributions\\nspeciﬁedbythemodeldistribution.Otherwise,theconditionaldistributionswe\\nneedtosamplefromaretheposteriordistributionsgiventheobservedvariables.\\nTheseposteriordistributionsareusuallynotexplicitlyspeciﬁedandparametrized\\ninthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswhere\\nthisisthecase,ancestralsamplingisnolongereﬃcient.\\nUnfortunately,ancestralsamplingisapplicableonlytodirectedmodels.We\\ncansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthis\\noftenrequiressolvingintractableinferenceproblems(todeterminethemarginal\\ndistributionovertherootnodesofthenewdirectedgraph)orrequiresintroducing\\nsomanyedgesthattheresultingdirectedmodelbecomesintractable.Sampling\\nfromanundirectedmodelwithoutﬁrstconvertingittoadirectedmodelseemsto\\nrequireresolvingcyclicaldependencies.Everyvariableinteractswitheveryother\\nvariable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately,\\ndrawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-pass\\nprocess.TheconceptuallysimplestapproachisGibbssampling.Supposewe\\nhaveagraphicalmodeloveran n-dimensionalvectorofrandomvariables x.We\\niterativelyvisiteachvariablex ianddrawasampleconditionedonalloftheother\\nvariables,from p(x i|x− i).Duetotheseparationpropertiesofthegraphical\\nmodel,wecanequivalentlyconditionononlytheneighborsofx i.Unfortunately,\\nafterwehavemadeonepassthroughthegraphicalmodelandsampledall n\\nvariables,westilldonothaveafairsamplefrom p(x).Instead,wemustrepeatthe\\n5 8 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='673e5f47-21db-4b03-ba5c-c85bd192452e', embedding=None, metadata={'page_label': '597', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nprocessandresampleall nvariablesusingtheupdatedvaluesoftheirneighbors.\\nAsymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfrom\\nthecorrectdistribution.Itcanbediﬃculttodeterminewhenthesampleshave\\nreachedasuﬃcientlyaccurateapproximationofthedesireddistribution.Sampling\\ntechniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailin\\nchapter.17\\n16.4AdvantagesofStructuredModeling\\nTheprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallow\\nustodramatically reducethecostofrepresentingprobabilitydistributionsaswell\\naslearningandinference.Samplingisalsoacceleratedinthecaseofdirected\\nmodels,whilethesituationcanbecomplicatedwithundirectedmodels.The\\nprimarymechanismthatallowsalloftheseoperationstouselessruntimeand\\nmemoryischoosingtonotmodelcertaininteractions. Graphicalmodelsconvey\\ninformationbyleavingedgesout.Anywherethereisnotanedge,themodel\\nspeciﬁestheassumptionthatwedonotneedtomodeladirectinteraction.\\nAlessquantiﬁablebeneﬁtofusingstructuredprobabilisticmodelsisthat\\ntheyallowustoexplicitlyseparaterepresentationofknowledgefromlearningof\\nknowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasierto\\ndevelopanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsand\\ninferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently,\\nwecandesignmodelsthatcapturetherelationshipswebelieveareimportantinour\\ndata.Wecanthencombinethesediﬀerentalgorithmsandstructuresandobtain\\naCartesianproductofdiﬀerentpossibilities.Itwouldbemuchmorediﬃcultto\\ndesignend-to-endalgorithmsforeverypossiblesituation.\\n16.5LearningaboutDependencies\\nAgoodgenerativemodelneedstoaccuratelycapturethedistributionoverthe\\nobservedor“visible”\\xa0variables v.Oftenthediﬀerentelementsofvarehighly\\ndependentoneachother.Inthecontextofdeeplearning,theapproachmost\\ncommonlyusedtomodelthesedependenciesistointroduceseverallatentor\\n“hidden”variables,h.Themodelcanthencapturedependenciesbetweenanypair\\nofvariablesv iandv jindirectly,viadirectdependenciesbetweenv iandh,and\\ndirectdependenciesbetweenandv h j.\\nAgoodmodelofvwhichdidnotcontainanylatentvariableswouldneedto\\n5 8 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b91cf86c-052b-4c8e-ba06-aa11f52db253', embedding=None, metadata={'page_label': '598', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nhaveverylargenumbersofparentspernodeinaBayesiannetworkorverylarge\\ncliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsis\\ncostly—bothinacomputational sense,becausethenumberofparametersthat\\nmustbestoredinmemoryscalesexponentiallywiththenumberofmembersina\\nclique,butalsoinastatisticalsense,becausethisexponentialnumberofparameters\\nrequiresawealthofdatatoestimateaccurately.\\nWhenthemodelisintendedtocapturedependenciesbetweenvisiblevariables\\nwithdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothe\\ngraphmustbedesignedtoconnectthosevariablesthataretightlycoupledand\\nomitedgesbetweenothervariables.Anentireﬁeldofmachinelearningcalled\\nstructurelearningisdevotedtothisproblemForagoodreferenceonstructure\\nlearning,see(KollerandFriedman2009,).Moststructurelearningtechniquesare\\naformofgreedysearch.Astructureisproposed,amodelwiththatstructure\\nistrained,thengivenascore.Thescorerewardshightrainingsetaccuracyand\\npenalizesmodelcomplexity.Candidatestructureswithasmallnumberofedges\\naddedorremovedarethenproposedasthenextstepofthesearch.Thesearch\\nproceedstoanewstructurethatisexpectedtoincreasethescore.\\nUsinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperform\\ndiscretesearchesandmultipleroundsoftraining.Aﬁxedstructureovervisible\\nandhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunits\\ntoimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameter\\nlearningtechniqueswecanlearnamodelwithaﬁxedstructurethatimputesthe\\nrightstructureonthemarginal . p()v\\nLatentvariableshaveadvantagesbeyondtheirroleineﬃcientlycapturing p(v).\\nThenewvariables halsoprovideanalternativerepresentationforv.Forexample,\\nasdiscussedinsection,themixtureofGaussiansmodellearnsalatentvariable 3.9.6\\nthatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.This\\nmeansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodo\\nclassiﬁcation.\\xa0Inchapterwesawhowsimpleprobabilisticmodelslikesparse 14\\ncodinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassiﬁer,\\norascoordinatesalongamanifold.Othermodelscanbeusedinthissameway,\\nbutdeepermodelsandmodelswithdiﬀerentkindsofinteractionscancreateeven\\nricherdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearning\\nbylearninglatentvariables.Often,givensomemodelofvandh,experimental\\nobservationsshowthat E[hv|]orargmaxh p( h v ,)isagoodfeaturemappingfor\\nv.\\n5 8 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3d6f8117-4843-41d8-96ca-d210292821f8', embedding=None, metadata={'page_label': '599', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\n16.6InferenceandApproximateInference\\nOneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabout\\nhowvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanask\\nwhatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwantto\\nextractfeatures E[hv|]describingtheobservedvariables v.Sometimesweneed\\ntosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodels\\nusingtheprincipleofmaximumlikelihood.Because\\nlog()= p v E h h∼ p (| v )[log( )log( )] p h v ,− p h v| ,(16.9)\\nweoftenwanttocompute p(h| v)inordertoimplementalearningrule.Allof\\ntheseareexamplesofinferenceproblemsinwhichwemustpredictthevalueof\\nsomevariablesgivenothervariables,orpredicttheprobabilitydistributionover\\nsomevariablesgiventhevalueofothervariables.\\nUnfortunately,formostinterestingdeepmodels,theseinferenceproblemsare\\nintractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.The\\ngraphstructureallowsustorepresentcomplicated,high-dimensionaldistributions\\nwithareasonablenumberofparameters,butthegraphsusedfordeeplearningare\\nusuallynotrestrictiveenoughtoalsoalloweﬃcientinference.\\nItisstraightforwardtoseethatcomputingthemarginalprobabilityofageneral\\ngraphicalmodelis#Phard.Thecomplexityclass#Pisageneralization ofthe\\ncomplexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblem\\nhasasolutionandﬁndingasolutionifoneexists.Problemsin#Prequirecounting\\nthenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethat\\nwedeﬁneagraphicalmodeloverthebinaryvariablesina3-SATproblem.\\xa0We\\ncanimposeauniformdistributionoverthesevariables.Wecanthenaddone\\nbinarylatentvariableperclausethatindicateswhethereachclauseissatisﬁed.\\nWecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesare\\nsatisﬁed.Thiscanbedonewithoutmakingalargeclique,bybuildingareduction\\ntreeoflatentvariables,witheachnodeinthetreereportingwhethertwoother\\nvariablesaresatisﬁed.Theleavesofthistreearethevariablesforeachclause.\\nTherootofthetreereportswhethertheentireproblemissatisﬁed.\\xa0Duetothe\\nuniformdistributionovertheliterals,themarginaldistributionovertherootofthe\\nreductiontreespeciﬁeswhatfractionofassignmentssatisfytheproblem.While\\nthisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpractical\\nreal-worldscenarios.\\nThismotivatestheuseofapproximate inference.In\\xa0thecontextof\\xa0deep\\nlearning,thisusuallyreferstovariationalinference,inwhichweapproximate the\\n5 8 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7fb803ee-f9ac-420e-aba2-8a5a37b9f3b7', embedding=None, metadata={'page_label': '600', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\ntruedistribution p(h| v)byseekinganapproximate distribution q(hv|)thatisas\\nclosetothetrueoneaspossible.Thisandothertechniquesaredescribedindepth\\ninchapter.19\\n16.7TheDeepLearningApproachtoStructuredProb-\\nabilisticModels\\nDeeplearningpractitioners generallyusethesamebasiccomputational toolsas\\nothermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels.\\nHowever,inthecontextofdeeplearning,weusuallymakediﬀerentdesigndecisions\\nabouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthat\\nhaveaverydiﬀerentﬂavorfrommoretraditionalgraphicalmodels.\\nDeeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthe\\ncontextofgraphicalmodels,wecandeﬁnethedepthofamodelintermsofthe\\ngraphicalmodelgraphratherthanthecomputational graph.Wecanthinkofa\\nlatentvariable h iasbeingatdepth jiftheshortestpathfrom h itoanobserved\\nvariableis jsteps.Weusuallydescribethedepthofthemodelasbeingthegreatest\\ndepthofanysuch h i.Thiskindofdepthisdiﬀerentfromthedepthinducedby\\nthecomputational graph.Manygenerativemodelsusedfordeeplearninghaveno\\nlatentvariablesoronlyonelayeroflatentvariables,butusedeepcomputational\\ngraphstodeﬁnetheconditionaldistributionswithinamodel.\\nDeeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen-\\ntations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretraining\\nshallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalways\\nhaveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhave\\nmorelatentvariablesthanobservedvariables.Complicated nonlinearinteractions\\nbetweenvariablesareaccomplishedviaindirectconnectionsthatﬂowthrough\\nmultiplelatentvariables.\\nBycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthat\\nareatleastoccasionallyobserved,evenifmanyofthevariablesaremissingat\\nrandomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-order\\ntermsandstructurelearningtocapturecomplicatednonlinearinteractionsbetween\\nvariables.Iftherearelatentvariables,theyareusuallyfewinnumber.\\nThewaythatlatentvariablesaredesignedalsodiﬀersindeeplearning.The\\ndeeplearningpractitionertypicallydoesnotintendforthelatentvariablesto\\ntakeonanyspeciﬁcsemanticsaheadoftime—thetrainingalgorithmisfreeto\\ninventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare\\n5 8 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30106f46-8d99-486c-9cfd-a3449d0c532d', embedding=None, metadata={'page_label': '601', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nusuallynotveryeasyforahumantointerpretafterthefact,thoughvisualization\\ntechniquesmayallowsomeroughcharacterization ofwhattheyrepresent.When\\nlatentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyare\\noftendesignedwithsomespeciﬁcsemanticsinmind—thetopicofadocument,\\ntheintelligenceofastudent,thediseasecausingapatient’ssymptoms,etc.These\\nmodelsareoftenmuchmoreinterpretable byhumanpractitioners andoftenhave\\nmoretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandare\\nnotreusableinasmanydiﬀerentcontextsasdeepmodels.\\nAnotherobviousdiﬀerenceisthekindofconnectivitytypicallyusedinthe\\ndeeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunits\\nthatareallconnectedtoothergroupsofunits,sothattheinteractionsbetween\\ntwogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodels\\nhaveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybe\\nindividuallydesigned.Thedesignofthemodelstructureistightlylinkedwith\\nthechoiceofinferencealgorithm.Traditionalapproachestographicalmodels\\ntypicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraint\\nistoolimiting,apopularapproximate inferencealgorithmisanalgorithmcalled\\nloopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithvery\\nsparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendto\\nconnecteachvisibleunitv itoverymanyhiddenunitsh j,sothathcanprovidea\\ndistributedrepresentationofv i(andprobablyseveralotherobservedvariablestoo).\\nDistributedrepresentationshavemanyadvantages,butfromthepointofview\\nofgraphicalmodelsandcomputational complexity,distributedrepresentations\\nhavethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughfor\\nthetraditionaltechniquesofexactinferenceandloopybeliefpropagationtobe\\nrelevant.Asaconsequence,oneofthemoststrikingdiﬀerencesbetweenthelarger\\ngraphicalmodelscommunityandthedeepgraphicalmodelscommunityisthat\\nloopybeliefpropagationisalmostneverusedfordeeplearning.Mostdeepmodels\\nareinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithms\\neﬃcient.Anotherconsiderationisthatdeeplearningmodelscontainaverylarge\\nnumberoflatentvariables,makingeﬃcientnumericalcodeessential.Thisprovides\\nanadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,for\\ngroupingtheunitsintolayerswithamatrixdescribingtheinteractionbetween\\ntwolayers.Thisallowstheindividualstepsofthealgorithmtobeimplemented\\nwitheﬃcientmatrixproductoperations,orsparselyconnectedgeneralizations ,like\\nblockdiagonalmatrixproductsorconvolutions.\\nFinally,thedeeplearningapproachtographicalmodelingischaracterizedby\\namarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntil\\nallquantitieswemightwantcanbecomputedexactly,weincreasethepowerof\\n5 8 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d3284f5c-a546-4810-a473-119f1f044d68', embedding=None, metadata={'page_label': '602', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nthemodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodels\\nwhosemarginaldistributionscannotbecomputed,andaresatisﬁedsimplytodraw\\napproximatesamplesfromthesemodels.Weoftentrainmodelswithanintractable\\nobjectivefunctionthatwecannotevenapproximate inareasonableamountof\\ntime,butwearestillabletoapproximately trainthemodelifwecaneﬃciently\\nobtainanestimateofthegradientofsuchafunction.Thedeeplearningapproach\\nisoftentoﬁgureoutwhattheminimumamountofinformationweabsolutely\\nneedis,andthentoﬁgureouthowtogetareasonableapproximation ofthat\\ninformationasquicklyaspossible.\\n1 6 . 7 . 1 E xa m p l e: T h e Rest ri ct ed B o l t zm a n n Ma c h i n e\\nTherestrictedBoltzmannmachine(RBM)(,)or Smolensky1986harmonium\\nisthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning.\\nTheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariables\\nthatmaybeusedtolearnarepresentationfortheinput.Inchapter,wewill20\\nseehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowthe\\nRBMexempliﬁesmanyofthepracticesusedinawidevarietyofdeepgraphical\\nmodels:\\xa0itsunitsareorganizedintolargegroupscalledlayers,theconnectivity\\nbetweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,the\\nmodelisdesignedtoalloweﬃcientGibbssampling,andtheemphasisofthemodel\\ndesignisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemantics\\nwerenotspeciﬁedbythedesigner.Later,insection,wewillrevisittheRBM 20.2\\ninmoredetail.\\nThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhidden\\nunits.Itsenergyfunctionis\\nE ,( v h b ) = −\\ue03ev c−\\ue03eh v−\\ue03eW h , (16.10)\\nwhere b, c,and Wareunconstrained,real-valued,learnableparameters.Wecan\\nseethatthemodelisdividedintotwogroupsofunits: vand h,andtheinteraction\\nbetweenthemisdescribedbyamatrix W.Themodelisdepictedgraphically\\ninﬁgure.Asthisﬁguremakesclear,animportantaspectofthismodelis 16.14\\nthattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenany\\ntwohiddenunits(hencethe“restricted,”ageneralBoltzmannmachinemayhave\\narbitraryconnections).\\nTherestrictionsontheRBMstructureyieldtheniceproperties\\np( ) = Π hv| i p(h i|v) (16.11)\\n5 8 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8fa59581-0fd0-4f76-aea8-97798d8b7009', embedding=None, metadata={'page_label': '603', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nh 1 h 1 h 2 h 2 h 3 h 3\\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\\nFigure16.14:AnRBMdrawnasaMarkovnetwork.\\nand\\np( ) = Π vh| i p(v i|h) . (16.12)\\nTheindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBM\\nweobtain:\\nP(h i= 1 ) = |v σ\\ue010\\nv\\ue03eW : , i+ b i\\ue011\\n, (16.13)\\nP(h i= 0 ) = 1 |v − σ\\ue010\\nv\\ue03eW : , i+ b i\\ue011\\n. (16.14)\\nTogetherthesepropertiesallowforeﬃcientblockGibbssampling,whichalter-\\nnatesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultane-\\nously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshownin\\nﬁgure.16.15\\nSincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itis\\neasytotakeitsderivatives.Forexample,\\n∂\\n∂ W i , jE ,(vh) = −v ih j . (16.15)\\nThesetwoproperties—eﬃcientGibbssamplingandeﬃcientderivatives—make\\ntrainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe 18\\ntrainedbycomputingsuchderivativesappliedtosamplesfromthemodel.\\nTrainingthemodelinducesarepresentation hofthedata v.Wecanoftenuse\\nE h h∼ p (| v )[] hasasetoffeaturestodescribe. v\\nOverall,theRBMdemonstratesthetypicaldeeplearningapproachtograph-\\nicalmodels:\\xa0representationlearningaccomplishedvialayersoflatentvariables,\\ncombinedwitheﬃcientinteractionsbetweenlayersparametrized bymatrices.\\nThelanguageofgraphicalmodelsprovidesanelegant,ﬂexibleandclearlanguage\\nfordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage,\\namongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels.\\n5 8 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='01a806b8-ce65-4dda-a888-d09259bee7de', embedding=None, metadata={'page_label': '604', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING\\nFigure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwith\\npermissionfrom(). LISA2008 ( L e f t )SamplesfromamodeltrainedonMNIST,drawn\\nusingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrow\\nrepresentstheoutputofanother1,000stepsofGibbssampling.Successivesamplesare\\nhighlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare ( R i g h t )\\nthistothesamplesandweightsofalinearfactormodel,showninﬁgure.Thesamples 13.2\\nherearemuchbetterbecausetheRBMprior p( h)isnotconstrainedtobefactorial.The\\nRBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand,\\ntheRBMposterior isfactorial,whilethesparsecodingposterior isnot, p( ) h v| p( ) h v|\\nsothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareable\\ntohavebothanon-factorialandanon-factorial. p() h p( ) h v|\\n5 8 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ba927342-5b51-4431-9b08-3ee02b68cde3', embedding=None, metadata={'page_label': '605', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 7\\nMon t e C arl o Me t h o d s\\nRandomizedalgorithmsfallintotworoughcategories:LasVegasalgorithmsand\\nMonteCarloalgorithms.LasVegasalgorithmsalwaysreturnpreciselythecorrect\\nanswer(orreportthattheyfailed).Thesealgorithmsconsumearandomamount\\nofresources,usuallymemoryortime.Incontrast,MonteCarloalgorithmsreturn\\nanswerswitharandomamountoferror.Theamountoferrorcantypicallybe\\nreducedbyexpendingmoreresources(usuallyrunningtimeandmemory).Forany\\nﬁxedcomputational budget,aMonteCarloalgorithmcanprovideanapproximate\\nanswer.\\nManyproblemsinmachinelearningaresodiﬃcultthatwecanneverexpectto\\nobtainpreciseanswerstothem.Thisexcludesprecisedeterministicalgorithmsand\\nLasVegasalgorithms.Instead,wemustusedeterministicapproximatealgorithms\\norMonteCarloapproximations.Bothapproachesareubiquitousinmachine\\nlearning.Inthischapter,wefocusonMonteCarlomethods.\\n17.1SamplingandMonteCarloMethods\\nManyimportanttechnologiesusedtoaccomplishmachinelearninggoalsarebased\\nondrawingsamplesfromsomeprobabilitydistributionandusingthesesamplesto\\nformaMonteCarloestimateofsomedesiredquantity.\\n1 7 . 1 . 1 Wh y S a m p l i n g ?\\nTherearemanyreasonsthatwemaywishtodrawsamplesfromaprobability\\ndistribution.Samplingprovidesaﬂexiblewaytoapproximatemanysumsand\\n590', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f9e36e42-d56c-4cef-806b-a288a1f0c7dd', embedding=None, metadata={'page_label': '606', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nintegralsatreducedcost.Sometimesweusethistoprovideasigniﬁcantspeedupto\\nacostlybuttractablesum,asinthecasewhenwesubsamplethefulltrainingcost\\nwithminibatches.Inothercases,ourlearningalgorithmrequiresustoapproximate\\nanintractablesumorintegral,suchasthegradientofthelogpartitionfunctionof\\nanundirectedmodel.Inmanyothercases,samplingisactuallyourgoal,inthe\\nsensethatwewanttotrainamodelthatcansamplefromthetrainingdistribution.\\n1 7 . 1 . 2 B a s i cs o f Mo n t e Ca rl o S a m p l i n g\\nWhenasumoranintegralcannotbecomputedexactly(forexamplethesum\\nhasanexponentialnumberoftermsandnoexactsimpliﬁcationisknown)itis\\noftenpossibletoapproximate itusingMonteCarlosampling.Theideaistoview\\nthesumorintegralasifitwasanexpectationundersomedistributionandto\\na p p r o x i m a t e t h e e x p e c t a t i o n b y a c o r r e s p o nding a v e r a g e.Let\\ns=\\ue058\\nxp f E () x() = x p[()] f x (17.1)\\nor\\ns=\\ue05a\\np f d E () x() x x= p[()] f x (17.2)\\nbethesumorintegraltoestimate,rewrittenasanexpectation,withtheconstraint\\nthat pisaprobabilitydistribution(forthesum)oraprobabilitydensity(forthe\\nintegral)overrandomvariable. x\\nWecanapproximate sbydrawing nsamples x( 1 ), . . . , x( ) nfrom pandthen\\nformingtheempiricalaverage\\nˆ s n=1\\nnn \\ue058\\ni = 1f( x( ) i) . (17.3)\\nThisapproximation isjustiﬁedbyafewdiﬀerentproperties.Theﬁrsttrivial\\nobservationisthattheestimator ˆ sisunbiased,since\\nE[ˆ s n] =1\\nnn \\ue058\\ni = 1E[( f x( ) i)] =1\\nnn \\ue058\\ni = 1s s .= (17.4)\\nButinaddition,thelawoflargenumbersstatesthatifthesamples x( ) iare\\ni.i.d.,thentheaverageconvergesalmostsurelytotheexpectedvalue:\\nlimn→∞ˆ s n= s , (17.5)\\n5 9 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ce5c241e-b026-4a48-aab5-e5f3899129d6', embedding=None, metadata={'page_label': '607', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nprovidedthatthevarianceoftheindividualterms,Var[ f( x( ) i)],isbounded.Tosee\\nthismoreclearly,considerthevarianceofˆ s nas nincreases.Thevariance Var[ˆ s n]\\ndecreasesandconvergesto0,solongasVar[( f x( ) i)] <∞:\\nVar[ˆ s n] =1\\nn2n\\ue058\\ni = 1Var[()] f x (17.6)\\n=Var[()] f x\\nn. (17.7)\\nThisconvenientresultalsotellsushowtoestimatetheuncertaintyinaMonte\\nCarloaverageorequivalentlytheamountofexpectederroroftheMonteCarlo\\napproximation.Wecomputeboththeempiricalaverageofthe f( x( ) i)andtheir\\nempiricalvariance,1andthendividetheestimatedvariancebythenumberof\\nsamples ntoobtainanestimatorofVar[ˆ s n].\\xa0Thecentrallimittheoremtells\\nusthatthedistributionoftheaverage, ˆ s n,convergestoanormaldistribution\\nwithmean sandvarianceV a r [ ( ) ] f x\\nn.Thisallowsustoestimateconﬁdenceintervals\\naroundtheestimate ˆ s n,usingthecumulativedistributionofthenormaldensity.\\nHowever,allthisreliesonourabilitytoeasilysamplefromthebasedistribution\\np( x),butdoingsoisnotalwayspossible.Whenitisnotfeasibletosamplefrom\\np,analternativeistouseimportancesampling,presentedinsection.A17.2\\nmoregeneralapproachistoformasequenceofestimatorsthatconvergetowards\\nthedistributionofinterest.ThatistheapproachofMonteCarloMarkovchains\\n(section).17.3\\n17.2ImportanceSampling\\nAnimportantstepinthedecompositionoftheintegrand(orsummand)usedbythe\\nMonteCarlomethodinequationisdecidingwhichpartoftheintegrandshould 17.2\\nplaytheroletheprobability p( x)andwhichpartoftheintegrandshouldplaythe\\nroleofthequantity f( x) whoseexpectedvalue(underthatprobabilitydistribution)\\nistobeestimated.Thereisnouniquedecompositionbecause p( x) f( x)canalways\\nberewrittenas\\np f q () x() = x () xp f() x() x\\nq() x, (17.8)\\nwherewenowsamplefrom qandaveragep f\\nq.Inmanycases,wewishtocompute\\nanexpectationforagiven pandan f,andthefactthattheproblemisspeciﬁed\\n1Th e u n b i a s e d e s t i m a t o r o f t h e v a ria n c e i s o f t e n p re f e rre d , i n wh i c h t h e s u m o f s q u a re d\\nd i ﬀ e re n c e s i s d i v i d e d b y i n s t e a d o f . n − 1 n\\n5 9 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad034b68-1a6e-47d6-bd97-db84027646f5', embedding=None, metadata={'page_label': '608', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nfromthestartasanexpectationsuggeststhatthis pand fwouldbeanatural\\nchoiceofdecomposition.However,theoriginalspeciﬁcationoftheproblemmay\\nnotbethetheoptimalchoiceintermsofthenumberofsamplesrequiredtoobtain\\nagivenlevelofaccuracy.\\xa0Fortunately,theformoftheoptimalchoice q∗canbe\\nderivedeasily.Theoptimal q∗correspondstowhatiscalledoptimalimportance\\nsampling.\\nBecauseoftheidentityshowninequation,anyMonteCarloestimator 17.8\\nˆ s p=1\\nnn \\ue058\\ni , = 1 x( ) i∼ pf( x( ) i) (17.9)\\ncanbetransformedintoanimportancesamplingestimator\\nˆ s q=1\\nnn \\ue058\\ni , = 1 x( ) i∼ qp( x( ) i)( f x( ) i)\\nq( x( ) i). (17.10)\\nWeseereadilythattheexpectedvalueoftheestimatordoesnotdependon: q\\nE q[ˆ s q] = E q[ˆ s p] = s . (17.11)\\nHowever,thevarianceofanimportancesamplingestimatorcanbegreatlysensitive\\ntothechoiceof.Thevarianceisgivenby q\\nVar[ˆ s q] = Var[p f() x() x\\nq() x] /n . (17.12)\\nTheminimumvarianceoccurswhenis q\\nq∗() = xp f() x|() x|\\nZ, (17.13)\\nwhere Zisthenormalization constant,chosensothat q∗( x)sumsorintegratesto\\n1asappropriate.Betterimportancesamplingdistributionsputmoreweightwhere\\ntheintegrandislarger.Infact,when f( x)doesnotchangesign,Var[ˆ s q∗]=0,\\nmeaningthat whentheoptimaldistributionisused. a s i ng l e s a m p l e i s s u ﬃ c i e nt\\nOfcourse,thisisonlybecausethecomputationof q∗hasessentiallysolvedthe\\noriginalproblem,soitisusuallynotpracticaltousethisapproachofdrawinga\\nsinglesamplefromtheoptimaldistribution.\\nAnychoiceofsamplingdistribution qisvalid(inthesenseofyieldingthe\\ncorrectexpectedvalue)and q∗istheoptimalone(inthesenseofyieldingminimum\\nvariance).Samplingfrom q∗isusuallyinfeasible,butotherchoicesof qcanbe\\nfeasiblewhilestillreducingthevariancesomewhat.\\n5 9 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='99cd814c-3b39-4356-a0cd-118345681ea4', embedding=None, metadata={'page_label': '609', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nAnotherapproachistousebiasedimportancesampling,whichhasthe\\nadvantageofnotrequiringnormalized por q.Inthecaseofdiscretevariables,the\\nbiasedimportancesamplingestimatorisgivenby\\nˆ s B I S=\\ue050n\\ni = 1p ( x( ) i)\\nq ( x( ) i )f( x( ) i)\\n\\ue050n\\ni = 1p ( x( ) i )\\nq ( x( ) i )(17.14)\\n=\\ue050n\\ni = 1p ( x( ) i)\\n˜ q ( x( ) i)f( x( ) i)\\n\\ue050n\\ni = 1p ( x( ) i)\\n˜ q ( x( ) i)(17.15)\\n=\\ue050n\\ni = 1˜ p ( x( ) i)\\n˜ q ( x( ) i )f( x( ) i)\\n\\ue050n\\ni = 1˜ p ( x( ) i )\\n˜ q ( x( ) i ), (17.16)\\nwhere ˜ pand˜ qaretheunnormalized formsof pand qandthe x( ) iarethesamples\\nfrom q.Thisestimatorisbiasedbecause E[ˆ s B I S]\\ue036= s,exceptasymptoticallywhen\\nn→∞andthedenominator ofequationconvergesto1.Hencethisestimator 17.14\\niscalledasymptoticallyunbiased.\\nAlthoughagoodchoiceof qcangreatlyimprovetheeﬃciencyofMonteCarlo\\nestimation,apoorchoiceof qcanmaketheeﬃciencymuchworse.Goingbackto\\nequation,weseethatiftherearesamplesof 17.12 qforwhichp f ( ) x| ( ) x|\\nq ( ) xislarge,\\nthenthevarianceoftheestimatorcangetverylarge.Thismayhappenwhen\\nq( x)istinywhileneither p( x)nor f( x)aresmallenoughtocancelit.The q\\ndistributionisusuallychosentobeaverysimpledistributionsothatitiseasy\\ntosamplefrom.When xishigh-dimensional,thissimplicityin qcausesitto\\nmatch por p f||poorly.When q( x( ) i)\\ue01d p( x( ) i)| f( x( ) i)|,importancesampling\\ncollectsuselesssamples(summingtinynumbersorzeros).Ontheotherhand,when\\nq( x( ) i)\\ue01c p( x( ) i)| f( x( ) i)|,whichwillhappenmorerarely,theratiocanbehuge.\\nBecausetheselattereventsarerare,theymaynotshowupinatypicalsample,\\nyieldingtypicalunderestimationof s,compensatedrarelybygrossoverestimation.\\nSuchverylargeorverysmallnumbersaretypicalwhen xishighdimensional,\\nbecauseinhighdimensionthedynamicrangeofjointprobabilities canbevery\\nlarge.\\nInspiteofthisdanger,importancesamplinganditsvariantshavebeenfound\\nveryusefulinmanymachinelearningalgorithms,includingdeeplearningalgorithms.\\nForexample,seetheuseofimportancesamplingtoacceleratetraininginneural\\nlanguagemodelswithalargevocabulary(section)orotherneuralnets 12.4.3.3\\nwithalargenumberofoutputs.Seealsohowimportancesamplinghasbeen\\nusedtoestimateapartitionfunction(thenormalization constantofaprobability\\n5 9 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ef51e126-1f00-4077-8f70-fefda3aa8eda', embedding=None, metadata={'page_label': '610', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\ndistribution)insection,andtoestimatethelog-likelihoodindeepdirected 18.7\\nmodelssuchasthevariationalautoencoder,insection.Importancesampling 20.10.3\\nmayalsobeusedtoimprovetheestimateofthegradientofthecostfunctionused\\ntotrainmodelparameterswithstochasticgradientdescent,particularlyformodels\\nsuchasclassiﬁerswheremostofthetotalvalueofthecostfunctioncomesfroma\\nsmallnumberofmisclassiﬁedexamples.Samplingmorediﬃcultexamplesmore\\nfrequentlycanreducethevarianceofthegradientinsuchcases(,). Hinton2006\\n17.3MarkovChainMonteCarloMethods\\nInmanycases,wewishtouseaMonteCarlotechniquebutthereisnotractable\\nmethodfordrawingexactsamplesfromthedistribution p m o de l( x)orfromagood\\n(lowvariance)importancesamplingdistribution q( x).Inthecontextofdeep\\nlearning,thismostoftenhappenswhen p m o de l( x)isrepresentedbyanundirected\\nmodel.Inthesecases,weintroduceamathematical toolcalledaMarkovchain\\ntoapproximately samplefrom p m o de l( x).ThefamilyofalgorithmsthatuseMarkov\\nchainstoperformMonteCarloestimatesiscalledMarkovchainMonteCarlo\\nmethods(MCMC).MarkovchainMonteCarlomethodsformachinelearningare\\ndescribedatgreaterlengthinKollerandFriedman2009().\\xa0Themoststandard,\\ngenericguaranteesforMCMCtechniquesareonlyapplicablewhenthemodel\\ndoesnotassignzeroprobabilitytoanystate.Therefore,itismostconvenient\\nto\\xa0present\\xa0these\\xa0techniques\\xa0assampling\\xa0froman\\xa0energy-basedmodel\\xa0(EBM)\\np( x)∝ −exp( E()) xasdescribedinsection.IntheEBMformulation,every 16.2.4\\nstateisguaranteedtohavenon-zeroprobability.MCMCmethodsareinfact\\nmorebroadlyapplicableandcanbeusedwithmanyprobabilitydistributionsthat\\ncontainzeroprobabilitystates.However,thetheoreticalguaranteesconcerningthe\\nbehaviorofMCMCmethodsmustbeprovenonacase-by-casebasisfordiﬀerent\\nfamiliesofsuchdistributions.Inthecontextofdeeplearning,itismostcommon\\ntorelyonthemostgeneraltheoreticalguaranteesthatnaturallyapplytoall\\nenergy-basedmodels.\\nTounderstandwhydrawingsamplesfromanenergy-basedmodelisdiﬃcult,\\nconsideranEBMoverjusttwovariables,deﬁningadistributionab.Inorder p( ,)\\ntosamplea,wemustdrawafrom p(ab|),andinordertosampleb,wemust\\ndrawitfrom p(ba|).Itseemstobeanintractablechicken-and-eggproblem.\\nDirectedmodelsavoidthisbecausetheirgraphisdirectedandacyclic.Toperform\\nancestralsamplingonesimplysampleseachofthevariablesintopologicalorder,\\nconditioningoneachvariable’sparents,whichareguaranteedtohavealreadybeen\\nsampled(section).Ancestralsamplingdeﬁnesaneﬃcient,single-passmethod 16.3\\n5 9 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='41b1b912-c155-4e77-a9af-a00a3b3530c8', embedding=None, metadata={'page_label': '611', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nofobtainingasample.\\nInanEBM,wecanavoidthischickenandeggproblembysamplingusinga\\nMarkovchain.ThecoreideaofaMarkovchainistohaveastate xthatbegins\\nasanarbitraryvalue.Overtime,werandomlyupdate xrepeatedly.Eventually\\nxbecomes(verynearly)afairsamplefrom p( x).Formally,aMarkovchainis\\ndeﬁnedbyarandomstate xandatransitiondistribution T( x\\ue030| x)specifying\\ntheprobabilitythatarandomupdatewillgotostate x\\ue030ifitstartsinstate x.\\nRunningtheMarkovchainmeansrepeatedlyupdatingthestate xtoavalue x\\ue030\\nsampledfrom T( x\\ue030| x).\\nTogainsometheoreticalunderstandingofhowMCMCmethodswork,itis\\nusefultoreparametrizetheproblem.First,werestrictourattentiontothecase\\nwheretherandomvariable xhascountablymanystates.Wecanthenrepresent\\nthestateasjustapositiveinteger x.\\xa0Diﬀerentintegervaluesof xmapbackto\\ndiﬀerentstatesintheoriginalproblem. x\\nConsiderwhathappenswhenweruninﬁnitelymanyMarkovchainsinparallel.\\nAllofthestatesofthediﬀerentMarkovchainsaredrawnfromsomedistribution\\nq( ) t( x),where tindicatesthenumberoftimestepsthathaveelapsed.Atthe\\nbeginning, q( 0 )issomedistributionthatweusedtoarbitrarilyinitialize xforeach\\nMarkovchain.Later, q( ) tisinﬂuencedbyalloftheMarkovchainstepsthathave\\nrunsofar.Ourgoalisfor q( ) t() xtoconvergeto. p x()\\nBecausewehavereparametrized theproblemintermsofpositiveinteger x,we\\ncandescribetheprobabilitydistributionusingavector,with q v\\nq i v (= x ) = i . (17.17)\\nConsiderwhathappenswhenweupdateasingleMarkovchain’sstate xtoa\\nnewstate x\\ue030.Theprobabilityofasinglestatelandinginstate x\\ue030isgivenby\\nq( + 1 ) t( x\\ue030) =\\ue058\\nxq( ) t()( x T x\\ue030| x .) (17.18)\\nUsingourintegerparametrization, wecanrepresenttheeﬀectofthetransition\\noperatorusingamatrix.Wedeﬁnesothat T A A\\nA i , j= ( T x\\ue030= = ) i| x j . (17.19)\\nUsingthisdeﬁnition,wecannowrewriteequation.Ratherthanwritingitin 17.18\\ntermsof qand Ttounderstandhowasinglestateisupdated,wemaynowuse v\\nand AtodescribehowtheentiredistributionoverallthediﬀerentMarkovchains\\n(runninginparallel)shiftsasweapplyanupdate:\\nv( ) t= A v( 1 ) t−. (17.20)\\n5 9 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54ebbc75-38b1-4d53-9ff2-e03469c643b5', embedding=None, metadata={'page_label': '612', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nApplyingtheMarkovchainupdaterepeatedlycorrespondstomultiplyingbythe\\nmatrix Arepeatedly.Inotherwords,wecanthinkoftheprocessasexponentiating\\nthematrix: A\\nv( ) t= Atv( 0 ). (17.21)\\nThematrix Ahasspecialstructurebecauseeachofitscolumnsrepresentsa\\nprobabilitydistribution.Suchmatricesarecalledstochasticmatrices.Ifthere\\nisanon-zeroprobabilityoftransitioningfromanystate xtoanyotherstate x\\ue030for\\nsomepower t,thenthePerron-Frobeniustheorem(,;Perron1907Frobenius1908,)\\nguaranteesthatthelargesteigenvalueisrealandequalto.Overtime,wecan 1\\nseethatalloftheeigenvaluesareexponentiated:\\nv( ) t=\\ue000\\nV λ Vdiag()− 1\\ue001tv( 0 )= () Vdiag λtV− 1v( 0 ).(17.22)\\nThisprocesscausesalloftheeigenvaluesthatarenotequaltotodecaytozero. 1\\nUndersomeadditionalmildconditions, Aisguaranteedtohaveonlyoneeigenvector\\nwitheigenvalue.Theprocessthusconvergestoa 1 stationarydistribution,\\nsometimesalsocalledthe .Atconvergence, equilibriumdistribution\\nv\\ue030= = A v v , (17.23)\\nandthissameconditionholdsforeveryadditionalstep.Thisisaneigenvector\\nequation.Tobeastationarypoint, vmustbeaneigenvectorwithcorresponding\\neigenvalue.Thisconditionguaranteesthatoncewehavereachedthestationary 1\\ndistribution,repeatedapplicationsofthetransitionsamplingproceduredonot\\nchangethe overthestatesofallthevariousMarkovchains(although d i s t r i b u t i o n\\ntransitionoperatordoeschangeeachindividualstate,ofcourse).\\nIfwehavechosen Tcorrectly,thenthestationarydistribution qwillbeequal\\ntothedistribution pwewishtosamplefrom.Wewilldescribehowtochoose T\\nshortly,insection.17.4\\nMostpropertiesofMarkovChainswithcountablestatescanbegeneralized\\ntocontinuousvariables.Inthissituation,someauthorscalltheMarkovChain\\naHarrischainbutweusethetermMarkovChaintodescribebothconditions.\\nIngeneral,aMarkovchainwithtransitionoperator Twillconverge,undermild\\nconditions,toaﬁxedpointdescribedbytheequation\\nq\\ue030( x\\ue030) = E x∼ q T( x\\ue030| x) , (17.24)\\nwhichinthediscretecaseisjustrewritingequation.When17.23 xisdiscrete,\\ntheexpectationcorrespondstoasum,andwhen xiscontinuous,theexpectation\\ncorrespondstoanintegral.\\n5 9 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0bcf81ec-20e1-4d8e-829a-ea0f3c9c5d43', embedding=None, metadata={'page_label': '613', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nRegardlessofwhetherthestateiscontinuousordiscrete,allMarkovchain\\nmethodsconsistofrepeatedlyapplyingstochasticupdatesuntileventuallythe\\nstatebeginstoyieldsamplesfromtheequilibriumdistribution.Runningthe\\nMarkovchainuntilitreachesitsequilibriumdistributioniscalled“burningin”\\ntheMarkovchain.Afterthechainhasreachedequilibrium,asequenceofinﬁnitely\\nmanysamplesmaybedrawnfromfromtheequilibriumdistribution.\\xa0Theyare\\nidenticallydistributedbutanytwosuccessivesampleswillbehighlycorrelated\\nwitheachother.Aﬁnitesequenceofsamplesmaythusnotbeveryrepresentative\\noftheequilibriumdistribution.Onewaytomitigatethisproblemistoreturn\\nonlyevery nsuccessivesamples,\\xa0sothatourestimateofthestatisticsofthe\\nequilibriumdistributionisnotasbiasedbythecorrelationbetweenanMCMC\\nsampleandthenextseveralsamples.Markovchainsarethusexpensivetouse\\nbecauseofthetimerequiredtoburnintotheequilibriumdistributionandthetime\\nrequiredtotransitionfromonesampletoanotherreasonablydecorrelatedsample\\nafterreachingequilibrium.Ifonedesirestrulyindependentsamples,onecanrun\\nmultipleMarkovchainsinparallel.Thisapproachusesextraparallelcomputation\\ntoeliminatelatency.ThestrategyofusingonlyasingleMarkovchaintogenerate\\nallsamplesandthestrategyofusingoneMarkovchainforeachdesiredsampleare\\ntwoextremes;deeplearningpractitioners usuallyuseanumberofchainsthatis\\nsimilartothenumberofexamplesinaminibatchandthendrawasmanysamples\\nasareneededfromthisﬁxedsetofMarkovchains.Acommonlyusednumberof\\nMarkovchainsis100.\\nAnotherdiﬃcultyisthatwedonotknowinadvancehowmanystepsthe\\nMarkovchainmustrunbeforereachingitsequilibriumdistribution.Thislengthof\\ntimeiscalledthemixingtime.ItisalsoverydiﬃculttotestwhetheraMarkov\\nchainhasreachedequilibrium.Wedonothaveapreciseenoughtheoryforguiding\\nusinansweringthisquestion.Theorytellsusthatthechainwillconverge,butnot\\nmuchmore.IfweanalyzetheMarkovchainfromthepointofviewofamatrix A\\nactingonavectorofprobabilities v,thenweknowthatthechainmixeswhen At\\nhaseﬀectivelylostalloftheeigenvaluesfrom Abesidestheuniqueeigenvalueof.1\\nThismeansthatthemagnitudeofthesecondlargesteigenvaluewilldeterminethe\\nmixingtime.However,inpractice,wecannotactuallyrepresentourMarkovchain\\nintermsofamatrix.Thenumberofstatesthatourprobabilisticmodelcanvisit\\nisexponentiallylargeinthenumberofvariables,soitisinfeasibletorepresent\\nv, A,ortheeigenvaluesof A.\\xa0Duetotheseandotherobstacles,weusuallydo\\nnotknowwhetheraMarkovchainhasmixed.Instead,wesimplyruntheMarkov\\nchainforanamountoftimethatweroughlyestimatetobesuﬃcient,anduse\\nheuristicmethodstodeterminewhetherthechainhasmixed.Theseheuristic\\nmethodsincludemanuallyinspectingsamplesormeasuringcorrelationsbetween\\n5 9 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3634f392-1b02-4653-b2ad-a97803150726', embedding=None, metadata={'page_label': '614', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nsuccessivesamples.\\n17.4GibbsSampling\\nSofarwehavedescribedhowtodrawsamplesfromadistribution q( x)byrepeatedly\\nupdating x x←\\ue030∼ T( x\\ue030| x).However,wehavenotdescribedhowtoensurethat\\nq( x)isausefuldistribution.Twobasicapproachesareconsideredinthisbook.\\nTheﬁrstoneistoderive Tfromagivenlearned p m o de l,describedbelowwiththe\\ncaseofsamplingfromEBMs.Thesecondoneistodirectlyparametrize Tand\\nlearnit,sothatitsstationarydistributionimplicitlydeﬁnesthe p m o de lofinterest.\\nExamplesofthissecondapproacharediscussedinsectionsand. 20.1220.13\\nInthecontextofdeeplearning,wecommonlyuseMarkovchainstodraw\\nsamplesfromanenergy-basedmodeldeﬁningadistribution p m o de l( x).Inthiscase,\\nwewantthe q( x)fortheMarkovchaintobe p m o de l( x).Toobtainthedesired\\nq() x,wemustchooseanappropriate T( x\\ue030| x).\\nAconceptuallysimpleandeﬀectiveapproachtobuildingaMarkovchain\\nthatsamplesfrom p m o de l( x)istouseGibbssampling,inwhichsamplingfrom\\nT( x\\ue030| x)isaccomplishedbyselectingonevariablex iandsamplingitfrom p m o de l\\nconditionedonitsneighborsintheundirectedgraph Gdeﬁningthestructureof\\ntheenergy-basedmodel.Itisalsopossibletosampleseveralvariablesatthesame\\ntimesolongastheyareconditionallyindependentgivenalloftheirneighbors.\\nAsshownintheRBMexampleinsection,allofthehiddenunitsofan 16.7.1\\nRBMmaybesampledsimultaneouslybecausetheyareconditionallyindependent\\nfromeachothergivenallofthevisibleunits.Likewise,allofthevisibleunitsmay\\nbesampledsimultaneouslybecausetheyareconditionallyindependentfromeach\\nothergivenallofthehiddenunits.Gibbssamplingapproachesthatupdatemany\\nvariablessimultaneouslyinthiswayarecalledblockGibbssampling.\\nAlternateapproachestodesigningMarkovchainstosamplefrom p m o de lare\\npossible.Forexample,theMetropolis-Hastingsalgorithmiswidelyusedinother\\ndisciplines.Inthecontextofthedeeplearningapproachtoundirectedmodeling,\\nitisraretouseanyapproachotherthanGibbssampling.Improvedsampling\\ntechniquesareonepossibleresearchfrontier.\\n17.5TheChallengeofMixingbetweenSeparatedModes\\nTheprimarydiﬃcultyinvolvedwithMCMCmethodsisthattheyhaveatendency\\ntomixpoorly.Ideally,successivesamplesfromaMarkovchaindesignedtosample\\n5 9 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a97a0a74-e4c8-40cd-99d4-af5c5e356184', embedding=None, metadata={'page_label': '615', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nfrom p( x)wouldbecompletelyindependentfromeachotherandwouldvisitmany\\ndiﬀerentregionsin xspaceproportionaltotheirprobability.Instead,especially\\ninhighdimensionalcases,MCMCsamplesbecomeverycorrelated.Werefer\\ntosuchbehaviorasslowmixingorevenfailuretomix.MCMCmethodswith\\nslowmixingcanbeseenasinadvertentlyperformingsomethingresemblingnoisy\\ngradientdescentontheenergyfunction,orequivalentlynoisyhillclimbingonthe\\nprobability,withrespecttothestateofthechain(therandomvariablesbeing\\nsampled).\\xa0Thechaintendstotakesmallsteps(inthespaceofthestateofthe\\nMarkovchain),fromaconﬁguration x( 1 ) t−toaconﬁguration x( ) t,withtheenergy\\nE( x( ) t)generallylowerorapproximately equaltotheenergy E( x( 1 ) t−),witha\\npreferenceformovesthatyieldlowerenergyconﬁgurations. Whenstartingfroma\\nratherimprobableconﬁguration(higherenergythanthetypicalonesfrom p( x)),\\nthechaintendstograduallyreducetheenergyofthestateandonlyoccasionally\\nmovetoanothermode.Oncethechainhasfoundaregionoflowenergy(for\\nexample,ifthevariablesarepixelsinanimage,aregionoflowenergymightbe\\naconnectedmanifoldofimagesofthesameobject),whichwecallamode,the\\nchainwilltendtowalkaroundthatmode(followingakindofrandomwalk).Once\\ninawhileitwillstepoutofthatmodeandgenerallyreturntoitor(ifitﬁnds\\nanescaperoute)movetowardsanothermode.Theproblemisthatsuccessful\\nescaperoutesarerareformanyinterestingdistributions,sotheMarkovchainwill\\ncontinuetosamplethesamemodelongerthanitshould.\\nThisisveryclearwhenweconsidertheGibbssamplingalgorithm(section).17.4\\nInthiscontext,considertheprobabilityofgoingfromonemodetoanearbymode\\nwithinagivennumberofsteps.Whatwilldeterminethatprobabilityistheshape\\nofthe“energybarrier”\\xa0betweenthesemodes.Transitionsbetweentwomodes\\nthatareseparatedbyahighenergybarrier(aregionoflowprobability)are\\nexponentiallylesslikely(intermsoftheheightoftheenergybarrier).Thisis\\nillustratedinﬁgure.Theproblemariseswhentherearemultiplemodeswith 17.1\\nhighprobabilitythatareseparatedbyregionsoflowprobability,especiallywhen\\neachGibbssamplingstepmustupdateonlyasmallsubsetofvariableswhose\\nvaluesarelargelydeterminedbytheothervariables.\\nAsasimpleexample,consideranenergy-basedmodelovertwovariablesaand\\nb,whicharebothbinarywithasign,takingonvalues −1 1and.If E(ab ,) =− wab\\nforsomelargepositivenumber w,thenthemodelexpressesastrongbeliefthata\\nandbhavethesamesign.ConsiderupdatingbusingaGibbssamplingstepwith\\na= 1.\\xa0Theconditionaldistributionoverbisgivenby P(b= 1|a= 1)= σ( w).\\nIf wislarge,thesigmoidsaturates,andtheprobabilityofalsoassigningbtobe\\n1iscloseto1.Likewise,ifa=−1,theprobabilityofassigningbtobe−1is\\ncloseto1.Accordingto P m o de l(ab ,),bothsignsofbothvariablesareequallylikely.\\n6 0 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9dec7e2a-215c-4f72-a6b8-6fc173ff940a', embedding=None, metadata={'page_label': '616', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nFigure17.1:PathsfollowedbyGibbssamplingforthreedistributions,withtheMarkov\\nchaininitializedatthemodeinbothcases. ( L e f t )Amultivariatenormaldistribution\\nwithtwoindependentvariables.Gibbssamplingmixeswellbecausethevariablesare\\nindependent.Amultivariatenormaldistributionwithhighlycorrelatedvariables. ( C e n t e r )\\nThecorrelationbetweenvariablesmakesitdiﬃcultfortheMarkovchaintomix.Because\\ntheupdateforeachvariablemustbeconditionedontheothervariable,thecorrelation\\nreducestherateatwhichtheMarkovchaincanmoveawayfromthestartingpoint.\\n( R i g h t )AmixtureofGaussianswithwidelyseparatedmodesthatarenotaxis-aligned.\\nGibbssamplingmixesveryslowlybecauseitisdiﬃculttochangemodeswhilealtering\\nonlyonevariableatatime.\\nAccordingto P m o de l(ab|),bothvariablesshouldhavethesamesign.Thismeans\\nthatGibbssamplingwillonlyveryrarelyﬂipthesignsofthesevariables.\\nInmorepracticalscenarios,thechallengeisevengreaterbecausewecarenot\\nonlyaboutmakingtransitionsbetweentwomodesbutmoregenerallybetween\\nallthemanymodesthatarealmodelmightcontain.Ifseveralsuchtransitions\\narediﬃcultbecauseofthediﬃcultyofmixingbetweenmodes,thenitbecomes\\nveryexpensivetoobtainareliablesetofsamplescoveringmostofthemodes,and\\nconvergenceofthechaintoitsstationarydistributionisveryslow.\\nSometimesthisproblemcanberesolvedbyﬁndinggroupsofhighlydependent\\nunitsandupdatingallofthemsimultaneouslyinablock.\\xa0Unfortunately,when\\nthedependenciesarecomplicated,itcanbecomputationally intractabletodrawa\\nsamplefromthegroup.Afterall,theproblemthattheMarkovchainwasoriginally\\nintroducedtosolveisthisproblemofsamplingfromalargegroupofvariables.\\nInthecontextofmodelswithlatentvariables,whichdeﬁneajointdistribution\\np m o de l( x h ,),weoftendrawsamplesof xbyalternatingbetweensamplingfrom\\np m o de l( x h|)andsamplingfrom p m o de l( h x|).Fromthepointofviewofmixing\\n6 0 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0e352f8-0e26-4134-9635-bf3fc60080f2', embedding=None, metadata={'page_label': '617', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\nFigure17.2:Anillustrationoftheslowmixingproblemindeepprobabilisticmodels.\\nEachpanelshouldbereadlefttoright,toptobottom. ( L e f t )Consecutivesamplesfrom\\nGibbssamplingappliedtoadeepBoltzmannmachinetrainedontheMNISTdataset.\\nConsecutivesamplesaresimilartoeachother.BecausetheGibbssamplingisperformed\\ninadeepgraphicalmodel,thissimilarityisbasedmoreonsemanticratherthanrawvisual\\nfeatures,butitisstilldiﬃcultfortheGibbschaintotransitionfromonemodeofthe\\ndistributiontoanother,forexamplebychangingthedigitidentity.Consecutive ( R i g h t )\\nancestralsamplesfromagenerativeadversarialnetwork.Becauseancestralsampling\\ngenerateseachsampleindependentlyfromtheothers,thereisnomixingproblem.\\nrapidly,wewouldlike p m o de l( h x|)tohaveveryhighentropy.However,fromthe\\npointofviewoflearningausefulrepresentationof h,wewouldlike htoencode\\nenoughinformationabout xtoreconstructitwell,whichimpliesthat hand x\\nshouldhaveveryhighmutualinformation. Thesetwogoalsareatoddswitheach\\nother.Weoftenlearngenerativemodelsthatverypreciselyencode xinto hbut\\narenotabletomixverywell.ThissituationarisesfrequentlywithBoltzmann\\nmachines—thesharperthedistributionaBoltzmannmachinelearns,theharder\\nitisforaMarkovchainsamplingfromthemodeldistributiontomixwell.This\\nproblemisillustratedinﬁgure.17.2\\nAllthiscouldmakeMCMCmethodslessusefulwhenthedistributionofinterest\\nhasamanifoldstructurewithaseparatemanifoldforeachclass:thedistribution\\nisconcentratedaroundmanymodesandthesemodesareseparatedbyvastregions\\nofhighenergy.Thistypeofdistributioniswhatweexpectinmanyclassiﬁcation\\nproblemsandwouldmakeMCMCmethodsconvergeveryslowlybecauseofpoor\\nmixingbetweenmodes.\\n6 0 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dda4a06e-b49b-4fde-89be-4b8eb7020689', embedding=None, metadata={'page_label': '618', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\n1 7 . 5 . 1 T em p eri n g t o Mi x b et w een Mo d es\\nWhenadistributionhassharppeaksofhighprobabilitysurroundedbyregionsof\\nlowprobability,itisdiﬃculttomixbetweenthediﬀerentmodesofthedistribution.\\nSeveraltechniquesforfastermixingarebasedonconstructingalternativeversions\\nofthetargetdistributioninwhichthepeaksarenotashighandthesurrounding\\nvalleysarenotaslow.Energy-basedmodelsprovideaparticularlysimplewayto\\ndoso.Sofar,wehavedescribedanenergy-basedmodelasdeﬁningaprobability\\ndistribution\\np E . () exp( x∝ −()) x (17.25)\\nEnergy-basedmodelsmaybeaugmentedwithanextraparameter βcontrolling\\nhowsharplypeakedthedistributionis:\\np β() exp( ()) x∝ − β E x . (17.26)\\nThe βparameterisoftendescribedasbeingthereciprocalofthetemperature,\\nreﬂectingtheoriginofenergy-basedmodelsinstatisticalphysics.Whenthe\\ntemperaturefallstozeroandrisestoinﬁnity,theenergy-basedmodelbecomes β\\ndeterministic.Whenthetemperaturerisestoinﬁnityand βfallstozero,the\\ndistribution(fordiscrete)becomesuniform. x\\nTypically,amodelistrainedtobeevaluatedat β= 1.However,wecanmake\\nuseofothertemperatures,particularlythosewhere β <1.Temperingisageneral\\nstrategyofmixingbetweenmodesof p 1rapidlybydrawingsampleswith. β <1\\nMarkovchainsbasedontemperedtransitions(,)temporarily Neal1994\\nsamplefromhigher-temperaturedistributionsinordertomixtodiﬀerentmodes,\\nthenresumesamplingfromtheunittemperaturedistribution.Thesetechniques\\nhavebeenappliedtomodelssuchasRBMs\\xa0(Salakhutdinov2010,).Another\\napproachistouseparalleltempering(,),inwhichtheMarkovchain Iba2001\\nsimulatesmanydiﬀerentstatesinparallel,atdiﬀerenttemperatures.Thehighest\\ntemperaturestatesmixslowly,whilethelowesttemperaturestates,attemperature\\n1,provideaccuratesamplesfromthemodel.Thetransitionoperatorincludes\\nstochasticallyswappingstatesbetweentwodiﬀerenttemperaturelevels,sothata\\nsuﬃcientlyhigh-probabilit ysamplefromahigh-temperatureslotcanjumpintoa\\nlowertemperatureslot.ThisapproachhasalsobeenappliedtoRBMs(Desjardins\\ne t a l . e t a l . ,;2010Cho,).\\xa0Althoughtemperingisapromisingapproach,at 2010\\nthispointithasnotallowedresearcherstomakeastrongadvanceinsolvingthe\\nchallengeofsamplingfromcomplexEBMs.Onepossiblereasonisthatthere\\narecriticaltemperaturesaroundwhichthetemperaturetransitionmustbe\\nveryslow(asthetemperatureisgraduallyreduced)inorderfortemperingtobe\\neﬀective.\\n6 0 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d219ae50-557d-4a16-af35-ca9e784e0b92', embedding=None, metadata={'page_label': '619', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER17.MONTECARLOMETHODS\\n1 7 . 5 . 2 D ep t h Ma y Hel p Mi xi n g\\nWhendrawingsamplesfromalatentvariablemodel p( h x ,),wehaveseenthatif\\np( h x|)encodes xtoowell,thensamplingfrom p( x h|)willnotchange xvery\\nmuchandmixingwillbepoor.Onewaytoresolvethisproblemistomake hbea\\ndeeprepresentation,thatencodesintoinsuchawaythataMarkovchainin x h\\nthespaceof hcanmixmoreeasily.Manyrepresentationlearningalgorithms,such\\nasautoencodersandRBMs,tendtoyieldamarginaldistributionover hthatis\\nmoreuniformandmoreunimodalthantheoriginaldatadistributionover x.Itcan\\nbearguedthatthisarisesfromtryingtominimizereconstructionerrorwhileusing\\nalloftheavailablerepresentationspace,becauseminimizingreconstructionerror\\noverthetrainingexampleswillbebetterachievedwhendiﬀerenttrainingexamples\\nareeasilydistinguishablefromeachotherin h-space,andthuswellseparated.\\nBengio2013a e t a l .()observedthatdeeperstacksofregularizedautoencodersor\\nRBMsyieldmarginaldistributionsinthetop-level h-spacethatappearedmore\\nspreadoutandmoreuniform,withlessofagapbetweentheregionscorresponding\\ntodiﬀerentmodes(categories,intheexperiments).TraininganRBMinthat\\nhigher-levelspaceallowedGibbssamplingtomixfasterbetweenmodes.Itremains\\nhoweverunclearhowtoexploitthisobservationtohelpbettertrainandsample\\nfromdeepgenerativemodels.\\nDespitethediﬃcultyofmixing,MonteCarlotechniquesareusefulandare\\noftenthebesttoolavailable.Indeed,theyaretheprimarytoolusedtoconfront\\ntheintractablepartitionfunctionofundirectedmodels,discussednext.\\n6 0 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c27487dc-26cb-4304-81d2-932f4c920079', embedding=None, metadata={'page_label': '620', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 8\\nC on f ron t i n g t h e P art i t i on\\nF u n ct i on\\nInsectionwesawthatmanyprobabilisticmodels(commonlyknownasundi- 16.2.2\\nrectedgraphicalmodels)aredeﬁnedbyanunnormalized probabilitydistribution\\n˜ p(x; θ).Wemustnormalize ˜ pbydividingbyapartitionfunction Z( θ)inorderto\\nobtainavalidprobabilitydistribution:\\np(;) =x θ1\\nZ() θ˜ p . (;)x θ (18.1)\\nThepartitionfunctionisanintegral(forcontinuousvariables)orsum(fordiscrete\\nvariables)overtheunnormalized probabilityofallstates:\\n\\ue05a\\n˜ p d() x x (18.2)\\nor \\ue058\\nx˜ p .() x (18.3)\\nThisoperationisintractableformanyinterestingmodels.\\nAswewillseeinchapter,severaldeeplearningmodelsaredesignedto 20\\nhaveatractablenormalizingconstant,oraredesignedtobeusedinwaysthatdo\\nnotinvolvecomputing p(x)atall.\\xa0However,othermodelsdirectlyconfrontthe\\nchallengeofintractablepartitionfunctions.Inthischapter,wedescribetechniques\\nusedfortrainingandevaluatingmodelsthathaveintractablepartitionfunctions.\\n605', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9e585f8d-75fb-4849-a6d7-e8e9f404ce40', embedding=None, metadata={'page_label': '621', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\n18.1TheLog-LikelihoodGradient\\nWhat\\xa0makes\\xa0learning\\xa0undirectedmodels\\xa0bymaximumlikelihood\\xa0particularly\\ndiﬃcultisthatthepartitionfunctiondependsontheparameters.Thegradientof\\nthelog-likelihoodwithrespecttotheparametershasatermcorrespondingtothe\\ngradientofthepartitionfunction:\\n∇ θlog(;) = px θ ∇ θlog ˜ p(;)x θ−∇ θlog() Z θ .(18.4)\\nThisisawell-knowndecompositionintothe p o si t i v e phaseand negat i v e\\nphaseoflearning.\\nFormostundirectedmodelsofinterest,thenegativephaseisdiﬃcult.Models\\nwithnolatentvariablesorwithfewinteractionsbetweenlatentvariablestypically\\nhaveatractablepositivephase.Thequintessentialexampleofamodelwitha\\nstraightforwardpositivephaseanddiﬃcultnegativephaseistheRBM,whichhas\\nhiddenunitsthatareconditionallyindependentfromeachothergiventhevisible\\nunits.Thecasewherethepositivephaseisdiﬃcult,withcomplicatedinteractions\\nbetweenlatentvariables,isprimarilycoveredinchapter.Thischapterfocuses 19\\nonthediﬃcultiesofthenegativephase.\\nLetuslookmorecloselyatthegradientof: log Z\\n∇ θlog Z (18.5)\\n=∇ θ Z\\nZ(18.6)\\n=∇ θ\\ue050\\nx˜ p()x\\nZ(18.7)\\n=\\ue050\\nx∇ θ˜ p()x\\nZ. (18.8)\\nFormodelsthatguarantee p(x) >0forallx,wecansubstitute exp(log ˜ p())x\\nfor˜ p()x:\\ue050\\nx∇ θexp(log ˜ p())x\\nZ(18.9)\\n=\\ue050\\nxexp(log ˜ p())x∇ θlog ˜ p()x\\nZ(18.10)\\n=\\ue050\\nx˜ p()x∇ θlog ˜ p()x\\nZ(18.11)\\n=\\ue058\\nxp()x∇ θlog ˜ p()x (18.12)\\n606', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d40dab24-1636-4a4e-a6a7-b56f3cc7f87e', embedding=None, metadata={'page_label': '622', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\n= E x x ∼ p ( )∇ θlog ˜ p .()x (18.13)\\nThisderivationmadeuseofsummationoverdiscrete x,butasimilarresult\\nappliesusingintegrationovercontinuous x.Inthecontinuousversionofthe\\nderivation,weuseLeibniz’srulefordiﬀerentiationundertheintegralsigntoobtain\\ntheidentity\\n∇ θ\\ue05a\\n˜ p d()x x=\\ue05a\\n∇ θ˜ p d . ()x x (18.14)\\nThisidentityisapplicableonlyundercertainregularityconditionson˜ pand∇ θ˜ p(x).\\nInmeasuretheoreticterms,theconditionsare:(i)Theunnormalized distribution˜ p\\nmustbeaLebesgue-integrablefunctionof xforeveryvalueof θ;(ii)Thegradient\\n∇ θ˜ p(x)mustexistforall θandalmostall x;(iii)Theremustexistanintegrable\\nfunction R( x)thatbounds ∇ θ˜ p(x)inthesensethatmax i|∂\\n∂ θ i˜ p(x)|≤ R( x)forall\\nθandalmostall x.Fortunately,mostmachinelearningmodelsofinteresthave\\ntheseproperties.\\nThisidentity\\n∇ θlog = Z E x x ∼ p ( )∇ θlog ˜ p()x (18.15)\\nisthebasisforavarietyofMonteCarlomethodsforapproximatelymaximizing\\nthelikelihoodofmodelswithintractablepartitionfunctions.\\nTheMonteCarloapproachtolearningundirectedmodelsprovidesanintuitive\\nframeworkinwhichwecanthinkofboththepositivephaseandthenegative\\nphase.Inthepositivephase,weincreaselog ˜ p(x)for xdrawnfromthedata.In\\nthenegativephase,wedecreasethepartitionfunctionbydecreasinglog ˜ p(x) drawn\\nfromthemodeldistribution.\\nInthedeeplearningliterature,itiscommontoparametrize log ˜ pintermsof\\nanenergyfunction(equation).Inthiscase,wecaninterpretthepositive 16.7\\nphaseaspushingdownontheenergyoftrainingexamplesandthenegativephase\\naspushingupontheenergyofsamplesdrawnfromthemodel,asillustratedin\\nﬁgure.18.1\\n18.2StochasticMaximumLikelihoodandContrastive\\nDivergence\\nThenaivewayofimplementing equation istocomputeitbyburningin 18.15\\nasetofMarkovchainsfromarandominitialization everytimethegradientis\\nneeded.Whenlearningisperformedusingstochasticgradientdescent,thismeans\\nthechainsmustbeburnedinoncepergradientstep.Thisapproachleadstothe\\n607', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74a1f31e-92c4-4029-88ba-97d0faca7a50', embedding=None, metadata={'page_label': '623', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\ntrainingprocedurepresentedinalgorithm .Thehighcostofburninginthe 18.1\\nMarkovchainsintheinnerloopmakesthisprocedurecomputationally infeasible,\\nbutthisprocedureisthestartingpointthatothermorepracticalalgorithmsaim\\ntoapproximate.\\nAl g o r i t hm 1 8 . 1AnaiveMCMCalgorithmformaximizingthelog-likelihood\\nwithanintractablepartitionfunctionusinggradientascent.\\nSet,thestepsize,toasmallpositivenumber. \\ue00f\\nSet k,thenumberofGibbssteps,highenoughtoallowburnin.Perhaps100to\\ntrainanRBMonasmallimagepatch.\\nwhi l enotconverged do\\nSampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset.\\ng←1\\nm\\ue050m\\ni = 1∇ θlog ˜ p(x( ) i;) θ.\\nInitializeasetof msamples {˜x( 1 ), . . . ,˜x( ) m}torandomvalues(e.g.,from\\nauniformornormaldistribution,orpossiblyadistributionwithmarginals\\nmatchedtothemodel’smarginals).\\nf o r do i k = 1to\\nf o r do j m = 1to\\n˜x( ) j←gibbs_update(˜x( ) j) .\\ne nd f o r\\ne nd f o r\\ngg←−1\\nm\\ue050m\\ni = 1∇ θlog ˜ p(˜x( ) i;) θ .\\nθ θ← + \\ue00f .g\\ne nd whi l e\\nWecanviewtheMCMCapproachtomaximumlikelihoodastryingtoachieve\\nbalancebetweentwoforces,onepushinguponthemodeldistributionwherethe\\ndataoccurs,andanotherpushingdownonthemodeldistributionwherethemodel\\nsamplesoccur.Figureillustratesthisprocess.Thetwoforcescorrespondto 18.1\\nmaximizing log ˜ pandminimizing log Z.Severalapproximations tothenegative\\nphasearepossible.Eachoftheseapproximationscanbeunderstoodasmaking\\nthenegativephasecomputationally cheaperbutalsomakingitpushdowninthe\\nwronglocations.\\nBecausethenegativephaseinvolvesdrawingsamplesfromthemodel’sdistri-\\nbution,wecanthinkofitasﬁndingpointsthatthemodelbelievesinstrongly.\\nBecausethenegativephaseactstoreducetheprobabilityofthosepoints,they\\naregenerallyconsideredtorepresentthemodel’sincorrectbeliefsabouttheworld.\\nTheyarefrequentlyreferredtointheliteratureas“hallucinations” or“fantasy\\nparticles.”Infact,thenegativephasehasbeenproposedasapossibleexplanation\\n608', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='59b5d218-5084-4cb0-89e6-34607ea26f9d', embedding=None, metadata={'page_label': '624', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nxp(x )The p o s i t i v e p h a s e\\np mo d e l ( ) x\\np d a t a ( ) x\\nxp(x )The n eg a t i v e p h a s e\\np mo d e l ( ) x\\np d a t a ( ) x\\nFigure18.1:Theviewofalgorithmashavinga“positivephase”and“negativephase.” 18.1\\n( L e f t )Inthepositivephase,wesamplepointsfromthedatadistribution,andpushupon\\ntheirunnormalizedprobability.Thismeanspointsthatarelikelyinthedatagetpushed\\nuponmore. ( R i g h t )Inthenegativephase,wesamplepointsfromthemodeldistribution,\\nandpushdownontheirunnormalizedprobability.Thiscounteractsthepositivephase’s\\ntendencytojustaddalargeconstanttotheunnormalizedprobabilityeverywhere.When\\nthedatadistributionandthemodeldistributionareequal,thepositivephasehasthe\\nsamechancetopushupatapointasthenegativephasehastopushdown.Whenthis\\noccurs,thereisnolongeranygradient(inexpectation)andtrainingmustterminate.\\nfordreaminginhumansandotheranimals(CrickandMitchison1983,),theidea\\nbeingthatthebrainmaintainsaprobabilisticmodeloftheworldandfollows\\nthegradientoflog ˜ pwhileexperiencingrealeventswhileawakeandfollowsthe\\nnegativegradientoflog ˜ ptominimize log Zwhilesleepingandexperiencingevents\\nsampledfromthecurrentmodel.Thisviewexplainsmuchofthelanguageusedto\\ndescribealgorithmswithapositiveandnegativephase,butithasnotbeenproven\\ntobecorrectwithneuroscientiﬁcexperiments.Inmachinelearningmodels,itis\\nusuallynecessarytousethepositiveandnegativephasesimultaneously,rather\\nthaninseparatetimeperiodsofwakefulnessandREMsleep.\\xa0Aswewillseein\\nsection,othermachinelearningalgorithmsdrawsamplesfromthemodel 19.5\\ndistributionforotherpurposesandsuchalgorithmscouldalsoprovideanaccount\\nforthefunctionofdreamsleep.\\nGiventhisunderstandingoftheroleofthepositiveandnegativephaseof\\nlearning,wecanattempttodesignalessexpensivealternativetoalgorithm .18.1\\nThemaincostofthenaiveMCMCalgorithmisthecostofburningintheMarkov\\nchainsfromarandominitialization ateachstep.\\xa0Anaturalsolutionistoinitialize\\ntheMarkovchainsfromadistributionthatisveryclosetothemodeldistribution,\\n609', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='75cdc64e-6626-46a4-8e1f-3947a0bb7e89', embedding=None, metadata={'page_label': '625', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nsothattheburninoperationdoesnottakeasmanysteps.\\nThe c o n t r ast i v e di v e r g e nc e(CD,orCD- ktoindicateCDwith kGibbssteps)\\nalgorithminitializestheMarkovchainateachstepwithsamplesfromthedata\\ndistribution(Hinton20002010,,).Thisapproachispresentedasalgorithm .18.2\\nObtainingsamplesfromthedatadistributionisfree,becausetheyarealready\\navailableinthedataset.Initially,thedatadistributionisnotclosetothemodel\\ndistribution,sothenegativephaseisnotveryaccurate.Fortunately,thepositive\\nphasecanstillaccuratelyincreasethemodel’sprobabilityofthedata.Afterthe\\npositivephasehashadsometimetoact,themodeldistributionisclosertothe\\ndatadistribution,andthenegativephasestartstobecomeaccurate.\\nAl g o r i t hm 1 8 . 2Thecontrastivedivergencealgorithm,usinggradientascentas\\ntheoptimization procedure.\\nSet,thestepsize,toasmallpositivenumber. \\ue00f\\nSet k,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling\\nfrom p(x; θ)tomixwheninitializedfrom pdata.Perhaps1-20totrainanRBM\\nonasmallimagepatch.\\nwhi l enotconverged do\\nSampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset.\\ng←1\\nm\\ue050m\\ni = 1∇ θlog ˜ p(x( ) i;) θ .\\nf o r do i m = 1to\\n˜x( ) i←x( ) i.\\ne nd f o r\\nf o r do i k = 1to\\nf o r do j m = 1to\\n˜x( ) j←gibbs_update(˜x( ) j) .\\ne nd f o r\\ne nd f o r\\ngg←−1\\nm\\ue050m\\ni = 1∇ θlog ˜ p(˜x( ) i;) θ .\\nθ θ← + \\ue00f .g\\ne nd whi l e\\nOfcourse,CDisstillanapproximation tothecorrectnegativephase.The\\nmainwaythatCDqualitativelyfailstoimplementthecorrectnegativephase\\nisthatitfailstosuppressregionsofhighprobabilitythatarefarfromactual\\ntrainingexamples.Theseregionsthathavehighprobabilityunderthemodelbut\\nlowprobabilityunderthedatageneratingdistributionarecalled spur i o us m o des.\\nFigureillustrateswhythishappens.Essentially,itisbecausemodesinthe 18.2\\nmodeldistributionthatarefarfromthedatadistributionwillnotbevisitedby\\n610', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ebfd5943-7702-413a-8036-db8bb3033316', embedding=None, metadata={'page_label': '626', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nxp(x )p mo d e l ( ) x\\np d a t a ( ) x\\nFigure18.2:\\xa0Anillustrationofhowthenegativephaseofcontrastivedivergence(algo-\\nrithm)canfailtosuppressspuriousmodes.Aspuriousmodeisamodethatis 18.2\\npresentinthemodeldistributionbutabsentinthedatadistribution.Becausecontrastive\\ndivergenceinitializesitsMarkovchainsfromdatapointsandrunstheMarkovchainfor\\nonlyafewsteps,itisunlikelytovisitmodesinthemodelthatarefarfromthedata\\npoints.Thismeansthatwhensamplingfromthemodel,wewillsometimesgetsamples\\nthatdonotresemblethedata.Italsomeansthatduetowastingsomeofitsprobability\\nmassonthesemodes,themodelwillstruggletoplacehighprobabilitymassonthecorrect\\nmodes.Forthepurposeofvisualization,thisﬁgureusesasomewhatsimpliﬁedconcept\\nofdistance—thespuriousmodeisfarfromthecorrectmodealongthenumberlinein\\nR.ThiscorrespondstoaMarkovchainbasedonmakinglocalmoveswithasingle x\\nvariablein R.Formostdeepprobabilisticmodels,theMarkovchainsarebasedonGibbs\\nsamplingandcanmakenon-localmovesofindividualvariablesbutcannotmoveallof\\nthevariablessimultaneously.Fortheseproblems,itisusuallybettertoconsidertheedit\\ndistancebetweenmodes,ratherthantheEuclideandistance.However,editdistanceina\\nhighdimensionalspaceisdiﬃculttodepictina2-Dplot.\\nMarkovchainsinitializedattrainingpoints,unlessisverylarge. k\\nCarreira-PerpiñanandHinton2005()showed\\xa0experimentallythatthe\\xa0CD\\nestimatorisbiasedforRBMsandfullyvisibleBoltzmannmachines,inthatit\\nconvergestodiﬀerentpointsthanthemaximumlikelihoodestimator.Theyargue\\nthatbecausethebiasissmall,CDcouldbeusedasaninexpensivewaytoinitialize\\namodelthatcouldlaterbeﬁne-tunedviamoreexpensiveMCMCmethods.Bengio\\nandDelalleau2009()showedthatCDcanbeinterpretedasdiscardingthesmallest\\ntermsofthecorrectMCMCupdategradient,whichexplainsthebias.\\nCDisusefulfortrainingshallowmodelslikeRBMs.Thesecaninturnbe\\nstackedtoinitializedeepermodelslikeDBNsorDBMs.\\xa0However,CDdoesnot\\nprovidemuchhelpfortrainingdeepermodelsdirectly.Thisisbecauseitisdiﬃcult\\n611', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c835f9e6-abd6-4f92-b837-26979ff95a22', embedding=None, metadata={'page_label': '627', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\ntoobtainsamplesofthehiddenunitsgivensamplesofthevisibleunits.Sincethe\\nhiddenunitsarenotincludedinthedata,initializingfromtrainingpointscannot\\nsolvetheproblem.Evenifweinitializethevisibleunitsfromthedata,wewillstill\\nneedtoburninaMarkovchainsamplingfromthedistributionoverthehidden\\nunitsconditionedonthosevisiblesamples.\\nTheCDalgorithmcanbethoughtofaspenalizingthemodelforhavinga\\nMarkovchainthatchangestheinputrapidlywhentheinputcomesfromthedata.\\nThismeanstrainingwithCDsomewhatresemblesautoencodertraining.Even\\nthoughCDismorebiasedthansomeoftheothertrainingmethods,itcanbe\\nusefulforpretrainingshallowmodelsthatwilllaterbestacked.Thisisbecause\\ntheearliestmodelsinthestackareencouragedtocopymoreinformationupto\\ntheirlatentvariables,therebymakingitavailabletothelatermodels.Thisshould\\nbethoughtofmoreofasanoften-exploitable sideeﬀectofCDtrainingratherthan\\naprincipleddesignadvantage.\\nSutskeverandTieleman2010()showedthattheCDupdatedirectionisnotthe\\ngradientofanyfunction.ThisallowsforsituationswhereCDcouldcycleforever,\\nbutinpracticethisisnotaseriousproblem.\\nAdiﬀerentstrategythatresolvesmanyoftheproblemswithCDistoinitial-\\nizetheMarkovchainsateachgradientstepwiththeirstatesfromtheprevious\\ngradientstep.Thisapproachwasﬁrstdiscoveredunderthename st o c hast i c m ax -\\ni m um l i k e l i ho o d(SML)intheappliedmathematics andstatisticscommunity\\n(Younes1998,)andlaterindependently rediscoveredunderthename p e r si st e n t\\nc o n t r ast i v e di v e r g e n c e(PCD,orPCD- ktoindicatetheuseof kGibbssteps\\nperupdate)inthedeeplearningcommunity(,).Seealgorithm . Tieleman2008 18.3\\nThebasicideaofthisapproachisthat,solongasthestepstakenbythestochastic\\ngradientalgorithmaresmall,thenthemodelfromthepreviousstepwillbesimilar\\ntothemodelfromthecurrentstep.Itfollowsthatthesamplesfromtheprevious\\nmodel’sdistributionwillbeveryclosetobeingfairsamplesfromthecurrent\\nmodel’sdistribution,soaMarkovchaininitializedwiththesesampleswillnot\\nrequiremuchtimetomix.\\nBecauseeachMarkovchainiscontinuallyupdatedthroughoutthelearning\\nprocess,ratherthanrestartedateachgradientstep,thechainsarefreetowander\\nfarenoughtoﬁndallofthemodel’smodes.SMListhusconsiderablymore\\nresistanttoformingmodelswithspuriousmodesthanCDis.Moreover,because\\nitispossibletostorethestateofallofthesampledvariables,whethervisibleor\\nlatent,SMLprovidesaninitialization pointforboththehiddenandvisibleunits.\\nCDisonlyabletoprovideaninitialization forthevisibleunits,andtherefore\\nrequiresburn-infordeepmodels.SMLisabletotraindeepmodelseﬃciently.\\n612', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a25e3112-6a87-48ed-a14e-f94ac06e0eac', embedding=None, metadata={'page_label': '628', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nMarlin 2010 e t a l .()comparedSMLtomanyoftheothercriteriapresentedin\\nthischapter.TheyfoundthatSMLresultsinthebesttestsetlog-likelihoodfor\\nanRBM,andthatiftheRBM’shiddenunitsareusedasfeaturesforanSVM\\nclassiﬁer,SMLresultsinthebestclassiﬁcationaccuracy.\\nSMLisvulnerabletobecominginaccurateifthestochasticgradientalgorithm\\ncanmovethemodelfasterthantheMarkovchaincanmixbetweensteps.This\\ncanhappenif kistoosmallor \\ue00fistoolarge.Thepermissiblerangeofvaluesis\\nunfortunately highlyproblem-dependent.Thereisnoknownwaytotestformally\\nwhetherthechainissuccessfullymixingbetweensteps.Subjectively,ifthelearning\\nrateistoohighforthenumberofGibbssteps,thehumanoperatorwillbeable\\ntoobservethatthereismuchmorevarianceinthenegativephasesamplesacross\\ngradientstepsratherthanacrossdiﬀerentMarkovchains.Forexample,amodel\\ntrainedonMNISTmightsampleexclusively7sononestep.Thelearningprocess\\nwillthenpushdownstronglyonthemodecorrespondingto7s,andthemodel\\nmightsampleexclusively9sonthenextstep.\\nAl g o r i t hm 1 8 . 3Thestochasticmaximumlikelihood/persistentcontrastive\\ndivergencealgorithmusinggradientascentastheoptimization procedure.\\nSet,thestepsize,toasmallpositivenumber. \\ue00f\\nSet k,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling\\nfrom p(x; θ+ \\ue00fg)toburnin,startingfromsamplesfrom p(x; θ).Perhaps1for\\nRBMonasmallimagepatch,or5-50foramorecomplicatedmodellikeaDBM.\\nInitializeasetof msamples {˜x( 1 ), . . . ,˜x( ) m}torandomvalues(e.g.,froma\\nuniformornormaldistribution,orpossiblyadistributionwithmarginalsmatched\\ntothemodel’smarginals).\\nwhi l enotconverged do\\nSampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset.\\ng←1\\nm\\ue050m\\ni = 1∇ θlog ˜ p(x( ) i;) θ .\\nf o r do i k = 1to\\nf o r do j m = 1to\\n˜x( ) j←gibbs_update(˜x( ) j) .\\ne nd f o r\\ne nd f o r\\ngg←−1\\nm\\ue050m\\ni = 1∇ θlog ˜ p(˜x( ) i;) θ .\\nθ θ← + \\ue00f .g\\ne nd whi l e\\nCaremustbetakenwhenevaluatingthesamplesfromamodeltrainedwith\\nSML.ItisnecessarytodrawthesamplesstartingfromafreshMarkovchain\\n613', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f13bd9c6-1bdb-44c5-bfc6-086b980ebc29', embedding=None, metadata={'page_label': '629', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\ninitializedfromarandomstartingpointafterthemodelisdonetraining.The\\nsamplespresentinthepersistentnegativechainsusedfortraininghavebeen\\ninﬂuencedbyseveralrecentversionsofthemodel,andthuscanmakethemodel\\nappeartohavegreatercapacitythanitactuallydoes.\\nBerglundandRaiko2013()performedexperimentstoexaminethebiasand\\nvarianceintheestimateofthegradientprovidedbyCDandSML.CDprovesto\\nhavelowervariancethantheestimatorbasedonexactsampling.SMLhashigher\\nvariance.ThecauseofCD’slowvarianceisitsuseofthesametrainingpoints\\ninboththepositiveandnegativephase.Ifthenegativephaseisinitializedfrom\\ndiﬀerenttrainingpoints,thevariancerisesabovethatoftheestimatorbasedon\\nexactsampling.\\nAllofthesemethodsbasedonusingMCMCtodrawsamplesfromthemodel\\ncaninprinciplebeusedwithalmostanyvariantofMCMC.Thismeansthat\\ntechniquessuchasSMLcanbeimprovedbyusinganyoftheenhancedMCMC\\ntechniquesdescribedinchapter,suchasparalleltempering( , 17 Desjardins e t a l .\\n2010Cho2010; e t a l .,).\\nOneapproachtoacceleratingmixingduringlearningreliesnotonchanging\\ntheMonteCarlosamplingtechnologybutratheronchangingtheparametrization\\nofthemodelandthecostfunction. F ast P CDorFPCD( , TielemanandHinton\\n2009)involvesreplacingtheparameters θofatraditionalmodelwithanexpression\\nθ θ= ( )slow+ θ( )fast. (18.16)\\nTherearenowtwiceasmanyparametersasbefore,andtheyareaddedtogether\\nelement-wisetoprovidetheparametersusedbytheoriginalmodeldeﬁnition.The\\nfastcopyoftheparametersistrainedwithamuchlargerlearningrate,allowing\\nittoadaptrapidlyinresponsetothenegativephaseoflearningandpushthe\\nMarkovchaintonewterritory.ThisforcestheMarkovchaintomixrapidly,though\\nthiseﬀectonlyoccursduringlearningwhilethefastweightsarefreetochange.\\nTypicallyonealsoappliessigniﬁcantweightdecaytothefastweights,encouraging\\nthemtoconvergetosmallvalues,afteronlytransientlytakingonlargevalueslong\\nenoughtoencouragetheMarkovchaintochangemodes.\\nOnekeybeneﬁttotheMCMC-basedmethodsdescribedinthissectionisthat\\ntheyprovideanestimateofthegradientoflog Z,andthuswecanessentially\\ndecomposetheproblemintothelog ˜ pcontributionandthelog Zcontribution.\\nWecanthenuseanyothermethodtotacklelog ˜ p(x),andjustaddournegative\\nphasegradientontotheothermethod’sgradient.Inparticular,thismeansthat\\nourpositivephasecanmakeuseofmethodsthatprovideonlyalowerboundon\\n˜ p.Mostoftheothermethodsofdealingwith log Zpresentedinthischapterare\\n614', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0a1bb4a4-1f23-4dc9-ae28-7f40906adfa0', embedding=None, metadata={'page_label': '630', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nincompatible withbound-basedpositivephasemethods.\\n18.3Pseudolikelihood\\nMonteCarloapproximations tothepartitionfunctionanditsgradientdirectly\\nconfrontthepartitionfunction.Otherapproachessidesteptheissue,bytraining\\nthemodelwithoutcomputingthepartitionfunction.Mostoftheseapproachesare\\nbasedontheobservationthatitiseasytocomputeratiosofprobabilities inan\\nundirectedprobabilisticmodel.Thisisbecausethepartitionfunctionappearsin\\nboththenumeratorandthedenominator oftheratioandcancelsout:\\np()x\\np()y=1\\nZ˜ p()x\\n1\\nZ˜ p()y=˜ p()x\\n˜ p()y. (18.17)\\nThepseudolikelihoodisbasedontheobservationthatconditionalprobabilities\\ntakethisratio-basedform,andthuscanbecomputedwithoutknowledgeofthe\\npartitionfunction.Supposethatwepartition xintoa,bandc,where acontains\\nthevariableswewanttoﬁndtheconditionaldistributionover,bcontainsthe\\nvariableswewanttoconditionon,andccontainsthevariablesthatarenotpart\\nofourquery.\\np( ) = ab|p ,(ab)\\np()b=p ,(ab)\\ue050\\na c , p , ,(abc)=˜ p ,(ab)\\ue050\\na c ,˜ p , ,(abc).(18.18)\\nThisquantityrequiresmarginalizing outa,whichcanbeaveryeﬃcientoperation\\nprovidedthataandcdonotcontainverymanyvariables.Intheextremecase,a\\ncanbeasinglevariableandccanbeempty,makingthisoperationrequireonlyas\\nmanyevaluationsof˜ pastherearevaluesofasinglerandomvariable.\\nUnfortunately,inordertocomputethelog-likelihood,weneedtomarginalize\\noutlargesetsofvariables.Ifthereare nvariablestotal,wemustmarginalizeaset\\nofsize.Bythechainruleofprobability, n−1\\nlog() = log( px p x 1)+log( p x 2| x 1)+ +( ··· p x n|x 1 : 1 n −) .(18.19)\\nInthiscase,wehavemade amaximallysmall,butccanbeaslargeasx 2 : n.What\\nifwesimplymovecintobtoreducethecomputational cost?Thisyieldsthe\\npseudolik e l i ho o d(,)objectivefunction,basedonpredictingthevalue Besag1975\\noffeature x igivenalloftheotherfeatures x − i:\\nn \\ue058\\ni = 1log( p x i| x − i) . (18.20)\\n615', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fccd180a-e708-4d3e-82a6-d0809f5d89ac', embedding=None, metadata={'page_label': '631', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nIfeachrandomvariablehas kdiﬀerentvalues,thisrequiresonly k n×evaluations\\nof˜ ptocompute,asopposedtothe knevaluationsneededtocomputethepartition\\nfunction.\\nThismaylooklikeanunprincipled hack,butitcanbeproventhatestimation\\nbymaximizingthepseudolikelihoodisasymptoticallyconsistent(,).Mase1995\\nOfcourse,inthecaseofdatasetsthatdonotapproachthelargesamplelimit,\\npseudolikelihoodmaydisplaydiﬀerentbehaviorfromthemaximumlikelihood\\nestimator.\\nItispossibletotradecomputational complexityfordeviationfrommaximum\\nlikelihoodbehaviorbyusingthe g e ner al i z e d pseudolikel i h o o destimator(Huang\\nandOgata2002,).Thegeneralizedpseudolikelihoodestimatoruses mdiﬀerentsets\\nS( ) i, i= 1 , . . . , mofindicesofvariablesthatappeartogetherontheleftsideofthe\\nconditioningbar.Intheextremecaseof m= 1and S( 1 )=1 , . . . , nthegeneralized\\npseudolikelihoodrecoversthelog-likelihood.\\xa0Intheextremecaseof m= nand\\nS( ) i={} i,thegeneralizedpseudolikelihoodrecoversthepseudolikelihood.The\\ngeneralizedpseudolikelihoodobjectivefunctionisgivenby\\nm \\ue058\\ni = 1log( pxS() i|x− S() i) . (18.21)\\nTheperformanceofpseudolikelihood-basedapproachesdependslargelyonhow\\nthemodelwillbeused.Pseudolikelihoodtendstoperformpoorlyontasksthat\\nrequireagoodmodelofthefulljoint p(x),suchasdensityestimationandsampling.\\nHowever,itcanperformbetterthanmaximumlikelihoodfortasksthatrequireonly\\ntheconditionaldistributionsusedduringtraining,suchasﬁllinginsmallamounts\\nofmissingvalues.Generalizedpseudolikelihoodtechniquesareespeciallypowerfulif\\nthedatahasregularstructurethatallowsthe Sindexsetstobedesignedtocapture\\nthemostimportantcorrelationswhileleavingoutgroupsofvariablesthatonly\\nhavenegligiblecorrelation.Forexample,innaturalimages,pixelsthatarewidely\\nseparatedinspacealsohaveweakcorrelation,sothegeneralizedpseudolikelihood\\ncanbeappliedwitheachsetbeingasmall,spatiallylocalizedwindow. S\\nOneweaknessofthepseudolikelihoodestimatoristhatitcannotbeusedwith\\notherapproximationsthatprovideonlyalowerboundon˜ p(x),suchasvariational\\ninference,whichwillbecoveredinchapter.Thisisbecause 19 ˜ pappearsinthe\\ndenominator. Alowerboundonthedenominator providesonlyanupperboundon\\ntheexpressionasawhole,andthereisnobeneﬁttomaximizinganupperbound.\\nThismakesitdiﬃculttoapplypseudolikelihoodapproachestodeepmodelssuch\\nasdeepBoltzmannmachines,sincevariationalmethodsareoneofthedominant\\napproachestoapproximately marginalizing outthemanylayersofhiddenvariables\\n616', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='25027af4-29e7-4bdb-baff-f46ede98ba23', embedding=None, metadata={'page_label': '632', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nthatinteractwitheachother.\\xa0However,pseudolikelihoodisstillusefulfordeep\\nlearning,becauseitcanbeusedtotrainsinglelayermodels,ordeepmodelsusing\\napproximateinferencemethodsthatarenotbasedonlowerbounds.\\nPseudolikelihoodhasamuchgreatercostpergradientstepthanSML,dueto\\nitsexplicitcomputationofalloftheconditionals.However,generalizedpseudo-\\nlikelihoodandsimilarcriteriacanstillperformwellifonlyonerandomlyselected\\nconditionaliscomputedperexample(Goodfellow2013b e t a l .,),therebybringing\\nthecomputational costdowntomatchthatofSML.\\nThoughthepseudolikelihoodestimatordoesnotexplicitlyminimize log Z,it\\ncanstillbethoughtofashavingsomethingresemblinganegativephase.The\\ndenominators ofeachconditionaldistributionresultinthelearningalgorithm\\nsuppressingtheprobabilityofallstatesthathaveonlyonevariablediﬀeringfrom\\natrainingexample.\\nSeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic\\neﬃciencyofpseudolikelihood.\\n18.4ScoreMatchingandRatioMatching\\nScorematching(,)providesanotherconsistentmeansoftraininga Hyvärinen2005\\nmodelwithoutestimating Zoritsderivatives.Thenamescorematchingcomes\\nfromterminologyinwhichthederivativesofalogdensitywithrespecttoits\\nargument,∇ xlog p( x),arecalledits sc o r e.Thestrategyusedbyscorematching\\nistominimizetheexpectedsquareddiﬀerencebetweenthederivativesofthe\\nmodel’slogdensitywithrespecttotheinputandthederivativesofthedata’slog\\ndensitywithrespecttotheinput:\\nL ,( x θ) =1\\n2||∇ xlog p m o de l(;) x θ−∇ xlog pdata() x||2\\n2(18.22)\\nJ() = θ1\\n2E pdata ( ) x L ,( x θ) (18.23)\\nθ∗= min\\nθJ() θ (18.24)\\nThisobjectivefunctionavoidsthediﬃcultiesassociatedwithdiﬀerentiating\\nthepartitionfunction Zbecause Zisnotafunctionof xandtherefore ∇ x Z= 0.\\nInitially,scorematchingappearstohaveanewdiﬃculty:\\xa0comput ingthescore\\nofthedatadistributionrequiresknowledgeofthetruedistributiongenerating\\nthetrainingdata, pdata.Fortunately,minimizingtheexpectedvalueofis L ,( x θ)\\n617', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5013c71e-96a5-4fda-a300-03915e444958', embedding=None, metadata={'page_label': '633', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nequivalenttominimizingtheexpectedvalueof\\n˜ L ,( x θ) =n \\ue058\\nj = 1\\ue020\\n∂2\\n∂ x2\\njlog p m o de l(;)+ x θ1\\n2\\ue012∂\\n∂ x jlog p m o de l(;) x θ\\ue0132\\ue021\\n(18.25)\\nwhereisthedimensionalityof. n x\\nBecausescorematchingrequirestakingderivativeswithrespecttox,itisnot\\napplicabletomodelsofdiscretedata.However,thelatentvariablesinthemodel\\nmaybediscrete.\\nLikethepseudolikelihood,scorematchingonlyworkswhenweareableto\\nevaluate log ˜ p(x)anditsderivativesdirectly.Itisnotcompatiblewithmethods\\nthatonlyprovidealowerboundonlog ˜ p(x),becausescorematchingrequires\\nthederivativesandsecondderivativesoflog ˜ p(x)andalowerboundconveysno\\ninformationaboutitsderivatives.Thismeansthatscorematchingcannotbe\\nappliedtoestimatingmodelswithcomplicatedinteractionsbetweenthehidden\\nunits,suchassparsecodingmodelsordeepBoltzmannmachines.Whilescore\\nmatchingcanbeusedtopretraintheﬁrsthiddenlayerofalargermodel,ithas\\nnotbeenappliedasapretrainingstrategyforthedeeperlayersofalargermodel.\\nThisisprobablybecausethehiddenlayersofsuchmodelsusuallycontainsome\\ndiscretevariables.\\nWhilescorematchingdoesnotexplicitlyhaveanegativephase,itcanbe\\nviewedasaversionofcontrastivedivergenceusingaspeciﬁckindofMarkovchain\\n(,).TheMarkovchaininthiscaseisnotGibbssampling,but Hyvärinen2007a\\nratheradiﬀerentapproachthatmakeslocalmovesguidedbythegradient.Score\\nmatchingisequivalenttoCDwiththistypeofMarkovchainwhenthesizeofthe\\nlocalmovesapproacheszero.\\nLyu2009()generalizedscorematchingtothediscretecase(butmadeanerror\\nintheirderivationthatwascorrectedby ()). Marlin e t a l .2010Marlin e t a l .\\n()foundthat 2010 g e ner al i z e d sc o r e m at c hi ng(GSM)doesnotworkinhigh\\ndimensionaldiscretespaceswheretheobservedprobabilityofmanyeventsis0.\\nAmoresuccessfulapproachtoextendingthebasicideasofscorematching\\ntodiscretedatais r at i o m at c hi ng(,).Ratiomatchingapplies Hyvärinen2007b\\nspeciﬁcallytobinarydata.Ratiomatchingconsistsofminimizingtheaverageover\\nexamplesofthefollowingobjectivefunction:\\nL( )RM() = x θ ,n \\ue058\\nj = 1\\uf8eb\\n\\uf8ed1\\n1+pmodel ( ; ) x θ\\npmodel ( ( ) ) ; ) f x , j θ\\uf8f6\\n\\uf8f82\\n,(18.26)\\n618', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='6c2664ab-6b37-4d90-b78c-21c00aa651f2', embedding=None, metadata={'page_label': '634', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nwhere returnswiththebitatpositionﬂipped.Ratiomatchingavoids f , j( x) x j\\nthepartitionfunctionusingthesametrickasthepseudolikelihoodestimator:ina\\nratiooftwoprobabilities, thepartitionfunctioncancelsout. () Marlin e t a l .2010\\nfoundthatratiomatchingoutperformsSML,pseudolikelihoodandGSMinterms\\noftheabilityofmodelstrainedwithratiomatchingtodenoisetestsetimages.\\nLikethepseudolikelihoodestimator,ratiomatchingrequires nevaluationsof˜ p\\nperdatapoint,makingitscomputational costperupdateroughly ntimeshigher\\nthanthatofSML.\\nAswiththepseudolikelihoodestimator,ratiomatchingcanbethoughtofas\\npushingdownonallfantasystatesthathaveonlyonevariablediﬀerentfroma\\ntrainingexample.Sinceratiomatchingappliesspeciﬁcallytobinarydata,this\\nmeansthatitactsonallfantasystateswithinHammingdistance1ofthedata.\\nRatiomatchingcanalsobeusefulasthebasisfordealingwithhigh-dimensional\\nsparsedata,suchaswordcountvectors.Thiskindofdataposesachallengefor\\nMCMC-basedmethodsbecausethedataisextremelyexpensivetorepresentin\\ndenseformat,yettheMCMCsamplerdoesnotyieldsparsevaluesuntilthemodel\\nhaslearnedtorepresentthesparsityinthedatadistribution.DauphinandBengio\\n()overcamethisissuebydesigninganunbiasedstochasticapproximation to 2013\\nratiomatching.Theapproximation evaluatesonlyarandomlyselectedsubsetof\\nthetermsoftheobjective,anddoesnotrequirethemodeltogeneratecomplete\\nfantasysamples.\\nSeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic\\neﬃciencyofratiomatching.\\n18.5DenoisingScoreMatching\\nInsomecaseswemaywishtoregularizescorematching,byﬁttingadistribution\\npsmoothed() = x\\ue05a\\npdata()( ) y q x y| d y (18.27)\\nratherthanthetrue pdata.Thedistribution q( x y|) isacorruptionprocess,usually\\nonethatformsbyaddingasmallamountofnoiseto. x y\\nDenoisingscorematchingisespeciallyusefulbecauseinpracticeweusuallydo\\nnothaveaccesstothetrue pdatabutratheronlyanempiricaldistributiondeﬁned\\nbysamplesfromit.Anyconsistentestimatorwill,givenenoughcapacity,make\\np m o de lintoasetofDiracdistributionscenteredonthetrainingpoints.Smoothing\\nby qhelpstoreducethisproblem,atthelossoftheasymptoticconsistencyproperty\\n619', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b07c3720-0744-4f7d-b76d-97e961e15295', embedding=None, metadata={'page_label': '635', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\ndescribedinsection. ()introducedaprocedurefor 5.4.5KingmaandLeCun2010\\nperformingregularizedscorematchingwiththesmoothingdistribution qbeing\\nnormallydistributednoise.\\nRecallfromsectionthatseveralautoencodertrainingalgorithmsare 14.5.1\\nequivalenttoscorematchingordenoisingscorematching.Theseautoencoder\\ntrainingalgorithmsare\\xa0therefore a\\xa0wayof\\xa0overcomingthe\\xa0partition function\\nproblem.\\n18.6Noise-ContrastiveEstimation\\nMosttechniquesforestimatingmodelswithintractablepartitionfunctionsdonot\\nprovideanestimateofthepartitionfunction.SMLandCDestimateonlythe\\ngradientofthelogpartitionfunction,ratherthanthepartitionfunctionitself.\\nScorematchingandpseudolikelihoodavoidcomputingquantitiesrelatedtothe\\npartitionfunctionaltogether.\\nNoi se - c o n t r ast i v e \\xa0 e st i m a t i o n \\xa0 ( N C E )(Gutmann\\xa0and\\xa0Hy varinen2010,\\xa0)\\ntakesadiﬀerentstrategy.Inthisapproach,theprobabilitydistributionestimated\\nbythemodelisrepresentedexplicitlyas\\nlog p m o de l() = log ˜ x pmodel(;)+x θ c , (18.28)\\nwhere cisexplicitlyintroducedasanapproximationof−log Z( θ).Ratherthan\\nestimatingonly θ,thenoisecontrastiveestimationproceduretreats casjust\\nanotherparameterandestimates θand csimultaneously,usingthesamealgorithm\\nforboth.Theresulting log p m o de l(x)thusmaynotcorrespondexactlytoavalid\\nprobabilitydistribution,butwillbecomecloserandclosertobeingvalidasthe\\nestimateofimproves. c1\\nSuchanapproachwouldnotbepossibleusingmaximumlikelihoodasthe\\ncriterionfortheestimator.Themaximumlikelihoodcriterionwouldchoosetoset\\nc c arbitrarilyhigh,ratherthansettingtocreateavalidprobabilitydistribution.\\nNCEworksbyreducingtheunsupervisedlearningproblemofestimating p(x)\\ntothatoflearningaprobabilisticbinaryclassiﬁerinwhichoneofthecategories\\ncorrespondstothedatageneratedbythemodel.Thissupervisedlearningproblem\\nisconstructedinsuchawaythatmaximumlikelihoodestimationinthissupervised\\n1NCEisalsoapplicabletoproblemswithatractablepartitionfunction,wherethereisno\\nneedtointroducetheextraparameter c.However,ithasgeneratedthemostinterestasameans\\nofestimatingmodelswithdiﬃcultpartitionfunctions.\\n620', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e61deac9-2a93-4b66-9e2c-1633ef5e4c42', embedding=None, metadata={'page_label': '636', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nlearningproblemdeﬁnesanasymptoticallyconsistentestimatoroftheoriginal\\nproblem.\\nSpeciﬁcally,weintroduceaseconddistribution,the noi se di st r i but i o n pnoise(x).\\nThenoisedistributionshouldbetractabletoevaluateandtosamplefrom.\\xa0We\\ncannowconstructamodeloverbothxandanew,binaryclassvariable y.Inthe\\nnewjointmodel,wespecifythat\\npjoint(= 1) = y1\\n2, (18.29)\\npjoint( = 1) = x| y p m o de l()x , (18.30)\\nand\\npjoint( = 0) = x| y pnoise()x . (18.31)\\nInotherwords, yisaswitchvariablethatdetermineswhetherwewillgenerate x\\nfromthemodelorfromthenoisedistribution.\\nWecanconstructasimilarjointmodeloftrainingdata.Inthiscase,the\\nswitchvariabledetermineswhetherwedraw xfromthe dat aorfromthenoise\\ndistribution.Formally, ptrain( y=1)=1\\n2, ptrain(x| y=1)= pdata(x),\\xa0and\\nptrain( = 0) = x| y pnoise()x.\\nWecannowjustusestandardmaximumlikelihoodlearningonthe sup e r v i se d\\nlearningproblemofﬁtting pjointto ptrain:\\nθ , c= argmax\\nθ , cE x , py ∼trainlog pjoint( ) y|x . (18.32)\\nThedistribution pjointisessentiallyalogisticregressionmodelappliedtothe\\ndiﬀerenceinlogprobabilities ofthemodelandthenoisedistribution:\\npjoint(= 1 ) = y |xp m o de l()x\\np m o de l()+x pnoise()x(18.33)\\n=1\\n1+pnoise ( ) x\\npmodel ( ) x(18.34)\\n=1\\n1+exp\\ue010\\nlogpnoise ( ) x\\npmodel ( ) x\\ue011 (18.35)\\n= σ\\ue012\\n−logpnoise()x\\np m o de l()x\\ue013\\n(18.36)\\n= (log σ p m o de l()log x− pnoise())x . (18.37)\\n621', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b754d97f-e067-4a71-96bd-6340ab025b25', embedding=None, metadata={'page_label': '637', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nNCEisthussimpletoapplysolongaslog ˜ pmodeliseasytoback-propagate\\nthrough,and,asspeciﬁedabove, pnoiseiseasytoevaluate(inordertoevaluate\\npjoint)andsamplefrom(inordertogeneratethetrainingdata).\\nNCEismostsuccessfulwhenappliedtoproblemswithfewrandomvariables,\\nbutcanworkwellevenifthoserandomvariablescantakeonahighnumberof\\nvalues.Forexample,ithasbeensuccessfullyappliedtomodelingtheconditional\\ndistributionoverawordgiventhecontextoftheword(MnihandKavukcuoglu,\\n2013).Thoughthewordmaybedrawnfromalargevocabulary,thereisonlyone\\nword.\\nWhenNCEisappliedtoproblemswithmanyrandomvariables,itbecomesless\\neﬃcient.Thelogisticregressionclassiﬁercanrejectanoisesamplebyidentifying\\nanyonevariablewhosevalueisunlikely.Thismeansthatlearningslowsdown\\ngreatlyafter p m o de lhaslearnedthebasicmarginalstatistics.Imaginelearninga\\nmodelofimagesoffaces,usingunstructuredGaussiannoiseas pnoise.If p m o de l\\nlearnsabouteyes,itcanrejectalmostallunstructurednoisesampleswithout\\nhavinglearnedanythingaboutotherfacialfeatures,suchasmouths.\\nTheconstraintthat pnoisemustbeeasytoevaluateandeasytosamplefrom\\ncanbeoverlyrestrictive.When pnoiseissimple,mostsamplesarelikelytobetoo\\nobviouslydistinctfromthedatatoforce p m o de ltoimprovenoticeably.\\nLikescorematchingandpseudolikelihood,NCEdoesnotworkifonlyalower\\nboundon˜ pisavailable.Suchalowerboundcouldbeusedtoconstructalower\\nboundon pjoint( y= 1|x),butitcanonlybeusedtoconstructanupperboundon\\npjoint( y= 0|x),whichappearsinhalfthetermsoftheNCEobjective.Likewise,\\nalowerboundon pnoiseisnotuseful,becauseitprovidesonlyanupperboundon\\npjoint(= 1 ) y |x.\\nWhenthemodeldistributioniscopiedtodeﬁneanewnoisedistributionbefore\\neachgradientstep,NCEdeﬁnesaprocedurecalled se l f - c o n t r ast i v e e st i m at i o n,\\nwhose\\xa0expected\\xa0gradientis\\xa0equivalentto\\xa0the\\xa0expected\\xa0gradientofmaximum\\nlikelihood(,).ThespecialcaseofNCEwherethenoisesamples Goodfellow2014\\nare\\xa0thosegenerated\\xa0by\\xa0themodel\\xa0suggests\\xa0thatmaximumlikelihood\\xa0can\\xa0be\\ninterpretedasaprocedurethatforcesamodeltoconstantlylearntodistinguish\\nrealityfromitsownevolvingbeliefs,whilenoisecontrastiveestimationachieves\\nsomereducedcomputational costbyonlyforcingthemodeltodistinguishreality\\nfromaﬁxedbaseline(thenoisemodel).\\nUsingthesupervisedtaskofclassifyingbetweentrainingsamplesandgenerated\\nsamples(withthemodelenergyfunctionusedindeﬁningtheclassiﬁer)toprovide\\nagradientonthemodelwasintroducedearlierinvariousforms(Welling e t a l .,\\n2003bBengio2009;,).\\n622', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6e4689f-cb85-46fa-851f-96e618b84a2d', embedding=None, metadata={'page_label': '638', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nNoise\\xa0contrastiveestimation\\xa0is\\xa0basedon\\xa0the\\xa0idea that\\xa0agood\\xa0generative\\nmodelshould\\xa0be\\xa0abletodistinguish\\xa0datafromnoise.Aclosely\\xa0relatedidea\\nisthat\\xa0agood\\xa0generativemodelshould\\xa0beabletogenerate\\xa0samples thatno\\nclassiﬁercandistinguishfromdata.Thisideayieldsgenerativeadversarialnetworks\\n(section).20.10.4\\n18.7EstimatingthePartitionFunction\\nWhilemuchofthischapterisdedicatedtodescribingmethodsthatavoidneeding\\ntocomputetheintractablepartitionfunction Z( θ)associatedwithanundirected\\ngraphicalmodel,inthissectionwediscussseveralmethodsfordirectlyestimating\\nthepartitionfunction.\\nEstimatingthepartitionfunctioncanbeimportantbecausewerequireitif\\nwewishtocomputethenormalizedlikelihoodofdata.Thisisoftenimportantin\\ne v a l u a t i ngthemodel,monitoringtrainingperformance,andcomparingmodelsto\\neachother.\\nForexample,imaginewehavetwomodels:model M Adeﬁningaprobabil-\\nitydistribution p A(x; θ A)=1\\nZ A˜ p A(x; θ A)andmodelM Bdeﬁningaprobability\\ndistribution p B(x; θ B)=1\\nZ B˜ p B(x; θ B).Acommonwaytocomparethemodels\\nistoevaluateandcomparethelikelihoodthatbothmodelsassigntoani.i.d.\\ntestdataset.Supposethetestsetconsistsof mexamples { x( 1 ), . . . , x( ) m}.If\\ue051\\ni p A(x( ) i; θ A) >\\ue051\\ni p B(x( ) i; θ B)orequivalentlyif\\n\\ue058\\nilog p A(x( ) i; θ A)−\\ue058\\nilog p B(x( ) i; θ B) 0 > ,(18.38)\\nthenwesaythatM AisabettermodelthanM B(or,atleast,itisabettermodel\\nofthetestset),inthesensethatithasabettertestlog-likelihood.Unfortunately,\\ntestingwhetherthisconditionholdsrequiresknowledgeofthepartitionfunction.\\nUnfortunately,equationseemstorequireevaluatingthelogprobabilitythat 18.38\\nthemodelassignstoeachpoint,whichinturnrequiresevaluatingthepartition\\nfunction.Wecansimplifythesituationslightlybyre-arrangingequation18.38\\nintoaformwhereweneedtoknowonlythe r at i oofthetwomodel’spartition\\nfunctions:\\n\\ue058\\nilog p A(x( ) i; θ A)−\\ue058\\nilog p B(x( ) i; θ B) =\\ue058\\ni\\ue020\\nlog˜ p A(x( ) i; θ A)\\n˜ p B(x( ) i; θ B)\\ue021\\n− mlogZ( θ A)\\nZ( θ B).\\n(18.39)\\n623', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='57e8214f-ba37-4f6d-bb1e-b16ebb47203b', embedding=None, metadata={'page_label': '639', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nWecanthusdeterminewhether M AisabettermodelthanM Bwithoutknowing\\nthepartitionfunctionofeithermodelbutonlytheirratio.Aswewillseeshortly,\\nwecanestimatethisratiousingimportancesampling,providedthatthetwomodels\\naresimilar.\\nIf,however,wewantedtocomputetheactualprobabilityofthetestdataunder\\neither M AorM B,wewouldneedtocomputetheactualvalueofthepartition\\nfunctions.Thatsaid,ifweknewtheratiooftwopartitionfunctions, r=Z ( θ B )\\nZ ( θ A ),\\nandweknewtheactualvalueofjustoneofthetwo,say Z( θ A),wecouldcompute\\nthevalueoftheother:\\nZ( θ B) = ( r Z θ A) =Z( θ B)\\nZ( θ A)Z( θ A) . (18.40)\\nAsimplewaytoestimatethe\\xa0partition functionistouse\\xa0aMonteCarlo\\nmethodsuchassimpleimportancesampling.Wepresenttheapproachinterms\\nofcontinuousvariablesusingintegrals,butitcanbereadilyappliedtodiscrete\\nvariablesbyreplacingtheintegralswithsummation.Weuseaproposaldistribution\\np 0(x)=1\\nZ0˜ p 0(x)whichsupportstractablesamplingandtractableevaluationof\\nboththepartitionfunction Z 0andtheunnormalized distribution˜ p 0()x.\\nZ 1=\\ue05a\\n˜ p 1()x dx (18.41)\\n=\\ue05ap 0()x\\np 0()x˜ p 1()x dx (18.42)\\n= Z 0\\ue05a\\np 0()x˜ p 1()x\\n˜ p 0()xdx (18.43)\\nˆ Z 1=Z 0\\nKK\\ue058\\nk = 1˜ p 1(x( ) k)\\n˜ p 0(x( ) k)st: . .x( ) k∼ p 0 (18.44)\\nInthelastline,wemakeaMonteCarloestimator,ˆ Z 1,oftheintegralusingsamples\\ndrawnfrom p 0(x)andthenweighteachsamplewiththeratiooftheunnormalized\\n˜ p 1andtheproposal p 0.\\nWeseealsothatthisapproachallowsustoestimatetheratiobetweenthe\\npartitionfunctionsas\\n1\\nKK \\ue058\\nk = 1˜ p 1(x( ) k)\\n˜ p 0(x( ) k)st: . .x( ) k∼ p 0 . (18.45)\\nThisvaluecanthenbeuseddirectlytocomparetwomodelsasdescribedin\\nequation.18.39\\n624', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a7d1aae7-97ed-45cd-a203-cfc8a320d77a', embedding=None, metadata={'page_label': '640', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nIfthedistribution p 0iscloseto p 1,equationcanbeaneﬀectivewayof 18.44\\nestimatingthepartitionfunction(Minka2005,).Unfortunately,mostofthetime\\np 1isbothcomplicated(usuallymultimodal)anddeﬁnedoverahighdimensional\\nspace.Itisdiﬃculttoﬁndatractable p 0thatissimpleenoughtoevaluatewhile\\nstillbeingcloseenoughto p 1toresultinahighqualityapproximation.If p 0and\\np 1arenotclose,mostsamplesfrom p 0willhavelowprobabilityunder p 1and\\nthereforemake(relatively)negligiblecontributiontothesuminequation.18.44\\nHavingfewsamples\\xa0withsigniﬁcantweightsinthis\\xa0sumwillresultinan\\nestimatorthatisofpoorqualityduetohighvariance.\\xa0This canbeunderstood\\nquantitativelythroughanestimateofthevarianceofourestimate ˆ Z 1:\\nˆVar\\ue010\\nˆ Z 1\\ue011\\n=Z 0\\nK2K \\ue058\\nk = 1\\ue020\\n˜ p 1(x( ) k)\\n˜ p 0(x( ) k)−ˆ Z 1\\ue0212\\n. (18.46)\\nThisquantityislargestwhenthereissigniﬁcantdeviationinthevaluesofthe\\nimportanceweights˜ p1 ( x() k)\\n˜ p0 ( x() k ).\\nWenowturntotworelatedstrategiesdevelopedtocopewiththechalleng-\\ningtaskofestimatingpartitionfunctionsforcomplexdistributionsoverhigh-\\ndimensionalspaces:\\xa0annealedimportancesamplingandbridgesampling.Both\\nstartwiththesimpleimportancesamplingstrategyintroducedaboveandboth\\nattempttoovercometheproblemoftheproposal p 0beingtoofarfrom p 1by\\nintroducingintermediatedistributionsthatattemptto between b r i d g e t h e g a p p 0\\nand p 1.\\n1 8 . 7 . 1 A n n ea l ed Im p o rt a n ce S a m p l i n g\\nInsituationswhere D K L( p 0\\ue06b p 1)islarge(i.e.,wherethereislittleoverlapbetween\\np 0and p 1),astrategycalled annealed i m p o r t anc e sampling(AIS)attempts\\ntobridgethegapbyintroducingintermediate distributions(,;, Jarzynski1997Neal\\n2001).Considerasequenceofdistributions p η0 , . . . , p η n,with 0 = η 0 < η 1 < <···\\nη n − 1 < η n= 1sothattheﬁrstandlastdistributionsinthesequenceare p 0and p 1\\nrespectively.\\nThisapproachallowsustoestimatethepartitionfunctionofamultimodal\\ndistributiondeﬁnedoverahigh-dimensionalspace(suchasthedistributiondeﬁned\\nbyatrainedRBM).Webeginwithasimplermodelwithaknownpartitionfunction\\n(suchasanRBMwithzeroesforweights)andestimatetheratiobetweenthetwo\\nmodel’spartitionfunctions.\\xa0Theestimateofthisratioisbasedontheestimate\\noftheratiosofasequenceofmanysimilardistributions,suchasthesequenceof\\nRBMswithweightsinterpolatingbetweenzeroandthelearnedweights.\\n625', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7003ebf1-a488-4e06-9edd-037e68c3e965', embedding=None, metadata={'page_label': '641', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nWecannowwritetheratioZ1\\nZ0as\\nZ 1\\nZ 0=Z 1\\nZ 0Z η1\\nZ η1···Z η n −1\\nZ η n −1(18.47)\\n=Z η1\\nZ 0Z η2\\nZ η1···Z η n −1\\nZ η n −2Z 1\\nZ η n −1(18.48)\\n=n − 1\\ue059\\nj = 0Z η j+1\\nZ η j(18.49)\\nProvidedthedistributions p η jand p η j + 1,forall0≤≤− j n1,aresuﬃciently\\nclose,wecanreliablyestimateeachofthefactorsZ η j+1\\nZ η jusingsimpleimportance\\nsamplingandthenusethesetoobtainanestimateofZ1\\nZ0.\\nWheredotheseintermediatedistributionscomefrom?Justastheoriginal\\nproposaldistribution p 0isadesignchoice,soisthesequenceofdistributions\\np η1 . . . p η n −1.Thatis,itcanbespeciﬁcallyconstructedtosuittheproblemdomain.\\nOnegeneral-purposeandpopularchoicefortheintermediate distributionsisto\\nusetheweightedgeometricaverageofthetargetdistribution p 1andthestarting\\nproposaldistribution(forwhichthepartitionfunctionisknown) p 0:\\np η j∝ pη j\\n1 p1 − η j\\n0 (18.50)\\nInordertosamplefromtheseintermediate distributions,wedeﬁneaseriesof\\nMarkovchaintransitionfunctions T η j( x\\ue030| x) thatdeﬁnetheconditionalprobability\\ndistributionoftransitioningto x\\ue030givenwearecurrentlyat x.Thetransition\\noperator T η j( x\\ue030| x)isdeﬁnedtoleave p η j() xinvariant:\\np η j() = x\\ue05a\\np η j( x\\ue030) T η j( x x|\\ue030) d x\\ue030(18.51)\\nThesetransitionsmaybeconstructedasanyMarkovchainMonteCarlomethod\\n(e.g.,Metropolis-Hastings,Gibbs),includingmethodsinvolvingmultiplepasses\\nthroughalloftherandomvariablesorotherkindsofiterations.\\nTheAISsamplingstrategyisthentogeneratesamplesfrom p 0andthenuse\\nthetransitionoperatorstosequentiallygeneratesamplesfromtheintermediate\\ndistributionsuntilwearriveatsamplesfromthetargetdistribution p 1:\\n•for k . . . K = 1\\n–Sample x( ) k\\nη1∼ p 0()x\\n626', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='59af49e9-0e99-4b7a-ab65-0eae734a9cf1', embedding=None, metadata={'page_label': '642', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\n–Sample x( ) k\\nη2∼ T η1(x( ) k\\nη2| x( ) k\\nη1)\\n–...\\n–Sample x( ) k\\nη n −1∼ T η n −2(x( ) k\\nη n −1| x( ) k\\nη n −2)\\n–Sample x( ) k\\nη n∼ T η n −1(x( ) k\\nη n| x( ) k\\nη n −1)\\n•end\\nForsample k,wecanderivetheimportanceweightbychainingtogetherthe\\nimportanceweightsforthejumpsbetweentheintermediatedistributionsgivenin\\nequation:18.49\\nw( ) k=˜ p η1( x( ) k\\nη1)\\n˜ p 0( x( ) k\\nη1)˜ p η2( x( ) k\\nη2)\\n˜ p η1( x( ) k\\nη2). . .˜ p 1( x( ) k\\n1)\\n˜ p η n −1( x( ) k\\nη n). (18.52)\\nToavoidnumericalissuessuchasoverﬂow,itisprobablybesttocompute log w( ) kby\\naddingandsubtractinglogprobabilities, ratherthancomputing w( ) kbymultiplying\\nanddividingprobabilities.\\nWiththesamplingprocedurethusdeﬁnedandtheimportanceweightsgiven\\ninequation,theestimateoftheratioofpartitionfunctionsisgivenby: 18.52\\nZ 1\\nZ 0≈1\\nKK\\ue058\\nk = 1w( ) k(18.53)\\nInordertoverifythatthisproceduredeﬁnesavalidimportancesampling\\nscheme,wecanshow(,)thattheAISprocedurecorrespondstosimple Neal2001\\nimportancesamplingonanextendedstatespacewithpointssampledoverthe\\nproductspace [ x η1 , . . . , x η n −1 , x 1].Todothis,wedeﬁnethedistributionoverthe\\nextendedspaceas:\\n˜ p( x η1 , . . . , x η n −1 , x 1) (18.54)\\n=˜ p 1( x 1)˜ T η n −1( x η n −1| x 1)˜ T η n −2( x η n −2| x η n −1) . . .˜ T η1( x η1| x η2) ,(18.55)\\nwhere ˜ T aisthereverseofthetransitionoperatordeﬁnedby T a(viaanapplication\\nofBayes’rule):\\n˜ T a( x\\ue030| x) =p a( x\\ue030)\\np a() xT a( x x|\\ue030) =˜ p a( x\\ue030)\\n˜ p a() xT a( x x|\\ue030) .(18.56)\\nPluggingtheaboveintotheexpressionforthejointdistributionontheextended\\nstatespacegiveninequation,weget:18.55\\n˜ p( x η1 , . . . , x η n −1 , x 1) (18.57)\\n627', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2d161cdb-aac9-4afd-b2e0-6ea77dbbe5a9', embedding=None, metadata={'page_label': '643', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\n=˜ p 1( x 1)˜ p η n −1( x η n −1)\\n˜ p η n −1( x 1)T η n −1( x 1| x η n −1)n − 2\\ue059\\ni = 1˜ p η i( x η i)\\n˜ p η i( x η i+1)T η i( x η i+1| x η i)\\n(18.58)\\n=˜ p 1( x 1)\\n˜ p η n −1( x 1)T η n −1( x 1| x η n −1)˜ p η1( x η1)n − 2 \\ue059\\ni = 1˜ p η i+1( x η i+1)\\n˜ p η i( x η i+1)T η i( x η i+1| x η i) .\\n(18.59)\\nWenowhavemeansofgeneratingsamplesfromthejointproposaldistribution\\nqovertheextendedsampleviaasamplingschemegivenabove,withthejoint\\ndistributiongivenby:\\nq( x η1 , . . . , x η n −1 , x 1) = p 0( x η1) T η1( x η2| x η1) . . . T η n −1( x 1| x η n −1) .(18.60)\\nWehaveajointdistributionontheextendedspacegivenbyequation.Taking18.59\\nq( x η1 , . . . , x η n −1 , x 1)astheproposaldistributionontheextendedstatespacefrom\\nwhichwewilldrawsamples,itremainstodeterminetheimportanceweights:\\nw( ) k=˜ p( x η1 , . . . , x η n −1 , x 1)\\nq( x η1 , . . . , x η n −1 , x 1)=˜ p 1( x( ) k\\n1)\\n˜ p η n −1( x( ) k\\nη n −1). . .˜ p η2( x( ) k\\nη2)\\n˜ p 1( x( ) k\\nη1)˜ p η1( x( ) k\\nη1)\\n˜ p 0( x( ) k\\n0).(18.61)\\nTheseweightsarethesameasproposedforAIS.ThuswecaninterpretAISas\\nsimpleimportancesamplingappliedtoanextendedstateanditsvalidityfollows\\nimmediatelyfromthevalidityofimportancesampling.\\nAnnealedimportancesampling(AIS)wasﬁrstdiscoveredby () Jarzynski1997\\nandthenagain,independently,by().Itiscurrentlythemostcommon Neal2001\\nwayofestimatingthepartitionfunctionforundirectedprobabilisticmodels.The\\nreasonsforthismayhavemoretodowiththepublicationofaninﬂuentialpaper\\n(SalakhutdinovandMurray2008,)describingitsapplicationtoestimatingthe\\npartitionfunctionofrestrictedBoltzmannmachinesanddeepbeliefnetworksthan\\nwithanyinherentadvantagethemethodhasovertheothermethoddescribed\\nbelow.\\nAdiscussionofthepropertiesoftheAISestimator(e.g..itsvarianceand\\neﬃciency)canbefoundin().Neal2001\\n1 8 . 7 . 2 B ri d g e S a m p l i n g\\nBridgesampling ()isanothermethodthat,likeAIS,addressesthe Bennett1976\\nshortcomingsofimportancesampling.Ratherthanchainingtogetheraseriesof\\n628', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e64472d2-16c1-4c80-95ba-0d3a302ed295', embedding=None, metadata={'page_label': '644', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nintermediatedistributions,bridgesamplingreliesonasingledistribution p ∗,known\\nasthebridge,tointerpolatebetweenadistributionwithknownpartitionfunction,\\np 0,andadistribution p 1forwhichwearetryingtoestimatethepartitionfunction\\nZ 1.\\nBridgesamplingestimatestheratio Z 1 / Z 0astheratiooftheexpectedimpor-\\ntanceweightsbetween˜ p 0and˜ p ∗andbetween˜ p 1and˜ p ∗:\\nZ 1\\nZ 0≈K \\ue058\\nk = 1˜ p ∗( x( ) k\\n0)\\n˜ p 0( x( ) k\\n0)\\ue02cK \\ue058\\nk = 1˜ p ∗( x( ) k\\n1)\\n˜ p 1( x( ) k\\n1)(18.62)\\nIfthebridgedistribution p ∗ischosencarefullytohavealargeoverlapofsupport\\nwithboth p 0and p 1,thenbridgesamplingcanallowthedistancebetweentwo\\ndistributions(ormoreformally, D K L( p 0\\ue06b p 1))tobemuchlargerthanwithstandard\\nimportancesampling.\\nItcanbeshownthattheoptimalbridgingdistributionisgivenby p( ) op t\\n∗(x)∝\\n˜ p0 ( ) ˜ x p1 ( ) x\\nr ˜ p0 ( ) + ˜ x p1 ( ) xwhere r= Z 1 / Z 0.Atﬁrst,thisappearstobeanunworkablesolution\\nasitwouldseemtorequiretheveryquantitywearetryingtoestimate, Z 1 / Z 0.\\nHowever,itispossibletostartwithacoarseestimateof randusetheresulting\\nbridgedistributiontoreﬁneourestimateiteratively(,).Thatis,we Neal2005\\niterativelyre-estimatetheratioanduseeachiterationtoupdatethevalueof. r\\nL i nk e d i m p o r t anc e samplingBothAISandbridgesamplinghavetheirad-\\nvantages.If D K L( p 0\\ue06b p 1)isnottoolarge(because p 0and p 1aresuﬃcientlyclose)\\nbridgesamplingcanbeamoreeﬀectivemeansofestimatingtheratioofpartition\\nfunctionsthanAIS.If,however,thetwodistributionsaretoofarapartforasingle\\ndistribution p ∗tobridgethegapthenonecanatleastuseAISwithpotentially\\nmanyintermediate distributionstospanthedistancebetween p 0and p 1.Neal\\n()showedhowhislinkedimportancesamplingmethodleveragedthepowerof 2005\\nthebridgesamplingstrategytobridgetheintermediatedistributionsusedinAIS\\ntosigniﬁcantlyimprovetheoverallpartitionfunctionestimates.\\nE st i m at i n g t he par t i t i o n f unc t i o n whi l e t r ai ni ngWhileAIShasbecome\\nacceptedasthestandardmethodforestimatingthepartitionfunctionformany\\nundirectedmodels,\\xa0itissuﬃcientlycomputationally intensivethatitremains\\ninfeasibletouseduringtraining.However,alternativestrategiesthathavebeen\\nexploredtomaintainanestimateofthepartitionfunctionthroughouttraining\\nUsingacombinationofbridgesampling,short-chainAISandparalleltempering,\\nDesjardins2011 e t a l .()devisedaschemetotrackthepartitionfunctionofan\\n629', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d0b2d646-af27-4463-a50d-cdd2297cf1d9', embedding=None, metadata={'page_label': '645', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION\\nRBMthroughoutthetrainingprocess.Thestrategyisbasedonthemaintenanceof\\nindependentestimatesofthepartitionfunctionsoftheRBMateverytemperature\\noperatingintheparalleltemperingscheme.Theauthorscombinedbridgesampling\\nestimatesoftheratiosofpartitionfunctionsofneighboringchains(i.e.from\\nparalleltempering)withAISestimatesacrosstimetocomeupwithalowvariance\\nestimateofthepartitionfunctionsateveryiterationoflearning.\\nThetoolsdescribedinthischapterprovidemanydiﬀerentwaysofovercoming\\ntheproblemofintractablepartitionfunctions,buttherecanbeseveralother\\ndiﬃcultiesinvolvedintrainingandusinggenerativemodels.Foremostamongthese\\nistheproblemofintractableinference,whichweconfrontnext.\\n630', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e95fd62c-e5f3-4d07-aa47-f61eba2f31d3', embedding=None, metadata={'page_label': '646', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 1 9\\nApproximateInference\\nManyprobabilisticmodelsarediﬃculttotrainbecauseitisdiﬃculttoperform\\ninferenceinthem.Inthecontextofdeeplearning,weusuallyhaveasetofvisible\\nvariables vandasetoflatentvariables h.Thechallengeofinferenceusually\\nreferstothediﬃcultproblemofcomputing p( h v|)ortakingexpectationswith\\nrespecttoit.Suchoperationsareoftennecessaryfortaskslikemaximumlikelihood\\nlearning.\\nManysimplegraphicalmodelswithonlyonehiddenlayer,suchasrestricted\\nBoltzmannmachinesandprobabilisticPCA,aredeﬁnedinawaythatmakes\\ninferenceoperationslikecomputing p( h v|),ortakingexpectationswithrespect\\ntoit,simple.Unfortunately,mostgraphicalmodelswithmultiplelayersofhidden\\nvariableshaveintractableposteriordistributions.Exactinferencerequiresan\\nexponentialamountoftimeinthesemodels.Evensomemodelswithonlyasingle\\nlayer,suchassparsecoding,havethisproblem.\\nInthischapter,weintroduceseveralofthetechniquesforconfrontingthese\\nintractableinferenceproblems.Later,inchapter,wewilldescribehowtouse 20\\nthesetechniquestotrainprobabilisticmodelsthatwouldotherwisebeintractable,\\nsuchasdeepbeliefnetworksanddeepBoltzmannmachines.\\nIntractableinferenceproblemsindeeplearningusuallyarisefrominteractions\\nbetweenlatentvariablesinastructuredgraphicalmodel.Seeﬁgureforsome19.1\\nexamples.Theseinteractionsmaybeduetodirectinteractionsinundirected\\nmodelsor“explainingaway”interactionsbetweenmutualancestorsofthesame\\nvisibleunitindirectedmodels.\\n631', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f231138f-33b9-4793-b38e-3ef541e553a5', embedding=None, metadata={'page_label': '647', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nFigure19.1:Intractableinferenceproblemsindeeplearningareusuallytheresultof\\ninteractionsbetweenlatentvariablesinastructuredgraphicalmodel.Thesecanbe\\nduetoedgesdirectlyconnectingonelatentvariabletoanother,orduetolongerpaths\\nthatareactivatedwhenthechildofaV-structureisobserved. ( L e f t )Asemi-restricted\\nBoltzmannmachine( ,)withconnectionsbetweenhidden OsinderoandHinton2008\\nunits.Thesedirectconnectionsbetweenlatentvariablesmaketheposteriordistribution\\nintractableduetolargecliquesoflatentvariables.AdeepBoltzmannmachine, ( C e n t e r )\\norganizedintolayersofvariableswithoutintra-layerconnections,stillhasanintractable\\nposteriordistributionduetotheconnectionsbetweenlayers.Thisdirectedmodel ( R i g h t )\\nhasinteractionsbetweenlatentvariableswhenthevisiblevariablesareobserved,because\\neverytwolatentvariablesareco-parents.Someprobabilisticmodelsareabletoprovide\\ntractableinferenceoverthelatentvariablesdespitehavingoneofthegraphstructures\\ndepictedabove.Thisispossibleiftheconditionalprobabilitydistributionsarechosento\\nintroduceadditionalindependencesbeyondthosedescribedbythegraph.Forexample,\\nprobabilisticPCAhasthegraphstructureshownintheright,yetstillhassimpleinference\\nduetospecialpropertiesofthespeciﬁcconditionaldistributionsituses(linear-Gaussian\\nconditionalswithmutuallyorthogonalbasisvectors).\\n6 3 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='143c5147-1c6f-4aa6-854c-62e7a808e765', embedding=None, metadata={'page_label': '648', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\n19.1InferenceasOptimization\\nManyapproachestoconfrontingtheproblemofdiﬃcultinferencemakeuseof\\ntheobservationthatexactinferencecanbedescribedasanoptimization problem.\\nApproximateinferencealgorithmsmaythenbederivedbyapproximatingthe\\nunderlyingoptimization problem.\\nToconstructtheoptimization problem,assumewehaveaprobabilisticmodel\\nconsistingofobservedvariables vandlatentvariables h.Wewouldliketocompute\\nthelogprobabilityoftheobserveddata, log p( v; θ).Sometimesitistoodiﬃcult\\ntocompute log p( v; θ)ifitiscostlytomarginalizeout h.Instead,wecancompute\\nalowerbound L( v θ , , q)onlog p( v; θ).Thisboundiscalledtheevidencelower\\nbound(ELBO).Anothercommonlyusednameforthislowerboundisthenegative\\nvariationalfreeenergy.Speciﬁcally,theevidencelowerboundisdeﬁnedtobe\\nL − ( ) = log(;) v θ , , q p v θ D K L(( )( ;)) q h v|\\ue06b p h v| θ(19.1)\\nwhereisanarbitraryprobabilitydistributionover. q h\\nBecausethediﬀerencebetweenlog p( v)and L( v θ , , q)isgivenbytheKL\\ndivergenceandbecausetheKLdivergenceisalwaysnon-negative,wecanseethat\\nLalwayshasatmostthesamevalueasthedesiredlogprobability.Thetwoare\\nequalifandonlyifisthesamedistributionas. q p( ) h v|\\nSurprisingly,Lcanbeconsiderablyeasiertocomputeforsomedistributions q.\\nSimplealgebrashowsthatwecanrearrange Lintoamuchmoreconvenientform:\\nL − ( ) =log(;) v θ , , q p v θ D K L(( )( ;)) q h v|\\ue06b p h v| θ (19.2)\\n=log(;) p v θ− E h∼ qlogq( ) h v|\\np( ) h v|(19.3)\\n=log(;) p v θ− E h∼ qlogq( ) h v|\\np , ( h v θ ; )\\np ( ; ) v θ(19.4)\\n=log(;) p v θ− E h∼ q[log( )log(;)+log(;)] q h v|− p h v , θ p v θ(19.5)\\n=− E h∼ q[log( )log(;)] q h v|− p h v , θ . (19.6)\\nThisyieldsthemorecanonicaldeﬁnitionoftheevidencelowerbound,\\nL( ) = v θ , , q E h∼ q[log( )]+() p h v , H q . (19.7)\\nForanappropriatechoiceof q,Listractabletocompute.Foranychoice\\nof q,Lprovidesalowerboundonthelikelihood.For q( h v|)thatarebetter\\n6 3 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3d5c3b41-0ecd-4702-8daf-1fdf7b44e9d4', embedding=None, metadata={'page_label': '649', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\napproximationsof p( h v|),thelowerbound Lwillbetighter,inotherwords,\\nclosertolog p( v).\\xa0When q( h v|)= p( h v|),theapproximation isperfect,and\\nL( ) = log(;) v θ , , q p v θ .\\nWecanthusthinkofinferenceastheprocedureforﬁndingthe qthatmaximizes\\nL.ExactinferencemaximizesLperfectlybysearchingoverafamilyoffunctions\\nqthatincludes p( h v|).Throughoutthischapter,wewillshowhowtoderive\\ndiﬀerentformsofapproximateinferencebyusingapproximateoptimization to\\nﬁnd q.Wecanmaketheoptimization procedurelessexpensivebutapproximate\\nbyrestrictingthefamilyofdistributions qtheoptimization isallowedtosearch\\noverorbyusinganimperfectoptimization procedurethatmaynotcompletely\\nmaximizebutmerelyincreaseitbyasigniﬁcantamount. L\\nNomatterwhatchoiceof qweuse,Lisalowerbound.Wecangettighter\\norlooserboundsthatarecheaperormoreexpensivetocomputedependingon\\nhowwechoosetoapproachthisoptimization problem.\\xa0Wecanobtainapoorly\\nmatched qbutreducethecomputational costbyusinganimperfectoptimization\\nprocedure,orbyusingaperfectoptimization procedureoverarestrictedfamilyof\\nqdistributions.\\n19.2ExpectationMaximization\\nTheﬁrstalgorithmweintroducebasedonmaximizingalowerbound Listhe\\nexpectationmaximization(EM)algorithm,apopulartrainingalgorithmfor\\nmodelswithlatentvariables.WedescribehereaviewontheEMalgorithm\\ndevelopedby ().Unlikemostoftheotheralgorithmswe NealandHinton1999\\ndescribeinthischapter,EMisnotanapproachtoapproximateinference,but\\nratheranapproachtolearningwithanapproximate posterior.\\nTheEMalgorithmconsistsofalternatingbetweentwostepsuntilconvergence:\\n•TheE-step(Expectationstep):Let θ( 0 )denotethevalueoftheparameters\\natthebeginningofthestep.Set q( h( ) i| v)= p( h( ) i| v( ) i; θ( 0 ))forall\\nindices iofthetrainingexamples v( ) iwewanttotrainon(bothbatchand\\nminibatchvariantsarevalid).Bythiswemean qisdeﬁnedintermsofthe\\nc u r r e ntparametervalueof θ( 0 );ifwevary θthen p( h v|; θ)willchangebut\\nq p ( ) h v|willremainequalto( ; h v| θ( 0 )).\\n•The (Maximization step):Completelyorpartiallymaximize M-step\\n\\ue058\\niL( v( ) i, , q θ) (19.8)\\n6 3 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f6ac663a-5a27-40b8-ae8a-0c55bfd23fc3', embedding=None, metadata={'page_label': '650', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nwithrespecttousingyouroptimization algorithmofchoice. θ\\nThiscanbeviewedasacoordinateascentalgorithmtomaximize L.Onone\\nstep,wemaximize Lwithrespectto q,andontheother,wemaximize Lwith\\nrespectto. θ\\nStochasticgradientascentonlatentvariablemodelscanbeseenasaspecial\\ncaseoftheEMalgorithmwheretheMstepconsistsoftakingasinglegradient\\nstep.OthervariantsoftheEMalgorithmcanmakemuchlargersteps.Forsome\\nmodelfamilies,theMstepcanevenbeperformedanalytically,jumpingallthe\\nwaytotheoptimalsolutionforgiventhecurrent. θ q\\nEventhoughtheE-stepinvolvesexactinference,wecanthinkoftheEM\\nalgorithmasusingapproximate inferenceinsomesense.Speciﬁcally,theM-step\\nassumesthatthesamevalueof qcanbeusedforallvaluesof θ.Thiswillintroduce\\nagapbetweenLandthetruelog p( v)astheM-stepmovesfurtherandfurther\\nawayfromthevalue θ( 0 )usedintheE-step.Fortunately,theE-stepreducesthe\\ngaptozeroagainasweentertheloopforthenexttime.\\nTheEMalgorithmcontainsafewdiﬀerentinsights.First,thereisthebasic\\nstructureofthelearningprocess,inwhichweupdatethemodelparametersto\\nimprovethelikelihoodofacompleteddataset,whereallmissingvariableshave\\ntheirvaluesprovidedbyanestimateoftheposteriordistribution.Thisparticular\\ninsightisnotuniquetotheEMalgorithm.Forexample,usinggradientdescentto\\nmaximizethelog-likelihoodalsohasthissameproperty;thelog-likelihoodgradient\\ncomputationsrequiretakingexpectationswithrespecttotheposteriordistribution\\noverthehiddenunits.\\xa0AnotherkeyinsightintheEMalgorithmisthatwecan\\ncontinuetouseonevalueof qevenafterwehavemovedtoadiﬀerentvalueof θ.\\nThisparticularinsightisusedthroughoutclassicalmachinelearningtoderivelarge\\nM-stepupdates.Inthecontextofdeeplearning,mostmodelsaretoocomplex\\ntoadmitatractablesolutionforanoptimallargeM-stepupdate,sothissecond\\ninsightwhichismoreuniquetotheEMalgorithmisrarelyused.\\n19.3MAPInferenceandSparseCoding\\nWeusuallyusetheterminferencetorefertocomputingtheprobabilitydistribution\\noveronesetofvariablesgivenanother.Whentrainingprobabilisticmodelswith\\nlatentvariables,weareusuallyinterestedincomputing p( h v|).Analternative\\nformofinferenceistocomputethesinglemostlikelyvalueofthemissingvariables,\\nratherthantoinfertheentiredistributionovertheirpossiblevalues.Inthecontext\\n6 3 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d0816054-96be-48c3-a5c3-a31a3134ce52', embedding=None, metadata={'page_label': '651', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\noflatentvariablemodels,thismeanscomputing\\nh∗= argmax\\nhp . ( ) h v| (19.9)\\nThisisknownasmaximumaposterioriinference,abbreviatedMAPinference.\\nMAPinferenceisusuallynotthoughtofasapproximate inference—it does\\ncomputetheexactmostlikelyvalueof h∗.However,ifwewishtodevelopa\\nlearningprocessbasedonmaximizing L( v h , , q),thenitishelpfultothinkofMAP\\ninferenceasaprocedurethatprovidesavalueof q.Inthissense,wecanthinkof\\nMAPinferenceasapproximateinference,becauseitdoesnotprovidetheoptimal\\nq.\\nRecallfromsectionthatexactinferenceconsistsofmaximizing 19.1\\nL( ) = v θ , , q E h∼ q[log( )]+() p h v , H q (19.10)\\nwithrespectto qoveranunrestrictedfamilyofprobabilitydistributions,using\\nanexactoptimization algorithm.WecanderiveMAPinferenceasaformof\\napproximateinferencebyrestrictingthefamilyofdistributions qmaybedrawn\\nfrom.Speciﬁcally,werequiretotakeonaDiracdistribution: q\\nq δ . ( ) = h v| ( ) h µ− (19.11)\\nThismeansthatwecannowcontrol qentirelyvia µ.DroppingtermsofLthat\\ndonotvarywith,weareleftwiththeoptimization problem µ\\nµ∗= argmax\\nµlog(= ) p h µ v , , (19.12)\\nwhichisequivalenttotheMAPinferenceproblem\\nh∗= argmax\\nhp . ( ) h v| (19.13)\\nWecanthusjustifyalearningproceduresimilartoEM,inwhichwealternate\\nbetweenperformingMAPinferencetoinfer h∗andthenupdate θtoincrease\\nlog p( h∗, v).AswithEM,thisisaformofcoordinateascentonL,wherewe\\nalternatebetweenusing\\xa0inference to\\xa0optimize Lwithrespect\\xa0to qandusing\\nparameterupdatestooptimize Lwithrespectto θ.Theprocedureasawholecan\\nbejustiﬁedbythefactthatLisalowerboundonlog p( v).InthecaseofMAP\\ninference,thisjustiﬁcationisrathervacuous,becausetheboundisinﬁnitelyloose,\\nduetotheDiracdistribution’sdiﬀerentialentropyofnegativeinﬁnity.However,\\naddingnoisetowouldmaketheboundmeaningfulagain. µ\\n6 3 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fcdc87b4-0643-4171-b68c-db65d9af38b6', embedding=None, metadata={'page_label': '652', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nMAPinferenceiscommonlyusedindeeplearningasbothafeatureextractor\\nandalearningmechanism.Itisprimarilyusedforsparsecodingmodels.\\nRecallfromsectionthatsparsecodingisalinearfactormodelthatimposes 13.4\\nasparsity-inducingprioronitshiddenunits.AcommonchoiceisafactorialLaplace\\nprior,with\\np h( i) =λ\\n2e−| λ h i|. (19.14)\\nThevisibleunitsarethengeneratedbyperformingalineartransformationand\\naddingnoise:\\np , β ( ) = (; + x h| N v W h b− 1I) . (19.15)\\nComputingorevenrepresenting p( h v|)isdiﬃcult.Everypairofvariables h i\\nand h jarebothparentsof v.Thismeansthatwhen visobserved,thegraphical\\nmodelcontainsanactivepathconnecting h iand h j.Allofthehiddenunitsthus\\nparticipateinonemassivecliquein p( h v|).IfthemodelwereGaussianthen\\ntheseinteractionscouldbemodeledeﬃcientlyviathecovariancematrix,butthe\\nsparsepriormakestheseinteractionsnon-Gaussian.\\nBecause p( h v|)isintractable,soisthecomputationofthelog-likelihoodand\\nitsgradient.Wethuscannotuseexactmaximumlikelihoodlearning.Instead,we\\nuseMAPinferenceandlearntheparametersbymaximizingtheELBOdeﬁnedby\\ntheDiracdistributionaroundtheMAPestimateof. h\\nIfweconcatenateallofthe hvectorsinthetrainingsetintoamatrix H,and\\nconcatenateallofthevectorsintoamatrix,thenthesparsecodinglearning v V\\nprocessconsistsofminimizing\\nJ ,( H W) =\\ue058\\ni , j| H i , j|+\\ue058\\ni , j\\ue010\\nV H W−\\ue03e\\ue0112\\ni , j.(19.16)\\nMostapplicationsofsparsecodingalsoinvolveweightdecayoraconstrainton\\nthenormsofthecolumnsof W,inordertopreventthepathologicalsolutionwith\\nextremelysmallandlarge. H W\\nWecanminimize Jbyalternatingbetweenminimization withrespectto H\\nandminimization withrespectto W.Bothsub-problemsareconvex.Infact,\\ntheminimization withrespectto Wisjustalinearregressionproblem.However,\\nminimization of Jwithrespecttobothargumentsisusuallynotaconvexproblem.\\nMinimization withrespectto Hrequiresspecializedalgorithmssuchasthe\\nfeature-signsearchalgorithm(,). Lee e t a l .2007\\n6 3 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='827de8a2-d286-4d00-b277-97c002b98696', embedding=None, metadata={'page_label': '653', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\n19.4VariationalInferenceandLearning\\nWe\\xa0have\\xa0seen\\xa0how\\xa0the\\xa0evidence\\xa0lo wer\\xa0bound L( v θ , , q)is\\xa0a\\xa0lower\\xa0bound\\xa0on\\nlog p( v; θ),howinferencecanbeviewedasmaximizing Lwithrespectto q,and\\nhowlearningcanbeviewedasmaximizing Lwithrespectto θ.Wehaveseen\\nthattheEMalgorithmallowsustomakelargelearningstepswithaﬁxed qand\\nthatlearningalgorithmsbasedonMAPinferenceallowustolearnusingapoint\\nestimateof p( h v|)ratherthaninferringtheentiredistribution.Nowwedevelop\\nthemoregeneralapproachtovariationallearning.\\nThecoreideabehindvariationallearningisthatwecanmaximize Lovera\\nrestrictedfamilyofdistributions q.Thisfamilyshouldbechosensothatitiseasy\\ntocompute E qlog p( h v ,).\\xa0Atypicalwaytodothisistointroduceassumptions\\nabouthowfactorizes. q\\nAcommonapproachtovariationallearningistoimposetherestrictionthat q\\nisafactorialdistribution:\\nq( ) = h v|\\ue059\\niq h( i| v) . (19.17)\\nThisiscalledthemeanﬁeldapproach.Moregenerally,wecanimposeanygraphi-\\ncalmodelstructurewechooseon q,toﬂexiblydeterminehowmanyinteractionswe\\nwantourapproximationtocapture.Thisfullygeneralgraphicalmodelapproach\\niscalledstructuredvariationalinference( ,). SaulandJordan1996\\nThebeautyofthevariationalapproachisthatwedonotneedtospecifya\\nspeciﬁcparametricformfor q.Wespecifyhowitshouldfactorize,butthenthe\\noptimization problemdeterminestheoptimalprobabilitydistributionwithinthose\\nfactorizationconstraints.Fordiscretelatentvariables,thisjustmeansthatwe\\nusetraditionaloptimization techniquestooptimizeaﬁnitenumberofvariables\\ndescribingthe qdistribution.Forcontinuouslatentvariables,thismeansthatwe\\nuseabranchofmathematics calledcalculusofvariationstoperformoptimization\\noveraspaceoffunctions,andactuallydeterminewhichfunctionshouldbeused\\ntorepresent q.Calculusof\\xa0variations\\xa0istheorigin\\xa0ofthenames\\xa0“variational\\nlearning”and“variationalinference,”thoughthesenamesapplyevenwhenthe\\nlatentvariablesarediscreteandcalculusofvariationsisnotneeded.Inthecase\\nofcontinuouslatentvariables,calculusofvariationsisapowerfultechniquethat\\nremovesmuchoftheresponsibilityfromthehumandesignerofthemodel,who\\nnowmustspecifyonlyhow qfactorizes,ratherthanneedingtoguesshowtodesign\\naspeciﬁcthatcanaccuratelyapproximate theposterior. q\\nBecauseL( v θ , , q)isdeﬁnedtobelog p( v; θ)− D K L( q( h v|)\\ue06b p( h v|; θ)),we\\ncanthinkofmaximizing Lwithrespectto qasminimizing D K L( q( h v|)\\ue06b p( h v|)).\\n6 3 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='360db617-66fb-47dc-8be4-78754ef0bb30', embedding=None, metadata={'page_label': '654', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nInthissense,weareﬁtting qto p.\\xa0However,wearedoingsowiththeopposite\\ndirectionoftheKLdivergencethanweareusedtousingforﬁttinganapproximation.\\nWhenweusemaximumlikelihoodlearningtoﬁtamodeltodata,weminimize\\nD K L( p da t a\\ue06b p m o de l).Asillustratedinﬁgure,thismeansthatmaximumlikelihood 3.6\\nencouragesthemodeltohavehighprobabilityeverywherethatthedatahashigh\\nprobability,\\xa0whileouroptimization-based inferenceprocedureencourages qto\\nhavelowprobabilityeverywherethetrueposteriorhaslowprobability.Both\\ndirectionsoftheKLdivergencecanhavedesirableandundesirableproperties.The\\nchoiceofwhichtousedependsonwhichpropertiesarethehighestpriorityfor\\neachapplication. Inthecaseoftheinferenceoptimization problem,wechoose\\ntouse D K L( q( h v|)\\ue06b p( h v|))forcomputational reasons.Speciﬁcally,computing\\nD K L( q( h v|)\\ue06b p( h v|))involvesevaluatingexpectationswithrespectto q,soby\\ndesigning qtobesimple,wecansimplifytherequiredexpectations.Theopposite\\ndirectionoftheKLdivergencewouldrequirecomputingexpectationswithrespect\\ntothetrueposterior.Becausetheformofthetrueposteriorisdeterminedby\\nthechoiceofmodel,wecannotdesignareduced-costapproachtocomputing\\nD K L(( )( )) p h v|\\ue06b q h v|exactly.\\n19.4.1DiscreteLatentVariables\\nVariationalinferencewithdiscretelatentvariablesisrelativelystraightforward.\\nWedeﬁneadistribution q,typicallyonewhereeachfactorof qisjustdeﬁned\\nbyalookuptableoverdiscretestates.\\xa0Inthesimplestcase, hisbinaryandwe\\nmakethemeanﬁeldassumptionthatfactorizesovereachindividual q h i.Inthis\\ncasewecanparametrize qwithavectorˆ hwhoseentriesareprobabilities. Then\\nq h( i= 1 ) =| v ˆh i.\\nAfterdetermininghowtorepresent q,wesimplyoptimizeitsparameters.In\\nthecaseofdiscretelatentvariables,thisisjustastandardoptimization problem.\\nInprincipletheselectionof qcouldbedonewithanyoptimization algorithm,such\\nasgradientdescent.\\nBecausethisoptimization mustoccurintheinnerloopofalearningalgorithm,\\nitmustbeveryfast.Toachievethisspeed,wetypicallyusespecialoptimization\\nalgorithmsthataredesignedtosolvecomparativelysmallandsimpleproblemsin\\nveryfewiterations.Apopularchoiceistoiterateﬁxedpointequations,inother\\nwords,tosolve\\n∂\\n∂ˆh iL= 0 (19.18)\\nforˆh i.Werepeatedlyupdatediﬀerentelementsofˆhuntilwesatisfyaconvergence\\n6 3 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1a50e616-1d54-44f3-8c3f-e58116e45ccb', embedding=None, metadata={'page_label': '655', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\ncriterion.\\nTomakethismoreconcrete,weshowhowtoapplyvariationalinferencetothe\\nbinarysparsecodingmodel(wepresentherethemodeldevelopedbyHenniges\\ne t a l .()butdemonstratetraditional,genericmeanﬁeldappliedtothemodel, 2010\\nwhiletheyintroduceaspecializedalgorithm).Thisderivationgoesintoconsiderable\\nmathematical detailandisintendedforthereaderwhowishestofullyresolve\\nanyambiguityinthehigh-levelconceptualdescriptionofvariationalinferenceand\\nlearningwehavepresentedsofar.Readerswhodonotplantoderiveorimplement\\nvariationallearningalgorithmsmaysafelyskiptothenextsectionwithoutmissing\\nanynewhigh-levelconcepts.Readerswhoproceedwiththebinarysparsecoding\\nexampleareencouragedtoreviewthelistofusefulpropertiesoffunctionsthat\\ncommonlyariseinprobabilisticmodelsinsection.Weusetheseproperties 3.10\\nliberallythroughoutthefollowingderivationswithouthighlightingexactlywhere\\nweuseeachone.\\nInthebinarysparsecodingmodel,theinput v∈ Rnisgeneratedfromthe\\nmodelbyaddingGaussiannoisetothesumof mdiﬀerentcomponentswhich\\ncaneachbepresentorabsent.Eachcomponentisswitchedonoroﬀbythe\\ncorrespondinghiddenunitin h∈{}01 ,m:\\np h( i= 1) = ( σ b i) (19.19)\\np , ( ) = (; v h| N v W h β− 1) (19.20)\\nwhere bisalearnablesetofbiases, Wisalearnableweightmatrix,and βisa\\nlearnable,diagonalprecisionmatrix.\\nTrainingthismodelwithmaximumlikelihoodrequirestakingthederivative\\nwithrespecttotheparameters.Considerthederivativewithrespecttooneofthe\\nbiases:\\n∂\\n∂ b ilog() p v (19.21)\\n=∂\\n∂ b ip() v\\np() v(19.22)\\n=∂\\n∂ b i\\ue050\\nhp ,( h v)\\np() v(19.23)\\n=∂\\n∂ b i\\ue050\\nhp p() h( ) v h|\\np() v(19.24)\\n6 4 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='198889ff-0a0c-408b-a797-1e15eb325acd', embedding=None, metadata={'page_label': '656', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nh 1 h 1 h 2 h 2 h 3 h 3\\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4\\nh 1 h 1\\nh 2 h 2h 3 h 3\\nh 4 h 4\\nFigure19.2:Thegraphstructureofabinarysparsecodingmodelwithfourhiddenunits.\\n( L e f t )Thegraphstructureof p( h v ,).Notethattheedgesaredirected,andthateverytwo\\nhiddenunitsareco-parentsofeveryvisibleunit.Thegraphstructureof ( R i g h t ) p( h v|).\\nInordertoaccountfortheactivepathsbetweenco-parents,theposteriordistribution\\nneedsanedgebetweenallofthehiddenunits.\\n=\\ue050\\nh p( ) v h|∂\\n∂ b ip() h\\np() v(19.25)\\n=\\ue058\\nhp( ) h v|∂\\n∂ b ip() h\\np() h(19.26)\\n= E h∼ | p ( h v )∂\\n∂ b ilog() p h . (19.27)\\nThisrequirescomputingexpectationswithrespectto p( h v|).Unfortunately,\\np( h v|)isacomplicateddistribution.Seeﬁgureforthegraphstructureof 19.2\\np( h v ,)and p( h v|).Theposteriordistributioncorrespondstothecompletegraph\\noverthehiddenunits,sovariableeliminationalgorithmsdonothelpustocompute\\ntherequiredexpectationsanyfasterthanbruteforce.\\nWecanresolvethisdiﬃcultybyusingvariationalinferenceandvariational\\nlearninginstead.\\nWecanmakeameanﬁeldapproximation:\\nq( ) = h v|\\ue059\\niq h( i| v) . (19.28)\\nThelatentvariablesofthebinarysparsecodingmodelarebinary,sotorepresent\\nafactorial qwesimplyneedtomodel mBernoullidistributions q( h i| v).Anatural\\nwaytorepresentthemeansoftheBernoullidistributionsiswithavectorˆhof\\nprobabilities, with q( h i=1| v)=ˆh i.Weimposearestrictionthatˆh iisnever\\nequalto0orto1,inordertoavoiderrorswhencomputing,forexample, logˆh i.\\nWewillseethatthevariationalinferenceequationsneverassignorto 01 ˆh i\\n6 4 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='047c76c7-0ab3-468b-ba2a-d140701c57b5', embedding=None, metadata={'page_label': '657', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nanalytically.However,inasoftwareimplementation,machineroundingerrorcould\\nresultinorvalues.Insoftware,wemaywishtoimplementbinarysparse 0 1\\ncodingusinganunrestrictedvectorofvariationalparameters zandobtain ˆ hvia\\ntherelation ˆh= σ( z).Wecanthussafelycompute logˆh ionacomputerbyusing\\ntheidentitylog( σ z i) = (− ζ− z i)relatingthesigmoidandthesoftplus.\\nTobeginourderivationofvariationallearninginthebinarysparsecoding\\nmodel,weshowthattheuseofthismeanﬁeldapproximationmakeslearning\\ntractable.\\nTheevidencelowerboundisgivenby\\nL( ) v θ , , q (19.29)\\n= E h∼ q[log( )]+() p h v , H q (19.30)\\n= E h∼ q[log()+log( )log( )] p h p v h|− q h v| (19.31)\\n= E h∼ q\\ue022m\\ue058\\ni = 1log( p h i)+n\\ue058\\ni = 1log( p v i|− h)m\\ue058\\ni = 1log( q h i| v)\\ue023\\n(19.32)\\n=m\\ue058\\ni = 1\\ue068\\nˆ h i(log( σ b i)log− ˆ h i)+(1−ˆ h i)(log( σ− b i)log(1 − −ˆ h i))\\ue069\\n(19.33)\\n+ E h∼ q\\ue022n\\ue058\\ni = 1log\\ue072\\nβ i\\n2 πexp\\ue012\\n−β i\\n2( v i− W i , : h)2\\ue013\\ue023\\n(19.34)\\n=m\\ue058\\ni = 1\\ue068\\nˆ h i(log( σ b i)log− ˆ h i)+(1−ˆ h i)(log( σ− b i)log(1 − −ˆ h i))\\ue069\\n(19.35)\\n+1\\n2n\\ue058\\ni = 1\\uf8ee\\n\\uf8f0logβ i\\n2 π− β i\\uf8eb\\n\\uf8ed v2\\ni−2 v i W i , :ˆh+\\ue058\\nj\\uf8ee\\n\\uf8f0 W2\\ni , jˆ h j+\\ue058\\nk j\\ue036=W i , j W i , kˆh jˆh k\\uf8f9\\n\\uf8fb\\uf8f6\\n\\uf8f8\\uf8f9\\n\\uf8fb .\\n(19.36)\\nWhiletheseequationsaresomewhatunappealingaesthetically,theyshowthatL\\ncanbeexpressedinasmallnumberofsimplearithmeticoperations.Theevidence\\nlowerbound Listhereforetractable.WecanuseLasareplacementforthe\\nintractablelog-likelihood.\\nInprinciple,wecouldsimplyrungradientascentonboth vand handthis\\nwouldmakeaperfectlyacceptablecombinedinferenceandtrainingalgorithm.\\nUsually,however,wedonotdothis,fortworeasons.First,thiswouldrequire\\nstoring ˆ hforeach v.Wetypicallypreferalgorithmsthatdonotrequireper-\\nexamplememory.Itisdiﬃculttoscalelearningalgorithmstobillionsofexamples\\nifwemustrememberadynamicallyupdatedvectorassociatedwitheachexample.\\n6 4 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc9becbf-04ba-4a14-8746-785cf3895db4', embedding=None, metadata={'page_label': '658', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nSecond,wewouldliketobeabletoextractthefeatures ˆhveryquickly,inorderto\\nrecognizethecontentof v.\\xa0Inarealisticdeployedsetting,wewouldneedtobe\\nabletocompute ˆhinrealtime.\\nForboththesereasons,wetypicallydonotusegradientdescenttocompute\\nthemeanﬁeldparameters ˆ h.Instead,werapidlyestimatethemwithﬁxedpoint\\nequations.\\nTheideabehindﬁxedpointequationsisthatweareseekingalocalmaximum\\nwithrespecttoˆh,\\xa0where ∇ hL( v θ , ,ˆh)= 0.Wecannoteﬃcientlysolvethis\\nequationwithrespecttoallofˆhsimultaneously.However,wecansolveforasingle\\nvariable:∂\\n∂ˆh iL( v θ , ,ˆh) = 0 . (19.37)\\nWecantheniterativelyapplythesolutiontotheequationfor i=1 , . . . , m,\\nandrepeatthecycleuntilwesatisfyaconvergecriterion.Commonconvergence\\ncriteriaincludestoppingwhenafullcycleofupdatesdoesnotimproveLbymore\\nthansometoleranceamount,orwhenthecycledoesnotchange ˆhbymorethan\\nsomeamount.\\nIteratingmeanﬁeldﬁxedpointequationsisageneraltechniquethatcan\\nprovidefastvariationalinferenceinabroadvarietyofmodels.Tomakethismore\\nconcrete,weshowhowtoderivetheupdatesforthebinarysparsecodingmodelin\\nparticular.\\nFirst,wemustwriteanexpressionforthederivativeswithrespecttoˆh i.To\\ndoso,wesubstituteequation intotheleftsideofequation: 19.36 19.37\\n∂\\n∂ˆh iL( v θ , ,ˆ h) (19.38)\\n=∂\\n∂ˆh i\\uf8ee\\n\\uf8f0m\\ue058\\nj = 1\\ue068\\nˆ h j(log( σ b j)log− ˆ h j)+(1−ˆ h j)(log( σ− b j)log(1 − −ˆ h j))\\ue069\\n(19.39)\\n+1\\n2n\\ue058\\nj = 1\\uf8ee\\n\\uf8f0logβ j\\n2 π− β j\\uf8eb\\n\\uf8ed v2\\nj−2 v j W j , :ˆh+\\ue058\\nk\\uf8ee\\n\\uf8f0 W2\\nj , kˆh k+\\ue058\\nl k\\ue036=W j , k W j , lˆh kˆh l\\uf8f9\\n\\uf8fb\\uf8f6\\n\\uf8f8\\uf8f9\\n\\uf8fb\\uf8f9\\n\\uf8fb\\n(19.40)\\n=log( σ b i)log− ˆh i− − 1+log(1ˆh i)+1log( − σ− b i) (19.41)\\n+n\\ue058\\nj = 1\\uf8ee\\n\\uf8f0 β j\\uf8eb\\n\\uf8ed v j W j , i−1\\n2W2\\nj , i−\\ue058\\nk i\\ue036=W j , k W j , iˆh k\\uf8f6\\n\\uf8f8\\uf8f9\\n\\uf8fb (19.42)\\n6 4 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0180611d-79fd-4bbb-b337-b072a71ebf4c', embedding=None, metadata={'page_label': '659', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\n= b i−logˆ h i+log(1−ˆh i)+ v\\ue03eβ W : , i−1\\n2W\\ue03e\\n: , i β W : , i−\\ue058\\nj i\\ue036=W\\ue03e\\n: , j β W : , iˆh j .(19.43)\\nToapplytheﬁxedpointupdateinferencerule,wesolvefortheˆ h ithatsets\\nequationto0:19.43\\nˆh i= σ\\uf8eb\\n\\uf8ed b i+ v\\ue03eβ W : , i−1\\n2W\\ue03e\\n: , i β W : , i−\\ue058\\nj i\\ue036=W\\ue03e\\n: , j β W : , iˆh j\\uf8f6\\n\\uf8f8 .(19.44)\\nAtthispoint,wecanseethatthereisacloseconnectionbetweenrecurrent\\nneuralnetworksandinferenceingraphicalmodels.Speciﬁcally,themeanﬁeld\\nﬁxedpointequationsdeﬁnedarecurrentneuralnetwork.Thetaskofthisnetwork\\nistoperforminference.Wehavedescribedhowtoderivethisnetworkfroma\\nmodeldescription,butitisalsopossibletotraintheinferencenetworkdirectly.\\nSeveralideasbasedonthisthemearedescribedinchapter.20\\nInthecaseofbinarysparsecoding,wecanseethattherecurrentnetwork\\nconnectionspeciﬁedbyequationconsistsofrepeatedlyupdatingthehidden 19.44\\nunitsbasedonthechangingvaluesoftheneighboringhiddenunits.Theinput\\nalwayssendsaﬁxedmessageof v\\ue03eβ Wtothehiddenunits,butthehiddenunits\\nconstantlyupdatethemessagetheysendtoeachother.Speciﬁcally,twounits ˆh i\\nandˆ h jinhibiteachotherwhentheirweightvectorsarealigned.Thisisaformof\\ncompetition—betweentwohiddenunitsthatbothexplaintheinput,onlytheone\\nthatexplainstheinputbestwillbeallowedtoremainactive.Thiscompetitionis\\nthemeanﬁeldapproximation’sattempttocapturetheexplainingawayinteractions\\ninthebinarysparsecodingposterior.Theexplainingawayeﬀectactuallyshould\\ncauseamulti-modalposterior,sothatifwedrawsamplesfromtheposterior,\\nsomesampleswillhaveoneunitactive,othersampleswillhavetheotherunit\\nactive,butveryfewsampleshavebothactive.Unfortunately,explainingaway\\ninteractionscannotbemodeledbythefactorial qusedformeanﬁeld,sothemean\\nﬁeldapproximation isforcedtochooseonemodetomodel.Thisisaninstanceof\\nthebehaviorillustratedinﬁgure.3.6\\nWecanrewriteequationintoanequivalentformthatrevealssomefurther 19.44\\ninsights:\\nˆh i= σ\\uf8eb\\n\\uf8ec\\uf8ed b i+\\uf8eb\\n\\uf8ed v−\\ue058\\nj i\\ue036=W : , jˆh j\\uf8f6\\n\\uf8f8\\ue03e\\nβ W : , i−1\\n2W\\ue03e\\n: , i β W : , i\\uf8f6\\n\\uf8f7\\uf8f8 .(19.45)\\nInthisreformulation,weseetheinputateachstepasconsistingof v−\\ue050\\nj i\\ue036=W : , jˆh j\\nratherthan v.Wecanthusthinkofunit iasattemptingtoencodetheresidual\\n6 4 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='da6f51fd-b98f-4166-97ac-13a2d988e94f', embedding=None, metadata={'page_label': '660', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nerrorin vgiventhecodeoftheotherunits.Wecanthusthinkofsparsecodingas\\naniterativeautoencoder,thatrepeatedlyencodesanddecodesitsinput,attempting\\ntoﬁxmistakesinthereconstructionaftereachiteration.\\nInthisexample,wehavederivedanupdaterulethatupdatesasingleunitat\\natime.Itwouldbeadvantageoustobeabletoupdatemoreunitssimultaneously.\\nSomegraphicalmodels,suchasdeepBoltzmannmachines,arestructuredinsucha\\nwaythatwecansolveformanyentriesofˆhsimultaneously.Unfortunately,binary\\nsparsecodingdoesnotadmitsuchblockupdates.Instead,wecanuseaheuristic\\ntechniquecalleddampingtoperformblockupdates.Inthedampingapproach,\\nwesolvefortheindividuallyoptimalvaluesofeveryelementofˆh,thenmoveallof\\nthevaluesinasmallstepinthatdirection.Thisapproachisnolongerguaranteed\\ntoincreaseLateachstep,butworkswellinpracticeformanymodels.SeeKoller\\nandFriedman2009()formoreinformationaboutchoosingthedegreeofsynchrony\\nanddampingstrategiesinmessagepassingalgorithms.\\n19.4.2CalculusofVariations\\nBeforecontinuingwithourpresentationofvariationallearning,wemustbrieﬂy\\nintroduceanimportantsetofmathematical toolsusedinvariationallearning:\\ncalculusofvariations.\\nManymachinelearningtechniquesarebasedonminimizingafunction J( θ)by\\nﬁndingtheinputvector θ∈ Rnforwhichittakesonitsminimalvalue.Thiscan\\nbeaccomplishedwithmultivariatecalculusandlinearalgebra,bysolvingforthe\\ncriticalpointswhere ∇ θ J( θ) = 0.Insomecases,weactuallywanttosolvefora\\nfunction f( x),suchaswhenwewanttoﬁndtheprobabilitydensityfunctionover\\nsomerandomvariable.Thisiswhatcalculusofvariationsenablesustodo.\\nAfunction\\xa0ofa\\xa0function fisknown\\xa0asafunctional J[ f].Muchas\\xa0we\\ncantakepartialderivativesofafunctionwithrespecttoelementsofitsvector-\\nvaluedargument,wecantakefunctionalderivatives,alsoknownasvariational\\nderivatives,ofafunctional J[ f]withrespecttoindividualvaluesofthefunction\\nf( x)atanyspeciﬁcvalueof x.Thefunctionalderivativeofthefunctional Jwith\\nrespecttothevalueofthefunctionatpointisdenoted f xδ\\nδ f x ( )J.\\nAcompleteformaldevelopmentoffunctionalderivativesisbeyondthescopeof\\nthisbook.Forourpurposes,itissuﬃcienttostatethatfordiﬀerentiablefunctions\\nf g y , () xanddiﬀerentiable functions ( x)withcontinuousderivatives,that\\nδ\\nδ f() x\\ue05a\\ng f , d (() x x) x=∂\\n∂ yg f , . (() x x) (19.46)\\n6 4 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='20823672-6b86-4e56-8d36-2f425a7df200', embedding=None, metadata={'page_label': '661', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nTogainsomeintuitionforthisidentity,onecanthinkof f( x)asbeingavector\\nwithuncountablymanyelements,indexedbyarealvector x.Inthis(somewhat\\nincompleteview),theidentityprovidingthefunctionalderivativesisthesameas\\nwewouldobtainforavector θ∈ Rnindexedbypositiveintegers:\\n∂\\n∂ θ i\\ue058\\njg θ( j , j) =∂\\n∂ θ ig θ( i , i .) (19.47)\\nManyresultsinothermachinelearningpublicationsarepresentedusingthemore\\ngeneralEuler-Lagrangeequationwhichallows gtodependonthederivatives\\nof faswellasthevalueof f,butwedonotneedthisfullygeneralformforthe\\nresultspresentedinthisbook.\\nTooptimizeafunctionwithrespecttoavector,wetakethegradientofthe\\nfunctionwithrespecttothevectorandsolveforthepointwhereeveryelementof\\nthegradientisequaltozero.Likewise,wecanoptimizeafunctionalbysolvingfor\\nthefunctionwherethefunctionalderivativeateverypointisequaltozero.\\nAsanexampleofhowthisprocessworks,considertheproblemofﬁndingthe\\nprobabilitydistributionfunctionover x∈ Rthathasmaximaldiﬀerentialentropy.\\nRecallthattheentropyofaprobabilitydistributionisdeﬁnedas p x()\\nH p[] = − E xlog() p x . (19.48)\\nForcontinuousvalues,theexpectationisanintegral:\\nH p[] = −\\ue05a\\np x p x d x . ()log() (19.49)\\nWecannotsimplymaximize H[ p] withrespecttothefunction p( x),becausethe\\nresultmightnotbeaprobabilitydistribution.Instead,weneedtouseLagrange\\nmultipliers\\xa0toadd\\xa0aconstraint\\xa0that p( x)integratesto\\xa01.Also,theentropy\\nincreaseswithoutboundasthevarianceincreases.Thismakesthequestionof\\nwhichdistributionhasthegreatestentropyuninteresting.Instead,weaskwhich\\ndistributionhasmaximalentropyforﬁxedvariance σ2.Finally,theproblem\\nisunderdetermined becausethedistributioncanbeshiftedarbitrarilywithout\\nchangingtheentropy.Toimposeauniquesolution,weaddaconstraintthatthe\\nmeanofthedistributionbe µ.\\xa0TheLagrangianfunctionalforthisoptimization\\nproblemis\\nL[] = p λ 1\\ue012\\ue05a\\np x d x()−1\\ue013\\n+ λ 2([] )+ E x− µ λ 3\\ue000\\nE[( ) x µ−2]− σ2\\ue001\\n+[] H p(19.50)\\n6 4 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fb25658e-cefa-4e44-8bb8-0fc7eab9053b', embedding=None, metadata={'page_label': '662', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\n=\\ue05a\\ue000\\nλ 1 p x λ ()+ 2 p x x λ ()+ 3 p x x µ ()(−)2− p x p x ()log()\\ue001d x λ− 1− µ λ 2− σ2λ 3 .\\n(19.51)\\nTominimizetheLagrangianwithrespectto p,wesetthefunctionalderivatives\\nequalto0:\\n∀ x ,δ\\nδ p x()L= λ 1+ λ 2 x λ+ 3( ) x µ−2−−1log() = 0 p x .(19.52)\\nThisconditionnowtellsusthefunctionalformof p( x).Byalgebraically\\nre-arrangingtheequation,weobtain\\np x() = exp\\ue000\\nλ 1+ λ 2 x λ+ 3( ) x µ−2−1\\ue001\\n. (19.53)\\nWeneverassumeddirectlythat p( x)wouldtakethisfunctionalform;we\\nobtainedtheexpressionitselfbyanalyticallyminimizingafunctional.Toﬁnish\\ntheminimization problem,wemustchoosethe λvaluestoensurethatallofour\\nconstraintsaresatisﬁed.Wearefreetochooseany λvalues,becausethegradient\\noftheLagrangianwithrespecttothe λvariablesiszerosolongastheconstraints\\naresatisﬁed.Tosatisfyalloftheconstraints,wemayset λ 1=1−log σ√\\n2 π,\\nλ 2= 0,and λ 3= −1\\n2 σ2toobtain\\np x x µ , σ () = (N;2) . (19.54)\\nThisisonereasonforusingthenormaldistributionwhenwedonotknowthe\\ntruedistribution.Becausethenormaldistributionhasthemaximumentropy,we\\nimposetheleastpossibleamountofstructurebymakingthisassumption.\\nWhileexaminingthecriticalpointsoftheLagrangianfunctionalfortheentropy,\\nwefoundonlyonecriticalpoint,correspondingtomaximizingtheentropyfor\\nﬁxedvariance.Whatabouttheprobabilitydistributionfunctionthat m i nim i z e s\\ntheentropy?Whydidwenotﬁndasecondcriticalpointcorrespondingtothe\\nminimum?Thereasonisthatthereisnospeciﬁcfunctionthatachievesminimal\\nentropy.Asfunctionsplacemoreprobabilitydensityonthetwopoints x= µ+ σ\\nand x= µ σ−,andplacelessprobabilitydensityonallothervaluesof x,theylose\\nentropywhilemaintainingthedesiredvariance.However,anyfunctionplacing\\nexactlyzeromassonallbuttwopointsdoesnotintegratetoone,andisnota\\nvalidprobabilitydistribution.Therethusisnosingleminimalentropyprobability\\ndistributionfunction,muchasthereisnosingleminimalpositiverealnumber.\\nInstead,wecansaythatthereisasequenceofprobabilitydistributionsconverging\\ntowardputtingmassonlyonthesetwopoints.Thisdegeneratescenariomaybe\\n6 4 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc59d3c5-3b9a-4178-a70e-82aef46d7359', embedding=None, metadata={'page_label': '663', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\ndescribedasamixtureofDiracdistributions.BecauseDiracdistributionsare\\nnotdescribedbyasingleprobabilitydistributionfunction,noDiracormixtureof\\nDiracdistributioncorrespondstoasinglespeciﬁcpointinfunctionspace.These\\ndistributionsarethusinvisibletoourmethodofsolvingforaspeciﬁcpointwhere\\nthefunctionalderivativesarezero.Thisisalimitationofthemethod.Distributions\\nsuchastheDiracmustbefoundbyothermethods,suchasguessingthesolution\\nandthenprovingthatitiscorrect.\\n19.4.3ContinuousLatentVariables\\nWhenourgraphicalmodelcontainscontinuouslatentvariables,\\xa0wemaystill\\nperformvariationalinferenceandlearningbymaximizing L.However,wemust\\nnowusecalculusofvariationswhenmaximizing withrespectto. L q( ) h v|\\nInmostcases,practitioners neednotsolveanycalculusofvariationsproblems\\nthemselves.Instead,thereisageneralequationforthemeanﬁeldﬁxedpoint\\nupdates.Ifwemakethemeanﬁeldapproximation\\nq( ) = h v|\\ue059\\niq h( i| v) , (19.55)\\nandﬁx q( h j| v)forall j\\ue036= i,thentheoptimal q( h i| v)maybeobtainedby\\nnormalizingtheunnormalized distribution\\n˜ q h( i| v) = exp\\ue000\\nE h − i∼ q ( h − i| v )log ˜ p ,( v h)\\ue001\\n(19.56)\\nsolongas pdoesnotassignprobabilitytoanyjointconﬁgurationofvariables. 0\\nCarryingouttheexpectationinsidetheequationwillyieldthecorrectfunctional\\nformof q( h i| v).Itisonlynecessarytoderivefunctionalformsof qdirectlyusing\\ncalculusofvariationsifonewishestodevelopanewformofvariationallearning;\\nequationyieldsthemeanﬁeldapproximation foranyprobabilisticmodel. 19.56\\nEquationisaﬁxedpointequation,designedtobeiterativelyappliedfor 19.56\\neachvalueof irepeatedlyuntilconvergence.However,italsotellsusmorethan\\nthat.Ittellsusthefunctionalformthattheoptimalsolutionwilltake,whether\\nwearrivetherebyﬁxedpointequationsornot.Thismeanswecantakethe\\nfunctionalformfromthatequationbutregardsomeofthevaluesthatappearinit\\nasparameters,thatwecanoptimizewithanyoptimization algorithmwelike.\\nAsanexample,consideraverysimpleprobabilisticmodel,withlatentvariables\\nh∈ R2andjustonevisiblevariable, v.Supposethat p( h)=N( h;0 , I)and\\np( v| h)=N( v; w\\ue03eh;1).Wecouldactuallysimplifythismodelbyintegrating\\nout h;theresultisjustaGaussiandistributionover v.\\xa0Themodelitselfisnot\\n6 4 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c5da7cc6-7009-464b-8f66-8a8607c6d9b1', embedding=None, metadata={'page_label': '664', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\ninteresting;wehaveconstructeditonlytoprovideasimpledemonstrationofhow\\ncalculusofvariationsmaybeappliedtoprobabilisticmodeling.\\nThetrueposteriorisgiven,uptoanormalizingconstant,by\\np( ) h v| (19.57)\\n∝ p ,( h v) (19.58)\\n=( p h 1)( p h 2)( ) p v h| (19.59)\\n∝exp\\ue012\\n−1\\n2\\ue002\\nh2\\n1+ h2\\n2+( v h− 1 w 1− h 2 w 2)2\\ue003\\ue013\\n(19.60)\\n=exp\\ue012\\n−1\\n2\\ue002\\nh2\\n1+ h2\\n2+ v2+ h2\\n1 w2\\n1+ h2\\n2 w2\\n2−2 v h 1 w 1−2 v h 2 w 2+2 h 1 w 1 h 2 w 2\\ue003\\ue013\\n.\\n(19.61)\\nDuetothepresenceofthetermsmultiplying h 1and h 2together,wecanseethat\\nthetrueposteriordoesnotfactorizeover h 1and h 2.\\nApplyingequation,weﬁndthat 19.56\\n˜ q h( 1| v) (19.62)\\n=exp\\ue000\\nE h 2∼ q ( h 2| v )log ˜ p ,( v h)\\ue001\\n(19.63)\\n=exp\\ue012\\n−1\\n2Eh 2∼ q ( h 2| v )\\ue002\\nh2\\n1+ h2\\n2+ v2+ h2\\n1 w2\\n1+ h2\\n2 w2\\n2(19.64)\\n−2 v h 1 w 1−2 v h 2 w 2+2 h 1 w 1 h 2 w 2]\\ue013\\n. (19.65)\\nFromthis,wecanseethatthereareeﬀectivelyonlytwovaluesweneedtoobtain\\nfrom q( h 2| v): E h 2∼ | q ( h v )[ h 2]and E h 2∼ | q ( h v )[ h2\\n2].\\xa0Writingtheseas\\ue068 h 2\\ue069and\\ue068 h2\\n2\\ue069,\\nweobtain\\n˜ q h( 1| v) = exp\\ue012\\n−1\\n2\\ue002\\nh2\\n1+\\ue068 h2\\n2\\ue069+ v2+ h2\\n1 w2\\n1+\\ue068 h2\\n2\\ue069 w2\\n2(19.66)\\n−2 v h 1 w 1−\\ue0682 v h 2\\ue069 w 2+2 h 1 w 1\\ue068 h 2\\ue069 w 2]\\ue013\\n.(19.67)\\nFromthis,wecanseethat˜ qhasthefunctionalformofaGaussian.Wecan\\nthusconclude q( h v|)=N( h; µ β ,− 1)where µanddiagonal βarevariational\\nparametersthatwecanoptimizeusinganytechniquewechoose.Itisimportant\\ntorecallthatwedidnoteverassumethat qwouldbeGaussian;itsGaussian\\nformwasderivedautomatically byusingcalculusofvariationstomaximize qwith\\n6 4 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c08006c4-86b9-4445-8492-4e737e978fad', embedding=None, metadata={'page_label': '665', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nrespecttoL.Usingthesameapproachonadiﬀerentmodelcouldyieldadiﬀerent\\nfunctionalformof. q\\nThiswasofcourse,justasmallcaseconstructedfordemonstrationpurposes.\\nForexamplesofrealapplicationsofvariationallearningwithcontinuousvariables\\ninthecontextofdeeplearning,see (). Goodfellow e t a l .2013d\\n19.4.4InteractionsbetweenLearningandInference\\nUsingapproximate inferenceaspartofalearningalgorithmaﬀectsthelearning\\nprocess,andthisinturnaﬀectstheaccuracyoftheinferencealgorithm.\\nSpeciﬁcally,thetrainingalgorithmtendstoadaptthemodelinawaythatmakes\\ntheapproximating assumptionsunderlyingtheapproximate inferencealgorithm\\nbecomemoretrue.Whentrainingtheparameters,variationallearningincreases\\nE h∼ qlog( ) p v h , . (19.68)\\nForaspeciﬁc v,thisincreases p( h v|)forvaluesof hthathavehighprobability\\nunder q( h v|)anddecreases p( h v|)forvaluesof hthathavelowprobability\\nunder . q( ) h v|\\nThisbehaviorcausesourapproximating assumptionstobecomeself-fulﬁlling\\nprophecies.Ifwetrainthemodelwithaunimodalapproximate posterior,wewill\\nobtainamodelwithatrueposteriorthatisfarclosertounimodalthanwewould\\nhaveobtainedbytrainingthemodelwithexactinference.\\nComputingthetrueamountofharmimposedonamodelbyavariational\\napproximationisthusverydiﬃcult.Thereexistseveralmethodsforestimating\\nlog p( v).Weoftenestimate log p( v; θ)aftertrainingthemodel,andﬁndthat\\nthegapwith L( v θ , , q)issmall.Fromthis,wecanconcludethatourvariational\\napproximationisaccurateforthespeciﬁcvalueof θthatweobtainedfromthe\\nlearningprocess.Weshouldnotconcludethatourvariationalapproximation is\\naccurateingeneralorthatthevariationalapproximation didlittleharmtothe\\nlearningprocess.Tomeasurethetrueamountofharminducedbythevariational\\napproximation,wewouldneedtoknow θ∗=max θlog p( v; θ).\\xa0Itispossiblefor\\nL( v θ , , q)≈log p( v; θ)andlog p( v; θ)\\ue01clog p( v; θ∗)toholdsimultaneously.If\\nmax qL( v θ ,∗, q)\\ue01clog p( v; θ∗),because θ∗inducestoocomplicatedofaposterior\\ndistributionforour qfamilytocapture,\\xa0then thelearningprocesswillnever\\napproach θ∗.Suchaproblemisverydiﬃculttodetect,becausewecanonlyknow\\nforsurethatithappenedifwehaveasuperiorlearningalgorithmthatcanﬁnd θ∗\\nforcomparison.\\n6 5 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='94361c22-6387-48d5-8d70-4aae6610bafb', embedding=None, metadata={'page_label': '666', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\n19.5LearnedApproximateInference\\nWehaveseenthatinferencecanbethoughtofasanoptimization procedure\\nthatincreasesthevalueofafunction L.Explicitlyperformingoptimization via\\niterativeproceduressuchasﬁxedpointequationsorgradient-basedoptimization\\nisoftenveryexpensiveandtime-consuming. Manyapproachestoinferenceavoid\\nthisexpensebylearningtoperformapproximateinference.\\xa0Speciﬁcally ,wecan\\nthinkoftheoptimization processasafunction fthatmapsaninput vtoan\\napproximatedistribution q∗=argmaxqL( v , q).Oncewethinkofthemulti-step\\niterativeoptimization processasjustbeingafunction,wecanapproximateitwith\\naneuralnetworkthatimplementsanapproximation ˆ f(;) v θ.\\n19.5.1Wake-Sleep\\nOneofthemaindiﬃcultieswithtrainingamodeltoinfer hfrom visthatwe\\ndonothaveasupervisedtrainingsetwithwhichtotrainthemodel.Givena v,\\nwedonotknowtheappropriate h.Themappingfrom vto hdependsonthe\\nchoiceofmodelfamily,andevolvesthroughoutthelearningprocessas θchanges.\\nThewake-sleepalgorithm(Hinton1995bFrey1996 e t a l .,; e t a l .,)resolvesthis\\nproblembydrawingsamplesofboth hand vfromthemodeldistribution.\\xa0For\\nexample,inadirectedmodel,thiscanbedonecheaplybyperformingancestral\\nsamplingbeginningat handendingat v.Theinferencenetworkcanthenbe\\ntrainedtoperformthereversemapping:\\xa0predicting which hcausedthepresent\\nv.Themaindrawbacktothisapproachisthatwewillonlybeabletotrainthe\\ninferencenetworkonvaluesof vthathavehighprobabilityunderthemodel.Early\\ninlearning,themodeldistributionwillnotresemblethedatadistribution,sothe\\ninferencenetworkwillnothaveanopportunitytolearnonsamplesthatresemble\\ndata.\\nInsectionwesawthatonepossibleexplanationfortheroleofdreamsleep 18.2\\ninhumanbeingsandanimalsisthatdreamscouldprovidethenegativephase\\nsamplesthatMonteCarlotrainingalgorithmsusetoapproximatethenegative\\ngradientofthelogpartitionfunctionofundirectedmodels.Anotherpossible\\nexplanationforbiologicaldreamingisthatitisprovidingsamplesfrom p( h v ,)\\nwhichcanbeusedtotrainaninferencenetworktopredict hgiven v.Insome\\nsenses,thisexplanationismoresatisfyingthanthepartitionfunctionexplanation.\\nMonteCarloalgorithmsgenerallydonotperformwelliftheyarerunusingonly\\nthepositivephaseofthegradientforseveralstepsthenwithonlythenegative\\nphaseofthegradientforseveralsteps.Humanbeingsandanimalsareusually\\nawakeforseveralconsecutivehoursthenasleepforseveralconsecutivehours.Itis\\n6 5 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42304f8e-75d5-4b56-879a-d7755a202a18', embedding=None, metadata={'page_label': '667', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nnotreadilyapparenthowthisschedulecouldsupportMonteCarlotrainingofan\\nundirectedmodel.Learningalgorithmsbasedonmaximizing Lcanberunwith\\nprolongedperiodsofimproving qandprolongedperiodsofimproving θ,however.\\nIftheroleofbiologicaldreamingistotrainnetworksforpredicting q,thenthis\\nexplainshowanimalsareabletoremainawakeforseveralhours(thelongerthey\\nareawake,thegreaterthegapbetweenLandlog p( v),butLwillremainalower\\nbound)andtoremainasleepforseveralhours(thegenerativemodelitselfisnot\\nmodiﬁedduringsleep)withoutdamagingtheirinternalmodels.Ofcourse,these\\nideasarepurelyspeculative,andthereisnohardevidencetosuggestthatdreaming\\naccomplisheseitherofthesegoals.Dreamingmayalsoservereinforcementlearning\\nratherthanprobabilisticmodeling,bysamplingsyntheticexperiencesfromthe\\nanimal’stransitionmodel,onwhichtotraintheanimal’spolicy.Orsleepmay\\nservesomeotherpurposenotyetanticipatedbythemachinelearningcommunity.\\n19.5.2OtherFormsofLearnedInference\\nThisstrategyoflearnedapproximateinferencehasalsobeenappliedtoother\\nmodels.SalakhutdinovandLarochelle2010()showedthatasinglepassina\\nlearnedinferencenetworkcouldyieldfasterinferencethaniteratingthemeanﬁeld\\nﬁxedpointequationsinaDBM.Thetrainingprocedureisbasedonrunningthe\\ninferencenetwork,thenapplyingonestepofmeanﬁeldtoimproveitsestimates,\\nandtrainingtheinferencenetworktooutputthisreﬁnedestimateinsteadofits\\noriginalestimate.\\nWehavealreadyseeninsectionthatthepredictivesparsedecomposition 14.8\\nmodeltrainsashallowencodernetworktopredictasparsecodefortheinput.\\nThiscanbeseenasahybridbetweenanautoencoderandsparsecoding.Itis\\npossibletodeviseprobabilisticsemanticsforthemodel,underwhichtheencoder\\nmaybeviewedasperforminglearnedapproximate MAPinference.Duetoits\\nshallowencoder,PSDisnotabletoimplementthekindofcompetitionbetween\\nunitsthatwehaveseeninmeanﬁeldinference.However,thatproblemcanbe\\nremediedbytrainingadeepencodertoperformlearnedapproximateinference,as\\nintheISTAtechnique( ,). GregorandLeCun2010b\\nLearned\\xa0approximate\\xa0inference\\xa0hasrecently\\xa0become\\xa0one\\xa0of\\xa0the\\xa0dominant\\napproachestogenerativemodeling,intheformofthevariationalautoencoder\\n(,; ,).Inthiselegantapproach,thereisnoneedto Kingma2013Rezende e t a l .2014\\nconstructexplicittargetsfortheinferencenetwork.Instead,theinferencenetwork\\nissimplyusedtodeﬁne L,andthentheparametersoftheinferencenetworkare\\nadaptedtoincrease.Thismodelisdescribedindepthlater,insection. L 20.10.3\\n6 5 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d51007f0-85eb-4779-a443-8cfa3a77eacd', embedding=None, metadata={'page_label': '668', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER19.APPROXIMATEINFERENCE\\nUsingapproximateinference,itispossibletotrainanduseawidevarietyof\\nmodels.Manyofthesemodelsaredescribedinthenextchapter.\\n6 5 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f0c986cc-5739-4d1e-9fdc-16b0bd4d7ca3', embedding=None, metadata={'page_label': '669', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='C h a p t e r 2 0\\nD e e p Ge n e rat i v e Mo d e l s\\nInthischapter,wepresentseveralofthespeciﬁckindsofgenerativemodelsthat\\ncanbebuiltandtrainedusingthetechniquespresentedinchapters–.Allof1619\\nthesemodelsrepresentprobabilitydistributionsovermultiplevariablesinsome\\nway.Someallowtheprobabilitydistributionfunctiontobeevaluatedexplicitly.\\nOthersdonotallowtheevaluationoftheprobabilitydistributionfunction,but\\nsupportoperationsthatimplicitlyrequireknowledgeofit,suchasdrawingsamples\\nfromthedistribution.Someofthesemodelsarestructuredprobabilisticmodels\\ndescribedintermsofgraphsandfactors,usingthelanguageofgraphicalmodels\\npresentedinchapter.\\xa0Otherscannoteasilybedescribedintermsoffactors, 16\\nbutrepresentprobabilitydistributionsnonetheless.\\n20.1BoltzmannMachines\\nBoltzmannmachineswereoriginallyintroducedasageneral“connectionist” ap-\\nproachtolearningarbitraryprobabilitydistributionsoverbinaryvectors(Fahlman\\ne t a l .,;1983Ackley1985Hinton1984HintonandSejnowski1986 e t a l .,; e t a l .,; ,).\\nVariantsoftheBoltzmannmachinethatincludeotherkindsofvariableshavelong\\nagosurpassedthepopularityoftheoriginal.Inthissectionwebrieﬂyintroduce\\nthebinaryBoltzmannmachineanddiscusstheissuesthatcomeupwhentryingto\\ntrainandperforminferenceinthemodel.\\nWedeﬁnetheBoltzmannmachineovera d-dimensionalbinaryrandomvector\\nx ∈{0 ,1}d.\\xa0TheBoltzmannmachineisanenergy-basedmodel(section),16.2.4\\n654', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7637c8a5-bf74-4a73-a8e1-5f66e6f67945', embedding=None, metadata={'page_label': '670', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nmeaningwedeﬁnethejointprobabilitydistributionusinganenergyfunction:\\nP() = xexp(()) − E x\\nZ, (20.1)\\nwhere E( x)istheenergyfunctionand Zisthepartitionfunctionthatensures\\nthat\\ue050\\nx P() = 1 x.TheenergyfunctionoftheBoltzmannmachineisgivenby\\nE() = x − x\\ue03eU x b−\\ue03ex , (20.2)\\nwhere Uisthe“weight”matrixofmodelparametersand bisthevectorofbias\\nparameters.\\nInthegeneralsettingoftheBoltzmannmachine,wearegivenasetoftraining\\nexamples,eachofwhichare n-dimensional.Equationdescribesthejoint 20.1\\nprobabilitydistributionovertheobservedvariables.Whilethisscenarioiscertainly\\nviable,itdoeslimitthekindsofinteractionsbetweentheobservedvariablesto\\nthosedescribedbytheweightmatrix.Speciﬁcally,itmeansthattheprobabilityof\\noneunitbeingonisgivenbyalinearmodel(logisticregression)fromthevaluesof\\ntheotherunits.\\nTheBoltzmannmachinebecomesmorepowerfulwhennotallthevariablesare\\nobserved.Inthiscase,thelatentvariables,canactsimilarlytohiddenunitsina\\nmulti-layerperceptronandmodelhigher-orderinteractionsamongthevisibleunits.\\nJustastheadditionofhiddenunitstoconvertlogisticregressionintoanMLPresults\\nintheMLPbeingauniversalapproximatoroffunctions,aBoltzmannmachine\\nwithhiddenunitsisnolongerlimitedtomodelinglinearrelationshipsbetween\\nvariables.Instead,theBoltzmannmachinebecomesauniversalapproximator of\\nprobabilitymassfunctionsoverdiscretevariables( ,). LeRouxandBengio2008\\nFormally,wedecomposetheunits xintotwosubsets:thevisibleunits vand\\nthelatent(orhidden)units.Theenergyfunctionbecomes h\\nE ,( v h v ) = −\\ue03eR v v−\\ue03eW h h−\\ue03eS h b−\\ue03ev c−\\ue03eh .(20.3)\\nBoltzmannMachineLearningLearningalgorithmsforBoltzmannmachines\\nareusuallybasedonmaximumlikelihood.AllBoltzmannmachineshavean\\nintractablepartitionfunction,sothemaximumlikelihoodgradientmustbeap-\\nproximatedusingthetechniquesdescribedinchapter.18\\nOneinterestingpropertyofBoltzmannmachineswhentrainedwithlearning\\nrulesbasedonmaximumlikelihoodisthattheupdateforaparticularweight\\nconnectingtwounitsdependsonlythestatisticsofthosetwounits,collected\\nunderdiﬀerentdistributions: P m o de l( v)and ˆ P da t a( v) P m o de l( h v|).Therestofthe\\n6 5 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eedeeccf-8b66-4715-b49f-1c8572d5a807', embedding=None, metadata={'page_label': '671', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nnetworkparticipatesinshapingthosestatistics,buttheweightcanbeupdated\\nwithoutknowinganythingabouttherestofthenetworkorhowthosestatisticswere\\nproduced.Thismeansthatthelearningruleis“local,”whichmakesBoltzmann\\nmachinelearningsomewhatbiologicallyplausible.\\xa0Itisconceivablethatifeach\\nneuronwerearandomvariableinaBoltzmannmachine,thentheaxonsand\\ndendritesconnectingtworandomvariablescouldlearnonlybyobservingtheﬁring\\npatternofthecellsthattheyactuallyphysicallytouch.Inparticular,inthe\\npositivephase,twounitsthatfrequentlyactivatetogetherhavetheirconnection\\nstrengthened.ThisisanexampleofaHebbianlearningrule(,)oftenHebb1949\\nsummarizedwiththemnemonic“ﬁretogether,wiretogether.”\\xa0Hebbian learning\\nrulesareamongtheoldesthypothesizedexplanationsforlearninginbiological\\nsystemsandremainrelevanttoday( ,). Giudice e t a l .2009\\nOtherlearningalgorithmsthatusemoreinformationthanlocalstatisticsseem\\ntorequireustohypothesizetheexistenceofmoremachinerythanthis.For\\nexample,forthebraintoimplementback-propagation inamultilayerperceptron,\\nitseemsnecessaryforthebraintomaintainasecondarycommunication networkfor\\ntransmittinggradientinformationbackwardsthroughthenetwork.Proposalsfor\\nbiologicallyplausibleimplementations(andapproximations)ofback-propagation\\nhavebeenmade(,;,)butremaintobevalidated,and Hinton2007aBengio2015\\nBengio2015()linksback-propagationofgradientstoinferenceinenergy-based\\nmodelssimilartotheBoltzmannmachine(butwithcontinuouslatentvariables).\\nThenegativephaseofBoltzmannmachinelearningissomewhatharderto\\nexplainfromabiologicalpointofview.Asarguedinsection,dreamsleep 18.2\\nmaybeaformofnegativephasesampling.Thisideaismorespeculativethough.\\n20.2RestrictedBoltzmannMachines\\nInventedunderthenameharmonium(,),restrictedBoltzmann Smolensky1986\\nmachinesaresomeofthemostcommonbuildingblocksofdeepprobabilisticmodels.\\nWehavebrieﬂydescribedRBMspreviously,insection.Herewereviewthe 16.7.1\\npreviousinformationandgointomoredetail.RBMsareundirectedprobabilistic\\ngraphicalmodelscontainingalayerofobservablevariablesandasinglelayerof\\nlatentvariables.RBMsmaybestacked(oneontopoftheother)toformdeeper\\nmodels.Seeﬁgureforsomeexamples.Inparticular,ﬁgureashowsthe 20.1 20.1\\ngraphstructureoftheRBMitself.Itisabipartitegraph,withnoconnections\\npermittedbetweenanyvariablesintheobservedlayerorbetweenanyunitsinthe\\nlatentlayer.\\n6 5 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b61f7ffe-6473-47a0-9e66-81263b03fd16', embedding=None, metadata={'page_label': '672', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nh 1 h 1 h 2 h 2 h 3 h 3\\nv 1 v 1 v 2 v 2 v 3 v 3h 4 h 4 h( 1 )\\n1 h( 1 )\\n1 h( 1 )\\n2 h( 1 )\\n2 h( 1 )\\n3 h( 1 )\\n3\\nv 1 v 1 v 2 v 2 v 3 v 3h( 2 )\\n1 h( 2 )\\n1 h( 2 )\\n2 h( 2 )\\n2h( 2 )\\n3h( 2 )\\n3\\nh( 1 )\\n4 h( 1 )\\n4\\n(a) (b)\\nh( 1 )\\n1h( 1 )\\n1h( 1 )\\n2h( 1 )\\n2h( 1 )\\n3 h( 1 )\\n3\\nv 1 v 1 v 2 v 2 v 3 v 3h( 2 )\\n1 h( 2 )\\n1 h( 2 )\\n2 h( 2 )\\n2 h( 2 )\\n3 h( 2 )\\n3\\nh( 1 )\\n4h( 1 )\\n4\\n(c)\\nFigure20.1:ExamplesofmodelsthatmaybebuiltwithrestrictedBoltzmannmachines.\\n( a )TherestrictedBoltzmannmachineitselfisanundirectedgraphicalmodelbasedon\\nabipartitegraph,withvisibleunitsinonepartofthegraphandhiddenunitsinthe\\notherpart.Therearenoconnectionsamongthevisibleunits,noranyconnectionsamong\\nthehiddenunits.\\xa0Typicallyeveryvisibleunitisconnectedtoeveryhiddenunitbutit\\nispossibletoconstructsparselyconnectedRBMssuchasconvolutionalRBMs.A ( b )\\ndeepbeliefnetworkisahybridgraphicalmodelinvolvingbothdirectedandundirected\\nconnections.LikeanRBM,ithasnointralayerconnections.However,aDBNhasmultiple\\nhiddenlayers,andthusthereareconnectionsbetweenhiddenunitsthatareinseparate\\nlayers.Allofthelocalconditionalprobabilitydistributionsneededbythedeepbelief\\nnetworkarecopieddirectlyfromthelocalconditionalprobabilitydistributionsofits\\nconstituentRBMs.Alternatively,wecouldalsorepresentthedeepbeliefnetworkwith\\nacompletelyundirectedgraph,butitwouldneedintralayerconnectionstocapturethe\\ndependenciesbetweenparents.AdeepBoltzmannmachineisanundirectedgraphical ( c )\\nmodelwithseverallayersoflatentvariables.LikeRBMsandDBNs,DBMslackintralayer\\nconnections.\\xa0DBMsarelesscloselytiedtoRBMsthanDBNsare.\\xa0Wheninitializinga\\nDBMfromastackofRBMs,itisnecessarytomodifytheRBMparametersslightly.Some\\nkindsofDBMsmaybetrainedwithoutﬁrsttrainingasetofRBMs.\\n6 5 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='92ce4440-ee2a-45a0-8deb-a5efe3f4ee70', embedding=None, metadata={'page_label': '673', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nWebeginwiththebinaryversionoftherestrictedBoltzmannmachine,butas\\nweseelaterthereareextensionstoothertypesofvisibleandhiddenunits.\\nMoreformally,lettheobservedlayerconsistofasetof n vbinaryrandom\\nvariableswhichwerefertocollectivelywiththevectorv.Werefertothelatentor\\nhiddenlayerof n hbinaryrandomvariablesas. h\\nLikethegeneralBoltzmannmachine,therestrictedBoltzmannmachineisan\\nenergy-basedmodelwiththejointprobabilitydistributionspeciﬁedbyitsenergy\\nfunction:\\nP , (= v vh= ) = h1\\nZexp(( )) − E v h , . (20.4)\\nTheenergyfunctionforanRBMisgivenby\\nE ,( v h b ) = −\\ue03ev c−\\ue03eh v−\\ue03eW h , (20.5)\\nandisthenormalizingconstantknownasthepartitionfunction: Z\\nZ=\\ue058\\nv\\ue058\\nhexp ( ) {− E v h ,} . (20.6)\\nItisapparentfromthedeﬁnitionofthepartitionfunction Zthatthenaivemethod\\nofcomputing Z(exhaustivelysummingoverallstates)couldbecomputationally\\nintractable,unlessacleverlydesignedalgorithmcouldexploitregularitiesinthe\\nprobabilitydistributiontocompute Zfaster.InthecaseofrestrictedBoltzmann\\nmachines, ()formallyprovedthatthepartitionfunction LongandServedio2010 Z\\nisintractable.Theintractablepartitionfunction Zimpliesthatthenormalized\\njointprobabilitydistributionisalsointractabletoevaluate. P() v\\n20.2.1ConditionalDistributions\\nThough P( v)isintractable,thebipartitegraphstructureoftheRBMhasthe\\nveryspecialpropertythatitsconditionaldistributions P(hv|)and P(vh|)are\\nfactorialandrelativelysimpletocomputeandtosamplefrom.\\nDerivingtheconditionaldistributionsfromthejointdistributionisstraightfor-\\nward:\\nP( ) = h v|P ,( h v)\\nP() v(20.7)\\n=1\\nP() v1\\nZexp\\ue06e\\nb\\ue03ev c+\\ue03eh v+\\ue03eW h\\ue06f\\n(20.8)\\n=1\\nZ\\ue030exp\\ue06e\\nc\\ue03eh v+\\ue03eW h\\ue06f\\n(20.9)\\n6 5 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='270636a0-f024-4c55-8cb0-65214a0bcd91', embedding=None, metadata={'page_label': '674', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\n=1\\nZ\\ue030exp\\uf8f1\\n\\uf8f2\\n\\uf8f3n h\\ue058\\nj = 1c j h j+n h\\ue058\\nj = 1v\\ue03eW : , j h j\\uf8fc\\n\\uf8fd\\n\\uf8fe(20.10)\\n=1\\nZ\\ue030n h\\ue059\\nj = 1exp\\ue06e\\nc j h j+ v\\ue03eW : , j h j\\ue06f\\n(20.11)\\nSinceweareconditioningonthevisibleunits v,wecantreattheseasconstant\\nwithrespecttothedistribution P(hv|).Thefactorialnatureoftheconditional\\nP(hv|)followsimmediately fromourabilitytowritethejointprobabilityover\\nthevector hastheproductof(unnormalized) distributionsovertheindividual\\nelements, h j.Itisnowasimplematterofnormalizingthedistributionsoverthe\\nindividualbinary h j.\\nP h( j= 1 ) =| v˜ P h( j= 1 )| v\\n˜ P h( j= 0 )+| v ˜ P h( j= 1 )| v(20.12)\\n=exp\\ue008\\nc j+ v\\ue03eW : , j\\ue009\\nexp0+exp {} { c j+ v\\ue03e W : , j}(20.13)\\n= σ\\ue010\\nc j+ v\\ue03eW : , j\\ue011\\n. (20.14)\\nWecannowexpressthefullconditionaloverthehiddenlayerasthefactorial\\ndistribution:\\nP( ) = h v|n h\\ue059\\nj = 1σ\\ue010\\n(2 1) (+ h− \\ue00c c W\\ue03ev)\\ue011\\nj.(20.15)\\nAsimilarderivationwillshowthattheotherconditionofinteresttous, P( v h|),\\nisalsoafactorialdistribution:\\nP( ) = v h|n v\\ue059\\ni = 1σ((2 1) (+ )) v− \\ue00c b W hi . (20.16)\\n20.2.2TrainingRestrictedBoltzmannMachines\\nBecausetheRBMadmitseﬃcientevaluationanddiﬀerentiation of˜ P( v)and\\neﬃcientMCMCsamplingintheformofblockGibbssampling,itcanreadilybe\\ntrainedwithanyofthetechniquesdescribedinchapterfortrainingmodels 18\\nthathaveintractablepartitionfunctions.\\xa0ThisincludesCD,SML(PCD),ratio\\nmatchingandsoon.Comparedtootherundirectedmodelsusedindeeplearning,\\ntheRBMisrelativelystraightforwardtotrainbecausewecancompute P(h| v)\\n6 5 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bf3269e-a07f-40d1-afc3-aa2915a21da8', embedding=None, metadata={'page_label': '675', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nexactlyinclosedform.Someotherdeepmodels,suchasthedeepBoltzmann\\nmachine,combineboththediﬃcultyofanintractablepartitionfunctionandthe\\ndiﬃcultyofintractableinference.\\n20.3DeepBeliefNetworks\\nDeepbeliefnetworks(DBNs)wereoneoftheﬁrstnon-convolutionalmodels\\ntosuccessfullyadmittrainingofdeeparchitectures(Hinton2006Hinton e t a l .,;,\\n2007b).Theintroduction ofdeepbeliefnetworksin2006beganthecurrentdeep\\nlearningrenaissance.Priortotheintroductionofdeepbeliefnetworks,deepmodels\\nwereconsideredtoodiﬃculttooptimize.Kernelmachineswithconvexobjective\\nfunctionsdominatedtheresearchlandscape.Deepbeliefnetworksdemonstrated\\nthatdeeparchitecturescanbesuccessful,byoutperformingkernelizedsupport\\nvectormachinesontheMNISTdataset( ,).Today,deepbelief Hinton e t a l .2006\\nnetworkshavemostlyfallenoutoffavorandarerarelyused,evencomparedto\\notherunsupervisedorgenerativelearningalgorithms,buttheyarestilldeservedly\\nrecognizedfortheirimportantroleindeeplearninghistory.\\nDeepbeliefnetworksaregenerativemodelswithseverallayersoflatentvariables.\\nThelatentvariablesaretypicallybinary,whilethevisibleunitsmaybebinary\\norreal.Therearenointralayerconnections.Usually,everyunitineachlayeris\\nconnectedtoeveryunitineachneighboringlayer,thoughitispossibletoconstruct\\nmoresparselyconnectedDBNs.Theconnectionsbetweenthetoptwolayersare\\nundirected.Theconnectionsbetweenallotherlayersaredirected,withthearrows\\npointedtowardthelayerthatisclosesttothedata.Seeﬁgurebforanexample. 20.1\\nADBNwith lhiddenlayerscontains lweightmatrices: W( 1 ), . . . , W( ) l.It\\nalsocontains l+1biasvectors: b( 0 ), . . . , b( ) l,with b( 0 )providingthebiasesforthe\\nvisiblelayer.TheprobabilitydistributionrepresentedbytheDBNisgivenby\\nP( h( ) l, h( 1 ) l−) exp∝\\ue010\\nb( ) l\\ue03eh( ) l+ b( 1 ) l−\\ue03eh( 1 ) l−+ h( 1 ) l−\\ue03eW( ) lh( ) l\\ue011\\n,(20.17)\\nP h(( ) k\\ni= 1 | h( + 1 ) k) = σ\\ue010\\nb( ) k\\ni+ W( + 1 ) k \\ue03e\\n: , i h( + 1 ) k\\ue011\\n∀∀∈ − i , k1 , . . . , l2 ,(20.18)\\nP v( i= 1 | h( 1 )) = σ\\ue010\\nb( 0 )\\ni+ W( 1 )\\ue03e\\n: , i h( 1 )\\ue011\\n∀ i .(20.19)\\nInthecaseofreal-valuedvisibleunits,substitute\\nv∼N\\ue010\\nv b;( 0 )+ W( 1 )\\ue03eh( 1 ), β− 1\\ue011\\n(20.20)\\n6 6 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4f89d213-c291-4851-bc52-531148302a89', embedding=None, metadata={'page_label': '676', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nwith βdiagonalfortractability.Generalizations tootherexponentialfamilyvisible\\nunitsarestraightforward,atleastintheory.ADBNwithonlyonehiddenlayeris\\njustanRBM.\\nTogenerateasamplefromaDBN,weﬁrstrunseveralstepsofGibbssampling\\nonthetoptwohiddenlayers.Thisstageisessentiallydrawingasamplefrom\\ntheRBMdeﬁnedbythetoptwohiddenlayers.Wecanthenuseasinglepassof\\nancestralsamplingthroughtherestofthemodeltodrawasamplefromthevisible\\nunits.\\nDeepbeliefnetworksincurmanyoftheproblemsassociatedwithbothdirected\\nmodelsandundirectedmodels.\\nInferenceinadeepbeliefnetworkisintractableduetotheexplainingaway\\neﬀectwithineachdirectedlayer,andduetotheinteractionbetweenthetwohidden\\nlayersthathaveundirectedconnections.Evaluatingormaximizingthestandard\\nevidencelowerboundonthelog-likelihoodisalsointractable,becausetheevidence\\nlowerboundtakestheexpectationofcliqueswhosesizeisequaltothenetwork\\nwidth.\\nEvaluatingormaximizingthelog-likelihoodrequiresnotjustconfrontingthe\\nproblemofintractableinferencetomarginalizeoutthelatentvariables,butalso\\ntheproblemofanintractablepartitionfunctionwithintheundirectedmodelof\\nthetoptwolayers.\\nTotrainadeepbeliefnetwork,onebeginsbytraininganRBMtomaximize\\nE v∼ pdatalog p( v)usingcontrastivedivergenceorstochasticmaximumlikelihood.\\nTheparametersoftheRBMthendeﬁnetheparametersoftheﬁrstlayerofthe\\nDBN.Next,asecondRBMistrainedtoapproximatelymaximize\\nE v∼ pdata Eh( 1 )∼ p( 1 ) ( h( 1 )| v )log p( 2 )( h( 1 )) (20.21)\\nwhere p( 1 )istheprobabilitydistributionrepresentedbytheﬁrstRBMand p( 2 )\\nistheprobabilitydistributionrepresentedbythesecondRBM.Inotherwords,\\nthesecondRBMistrainedtomodelthedistributiondeﬁnedbysamplingthe\\nhiddenunitsoftheﬁrstRBM,whentheﬁrstRBMisdrivenbythedata.\\xa0This\\nprocedurecanberepeatedindeﬁnitely,toaddasmanylayerstotheDBNas\\ndesired,witheachnewRBMmodelingthesamplesofthepreviousone.EachRBM\\ndeﬁnesanotherlayeroftheDBN.Thisprocedurecanbejustiﬁedasincreasinga\\nvariationallowerboundonthelog-likelihoodofthedataundertheDBN(Hinton\\ne t a l .,).2006\\nInmostapplications,noeﬀortismadetojointlytraintheDBNafterthegreedy\\nlayer-wiseprocedureiscomplete.However,itispossibletoperformgenerative\\nﬁne-tuningusingthewake-sleepalgorithm.\\n6 6 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1741d3b8-c8e8-4b06-9870-ed6d9b2d0758', embedding=None, metadata={'page_label': '677', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nThetrainedDBNmaybeuseddirectlyasagenerativemodel,butmostofthe\\ninterestinDBNsarosefromtheirabilitytoimproveclassiﬁcationmodels.Wecan\\ntaketheweightsfromtheDBNandusethemtodeﬁneanMLP:\\nh( 1 )= σ\\ue010\\nb( 1 )+ v\\ue03eW( 1 )\\ue011\\n. (20.22)\\nh( ) l= σ\\ue010\\nb( ) l\\ni+ h( 1 ) l−\\ue03eW( ) l\\ue011\\n∀∈ l2 , . . . , m ,(20.23)\\nAfterinitializingthisMLPwiththeweightsandbiaseslearnedviagenerative\\ntrainingoftheDBN,wemaytraintheMLPtoperformaclassiﬁcationtask.This\\nadditionaltrainingoftheMLPisanexampleofdiscriminativeﬁne-tuning.\\nThisspeciﬁcchoiceofMLPissomewhatarbitrary,comparedtomanyofthe\\ninferenceequationsinchapterthatarederivedfromﬁrstprinciples.ThisMLP 19\\nisaheuristicchoicethatseemstoworkwellinpracticeandisusedconsistently\\nintheliterature.Manyapproximate inferencetechniquesaremotivatedbytheir\\nabilitytoﬁndamaximally variationallowerboundonthelog-likelihood t i g h t\\nundersomesetofconstraints.Onecanconstructavariationallowerboundonthe\\nlog-likelihoodusingthehiddenunitexpectationsdeﬁnedbytheDBN’sMLP,but\\nthisistrueofprobabilitydistributionoverthehiddenunits,andthereisno a ny\\nreasontobelievethatthisMLPprovidesaparticularlytightbound.\\xa0Inparticular,\\ntheMLPignoresmanyimportantinteractionsintheDBNgraphicalmodel.The\\nMLPpropagatesinformationupwardfromthevisibleunitstothedeepesthidden\\nunits,butdoesnotpropagateanyinformationdownwardorsideways.TheDBN\\ngraphicalmodelhasexplainingawayinteractionsbetweenallofthehiddenunits\\nwithinthesamelayeraswellastop-downinteractionsbetweenlayers.\\nWhilethelog-likelihoodofaDBNisintractable,itmaybeapproximatedwith\\nAIS(SalakhutdinovandMurray2008,).Thispermitsevaluatingitsqualityasa\\ngenerativemodel.\\nTheterm“deepbeliefnetwork”iscommonlyusedincorrectlytorefertoany\\nkindofdeepneuralnetwork,evennetworkswithoutlatentvariablesemantics.\\nTheterm“deepbeliefnetwork”shouldreferspeciﬁcallytomodelswithundirected\\nconnectionsinthedeepestlayeranddirectedconnectionspointingdownward\\nbetweenallotherpairsofconsecutivelayers.\\nTheterm“deepbeliefnetwork”mayalsocausesomeconfusionbecausethe\\nterm“beliefnetwork”issometimesusedtorefertopurelydirectedmodels,while\\ndeepbeliefnetworkscontainanundirectedlayer.Deepbeliefnetworksalsoshare\\ntheacronymDBNwithdynamicBayesiannetworks(DeanandKanazawa1989,),\\nwhichareBayesiannetworksforrepresentingMarkovchains.\\n6 6 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1cce90fa-77e7-41d0-98fc-a9c08f7f10ac', embedding=None, metadata={'page_label': '678', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nh( 1 )\\n1 h( 1 )\\n1 h( 1 )\\n2 h( 1 )\\n2 h( 1 )\\n3 h( 1 )\\n3\\nv 1 v 1 v 2 v 2 v 3 v 3h( 2 )\\n1 h( 2 )\\n1 h( 2 )\\n2 h( 2 )\\n2h( 2 )\\n3h( 2 )\\n3\\nh( 1 )\\n4 h( 1 )\\n4\\nFigure20.2:ThegraphicalmodelforadeepBoltzmannmachinewithonevisiblelayer\\n(bottom)andtwohiddenlayers.Connectionsareonlybetweenunitsinneighboringlayers.\\nTherearenointralayerlayerconnections.\\n20.4DeepBoltzmannMachines\\nAdeepBoltzmannmachineorDBM(Salakhutdino vandHinton2009a,)is\\nanotherkindofdeep,generativemodel.\\xa0Unlikethedeepbeliefnetwork(DBN),\\nitisanentirelyundirectedmodel.UnliketheRBM,theDBMhasseverallayers\\noflatentvariables(RBMshavejustone).\\xa0ButliketheRBM,withineachlayer,\\neachofthevariablesaremutuallyindependent,conditionedonthevariablesin\\ntheneighboringlayers.Seeﬁgureforthegraphstructure.DeepBoltzmann 20.2\\nmachineshavebeenappliedtoavarietyoftasksincludingdocumentmodeling\\n(Srivastava2013 e t a l .,).\\nLikeRBMsandDBNs,\\xa0DBMstypicallycontainonlybinaryunits—aswe\\nassumeforsimplicityofourpresentationofthemodel—butitisstraightforward\\ntoincludereal-valuedvisibleunits.\\nADBMisanenergy-basedmodel,meaningthatthethejointprobability\\ndistributionoverthemodelvariablesisparametrized byanenergyfunction E.In\\nthecaseofadeepBoltzmannmachinewithonevisiblelayer, v,andthreehidden\\nlayers, h( 1 ), h( 2 )and h( 3 ),thejointprobabilityisgivenby:\\nP\\ue010\\nv h ,( 1 ), h( 2 ), h( 3 )\\ue011\\n=1\\nZ() θexp\\ue010\\n− E ,( v h( 1 ), h( 2 ), h( 3 );) θ\\ue011\\n.(20.24)\\nTosimplifyourpresentation,weomitthebiasparametersbelow.TheDBMenergy\\nfunctionisthendeﬁnedasfollows:\\nE ,( v h( 1 ), h( 2 ), h( 3 );) = θ − v\\ue03eW( 1 )h( 1 )− h( 1 )\\ue03eW( 2 )h( 2 )− h( 2 )\\ue03eW( 3 )h( 3 ).\\n(20.25)\\n6 6 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a55e226a-5cf0-483b-baec-874c4d249044', embedding=None, metadata={'page_label': '679', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nh( 1 )\\n1 h( 1 )\\n1 h( 1 )\\n2 h( 1 )\\n2h( 1 )\\n3h( 1 )\\n3\\nv 1 v 1 v 2 v 2h( 2 )\\n1h( 2 )\\n1h( 2 )\\n2h( 2 )\\n2h( 2 )\\n3 h( 2 )\\n3h( 3 )\\n1 h( 3 )\\n1 h( 3 )\\n2 h( 3 )\\n2\\nv1\\nv2h( 2 )\\n1 h( 2 )\\n1\\nh( 2 )\\n2 h( 2 )\\n2\\nh( 2 )\\n3h( 2 )\\n3\\nh( 1 )\\n1 h( 1 )\\n1\\nh( 1 )\\n2 h( 1 )\\n2\\nh( 1 )\\n3 h( 1 )\\n3h( 3 )\\n1 h( 3 )\\n1\\nh( 3 )\\n2 h( 3 )\\n2\\nFigure20.3:AdeepBoltzmannmachine,re-arrangedtorevealitsbipartitegraphstructure.\\nIncomparisontotheRBMenergyfunction(equation),theDBMenergy 20.5\\nfunctionincludesconnectionsbetweenthehiddenunits(latentvariables)inthe\\nformoftheweightmatrices( W( 2 )and W( 3 )).Aswewillsee,theseconnections\\nhavesigniﬁcantconsequencesforboththemodelbehavioraswellashowwego\\naboutperforminginferenceinthemodel.\\nIncomparisontofullyconnectedBoltzmannmachines(witheveryunitcon-\\nnectedtoeveryotherunit),theDBMoﬀerssomeadvantagesthataresimilar\\ntothoseoﬀeredbytheRBM.Speciﬁcally,asillustratedinﬁgure,theDBM20.3\\nlayerscanbeorganizedintoabipartitegraph,withoddlayersononesideand\\nevenlayersontheother.Thisimmediatelyimpliesthatwhenweconditiononthe\\nvariablesintheevenlayer,thevariablesintheoddlayersbecomeconditionally\\nindependent.Ofcourse,whenweconditiononthevariablesintheoddlayers,the\\nvariablesintheevenlayersalsobecomeconditionallyindependent.\\nThebipartitestructureoftheDBMmeansthatwecanapplythesameequa-\\ntionswehavepreviouslyusedfortheconditionaldistributionsofanRBMto\\ndeterminetheconditionaldistributionsinaDBM.Theunitswithinalayerare\\nconditionallyindependentfromeachothergiventhevaluesoftheneighboring\\nlayers,sothedistributionsoverbinaryvariablescanbefullydescribedbythe\\nBernoulliparametersgivingtheprobabilityofeachunitbeingactive.Inour\\nexamplewithtwohiddenlayers,theactivationprobabilities aregivenby:\\nP v( i= 1 | h( 1 )) = σ\\ue010\\nW( 1 )\\ni , : h( 1 )\\ue011\\n, (20.26)\\n6 6 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b20e9bf1-37df-4037-94ac-0e84448a48c6', embedding=None, metadata={'page_label': '680', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nP h(( 1 )\\ni= 1 | v h ,( 2 )) = σ\\ue010\\nv\\ue03eW( 1 )\\n: , i+ W( 2 )\\ni , : h( 2 )\\ue011\\n(20.27)\\nand\\nP h(( 2 )\\nk= 1 | h( 1 )) = σ\\ue010\\nh( 1 )\\ue03eW( 2 )\\n: , k\\ue011\\n. (20.28)\\nThebipartitestructuremakesGibbssamplinginadeepBoltzmannmachine\\neﬃcient.\\xa0ThenaiveapproachtoGibbssamplingistoupdateonlyonevariable\\natatime.RBMsallowallofthevisibleunitstobeupdatedinoneblockandall\\nofthehiddenunitstobeupdatedinasecondblock.Onemightnaivelyassume\\nthataDBMwith llayersrequires l+1updates,witheachiterationupdatinga\\nblockconsistingofonelayerofunits.Instead,itispossibletoupdateallofthe\\nunitsinonlytwoiterations.Gibbssamplingcanbedividedintotwoblocksof\\nupdates,oneincludingallevenlayers(includingthevisiblelayer)andtheother\\nincludingalloddlayers.DuetothebipartiteDBMconnectionpattern,given\\ntheevenlayers,thedistributionovertheoddlayersisfactorialandthuscanbe\\nsampledsimultaneouslyandindependentlyasablock.Likewise,giventheodd\\nlayers,theevenlayerscanbesampledsimultaneouslyandindependentlyasa\\nblock.Eﬃcientsamplingisespeciallyimportantfortrainingwiththestochastic\\nmaximumlikelihoodalgorithm.\\n20.4.1InterestingProperties\\nDeepBoltzmannmachineshavemanyinterestingproperties.\\nDBMsweredevelopedafterDBNs.ComparedtoDBNs,theposteriordistribu-\\ntion P( h v|)issimplerforDBMs.Somewhatcounterintuitively,thesimplicityof\\nthisposteriordistributionallowsricherapproximationsoftheposterior.Inthecase\\noftheDBN,weperformclassiﬁcationusingaheuristicallymotivatedapproximate\\ninferenceprocedure,inwhichweguessthatareasonablevalueforthemeanﬁeld\\nexpectationofthehiddenunitscanbeprovidedbyanupwardpassthroughthe\\nnetworkinanMLPthatusessigmoidactivationfunctionsandthesameweightsas\\ntheoriginalDBN.distribution A ny Q( h)maybeusedtoobtainavariationallower\\nboundonthelog-likelihood.Thisheuristicprocedurethereforeallowsustoobtain\\nsuchabound.However,theboundisnotexplicitlyoptimizedinanyway,sothe\\nboundmaybefarfromtight.Inparticular,theheuristicestimateof Qignores\\ninteractionsbetweenhiddenunitswithinthesamelayeraswellasthetop-down\\nfeedbackinﬂuenceofhiddenunitsindeeperlayersonhiddenunitsthatarecloser\\ntotheinput.BecausetheheuristicMLP-basedinferenceprocedureintheDBN\\nisnotabletoaccountfortheseinteractions, theresulting Qispresumablyfar\\n6 6 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='465af405-6774-4e99-a23d-e8531abce4eb', embedding=None, metadata={'page_label': '681', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nfromoptimal.InDBMs,allofthehiddenunitswithinalayerareconditionally\\nindependentgiventheotherlayers.\\xa0Thislackofintralayerinteractionmakesit\\npossibletouseﬁxedpointequationstoactuallyoptimizethevariationallower\\nboundandﬁndthetrueoptimalmeanﬁeldexpectations(towithinsomenumerical\\ntolerance).\\nTheuseofpropermeanﬁeldallowstheapproximate inferenceprocedurefor\\nDBMstocapturetheinﬂuenceoftop-downfeedbackinteractions. Thismakes\\nDBMsinterestingfromthepointofviewofneuroscience,becausethehumanbrain\\nisknowntousemanytop-downfeedbackconnections.Becauseofthisproperty,\\nDBMshavebeenusedascomputational modelsofrealneuroscientiﬁcphenomena\\n(,; Series e t a l .2010Reichert2011 e t a l .,).\\nOneunfortunatepropertyofDBMsisthatsamplingfromthemisrelatively\\ndiﬃcult.DBNsonlyneedtouseMCMCsamplingintheirtoppairoflayers.The\\notherlayersareusedonlyattheendofthesamplingprocess,inoneeﬃcient\\nancestralsamplingpass.TogenerateasamplefromaDBM,itisnecessaryto\\nuseMCMCacrossalllayers,witheverylayerofthemodelparticipating inevery\\nMarkovchaintransition.\\n20.4.2DBMMeanFieldInference\\nTheconditionaldistributionoveroneDBMlayergiventheneighboringlayersis\\nfactorial.IntheexampleoftheDBMwithtwohiddenlayers,thesedistributions\\nare P( v h|( 1 )), P( h( 1 )| v h ,( 2 ))and P( h( 2 )| h( 1 )).Thedistributionover a l l\\nhiddenlayersgenerallydoesnotfactorizebecauseofinteractionsbetweenlayers.\\nIntheexamplewithtwohiddenlayers, P( h( 1 ), h( 2 )| v)doesnotfactorizeduedue\\ntotheinteractionweights W( 2 )between h( 1 )and h( 2 )whichrenderthesevariables\\nmutuallydependent.\\nAswasthecasewiththeDBN,wearelefttoseekoutmethodstoapproximate\\ntheDBMposteriordistribution.\\xa0However,unliketheDBN,theDBMposterior\\ndistributionovertheirhiddenunits—whilecomplicated—is easytoapproximate\\nwithavariationalapproximation(asdiscussedinsection),\\xa0speciﬁcallya 19.4\\nmeanﬁeldapproximation. Themeanﬁeldapproximation isasimpleformof\\nvariationalinference,wherewerestricttheapproximatingdistributiontofully\\nfactorialdistributions.InthecontextofDBMs,themeanﬁeldequationscapture\\nthebidirectionalinteractionsbetweenlayers.Inthissectionwederivetheiterative\\napproximateinferenceprocedureoriginallyintroducedinSalakhutdinovandHinton\\n().2009a\\nInvariationalapproximations toinference,weapproachthetaskofapproxi-\\n6 6 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='21c05c0b-cbec-49d2-89a9-dcd951d95be4', embedding=None, metadata={'page_label': '682', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nmatingaparticulartargetdistribution—inourcase,theposteriordistributionover\\nthehiddenunitsgiventhevisibleunits—bysomereasonablysimplefamilyofdis-\\ntributions.Inthecaseofthemeanﬁeldapproximation, theapproximating family\\nisthesetofdistributionswherethehiddenunitsareconditionallyindependent.\\nWenowdevelopthemeanﬁeldapproachfortheexamplewithtwohidden\\nlayers.Let Q( h( 1 ), h( 2 )| v)betheapproximation of P( h( 1 ), h( 2 )| v).Themean\\nﬁeldassumptionimpliesthat\\nQ( h( 1 ), h( 2 )| v) =\\ue059\\njQ h(( 1 )\\nj| v)\\ue059\\nkQ h(( 2 )\\nk| v) .(20.29)\\nThemeanﬁeldapproximationattemptstoﬁndamemberofthisfamilyof\\ndistributionsthatbestﬁtsthetrueposterior P( h( 1 ), h( 2 )| v).\\xa0Importantly ,the\\ninferenceprocessmustberunagaintoﬁndadiﬀerentdistribution Qeverytime\\nweuseanewvalueof. v\\nOnecanconceiveofmanywaysofmeasuringhowwell Q( h v|)ﬁts P( h v|).\\nThemeanﬁeldapproachistominimize\\nKL( ) = Q P\\ue06b\\ue058\\nhQ( h( 1 ), h( 2 )| v)log\\ue020\\nQ( h( 1 ), h( 2 )| v)\\nP( h( 1 ) , h( 2 )| v)\\ue021\\n.(20.30)\\nIngeneral,wedonothavetoprovideaparametricformoftheapproximating\\ndistributionbeyondenforcingtheindependenceassumptions.Thevariational\\napproximationprocedureisgenerallyabletorecoverafunctionalformofthe\\napproximatedistribution.However,inthecaseofameanﬁeldassumptionon\\nbinaryhiddenunits(thecasewearedevelopinghere)thereisnolossofgenerality\\nresultingfromﬁxingaparametrization ofthemodelinadvance.\\nWeparametrize QasaproductofBernoullidistributions,thatisweassociate\\ntheprobabilityofeachelementof h( 1 )withaparameter.Speciﬁcally,foreach j,\\nˆh( 1 )\\nj= Q( h( 1 )\\nj= 1| v),where ˆh( 1 )\\nj∈[0 ,1]andforeach k,ˆh( 2 )\\nk= Q( h( 2 )\\nk= 1| v),\\nwhere ˆ h( 2 )\\nk∈[01] ,.Thuswehavethefollowingapproximationtotheposterior:\\nQ( h( 1 ), h( 2 )| v) =\\ue059\\njQ h(( 1 )\\nj| v)\\ue059\\nkQ h(( 2 )\\nk| v) (20.31)\\n=\\ue059\\nj(ˆ h( 1 )\\nj)h( 1 )\\nj(1−ˆh( 1 )\\nj)( 1− h( 1 )\\nj )×\\ue059\\nk(ˆh( 2 )\\nk)h( 2 )\\nk(1−ˆh( 2 )\\nk)( 1− h( 2 )\\nk).\\n(20.32)\\nOfcourse,forDBMswithmorelayerstheapproximateposteriorparametrization\\ncanbeextendedintheobviousway,exploitingthebipartitestructureofthegraph\\n6 6 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='583cd0e2-065f-4f78-9815-d4b596398a62', embedding=None, metadata={'page_label': '683', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\ntoupdatealloftheevenlayerssimultaneouslyandthentoupdatealloftheodd\\nlayerssimultaneously,followingthesamescheduleasGibbssampling.\\nNowthatwehavespeciﬁedourfamilyofapproximating distributions Q,it\\nremainstospecifyaprocedureforchoosingthememberofthisfamilythatbest\\nﬁts P.Themoststraightforwardwaytodothisistousethemeanﬁeldequations\\nspeciﬁedbyequation.Theseequationswerederivedbysolvingforwherethe 19.56\\nderivativesofthevariationallowerboundarezero.Theydescribeinanabstract\\nmannerhowtooptimizethevariationallowerboundforanymodel,simplyby\\ntakingexpectationswithrespectto. Q\\nApplyingthesegeneralequations,weobtaintheupdaterules(again,ignoring\\nbiasterms):\\nˆh( 1 )\\nj= σ\\ue020\\ue058\\niv i W( 1 )\\ni , j+\\ue058\\nk\\ue030W( 2 )\\nj , k\\ue030ˆ h( 2 )\\nk\\ue030\\ue021\\n, j∀ (20.33)\\nˆh( 2 )\\nk= σ\\uf8eb\\n\\uf8ed\\ue058\\nj\\ue030W( 2 )\\nj\\ue030 , kˆh( 1 )\\nj\\ue030\\uf8f6\\n\\uf8f8 , k .∀ (20.34)\\nAtaﬁxedpointofthissystemofequations,wehavealocalmaximumofthe\\nvariationallowerbound L( Q).Thustheseﬁxedpointupdateequationsdeﬁnean\\niterativealgorithmwherewealternateupdatesofˆh( 1 )\\nj(usingequation)and20.33\\nupdatesofˆh( 2 )\\nk(usingequation).OnsmallproblemssuchasMNIST,asfew 20.34\\nasteniterationscanbesuﬃcienttoﬁndanapproximate positivephasegradient\\nforlearning,andﬁftyusuallysuﬃcetoobtainahighqualityrepresentationof\\nasinglespeciﬁcexampletobeusedforhigh-accuracy classiﬁcation.Extending\\napproximatevariationalinferencetodeeperDBMsisstraightforward.\\n20.4.3DBMParameterLearning\\nLearningintheDBMmustconfrontboththechallengeofanintractablepartition\\nfunction,usingthetechniquesfromchapter,andthechallengeofanintractable 18\\nposteriordistribution,usingthetechniquesfromchapter.19\\nAsdescribedinsection,variationalinferenceallowstheconstructionof 20.4.2\\nadistribution Q( h v|)thatapproximates theintractable P( h v|).Learningthen\\nproceedsbymaximizing L( v θ , Q ,),thevariationallowerboundontheintractable\\nlog-likelihood, . log(;) P v θ\\n6 6 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3112f43b-d302-4c0c-8ad3-4d21a84ef512', embedding=None, metadata={'page_label': '684', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nForadeepBoltzmannmachinewithtwohiddenlayers,isgivenby L\\nL( ) = Q , θ\\ue058\\ni\\ue058\\nj\\ue030v i W( 1 )\\ni , j\\ue030ˆh( 1 )\\nj\\ue030+\\ue058\\nj\\ue030\\ue058\\nk\\ue030ˆh( 1 )\\nj\\ue030 W( 2 )\\nj\\ue030 , k\\ue030ˆh( 2 )\\nk\\ue030− H log()+ Z θ () Q .(20.35)\\nThisexpressionstillcontainsthelogpartitionfunction, log Z( θ).Becauseadeep\\nBoltzmannmachinecontainsrestrictedBoltzmannmachinesascomponents,the\\nhardnessresultsforcomputingthepartitionfunctionandsamplingthatapplyto\\nrestrictedBoltzmannmachinesalsoapplytodeepBoltzmannmachines.Thismeans\\nthatevaluatingtheprobabilitymassfunctionofaBoltzmannmachinerequires\\napproximatemethodssuchasannealedimportancesampling.Likewise,training\\nthemodelrequiresapproximationstothegradientofthelogpartitionfunction.See\\nchapterforageneraldescriptionofthesemethods.DBMsaretypicallytrained 18\\nusingstochasticmaximumlikelihood.Manyoftheothertechniquesdescribedin\\nchapterarenotapplicable.Techniquessuchaspseudolikelihoodrequirethe 18\\nabilitytoevaluatetheunnormalized probabilities, ratherthanmerelyobtaina\\nvariationallowerboundonthem.ContrastivedivergenceisslowfordeepBoltzmann\\nmachinesbecausetheydonotalloweﬃcientsamplingofthehiddenunitsgiventhe\\nvisibleunits—instead,contrastivedivergencewouldrequireburninginaMarkov\\nchaineverytimeanewnegativephasesampleisneeded.\\nThenon-variationalversionofstochasticmaximumlikelihoodalgorithmwas\\ndiscussedearlier,insection.\\xa0Variationalstochasticmaximumlikelihoodas 18.2\\nappliedtotheDBMisgiveninalgorithm .Recallthatwedescribeasimpliﬁed 20.1\\nvarientoftheDBMthatlacksbiasparameters;includingthemistrivial.\\n20.4.4Layer-WisePretraining\\nUnfortunately,trainingaDBMusingstochasticmaximumlikelihood(asdescribed\\nabove)fromarandominitialization usuallyresultsinfailure.Insomecases,the\\nmodelfailstolearntorepresentthedistributionadequately.Inothercases,the\\nDBMmayrepresentthedistributionwell,butwithnohigherlikelihoodthancould\\nbeobtainedwithjustanRBM.ADBMwithverysmallweightsinallbuttheﬁrst\\nlayerrepresentsapproximatelythesamedistributionasanRBM.\\nVarioustechniquesthatpermitjointtraininghavebeendevelopedandare\\ndescribedinsection.However,theoriginalandmostpopularmethodfor 20.4.5\\novercomingthejointtrainingproblemofDBMsisgreedylayer-wisepretraining.\\nInthismethod,eachlayeroftheDBMistrainedinisolationasanRBM.The\\nﬁrstlayeristrainedtomodeltheinputdata.EachsubsequentRBMistrainedto\\nmodelsamplesfromthepreviousRBM’sposteriordistribution.\\xa0Afterallofthe\\n6 6 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='04bb3510-40c0-4d85-a32f-56f957f07d5e', embedding=None, metadata={'page_label': '685', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nAlgorithm20.1Thevariationalstochasticmaximumlikelihoodalgorithmfor\\ntrainingaDBMwithtwohiddenlayers.\\nSet,thestepsize,toasmallpositivenumber \\ue00f\\nSet k,thenumberofGibbssteps,highenoughtoallowaMarkovchainof\\np( v h ,( 1 ), h( 2 ); θ+ \\ue00f∆ θ)toburnin,startingfromsamplesfrom p( v h ,( 1 ), h( 2 ); θ).\\nInitializethreematrices,˜ V,˜ H( 1 )and ˜ H( 2 )eachwith mrowssettorandom\\nvalues(e.g.,fromBernoullidistributions,possiblywithmarginalsmatchedto\\nthemodel’smarginals).\\nwhilenotconverged(learningloop)do\\nSampleaminibatchof mexamplesfromthetrainingdataandarrangethem\\nastherowsofadesignmatrix. V\\nInitializematrices ˆ H( 1 )and ˆ H( 2 ),possiblytothemodel’smarginals.\\nwhilenotconverged(meanﬁeldinferenceloop)do\\nˆ H( 1 )← σ\\ue010\\nV W( 1 )+ˆ H( 2 )W( 2 )\\ue03e\\ue011\\n.\\nˆ H( 2 )← σ\\ue010\\nˆ H( 1 )W( 2 )\\ue011\\n.\\nendwhile\\n∆W( 1 )←1\\nmV\\ue03eˆ H( 1 )\\n∆W( 2 )←1\\nmˆ H( 1 )\\ue03eˆ H( 2 )\\nfor do l k = 1to(Gibbssampling)\\nGibbsblock1:\\n∀ i , j ,˜ V i , jsampledfrom P(˜ V i , j= 1) = σ\\ue012\\nW( 1 )\\nj , :\\ue010\\n˜ H( 1 )\\ni , :\\ue011\\ue03e\\ue013\\n.\\n∀ i , j ,˜ H( 2 )\\ni , jsampledfrom P(˜ H( 2 )\\ni , j= 1) = σ\\ue010\\n˜ H( 1 )\\ni , : W( 2 )\\n: , j\\ue011\\n.\\nGibbsblock2:\\n∀ i , j ,˜ H( 1 )\\ni , jsampledfrom P(˜ H( 1 )\\ni , j= 1) = σ\\ue010\\n˜ V i , : W( 1 )\\n: , j+˜ H( 2 )\\ni , : W( 2 )\\ue03e\\nj , :\\ue011\\n.\\nendfor\\n∆W( 1 )←∆W( 1 )−1\\nmV\\ue03e˜ H( 1 )\\n∆W( 2 )←∆W( 2 )−1\\nm˜ H( 1 )\\ue03e˜ H( 2 )\\nW( 1 )← W( 1 )+ \\ue00f∆W( 1 )(thisisacartoonillustration,inpracticeuseamore\\neﬀectivealgorithm,suchasmomentumwithadecayinglearningrate)\\nW( 2 )← W( 2 )+∆ \\ue00fW( 2 )\\nendwhile\\n6 7 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1ca3d34d-cbbd-43f8-be68-167bb161ed9c', embedding=None, metadata={'page_label': '686', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nRBMshavebeentrainedinthisway,theycanbecombinedtoformaDBM.The\\nDBMmaythenbetrainedwithPCD.TypicallyPCDtrainingwillmakeonlya\\nsmallchangeinthemodel’sparametersanditsperformanceasmeasuredbythe\\nlog-likelihooditassignstothedata,oritsabilitytoclassifyinputs.Seeﬁgure20.4\\nforanillustrationofthetrainingprocedure.\\nThisgreedylayer-wisetrainingprocedureisnotjustcoordinateascent.Itbears\\nsomepassingresemblancetocoordinateascentbecauseweoptimizeonesubsetof\\ntheparametersateachstep.Thetwomethodsdiﬀerbecausethegreedylayer-wise\\ntrainingprocedureusesadiﬀerentobjectivefunctionateachstep.\\nGreedylayer-wisepretrainingofaDBMdiﬀersfromgreedylayer-wisepre-\\ntrainingofaDBN.TheparametersofeachindividualRBMmaybecopiedto\\nthecorrespondingDBNdirectly.InthecaseoftheDBM,theRBMparameters\\nmustbemodiﬁedbeforeinclusionintheDBM.Alayerinthemiddleofthestack\\nofRBMsistrainedwithonlybottom-upinput,butafterthestackiscombined\\ntoformtheDBM,thelayerwillhavebothbottom-upandtop-downinput.\\xa0To\\naccountforthiseﬀect,SalakhutdinovandHinton2009a()advocatedividingthe\\nweightsofallbutthetopandbottomRBMinhalfbeforeinsertingthemintothe\\nDBM.Additionally,thebottomRBMmustbetrainedusingtwo“copies”ofeach\\nvisibleunitandtheweightstiedtobeequalbetweenthetwocopies.Thismeans\\nthattheweightsareeﬀectivelydoubledduringtheupwardpass.Similarly,thetop\\nRBMshouldbetrainedwithtwocopiesofthetopmostlayer.\\nObtainingthestateoftheartresultswiththedeepBoltzmannmachinerequires\\namodiﬁcationofthestandardSMLalgorithm,whichistouseasmallamountof\\nmeanﬁeldduringthenegativephaseofthejointPCDtrainingstep(Salakhutdinov\\nandHinton2009a,).\\xa0Speciﬁcally,theexpectationoftheenergygradientshould\\nbecomputedwithrespecttothemeanﬁelddistributioninwhichalloftheunits\\nareindependentfromeachother.Theparametersofthismeanﬁelddistribution\\nshouldbeobtainedbyrunningthemeanﬁeldﬁxedpointequationsforjustone\\nstep.See ()foracomparisonoftheperformanceofcentered Goodfellow e t a l .2013b\\nDBMswithandwithouttheuseofpartialmeanﬁeldinthenegativephase.\\n20.4.5JointlyTrainingDeepBoltzmannMachines\\nClassicDBMsrequiregreedyunsupervisedpretraining,andtoperformclassiﬁcation\\nwell,requireaseparateMLP-basedclassiﬁerontopofthehiddenfeaturesthey\\nextract.Thishassomeundesirableproperties.Itishardtotrackperformance\\nduringtrainingbecausewecannotevaluatepropertiesofthefullDBMwhile\\ntrainingtheﬁrstRBM.Thus,itishardtotellhowwellourhyperparameters\\n6 7 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e0453c0b-6bd8-4cec-a62a-7e5d4c969ffd', embedding=None, metadata={'page_label': '687', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nd)a) b)\\nc )\\nFigure20.4:ThedeepBoltzmannmachinetrainingprocedureusedtoclassifytheMNIST\\ndataset(SalakhutdinovandHinton2009aSrivastava2014 ,; e t a l .,).TrainanRBM ( a )\\nbyusingCDtoapproximatelymaximizelog P( v).TrainasecondRBMthatmodels ( b )\\nh( 1 )andtargetclassybyusingCD- ktoapproximatelymaximizelog P( h( 1 ),y)where\\nh( 1 )isdrawnfromtheﬁrstRBM’sposteriorconditionedonthedata.Increase kfrom1\\nto20duringlearning.CombinethetwoRBMsintoaDBM.Trainittoapproximately ( c )\\nmaximizelog P(v ,y)usingstochasticmaximumlikelihoodwith k= 5.Delete ( d )yfrom\\nthemodel.Deﬁneanewsetoffeatures h( 1 )and h( 2 )thatareobtainedbyrunningmean\\nﬁeldinferenceinthemodellackingy.UsethesefeaturesasinputtoanMLPwhose\\nstructureisthesameasanadditionalpassofmeanﬁeld,withanadditionaloutputlayer\\nfortheestimateofy.InitializetheMLP’sweightstobethesameastheDBM’sweights.\\nTraintheMLPtoapproximatelymaximizelog P(y|v)usingstochasticgradientdescent\\nanddropout.Figurereprintedfrom( ,). Goodfellow e t a l .2013b\\n6 7 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0f537c1c-d283-4dbf-bc82-1e0fc5a59b1a', embedding=None, metadata={'page_label': '688', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nareworkinguntilquitelateinthetrainingprocess.Softwareimplementations\\nofDBMsneedtohavemanydiﬀerentcomponentsforCDtrainingofindividual\\nRBMs,PCDtrainingofthefullDBM,andtrainingbasedonback-propagation\\nthroughtheMLP.Finally,theMLPontopoftheBoltzmannmachinelosesmany\\noftheadvantagesoftheBoltzmannmachineprobabilisticmodel,suchasbeing\\nabletoperforminferencewhensomeinputvaluesaremissing.\\nTherearetwomainwaystoresolvethejointtrainingproblemofthedeep\\nBoltzmannmachine.The\\xa0ﬁrstisthecentereddeepBoltzmann\\xa0machine\\n(MontavonandMuller2012,),whichreparametrizes themodelinordertomake\\ntheHessianofthecostfunctionbetter-conditionedatthebeginningofthelearning\\nprocess.Thisyieldsamodelthatcanbetrainedwithoutagreedylayer-wise\\npretrainingstage.Theresultingmodelobtainsexcellenttestsetlog-likelihood\\nandproduceshighqualitysamples.Unfortunately,itremainsunabletocompete\\nwithappropriately regularizedMLPsasaclassiﬁer.Thesecondwaytojointly\\ntrainadeepBoltzmannmachineistouseamulti-predictiondeepBoltzmann\\nmachine(Goodfellow2013b e t a l .,).Thismodelusesanalternativetraining\\ncriterionthatallowstheuseoftheback-propagationalgorithminordertoavoid\\ntheproblemswithMCMCestimatesofthegradient.Unfortunately,\\xa0thenew\\ncriteriondoesnotleadtogoodlikelihoodorsamples,but,comparedtotheMCMC\\napproach,itdoesleadtosuperiorclassiﬁcationperformanceandabilitytoreason\\nwellaboutmissinginputs.\\nThecenteringtrickfortheBoltzmannmachineiseasiesttodescribeifwe\\nreturntothegeneralviewofaBoltzmannmachineasconsistingofasetofunits\\nxwithaweightmatrix Uandbiases b.Recallfromequationthatheenergy 20.2\\nfunctionisgivenby\\nE() = x − x\\ue03eU x b−\\ue03ex . (20.36)\\nUsing\\xa0diﬀerent\\xa0sparsity\\xa0patternsin\\xa0theweight\\xa0matrix U,\\xa0wecan\\xa0implemen t\\nstructuresofBoltzmannmachines,suchasRBMs,orDBMswithdiﬀerentnumbers\\noflayers.Thisisaccomplishedbypartitioning xintovisibleandhiddenunitsand\\nzeroingoutelementsof Uforunitsthatdonotinteract.ThecenteredBoltzmann\\nmachineintroducesavectorthatissubtractedfromallofthestates: µ\\nE\\ue030(; ) = ( ) x U b , − x µ−\\ue03eU x µ x µ (−)(− −)\\ue03eb .(20.37)\\nTypically µisahyperparameterﬁxedatthebeginningoftraining.Itisusu-\\nallychosentomakesurethat x µ− ≈0whenthemodelisinitialized.This\\nreparametrization doesnotchangethesetofprobabilitydistributionsthatthe\\nmodelcanrepresent,butitdoeschangethedynamicsofstochasticgradientdescent\\nappliedtothelikelihood.Speciﬁcally,inmanycases,thisreparametrization results\\n6 7 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='33926cf2-6f3d-4b10-b2a5-4fa7aaa89f48', embedding=None, metadata={'page_label': '689', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\ninaHessianmatrixthatisbetterconditioned. ()experimentally Melchior e t a l .2013\\nconﬁrmedthattheconditioningoftheHessianmatriximproves,andobservedthat\\nthecenteringtrickisequivalenttoanotherBoltzmannmachinelearningtechnique,\\ntheenhancedgradient(,).Theimprovedconditioningofthe Cho e t a l .2011\\nHessianmatrixallowslearningtosucceed,evenindiﬃcultcasesliketraininga\\ndeepBoltzmannmachinewithmultiplelayers.\\nTheotherapproachtojointlytrainingdeepBoltzmannmachinesisthemulti-\\npredictiondeepBoltzmannmachine(MP-DBM)whichworksbyviewingthemean\\nﬁeldequationsasdeﬁningafamilyofrecurrentnetworksforapproximately solving\\neverypossibleinferenceproblem( ,).Ratherthantraining Goodfellow e t a l .2013b\\nthemodeltomaximizethelikelihood,themodelistrainedtomakeeachrecurrent\\nnetworkobtainanaccurateanswertothecorrespondinginferenceproblem.The\\ntrainingprocessisillustratedinﬁgure.\\xa0Itconsistsofrandomlysamplinga 20.5\\ntrainingexample,randomlysamplingasubsetofinputstotheinferencenetwork,\\nandthentrainingtheinferencenetworktopredictthevaluesoftheremaining\\nunits.\\nThisgeneralprincipleofback-propagating throughthecomputational graph\\nforapproximateinferencehasbeenappliedtoothermodels(Stoyanov2011 e t a l .,;\\nBrakel2013 e t a l .,).InthesemodelsandintheMP-DBM,theﬁnallossisnot\\nthelowerboundonthelikelihood.Instead,theﬁnallossistypicallybasedon\\ntheapproximateconditionaldistributionthattheapproximate inferencenetwork\\nimposesoverthemissingvalues.Thismeansthatthetrainingofthesemodels\\nissomewhatheuristicallymotivated.Ifweinspectthe p( v)representedbythe\\nBoltzmannmachinelearnedbytheMP-DBM,ittendstobesomewhatdefective,\\ninthesensethatGibbssamplingyieldspoorsamples.\\nBack-propagationthroughtheinferencegraphhastwomainadvantages.First,\\nittrainsthemodelasitisreallyused—withapproximate inference.Thismeans\\nthatapproximateinference,forexample,toﬁllinmissinginputs,ortoperform\\nclassiﬁcationdespitethepresenceofmissinginputs,ismoreaccurateintheMP-\\nDBMthanintheoriginalDBM.TheoriginalDBMdoesnotmakeanaccurate\\nclassiﬁeronitsown;thebestclassiﬁcationresultswiththeoriginalDBMwere\\nbasedontrainingaseparateclassiﬁertousefeaturesextractedbytheDBM,\\nratherthanbyusinginferenceintheDBMtocomputethedistributionoverthe\\nclasslabels.MeanﬁeldinferenceintheMP-DBMperformswellasaclassiﬁer\\nwithoutspecialmodiﬁcations.Theotheradvantageofback-propagating through\\napproximateinferenceisthatback-propagationcomputestheexactgradientof\\ntheloss.Thisisbetterforoptimization thantheapproximate gradientsofSML\\ntraining,whichsuﬀerfrombothbiasandvariance.ThisprobablyexplainswhyMP-\\n6 7 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9945555a-9988-48f3-b217-150980d7c76b', embedding=None, metadata={'page_label': '690', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nFigure20.5:Anillustrationofthemulti-predictiontrainingprocessforadeepBoltzmann\\nmachine.Eachrowindicatesadiﬀerentexamplewithinaminibatchforthesametraining\\nstep.\\xa0Eachcolumnrepresentsatimestepwithinthemeanﬁeldinferenceprocess.\\xa0For\\neachexample,wesampleasubsetofthedatavariablestoserveasinputstotheinference\\nprocess.Thesevariablesareshadedblacktoindicateconditioning.Wethenrunthe\\nmeanﬁeldinferenceprocess,witharrowsindicatingwhichvariablesinﬂuencewhichother\\nvariablesintheprocess.Inpracticalapplications,weunrollmeanﬁeldforseveralsteps.\\nInthisillustration,weunrollforonlytwosteps.Dashedarrowsindicatehowtheprocess\\ncouldbeunrolledformoresteps.Thedatavariablesthatwerenotusedasinputstothe\\ninferenceprocessbecometargets,shadedingray.Wecanviewtheinferenceprocessfor\\neachexampleasarecurrentnetwork.Weusegradientdescentandback-propagationto\\ntraintheserecurrentnetworkstoproducethecorrecttargetsgiventheirinputs.This\\ntrainsthemeanﬁeldprocessfortheMP-DBMtoproduceaccurateestimates.Figure\\nadaptedfrom (). Goodfellow e t a l .2013b\\n6 7 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f9c57a33-d965-415e-8513-2502383432e6', embedding=None, metadata={'page_label': '691', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nDBMsmaybetrainedjointlywhileDBMsrequireagreedylayer-wisepretraining.\\nThedisadvantageofback-propagatingthroughtheapproximate inferencegraphis\\nthatitdoesnotprovideawaytooptimizethelog-likelihood,butratheraheuristic\\napproximationofthegeneralizedpseudolikelihood.\\nTheMP-DBMinspiredtheNADE- k(Raiko2014 e t a l .,)extensiontothe\\nNADEframework,whichisdescribedinsection.20.10.10\\nTheMP-DBMhassomeconnectionstodropout.Dropoutsharesthesamepa-\\nrametersamongmanydiﬀerentcomputational graphs,withthediﬀerencebetween\\neachgraphbeingwhetheritincludesorexcludeseachunit.TheMP-DBMalso\\nsharesparametersacrossmanycomputational graphs.InthecaseoftheMP-DBM,\\nthediﬀerencebetweenthegraphsiswhethereachinputunitisobservedornot.\\nWhenaunitisnotobserved,theMP-DBMdoesnotdeleteitentirelyasdropout\\ndoes.Instead,theMP-DBMtreatsitasalatentvariabletobeinferred.Onecould\\nimagineapplyingdropouttotheMP-DBMbyadditionallyremovingsomeunits\\nratherthanmakingthemlatent.\\n20.5BoltzmannMachinesforReal-ValuedData\\nWhileBoltzmannmachineswereoriginallydevelopedforusewithbinarydata,\\nmanyapplicationssuchasimageandaudiomodelingseemtorequiretheability\\ntorepresentprobabilitydistributionsoverrealvalues.Insomecases,itispossible\\ntotreatreal-valueddataintheinterval[0,1]asrepresentingtheexpectationofa\\nbinaryvariable.Forexample, ()treatsgrayscaleimagesinthetraining Hinton2000\\nsetasdeﬁning[0,1]probabilityvalues.Eachpixeldeﬁnestheprobabilityofa\\nbinaryvaluebeing1,andthebinarypixelsareallsampledindependentlyfrom\\neachother.Thisisacommonprocedureforevaluatingbinarymodelsongrayscale\\nimagedatasets.However,itisnotaparticularlytheoreticallysatisfyingapproach,\\nandbinaryimagessampledindependentlyinthiswayhaveanoisyappearance.In\\nthissection,wepresentBoltzmannmachinesthatdeﬁneaprobabilitydensityover\\nreal-valueddata.\\n20.5.1Gaussian-BernoulliRBMs\\nRestrictedBoltzmannmachinesmaybedevelopedformanyexponentialfamily\\nconditionaldistributions(Welling2005 e t a l .,).Ofthese,themostcommonisthe\\nRBMwithbinaryhiddenunitsandreal-valuedvisibleunits,withtheconditional\\ndistributionoverthevisibleunitsbeingaGaussiandistributionwhosemeanisa\\nfunctionofthehiddenunits.\\n6 7 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b67ee0ab-25bd-4e48-af87-199c8cea7ed0', embedding=None, metadata={'page_label': '692', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nTherearemanywaysofparametrizing Gaussian-Bernoulli RBMs.Onechoice\\niswhethertouseacovariancematrixoraprecisionmatrixfortheGaussian\\ndistribution.Herewepresenttheprecisionformulation.Themodiﬁcationtoobtain\\nthecovarianceformulationisstraightforward.\\xa0Wewishtohavetheconditional\\ndistribution\\np , ( ) = (; v h| N v W h β− 1) . (20.38)\\nWecanﬁndthetermsweneedtoaddtotheenergyfunctionbyexpandingthe\\nunnormalized logconditionaldistribution:\\nlog (;N v W h β ,− 1) = −1\\n2( ) v W h −\\ue03eβ v W h β (− )+( f) .(20.39)\\nHere fencapsulatesallthetermsthatareafunctiononlyoftheparameters\\nandnottherandomvariablesinthemodel.Wecandiscard fbecauseitsonly\\nroleistonormalizethedistribution,andthepartitionfunctionofwhateverenergy\\nfunctionwechoosewillcarryoutthatrole.\\nIfweincludealloftheterms(withtheirsignﬂipped)involving vfromequa-\\ntioninourenergyfunctionanddonotaddanyothertermsinvolving 20.39 v,then\\nourenergyfunctionwillrepresentthedesiredconditional . p( ) v h|\\nWehavesomefreedomregardingtheotherconditionaldistribution, p( h v|).\\nNotethatequationcontainsaterm 20.39\\n1\\n2h\\ue03eW\\ue03eβ W h . (20.40)\\nThistermcannotbeincludedinitsentiretybecauseitincludes h i h jterms.These\\ncorrespondtoedgesbetweenthehiddenunits.Ifweincludedtheseterms,we\\nwouldhavealinearfactormodelinsteadofarestrictedBoltzmannmachine.When\\ndesigningourBoltzmannmachine,wesimplyomitthese h i h jcrossterms.Omitting\\nthemdoesnotchangetheconditional p( v h|)soequationisstillrespected. 20.39\\nHowever,westillhaveachoiceaboutwhethertoincludethetermsinvolvingonly\\nasingle h i.Ifweassumeadiagonalprecisionmatrix,weﬁndthatforeachhidden\\nunit h iwehaveaterm\\n1\\n2h i\\ue058\\njβ j W2\\nj , i . (20.41)\\nIntheabove,weusedthefactthat h2\\ni= h ibecause h i∈{0 ,1}.Ifweincludethis\\nterm(withitssignﬂipped)intheenergyfunction,thenitwillnaturallybias h i\\ntobeturnedoﬀwhentheweightsforthatunitarelargeandconnectedtovisible\\nunitswithhighprecision.Thechoiceofwhetherornottoincludethisbiasterm\\ndoesnotaﬀectthefamilyofdistributionsthemodelcanrepresent(assumingthat\\n6 7 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='73661dba-9a6e-4a0e-bb15-2a8b84a87412', embedding=None, metadata={'page_label': '693', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nweincludebiasparametersforthehiddenunits)butitdoesaﬀectthelearning\\ndynamicsofthemodel.Includingthetermmayhelpthehiddenunitactivations\\nremainreasonableevenwhentheweightsrapidlyincreaseinmagnitude.\\nOnewaytodeﬁnetheenergyfunctiononaGaussian-Bernoulli RBMisthus\\nE ,( v h) =1\\n2v\\ue03e( )( ) β v\\ue00c − v β\\ue00c\\ue03eW h b−\\ue03eh(20.42)\\nbutwemayalsoaddextratermsorparametrizetheenergyintermsofthevariance\\nratherthanprecisionifwechoose.\\nInthisderivation,wehavenotincludedabiastermonthevisibleunits,butone\\ncouldeasilybeadded.Oneﬁnalsourceofvariabilityintheparametrization ofa\\nGaussian-Bernoulli RBMisthechoiceofhowtotreattheprecisionmatrix.Itmay\\neitherbeﬁxedtoaconstant(perhapsestimatedbasedonthemarginalprecision\\nofthedata)orlearned.Itmayalsobeascalartimestheidentitymatrix,orit\\nmaybeadiagonalmatrix.Typicallywedonotallowtheprecisionmatrixtobe\\nnon-diagonal inthiscontext,becausesomeoperationsontheGaussiandistribution\\nrequireinvertingthematrix,andadiagonalmatrixcanbeinvertedtrivially.In\\nthesectionsahead,wewillseethatotherformsofBoltzmannmachinespermit\\nmodelingthecovariancestructure,usingvarioustechniquestoavoidinvertingthe\\nprecisionmatrix.\\n20.5.2UndirectedModelsofConditionalCovariance\\nWhiletheGaussianRBMhasbeenthecanonicalenergymodelforreal-valued\\ndata, ()arguethattheGaussianRBMinductivebiasisnot Ranzato e t a l .2010a\\nwellsuitedtothestatisticalvariationspresentinsometypesofreal-valueddata,\\nespeciallynaturalimages.Theproblemisthatmuchoftheinformationcontent\\npresentinnaturalimagesisembeddedinthecovariancebetweenpixelsratherthan\\nintherawpixelvalues.Inotherwords,itistherelationshipsbetweenpixelsand\\nnottheirabsolutevalueswheremostoftheusefulinformationinimagesresides.\\nSincetheGaussianRBMonlymodelstheconditionalmeanoftheinputgiventhe\\nhiddenunits,itcannotcaptureconditionalcovarianceinformation. Inresponse\\ntothesecriticisms,alternativemodelshavebeenproposedthatattempttobetter\\naccountforthecovarianceofreal-valueddata.Thesemodelsincludethemeanand\\ncovarianceRBM(mcRBM1),themean-productof t-distribution(mPoT)model\\nandthespikeandslabRBM(ssRBM).\\n1Th e t e rm “ m c R B M ” i s p ro n o u n c e d b y s a y i n g t h e n a m e o f t h e l e t t e rs M - C- R - B - M ; t h e “ m c ”\\ni s n o t p ro n o u n c e d l i k e t h e “ M c ” i n “ M c D o n a l d ’ s . ”\\n6 7 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f85dd102-a485-42ee-9cf8-cc38bfe8dfa2', embedding=None, metadata={'page_label': '694', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nMeanandCovarianceRBMThemcRBMusesitshiddenunitstoindepen-\\ndentlyencodetheconditionalmeanandcovarianceofallobservedunits.The\\nmcRBMhiddenlayerisdividedintotwogroupsofunits:meanunitsandcovariance\\nunits.ThegroupthatmodelstheconditionalmeanissimplyaGaussianRBM.\\nTheotherhalfisacovarianceRBM( ,),alsocalledacRBM, Ranzato e t a l .2010a\\nwhosecomponentsmodeltheconditionalcovariancestructure,asdescribedbelow.\\nSpeciﬁcally,withbinarymeanunits h( ) mandbinarycovarianceunits h( ) c,the\\nmcRBMmodelisdeﬁnedasthecombinationoftwoenergyfunctions:\\nE m c( x h ,( ) m, h( ) c) = E m( x h ,( ) m)+ E c( x h ,( ) c) ,(20.43)\\nwhere E misthestandardGaussian-Bernoulli RBMenergyfunction:2\\nE m( x h ,( ) m) =1\\n2x\\ue03ex−\\ue058\\njx\\ue03eW : , j h( ) m\\nj−\\ue058\\njb( ) m\\nj h( ) m\\nj ,(20.44)\\nand E cisthecRBMenergy\\xa0function that\\xa0models\\xa0the conditionalcovariance\\ninformation:\\nE c( x h ,( ) c) =1\\n2\\ue058\\njh( ) c\\nj\\ue010\\nx\\ue03er( ) j\\ue0112\\n−\\ue058\\njb( ) c\\nj h( ) c\\nj .(20.45)\\nTheparameter r( ) jcorrespondstothecovarianceweightvectorassociatedwith\\nh( ) c\\njand b( ) cisavectorofcovarianceoﬀsets.Thecombinedenergyfunctiondeﬁnes\\najointdistribution:\\np m c( x h ,( ) m, h( ) c) =1\\nZexp\\ue06e\\n− E m c( x h ,( ) m, h( ) c)\\ue06f\\n,(20.46)\\nandacorrespondingconditionaldistributionovertheobservationsgiven h( ) mand\\nh( ) casamultivariateGaussiandistribution:\\np m c( x h|( ) m, h( ) c) = N\\uf8eb\\n\\uf8ed x C;m c\\nx h|\\uf8eb\\n\\uf8ed\\ue058\\njW : , j h( ) m\\nj\\uf8f6\\n\\uf8f8 , Cm c\\nx h|\\uf8f6\\n\\uf8f8 .(20.47)\\nNotethatthecovariancematrix Cm c\\nx h|=\\ue010\\ue050\\nj h( ) c\\nj r( ) jr( ) j\\ue03e+ I\\ue011− 1\\nisnon-diagonal\\nandthat WistheweightmatrixassociatedwiththeGaussianRBMmodelingthe\\n2Th i s v e rs i o n o f t h e Ga u s s i a n - B e rn o u l l i R B M e n e rg y f u n c t i o n a s s u m e s t h e i m a g e d a t a h a s\\nz e ro m e a n , p e r p i x e l . P i x e l o ﬀ s e t s c a n e a s i l y b e a d d e d t o t h e m o d e l t o a c c o u n t f o r n o n z e ro p i x e l\\nm e a n s .\\n6 7 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='22c0e747-8e45-41d4-ba28-ff7e3ef53a2a', embedding=None, metadata={'page_label': '695', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nconditionalmeans.ItisdiﬃculttotrainthemcRBMviacontrastivedivergenceor\\npersistentcontrastivedivergencebecauseofitsnon-diagonal conditionalcovariance\\nstructure.CDandPCDrequiresamplingfromthejointdistributionof x h ,( ) m, h( ) c\\nwhich,inastandardRBM,isaccomplishedbyGibbssamplingovertheconditionals.\\nHowever,inthemcRBM,samplingfrom p m c( x h|( ) m, h( ) c)requirescomputing\\n( Cm c)− 1ateveryiterationoflearning.Thiscanbeanimpracticalcomputational\\nburdenforlargerobservations. ()avoiddirectsampling RanzatoandHinton2010\\nfromtheconditional p m c( x h|( ) m, h( ) c)bysamplingdirectlyfromthemarginal\\np( x)usingHamiltonian(hybrid)MonteCarlo(,)onthemcRBMfree Neal1993\\nenergy.\\nMean-ProductofStudent’s-distributions t Themean-productofStudent’s\\nt-distribution(mPoT)model( ,)extendsthePoTmodel( Ranzato e t a l .2010b Welling\\ne t a l .,)inamannersimilartohowthemcRBMextendsthecRBM.This 2003a\\nisachievedbyincludingnonzeroGaussianmeansbytheadditionofGaussian\\nRBM-likehiddenunits.LikethemcRBM,thePoTconditionaldistributionoverthe\\nobservationisamultivariateGaussian(withnon-diagonal covariance)distribution;\\nhowever,unlikethemcRBM,thecomplementaryconditionaldistributionoverthe\\nhiddenvariablesisgivenbyconditionallyindependentGammadistributions.The\\nGammadistributionG( k , θ) isaprobabilitydistributionoverpositiverealnumbers,\\nwithmean k θ.Itisnotnecessarytohaveamoredetailedunderstandingofthe\\nGammadistributiontounderstandthebasicideasunderlyingthemPoTmodel.\\nThemPoTenergyfunctionis:\\nE m P o T( x h ,( ) m, h( ) c) (20.48)\\n= E m( x h ,( ) m)+\\ue058\\nj\\ue012\\nh( ) c\\nj\\ue012\\n1+1\\n2\\ue010\\nr( ) j\\ue03ex\\ue0112\\ue013\\n+(1− γ j)log h( ) c\\nj\\ue013\\n(20.49)\\nwhere r( ) jisthecovarianceweightvectorassociatedwithunit h( ) c\\njand E m( x h ,( ) m)\\nisasdeﬁnedinequation.20.44\\nJustaswiththemcRBM,themPoTmodelenergyfunctionspeciﬁesamul-\\ntivariateGaussian,withaconditionaldistributionover xthathasnon-diagonal\\ncovariance.LearninginthemPoTmodel—again,likethemcRBM—iscompli-\\ncatedbytheinabilityto\\xa0samplefromthenon-diagonal Gaussianconditional\\np m P o T( x h|( ) m, h( ) c),so ()alsoadvocatedirectsamplingof Ranzato e t a l .2010b\\np() xviaHamiltonian(hybrid)MonteCarlo.\\n6 8 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='951f7172-23d3-47b5-adc4-e968f8b46b27', embedding=None, metadata={'page_label': '696', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nSpikeandSlabRestrictedBoltzmannMachinesSpikeandslabrestricted\\nBoltzmannmachines( ,)orssRBMsprovideanothermeans Courville e t a l .2011\\nofmodelingthecovariancestructureofreal-valueddata.ComparedtomcRBMs,\\nssRBMshavetheadvantageofrequiringneithermatrixinversionnorHamiltonian\\nMonteCarlomethods.LikethemcRBMandthemPoTmodel,thessRBM’sbinary\\nhiddenunitsencodetheconditionalcovarianceacrosspixelsthroughtheuseof\\nauxiliaryreal-valuedvariables.\\nThespikeandslabRBMhastwosetsofhiddenunits:binaryspikeunits h,\\nandreal-valuedslabunits s.Themeanofthevisibleunitsconditionedonthe\\nhiddenunitsisgivenby( h s\\ue00c) W\\ue03e.Inotherwords,eachcolumn W : , ideﬁnesa\\ncomponentthatcanappearintheinputwhen h i= 1.Thecorrespondingspike\\nvariableh idetermineswhetherthatcomponentispresentatall.Thecorresponding\\nslabvariables ideterminestheintensityofthatcomponent,ifitispresent.When\\naspikevariableisactive,thecorrespondingslabvariableaddsvariancetothe\\ninputalongtheaxisdeﬁnedby W : , i.Thisallowsustomodelthecovarianceofthe\\ninputs.Fortunately,contrastivedivergenceandpersistentcontrastivedivergence\\nwithGibbssamplingarestillapplicable.Thereisnoneedtoinvertanymatrix.\\nFormally,thessRBMmodelisdeﬁnedviaitsenergyfunction:\\nE s s( ) = x s h , , −\\ue058\\nix\\ue03eW : , i s i h i+1\\n2x\\ue03e\\ue020\\nΛ+\\ue058\\niΦ i h i\\ue021\\nx (20.50)\\n+1\\n2\\ue058\\niα i s2\\ni−\\ue058\\niα i µ i s i h i−\\ue058\\nib i h i+\\ue058\\niα i µ2\\ni h i ,(20.51)\\nwhere b iistheoﬀsetofthespike h iandΛisadiagonalprecisionmatrixonthe\\nobservations x.Theparameter α i >0isascalarprecisionparameterforthe\\nreal-valuedslabvariable s i.Theparameter Φ iisanon-negativediagonalmatrix\\nthatdeﬁnesan h-modulatedquadraticpenaltyon x.Each µ iisameanparameter\\nfortheslabvariable s i.\\nWiththejointdistributiondeﬁnedviatheenergyfunction,itisrelatively\\nstraightforwardto\\xa0derivethessRBM\\xa0conditionaldistributions.For\\xa0example,\\nbymarginalizing outtheslabvariables s,theconditionaldistributionoverthe\\nobservationsgiventhebinaryspikevariablesisgivenby: h\\np s s( )= x h|1\\nP() h1\\nZ\\ue05a\\nexp ( ) {− E x s h , ,} d s(20.52)\\n=N\\ue020\\nx C;s s\\nx h|\\ue058\\niW : , i µ i h i , Cs s\\nx h|\\ue021\\n(20.53)\\n6 8 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='44d9aad3-a942-427e-b8f6-d702a081e0e0', embedding=None, metadata={'page_label': '697', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nwhere Cs s\\nx h|=\\ue000\\nΛ+\\ue050\\niΦ i h i−\\ue050\\niα− 1\\ni h i W : , i W\\ue03e\\n: , i\\ue001− 1.Thelastequalityholdsonlyif\\nthecovariancematrix Cs s\\nx h|ispositivedeﬁnite.\\nGatingbythespikevariablesmeansthatthetruemarginaldistributionover\\nhs\\ue00cissparse.Thisisdiﬀerentfromsparsecoding,wheresamplesfromthemodel\\n“almostnever”(inthemeasuretheoreticsense)containzerosinthecode,andMAP\\ninferenceisrequiredtoimposesparsity.\\nComparingthessRBMtothemcRBMandthemPoTmodels,thessRBM\\nparametrizes theconditionalcovarianceoftheobservationinasigniﬁcantlydiﬀerent\\nway.ThemcRBMandmPoTbothmodelthecovariancestructureoftheobservation\\nas\\ue010\\ue050\\nj h( ) c\\nj r( ) jr( ) j\\ue03e+ I\\ue011− 1\\n,usingtheactivationofthehiddenunits h j >0to\\nenforceconstraintsontheconditionalcovarianceinthedirection r( ) j.Incontrast,\\nthessRBMspeciﬁestheconditionalcovarianceoftheobservationsusingthehidden\\nspikeactivations h i= 1topinchtheprecisionmatrixalongthedirectionspeciﬁed\\nbythecorrespondingweightvector.\\xa0ThessRBMconditionalcovarianceisvery\\nsimilartothatgivenbyadiﬀerentmodel:theproductofprobabilisticprincipal\\ncomponentsanalysis(PoPPCA)(WilliamsandAgakov2002,).Intheovercomplete\\nsetting,sparseactivationswiththessRBMparametrization permitsigniﬁcant\\nvariance(abovethenominalvariancegivenbyΛ− 1)onlyintheselecteddirections\\nofthesparselyactivated h i.\\xa0InthemcRBMormPoTmodels,anovercomplete\\nrepresentationwouldmeanthattocapturevariationinaparticulardirectionin\\ntheobservationspacerequiresremovingpotentiallyallconstraintswithpositive\\nprojectioninthatdirection.\\xa0This wouldsuggestthatthesemodelsarelesswell\\nsuitedtotheovercompletesetting.\\nTheprimarydisadvantageofthespikeandslabrestrictedBoltzmannmachine\\nisthatsomesettingsoftheparameterscancorrespondtoacovariancematrix\\nthatisnotpositivedeﬁnite.Suchacovariancematrixplacesmoreunnormalized\\nprobabilityonvaluesthatarefartherfromthemean,causingtheintegralover\\nallpossibleoutcomestodiverge.Generallythisissuecanbeavoidedwithsimple\\nheuristictricks.Thereisnotyetanytheoreticallysatisfyingsolution.Using\\nconstrainedoptimization toexplicitlyavoidtheregionswheretheprobabilityis\\nundeﬁnedisdiﬃculttodowithoutbeingoverlyconservativeandalsopreventing\\nthemodelfromaccessinghigh-performingregionsofparameterspace.\\nQualitatively,convolutionalvariantsofthessRBMproduceexcellentsamples\\nofnaturalimages.Someexamplesareshowninﬁgure.16.1\\nThessRBMallowsforseveralextensions.Includinghigher-orderinteractions\\nandaverage-poolingoftheslabvariables( ,)enablesthemodel Courville e t a l .2014\\ntolearnexcellentfeaturesforaclassiﬁerwhenlabeleddataisscarce.\\xa0Addinga\\n6 8 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a011f20-b939-4c13-82e4-17ac751356ba', embedding=None, metadata={'page_label': '698', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\ntermtotheenergyfunctionthatpreventsthepartitionfunctionfrombecoming\\nundeﬁnedresultsinasparsecodingmodel,spikeandslabsparsecoding(Goodfellow\\ne t a l .,),alsoknownasS3C. 2013d\\n20.6ConvolutionalBoltzmannMachines\\nAsseeninchapter,extremelyhighdimensionalinputssuchasimagesplace 9\\ngreatstrainonthecomputation,memoryandstatisticalrequirementsofmachine\\nlearningmodels.Replacingmatrixmultiplication bydiscreteconvolutionwitha\\nsmallkernelisthestandardwayofsolvingtheseproblemsforinputsthathave\\ntranslationinvariantspatialortemporalstructure. () DesjardinsandBengio2008\\nshowedthatthisapproachworkswellwhenappliedtoRBMs.\\nDeepconvolutionalnetworksusuallyrequireapoolingoperationsothatthe\\nspatialsizeofeachsuccessivelayerdecreases.Feedforwardconvolutionalnetworks\\noftenuseapoolingfunctionsuchasthemaximumoftheelementstobepooled.\\nItisunclearhowtogeneralizethistothesettingofenergy-basedmodels.We\\ncouldintroduceabinarypoolingunitpover nbinarydetectorunits dandenforce\\np=max i d ibysettingtheenergyfunctiontobe∞wheneverthatconstraintis\\nviolated.Thisdoesnotscalewellthough,asitrequiresevaluating 2ndiﬀerent\\nenergyconﬁgurations tocomputethenormalization constant.Forasmall 3×3\\npoolingregionthisrequires 29= 512energyfunctionevaluationsperpoolingunit!\\nLee2009 e t a l .()developedasolutiontothisproblemcalledprobabilistic\\nmaxpooling(nottobeconfusedwith“stochasticpooling,”whichisatechnique\\nforimplicitlyconstructingensemblesofconvolutionalfeedforwardnetworks).The\\nstrategybehindprobabilisticmaxpoolingistoconstrainthedetectorunitsso\\natmostonemaybeactiveatatime.Thismeansthereareonly n+ 1total\\nstates(onestateforeachofthe ndetectorunitsbeingon,andanadditionalstate\\ncorrespondingtoallofthedetectorunitsbeingoﬀ).Thepoolingunitisonif\\nandonlyifoneofthedetectorunitsison.Thestatewithallunitsoﬀisassigned\\nenergyzero.Wecanthinkofthisasdescribingamodelwithasinglevariablethat\\nhas n+1states,orequivalentlyasamodelthathas n+1variablesthatassigns\\nenergytoallbutjointassignmentsofvariables. ∞ n+1\\nWhileeﬃcient,probabilisticmaxpoolingdoesforcethedetectorunitstobe\\nmutuallyexclusive,whichmaybeausefulregularizingconstraintinsomecontexts\\noraharmfullimitonmodelcapacityinothercontexts.Italsodoesnotsupport\\noverlappingpoolingregions.Overlapping poolingregionsareusuallyrequired\\ntoobtainthebestperformancefromfeedforwardconvolutionalnetworks,sothis\\nconstraintprobablygreatlyreducestheperformanceofconvolutionalBoltzmann\\n6 8 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='37837c10-454f-4c1b-9c1b-124f5fc45474', embedding=None, metadata={'page_label': '699', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nmachines.\\nLee2009 e t a l .()demonstratedthatprobabilisticmaxpoolingcouldbeused\\ntobuildconvolutionaldeepBoltzmannmachines.3Thismodelisabletoperform\\noperationssuchasﬁllinginmissingportionsofitsinput.Whileintellectually\\nappealing,thismodelischallengingtomakeworkinpractice,andusuallydoes\\nnotperformaswellasaclassiﬁerastraditionalconvolutionalnetworkstrained\\nwithsupervisedlearning.\\nManyconvolutionalmodelsworkequallywellwithinputsofmanydiﬀerent\\nspatialsizes.ForBoltzmannmachines,itisdiﬃculttochangetheinputsize\\nforavarietyofreasons.\\xa0Thepartitionfunctionchangesasthesizeoftheinput\\nchanges.Moreover,manyconvolutionalnetworksachievesizeinvariancebyscaling\\nupthesizeoftheirpoolingregionsproportionaltothesizeoftheinput,butscaling\\nBoltzmannmachinepoolingregionsisawkward.Traditionalconvolutionalneural\\nnetworkscanuseaﬁxednumberofpoolingunitsanddynamicallyincreasethe\\nsizeoftheirpoolingregionsinordertoobtainaﬁxed-sizerepresentationofa\\nvariable-sizedinput.ForBoltzmannmachines,largepoolingregionsbecometoo\\nexpensiveforthenaiveapproach.\\xa0The approachof()ofmaking Lee e t a l .2009\\neachofthedetectorunitsinthesamepoolingregionmutuallyexclusivesolves\\nthecomputational problems,butstilldoesnotallowvariable-sizepoolingregions.\\nForexample,supposewelearnamodelwith 2×2probabilisticmaxpoolingover\\ndetectorunitsthatlearnedgedetectors.\\xa0Thisenforcestheconstraintthatonly\\noneoftheseedgesmayappearineach2×2region.Ifwethenincreasethesizeof\\ntheinputimageby50%ineachdirection,wewouldexpectthenumberofedgesto\\nincreasecorrespondingly.Instead,ifweincreasethesizeofthepoolingregionsby\\n50%ineachdirectionto3×3,thenthemutualexclusivityconstraintnowspeciﬁes\\nthateachoftheseedgesmayonlyappearonceina3×3region.Aswegrow\\namodel’sinputimageinthisway,themodelgeneratesedgeswithlessdensity.\\nOfcourse,theseissuesonlyarisewhenthemodelmustusevariableamountsof\\npoolinginordertoemitaﬁxed-sizeoutputvector.Modelsthatuseprobabilistic\\nmaxpoolingmaystillacceptvariable-sizedinputimagessolongastheoutputof\\nthemodelisafeaturemapthatcanscaleinsizeproportionaltotheinputimage.\\nPixelsattheboundaryoftheimagealsoposesomediﬃculty,whichisexac-\\nerbatedbythefactthatconnectionsinaBoltzmannmachinearesymmetric.If\\nwedonotimplicitlyzero-padtheinput,thentherearefewerhiddenunitsthan\\nvisibleunits,andthevisibleunitsattheboundaryoftheimagearenotmodeled\\n3Th e p u b l i c a t i o n d e s c rib e s t h e m o d e l a s a “ d e e p b e l i e f n e t w o rk ” b u t b e c a u s e i t c a n b e d e s c rib e d\\na s a p u re l y u n d i re c t e d m o d e l with t ra c t a b l e l a y e r- wis e m e a n ﬁ e l d ﬁ x e d p o i n t u p d a t e s , i t b e s t ﬁ t s\\nt h e d e ﬁ n i t i o n o f a d e e p B o l t z m a n n m a c h i n e .\\n6 8 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3faec538-d5b3-43ba-b2d8-4e7588e445b8', embedding=None, metadata={'page_label': '700', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nwellbecausetheylieinthereceptiveﬁeldoffewerhiddenunits.However,ifwedo\\nimplicitlyzero-padtheinput,thenthehiddenunitsattheboundaryaredrivenby\\nfewerinputpixels,andmayfailtoactivatewhenneeded.\\n20.7BoltzmannMachinesforStructuredorSequential\\nOutputs\\nInthestructuredoutputscenario,wewishtotrainamodelthatcanmapfrom\\nsomeinput xtosomeoutput y,andthediﬀerententriesof yarerelatedtoeach\\notherandmustobeysomeconstraints.Forexample,inthespeechsynthesistask,\\nyisawaveform,andtheentirewaveformmustsoundlikeacoherentutterance.\\nAnaturalwaytorepresenttherelationshipsbetweentheentriesin yisto\\nuseaprobabilitydistribution p(y| x).Boltzmannmachines,extendedtomodel\\nconditionaldistributions,cansupplythisprobabilisticmodel.\\nThesametoolofconditionalmodelingwithaBoltzmannmachinecanbeused\\nnotjustforstructuredoutputtasks,butalsoforsequencemodeling.Inthelatter\\ncase,ratherthanmappinganinput xtoanoutput y,themodelmustestimatea\\nprobabilitydistributionoverasequenceofvariables, p(x( 1 ), . . . ,x( ) τ).Conditional\\nBoltzmannmachinescanrepresentfactorsoftheform p(x( ) t|x( 1 ), . . . ,x( 1 ) t−)in\\nordertoaccomplishthistask.\\nAnimportantsequencemodelingtaskforthevideogameandﬁlmindustry\\nismodelingsequencesofjointanglesofskeletonsusedtorender3-Dcharacters.\\nThesesequencesareoftencollectedusingmotioncapturesystemstorecordthe\\nmovementsofactors.Aprobabilisticmodelofacharacter’smovementallows\\nthegenerationofnew,\\xa0previouslyunseen,\\xa0but\\xa0realisticanimations.Tosolve\\nthissequencemodelingtask,Taylor2007 e t a l .()introducedaconditionalRBM\\nmodeling p( x( ) t| x( 1 ) t−, . . . , x( ) t m−)forsmall m.ThemodelisanRBMover\\np( x( ) t)whosebiasparametersarealinearfunctionofthepreceding mvaluesof x.\\nWhenweconditionondiﬀerentvaluesof x( 1 ) t−andearliervariables,wegetanew\\nRBMoverx.TheweightsintheRBMoverxneverchange,butbyconditioningon\\ndiﬀerentpastvalues,wecanchangetheprobabilityofdiﬀerenthiddenunitsinthe\\nRBMbeingactive.Byactivatinganddeactivatingdiﬀerentsubsetsofhiddenunits,\\nwecanmakelargechangestotheprobabilitydistributioninducedonx.\\xa0Other\\nvariantsofconditionalRBM(,)andothervariantsofsequence Mnih e t a l .2011\\nmodelingusingconditionalRBMsarepossible(TaylorandHinton2009Sutskever ,;\\ne t a l .,;2009Boulanger-Lewandowski2012 e t a l .,).\\nAnothersequencemodelingtaskistomodelthedistributionoversequences\\n6 8 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5c38cead-da4c-4f20-aec6-7aea20c7479f', embedding=None, metadata={'page_label': '701', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nofmusicalnotesusedtocomposesongs.Boulanger-Lewandowski2012 e t a l .()\\nintroducedtheRNN-RBM sequencemodelandappliedittothistask.The\\nRNN-RBMisagenerativemodelofasequenceofframes x( ) tconsistingofanRNN\\nthatemitstheRBMparametersforeachtimestep.Unlikepreviousapproaches\\ninwhichonlythebiasparametersoftheRBMvariedfromonetimesteptothe\\nnext,theRNN-RBMusestheRNNtoemitalloftheparametersoftheRBM,\\nincludingtheweights.Totrainthemodel,weneedtobeabletoback-propagate\\nthegradientofthelossfunctionthroughtheRNN.Thelossfunctionisnotapplied\\ndirectlytotheRNNoutputs.Instead,itisappliedtotheRBM.Thismeansthat\\nwemustapproximately diﬀerentiatethelosswithrespecttotheRBMparameters\\nusingcontrastivedivergenceorarelatedalgorithm.\\xa0This approximate gradient\\nmaythenbeback-propagated throughtheRNNusingtheusualback-propagation\\nthroughtimealgorithm.\\n20.8OtherBoltzmannMachines\\nManyothervariantsofBoltzmannmachinesarepossible.\\nBoltzmannmachinesmaybeextendedwithdiﬀerenttrainingcriteria.Wehave\\nfocusedonBoltzmannmachinestrainedtoapproximately maximizethegenerative\\ncriterion log p( v).ItisalsopossibletotraindiscriminativeRBMsthataimto\\nmaximize log p( y| v)instead( ,).Thisapproachoften LarochelleandBengio2008\\nperformsthebestwhenusingalinearcombinationofboththegenerativeand\\nthediscriminativecriteria.Unfortunately,RBMsdonotseemtobeaspowerful\\nsupervisedlearnersasMLPs,atleastusingexistingmethodology.\\nMostBoltzmannmachinesusedinpracticehaveonlysecond-orderinteractions\\nintheirenergyfunctions,meaningthattheirenergyfunctionsarethesumofmany\\ntermsandeachindividualtermonlyincludestheproductbetweentworandom\\nvariables.Anexampleofsuchatermis v i W i , j h j.Itisalsopossibletotrain\\nhigher-orderBoltzmannmachines(,)whoseenergyfunctionterms Sejnowski1987\\ninvolvetheproductsbetweenmanyvariables.Three-wayinteractionsbetweena\\nhiddenunitandtwodiﬀerentimagescanmodelspatialtransformationsfromone\\nframeofvideotothenext(MemisevicandHinton20072010,,).Multiplication bya\\none-hotclassvariablecanchangetherelationshipbetweenvisibleandhiddenunits\\ndependingonwhichclassispresent( ,).Onerecentexample NairandHinton2009\\noftheuseofhigher-orderinteractionsisaBoltzmannmachinewithtwogroupsof\\nhiddenunits,withonegroupofhiddenunitsthatinteractwithboththevisible\\nunits vandtheclasslabel y,andanothergroupofhiddenunitsthatinteractonly\\nwiththe vinputvalues(,).Thiscanbeinterpretedasencouraging Luo e t a l .2011\\n6 8 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a1bb8e25-abdf-4154-8d5c-b3b2e97d3aa0', embedding=None, metadata={'page_label': '702', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nsomehiddenunitstolearntomodeltheinputusingfeaturesthatarerelevantto\\ntheclassbutalsotolearnextrahiddenunitsthatexplainnuisancedetailsthat\\narenecessaryforthesamplesof vtoberealisticbutdonotdeterminetheclass\\noftheexample.Anotheruseofhigher-orderinteractionsistogatesomefeatures.\\nSohn2013 e t a l .()introducedaBoltzmannmachinewiththird-orderinteractions\\nwithbinarymaskvariablesassociatedwitheachvisibleunit.Whenthesemasking\\nvariablesaresettozero,theyremovetheinﬂuenceofavisibleunitonthehidden\\nunits.Thisallowsvisibleunitsthatarenotrelevanttotheclassiﬁcationproblem\\ntoberemovedfromtheinferencepathwaythatestimatestheclass.\\nMoregenerally,theBoltzmannmachineframeworkisarichspaceofmodels\\npermittingmanymoremodelstructuresthanhavebeenexploredsofar.Developing\\nanewformofBoltzmannmachinerequiressomemorecareandcreativitythan\\ndevelopinganewneuralnetworklayer,becauseitisoftendiﬃculttoﬁndanenergy\\nfunctionthatmaintainstractabilityofallofthediﬀerentconditionaldistributions\\nneededtousetheBoltzmannmachine,butdespitethisrequiredeﬀorttheﬁeld\\nremainsopentoinnovation.\\n20.9Back-PropagationthroughRandomOperations\\nTraditionalneuralnetworksimplementadeterministictransformationofsome\\ninputvariables x.Whendevelopinggenerativemodels,weoftenwishtoextend\\nneuralnetworkstoimplementstochastictransformationsof x.Onestraightforward\\nwaytodothisistoaugmenttheneuralnetworkwithextrainputs zthatare\\nsampledfromsomesimpleprobabilitydistribution,suchasauniformorGaussian\\ndistribution.Theneuralnetworkcanthencontinuetoperformdeterministic\\ncomputationinternally,\\xa0butthefunction f( x z ,)willappearstochasticto\\xa0an\\nobserverwhodoesnothaveaccessto z.Providedthat fiscontinuousand\\ndiﬀerentiable,wecanthencomputethegradientsnecessaryfortrainingusing\\nback-propagationasusual.\\nAsanexample,letusconsidertheoperationconsistingofdrawingsamplesy\\nfromaGaussiandistributionwithmeanandvariance µ σ2:\\ny∼N( µ , σ2) . (20.54)\\nBecauseanindividualsampleofyisnotproducedbyafunction,butratherby\\nasamplingprocesswhoseoutputchangeseverytimewequeryit,itmayseem\\ncounterintuitivetotakethederivativesof ywithrespecttotheparametersof\\nitsdistribution, µand σ2.However,\\xa0wecanrewritethesamplingprocessas\\n6 8 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='95ff7cdd-6ef5-469e-b0f5-a9882cb4ed65', embedding=None, metadata={'page_label': '703', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\ntransforminganunderlyingrandomvaluez∼N( z;0 ,1)toobtainasamplefrom\\nthedesireddistribution:\\ny µ σ z = + (20.55)\\nWearenowabletoback-propagatethroughthesamplingoperation,byregard-\\ningitasadeterministicoperationwithanextrainputz.Crucially,theextrainput\\nisarandomvariablewhosedistributionisnotafunctionofanyofthevariables\\nwhosederivativeswewanttocalculate.\\xa0The resulttellsushowaninﬁnitesimal\\nchangein µor σwouldchangetheoutputifwecouldrepeatthesamplingoperation\\nagainwiththesamevalueofz.\\nBeingabletoback-propagate throughthissamplingoperationallowsusto\\nincorporateitintoalargergraph.Wecanbuildelementsofthegraphontopofthe\\noutputofthesamplingdistribution.Forexample,wecancomputethederivatives\\nofsomelossfunction J( y).Wecanalsobuildelementsofthegraphwhoseoutputs\\naretheinputsortheparametersofthesamplingoperation.Forexample,wecould\\nbuildalargergraphwith µ= f( x; θ)and σ= g( x; θ).Inthisaugmentedgraph,\\nwecanuseback-propagationthroughthesefunctionstoderive∇ θ J y().\\nTheprincipleusedinthisGaussiansamplingexampleismoregenerallyappli-\\ncable.Wecanexpressanyprobabilitydistributionoftheform p(y; θ)or p(y| x; θ)\\nas p(y| ω),where ωisavariablecontainingbothparameters θ,andifapplicable,\\ntheinputs x.Givenavalue ysampledfromdistribution p(y| ω),where ωmayin\\nturnbeafunctionofothervariables,wecanrewrite\\ny y ∼ p(| ω) (20.56)\\nas\\ny z ω = ( f;) , (20.57)\\nwhere zisasourceofrandomness.Wemaythencomputethederivativesof ywith\\nrespectto ωusingtraditionaltoolssuchastheback-propagation algorithmapplied\\nto f,solongas fiscontinuousanddiﬀerentiable almosteverywhere.Crucially, ω\\nmustnotbeafunctionof z,and zmustnotbeafunctionof ω.Thistechnique\\nisoftencalledthereparametrizationtrick,stochasticback-propagationor\\nperturbationanalysis.\\nTherequirementthat fbecontinuousanddiﬀerentiableofcourserequires y\\ntobecontinuous.Ifwewishtoback-propagate throughasamplingprocessthat\\nproducesdiscrete-valuedsamples,itmaystillbepossibletoestimateagradienton\\nω,usingreinforcementlearningalgorithmssuchasvariantsoftheREINFORCE\\nalgorithm(,),discussedinsection. Williams1992 20.9.1\\n6 8 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c0983f97-aeab-4d56-932c-bbfb1f49ff22', embedding=None, metadata={'page_label': '704', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nInneuralnetworkapplications,wetypicallychoose ztobedrawnfromsome\\nsimpledistribution,suchasaunituniformorunitGaussiandistribution,and\\nachievemorecomplexdistributionsbyallowingthedeterministicportionofthe\\nnetworktoreshapeitsinput.\\nTheideaofpropagatinggradientsoroptimizingthroughstochasticoperations\\ndatesbacktothemid-twentiethcentury(,;,)andwas Price1958Bonnet1964\\nﬁrstusedformachinelearninginthecontextofreinforcementlearning(,Williams\\n1992).\\xa0Morerecently,ithasbeenappliedtovariationalapproximations(Opper\\nandArchambeau2009,)andstochasticorgenerativeneuralnetworks(Bengio\\ne t a l .,;,; 2013bKingma2013KingmaandWelling2014baRezende2014 ,,; e t a l .,;\\nGoodfellow2014c e t a l .,).Manynetworks,suchasdenoisingautoencodersor\\nnetworksregularized\\xa0with dropout,\\xa0are\\xa0also naturally\\xa0designedto\\xa0take\\xa0noise\\nasaninputwithoutrequiringanyspecialreparametrization tomakethenoise\\nindependentfromthemodel.\\n20.9.1Back-PropagatingthroughDiscreteStochasticOperations\\nWhenamodelemitsadiscretevariable y,thereparametrization trickisnot\\napplicable.Suppose\\xa0thatthemodel\\xa0takesinputs xandparameters θ,\\xa0both\\nencapsulatedinthevector ω,andcombinesthemwithrandomnoise ztoproduce\\ny:\\ny z ω = ( f;) . (20.58)\\nBecause yisdiscrete, fmustbeastepfunction.Thederivativesofastepfunction\\narenotusefulatanypoint.Rightateachstepboundary,thederivativesare\\nundeﬁned,butthatisasmallproblem.Thelargeproblemisthatthederivatives\\narezeroalmosteverywhere,ontheregionsbetweenstepboundaries.Thederivatives\\nofanycostfunction J( y)thereforedonotgiveanyinformationforhowtoupdate\\nthemodelparameters . θ\\nTheREINFORCEalgorithm(REwardIncrement=Non-negativeFactor ×\\nOﬀsetReinforcement×Characteristic Eligibility)providesaframeworkdeﬁninga\\nfamilyofsimplebutpowerfulsolutions(,).\\xa0Thecoreideaisthat Williams1992\\neventhough J( f( z; ω))isastepfunctionwithuselessderivatives,theexpected\\ncost E z z∼ p ( ) J f((;)) z ωisoftenasmoothfunctionamenabletogradientdescent.\\nAlthoughthatexpectationistypicallynottractablewhen yishigh-dimensional\\n(oristheresultofthecompositionofmanydiscretestochasticdecisions),itcanbe\\nestimatedwithoutbiasusingaMonteCarloaverage.Thestochasticestimateof\\nthegradientcanbeusedwithSGDorotherstochasticgradient-basedoptimization\\ntechniques.\\n6 8 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='92aec64e-a149-4e1a-888e-a5f9464a4857', embedding=None, metadata={'page_label': '705', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nThesimplestversionofREINFORCEcanbederivedbysimplydiﬀerentiating\\ntheexpectedcost:\\nE z[()] = J y\\ue058\\nyJ p() y() y (20.59)\\n∂ J E[()] y\\n∂ ω=\\ue058\\nyJ() y∂ p() y\\n∂ ω(20.60)\\n=\\ue058\\nyJ p() y() y∂ plog() y\\n∂ ω(20.61)\\n≈1\\nmm\\ue058\\ny( ) i∼ p , i ( ) y = 1J( y( ) i)∂ plog( y( ) i)\\n∂ ω.(20.62)\\nEquationreliesontheassumptionthat 20.60 Jdoesnotreference ωdirectly.Itis\\ntrivialtoextendtheapproachtorelaxthisassumption.Equationexploits20.61\\nthederivativeruleforthelogarithm,∂ p l o g ( ) y\\n∂ ω=1\\np ( ) y∂ p ( ) y\\n∂ ω.Equation gives20.62\\nanunbiasedMonteCarloestimatorofthegradient.\\nAnywherewewrite p( y)inthissection,onecouldequallywrite p( y x|).This\\nisbecause p( y)isparametrized by ω,and ωcontainsboth θand x,if xispresent.\\nOneissuewiththeabovesimpleREINFORCEestimatoristhatithasavery\\nhighvariance,sothatmanysamplesof yneedtobedrawntoobtainagood\\nestimatorofthegradient,orequivalently,ifonlyonesampleisdrawn,SGDwill\\nconvergeveryslowlyandwillrequireasmallerlearningrate.Itispossibleto\\nconsiderablyreducethevarianceofthatestimatorbyusingvariancereduction\\nmethods(,;,).Theideaistomodifytheestimatorso Wilson1984L’Ecuyer1994\\nthatitsexpectedvalueremainsunchangedbutitsvariancegetreduced.\\xa0Inthe\\ncontextofREINFORCE,theproposedvariancereductionmethodsinvolvethe\\ncomputationofabaselinethatisusedtooﬀset J( y).Notethatanyoﬀset b( ω)\\nthatdoesnotdependon ywouldnotchangetheexpectationoftheestimated\\ngradientbecause\\nE p ( ) y\\ue014∂ plog() y\\n∂ ω\\ue015\\n=\\ue058\\nyp() y∂ plog() y\\n∂ ω(20.63)\\n=\\ue058\\ny∂ p() y\\n∂ ω(20.64)\\n=∂\\n∂ ω\\ue058\\nyp() = y∂\\n∂ ω1 = 0 ,(20.65)\\n6 9 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4dd3d0c-eb02-4f1a-acd3-c63f81da95bf', embedding=None, metadata={'page_label': '706', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nwhichmeansthat\\nE p ( ) y\\ue014\\n(()()) J y− b ω∂ plog() y\\n∂ ω\\ue015\\n= E p ( ) y\\ue014\\nJ() y∂ plog() y\\n∂ ω\\ue015\\n− b E() ω p ( ) y\\ue014∂ plog() y\\n∂ ω\\ue015\\n(20.66)\\n= E p ( ) y\\ue014\\nJ() y∂ plog() y\\n∂ ω\\ue015\\n. (20.67)\\nFurthermore,wecanobtaintheoptimal b( ω) bycomputingthevarianceof( J( y)−\\nb( ω))∂ p l o g ( ) y\\n∂ ωunder p( y)andminimizingwithrespectto b( ω).Whatweﬁndis\\nthatthisoptimalbaseline b∗() ω iisdiﬀerentforeachelement ω iofthevector: ω\\nb∗() ω i=Ep ( ) y\\ue068\\nJ() y∂ p l o g ( ) y\\n∂ ω i2\\ue069\\nE p ( ) y\\ue068\\n∂ p l o g ( ) y\\n∂ ω i2\\ue069 . (20.68)\\nThegradientestimatorwithrespectto ω ithenbecomes\\n(()() J y− b ω i)∂ plog() y\\n∂ ω i(20.69)\\nwhere b( ω) iestimatestheabove b∗( ω) i.Theestimate bisusuallyobtainedby\\naddingextraoutputstotheneuralnetworkandtrainingthenewoutputstoestimate\\nE p ( ) y[ J( y)∂ p l o g ( ) y\\n∂ ω i2]and E p ( ) y\\ue068\\n∂ p l o g ( ) y\\n∂ ω i2\\ue069\\nforeachelementof ω.Theseextra\\noutputscanbetrainedwiththemeansquarederrorobjective,usingrespectively\\nJ( y)∂ p l o g ( ) y\\n∂ ω i2and∂ p l o g ( ) y\\n∂ ω i2astargetswhen yissampledfrom p( y),foragiven\\nω.Theestimate bmaythenberecoveredbysubstitutingtheseestimatesinto\\nequation. ()preferredtouseasinglesharedoutput 20.68MnihandGregor2014\\n(acrossallelements iof ω)trainedwiththetarget J( y),usingasbaseline b( ω)≈\\nE p ( ) y[()] J y.\\nVariancereductionmethodshavebeenintroducedinthereinforcementlearning\\ncontext( ,; Sutton e t a l .2000WeaverandTao2001,),generalizingpreviouswork\\nonthecaseofbinaryrewardbyDayan1990Bengio2013bMnih ().\\xa0See e t a l .(),\\nandGregor2014Ba2014Mnih2014Xu2015 (), e t a l .(), e t a l .(),or e t a l .()for\\nexamplesofmodernusesoftheREINFORCEalgorithmwithreducedvariancein\\nthecontextofdeeplearning.Inadditiontotheuseofaninput-dependentbaseline\\nb( ω) ( , ()foundthatthescaleof MnihandGregor2014 J( y)− b( ω))couldbe\\nadjustedduringtrainingbydividingitbyitsstandarddeviationestimatedbya\\nmovingaverageduringtraining,asakindofadaptivelearningrate,tocounter\\ntheeﬀectofimportantvariationsthatoccurduringthecourseoftraininginthe\\n6 9 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d345ac85-c885-4501-ba0b-47c1e929ae7a', embedding=None, metadata={'page_label': '707', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nmagnitudeofthisquantity. ()calledthisheuristic MnihandGregor2014 variance\\nnormalization.\\nREINFORCE-basedestimatorscanbeunderstoodasestimatingthegradient\\nbycorrelatingchoicesof ywithcorrespondingvaluesof J( y).Ifagoodvalueof y\\nisunlikelyunderthecurrentparametrization, itmighttakealongtimetoobtainit\\nbychance,andgettherequiredsignalthatthisconﬁgurationshouldbereinforced.\\n20.10DirectedGenerativeNets\\nAsdiscussedinchapter,directedgraphicalmodelsmakeupaprominentclass 16\\nofgraphicalmodels.Whiledirectedgraphicalmodelshavebeenverypopular\\nwithinthegreatermachinelearningcommunity,withinthesmallerdeeplearning\\ncommunitytheyhaveuntilroughly2013beenovershadowedbyundirectedmodels\\nsuchastheRBM.\\nInthissectionwereviewsomeofthestandarddirectedgraphicalmodelsthat\\nhavetraditionallybeenassociatedwiththedeeplearningcommunity.\\nWehavealreadydescribeddeepbeliefnetworks,whichareapartiallydirected\\nmodel.Wehavealsoalreadydescribedsparsecodingmodels,whichcanbethought\\nofasshallowdirectedgenerativemodels.Theyareoftenusedasfeaturelearners\\ninthecontextofdeeplearning,thoughtheytendtoperformpoorlyatsample\\ngenerationanddensityestimation.Wenowdescribeavarietyofdeep,fullydirected\\nmodels.\\n20.10.1SigmoidBeliefNets\\nSigmoidbeliefnetworks(,)areasimpleformofdirectedgraphicalmodel Neal1990\\nwithaspeciﬁckindofconditionalprobabilitydistribution.Ingeneral,wecan\\nthinkofasigmoidbeliefnetworkashavingavectorofbinarystates s,witheach\\nelementofthestateinﬂuencedbyitsancestors:\\np s( i) = σ\\uf8eb\\n\\uf8ed\\ue058\\nj < iW j , i s j+ b i\\uf8f6\\n\\uf8f8 . (20.70)\\nThemostcommonstructureofsigmoidbeliefnetworkisonethatisdivided\\nintomanylayers,withancestralsamplingproceedingthroughaseriesofmany\\nhiddenlayersandthenultimatelygeneratingthevisiblelayer.Thisstructureis\\nverysimilartothedeepbeliefnetwork,exceptthattheunitsatthebeginningof\\n6 9 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='35e6f696-e5bf-4b2d-9b4f-3ede0aa892a3', embedding=None, metadata={'page_label': '708', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nthesamplingprocessareindependentfromeachother,ratherthansampledfrom\\narestrictedBoltzmannmachine.\\xa0Suchastructureisinterestingforavarietyof\\nreasons.Onereasonisthatthestructureisauniversalapproximator ofprobability\\ndistributionsoverthevisibleunits,inthesensethatitcanapproximate any\\nprobabilitydistributionoverbinaryvariablesarbitrarilywell,givenenoughdepth,\\nevenifthewidthoftheindividuallayersisrestrictedtothedimensionalityofthe\\nvisiblelayer(SutskeverandHinton2008,).\\nWhilegeneratingasampleofthevisibleunitsisveryeﬃcientinasigmoid\\nbeliefnetwork,mostotheroperationsarenot.Inferenceoverthehiddenunitsgiven\\nthevisibleunitsisintractable.Meanﬁeldinferenceisalsointractablebecausethe\\nvariationallowerboundinvolvestakingexpectationsofcliquesthatencompass\\nentirelayers.Thisproblemhasremaineddiﬃcultenoughtorestrictthepopularity\\nofdirecteddiscretenetworks.\\nOneapproachforperforminginferenceinasigmoidbeliefnetworkistoconstruct\\nadiﬀerentlowerboundthatisspecializedforsigmoidbeliefnetworks(,Saul e t a l .\\n1996).Thisapproachhasonlybeenappliedtoverysmallnetworks.Another\\napproachistouselearnedinferencemechanismsasdescribedinsection.The19.5\\nHelmholtzmachine(Dayan1995DayanandHinton1996 e t a l .,; ,)isasigmoidbelief\\nnetworkcombinedwithaninferencenetworkthatpredictstheparametersofthe\\nmeanﬁelddistributionoverthehiddenunits.Modernapproaches( ,Gregor e t a l .\\n2014MnihandGregor2014 ; ,)tosigmoidbeliefnetworksstillusethisinference\\nnetworkapproach.Thesetechniquesremaindiﬃcultduetothediscretenatureof\\nthelatentvariables.Onecannotsimplyback-propagate throughtheoutputofthe\\ninferencenetwork,butinsteadmustusetherelativelyunreliablemachineryforback-\\npropagatingthroughdiscretesamplingprocesses,describedinsection.Recent 20.9.1\\napproachesbasedonimportancesampling,reweightedwake-sleep(Bornscheinand\\nBengio2015 Bornschein2015 ,)andbidirectional Helmholtzmachines( e t a l .,)\\nmakeitpossibletoquicklytrainsigmoidbeliefnetworksandreachstate-of-the-art\\nperformanceonbenchmarktasks.\\nAspecialcaseofsigmoidbeliefnetworksisthecasewheretherearenolatent\\nvariables.Learninginthiscaseiseﬃcient,becausethereisnoneedtomarginalize\\nlatentvariablesoutofthelikelihood.\\xa0Afamilyofmodelscalledauto-regressive\\nnetworksgeneralizethisfullyvisiblebeliefnetworktootherkindsofvariables\\nbesidesbinaryvariablesandotherstructuresofconditionaldistributionsbesideslog-\\nlinearrelationships.Auto-regressive networksaredescribedlater,insection.20.10.7\\n6 9 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a73c4871-9358-42a6-a5ae-fc085b11f1d6', embedding=None, metadata={'page_label': '709', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\n20.10.2DiﬀerentiableGeneratorNets\\nManygenerativemodelsarebasedontheideaofusingadiﬀerentiablegenerator\\nnetwork.Themodeltransformssamplesoflatentvariables ztosamples xor\\ntodistributionsoversamples xusingadiﬀerentiablefunction g( z; θ( ) g)whichis\\ntypicallyrepresentedbyaneuralnetwork.Thismodelclassincludesvariational\\nautoencoders,\\xa0whichpair\\xa0thegeneratornetwithaninferencenet,generative\\nadversarial\\xa0networks,\\xa0which\\xa0pair the\\xa0generator\\xa0net work\\xa0witha\\xa0discriminator\\nnetwork,andtechniquesthattraingeneratornetworksinisolation.\\nGeneratornetworksareessentiallyjustparametrized computational procedures\\nforgeneratingsamples,wherethearchitectureprovidesthefamilyofpossible\\ndistributionstosamplefromandtheparametersselectadistributionfromwithin\\nthatfamily.\\nAsanexample,thestandardprocedurefordrawingsamplesfromanormal\\ndistributionwithmean µandcovariance Σistofeedsamples zfromanormal\\ndistributionwithzeromeanandidentitycovarianceintoaverysimplegenerator\\nnetwork.Thisgeneratornetworkcontainsjustoneaﬃnelayer:\\nx z L z = ( g) = + µ (20.71)\\nwhereisgivenbytheCholeskydecompositionof. L Σ\\nPseudorandomnumbergeneratorscanalsousenonlineartransformationsof\\nsimpledistributions.Forexample,inversetransformsampling(Devroye2013,)\\ndrawsascalar zfrom U(0 ,1)andappliesanonlineartransformationtoascalar\\nx.Inthiscase g( z)isgivenbytheinverseofthecumulativedistributionfunction\\nF( x) =\\ue052x\\n−∞p( v) d v.Ifweareabletospecify p( x),integrateover x,andinvertthe\\nresultingfunction,wecansamplefromwithoutusingmachinelearning. p x()\\nTogeneratesamplesfrommorecomplicateddistributionsthatarediﬃcult\\ntospecifydirectly,\\xa0diﬃculttointegrateover,\\xa0orwhoseresultingintegralsare\\ndiﬃculttoinvert,weuseafeedforwardnetworktorepresentaparametricfamily\\nofnonlinearfunctions g,andusetrainingdatatoinfertheparametersselecting\\nthedesiredfunction.\\nWecanthinkof gasprovidinganonlinearchangeofvariablesthattransforms\\nthedistributionoverintothedesireddistributionover. z x\\nRecallfromequationthat,forinvertible,diﬀerentiable,continuous, 3.47 g\\np z() = z p x(()) g z\\ue00c\\ue00c\\ue00c\\ue00cdet(∂ g\\n∂ z)\\ue00c\\ue00c\\ue00c\\ue00c. (20.72)\\n6 9 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2adaa909-27c5-451d-8789-f5e4e6acbbf2', embedding=None, metadata={'page_label': '710', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nThisimplicitlyimposesaprobabilitydistributionover:x\\np x() = xp z( g− 1()) x\\ue00c\\ue00c\\ue00cdet(∂ g\\n∂ z)\\ue00c\\ue00c\\ue00c. (20.73)\\nOfcourse,thisformulamaybediﬃculttoevaluate,dependingonthechoiceof\\ng,soweoftenuseindirectmeansoflearning g,ratherthantryingtomaximize\\nlog() p xdirectly.\\nInsomecases,ratherthanusing gtoprovideasampleof xdirectly,weuse g\\ntodeﬁneaconditionaldistributionover x.Forexample,wecoulduseagenerator\\nnetwhoseﬁnallayerconsistsofsigmoidoutputstoprovidethemeanparameters\\nofBernoullidistributions:\\np(x i= 1 ) = () | z g z i . (20.74)\\nInthiscase,whenweuse gtodeﬁne p( x z|),weimposeadistributionover xby\\nmarginalizing : z\\np() = x E z p . ( ) x z| (20.75)\\nBothapproachesdeﬁneadistribution p g( x)andallowustotrainvarious\\ncriteriaof p gusingthereparametrization trickofsection.20.9\\nThetwodiﬀerentapproachestoformulatinggeneratornets—emittingthe\\nparametersofaconditionaldistributionversusdirectlyemittingsamples—have\\ncomplementarystrengthsandweaknesses.Whenthegeneratornetdeﬁnesa\\nconditionaldistributionover x,itiscapableofgeneratingdiscretedataaswellas\\ncontinuousdata.Whenthegeneratornetprovidessamplesdirectly,itiscapableof\\ngeneratingonlycontinuousdata(wecouldintroducediscretizationintheforward\\npropagation, butdoingsowouldmeanthemodelcouldnolongerbetrainedusing\\nback-propagation).Theadvantagetodirectsamplingisthatwearenolonger\\nforcedtouseconditionaldistributionswhoseformcanbeeasilywrittendownand\\nalgebraically manipulated byahumandesigner.\\nApproachesbasedondiﬀerentiable generatornetworksaremotivatedbythe\\nsuccessof\\xa0gradient\\xa0descentappliedtodiﬀerentiable feedforwardnetworksfor\\nclassiﬁcation.\\xa0Inthecontextofsupervisedlearning,deepfeedforwardnetworks\\ntrainedwithgradient-basedlearningseempracticallyguaranteedtosucceedgiven\\nenoughhiddenunitsandenoughtrainingdata.Canthissamerecipeforsuccess\\ntransfertogenerativemodeling?\\nGenerativemodelingseemstobemorediﬃcultthanclassiﬁcationorregression\\nbecausethelearningprocessrequiresoptimizingintractablecriteria.Inthecontext\\n6 9 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ec37de5a-47a5-4f87-b575-63120b5b8a02', embedding=None, metadata={'page_label': '711', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nofdiﬀerentiablegeneratornets,thecriteriaareintractablebecausethedatadoes\\nnotspecifyboththeinputs zandtheoutputs xofthegeneratornet.Inthecase\\nofsupervisedlearning,boththeinputs xandtheoutputs yweregiven,andthe\\noptimization procedureneedsonlytolearnhowtoproducethespeciﬁedmapping.\\nInthecaseofgenerativemodeling,thelearningprocedureneedstodeterminehow\\ntoarrangespaceinausefulwayandadditionallyhowtomapfromto. z z x\\nDosovitskiy2015 e t a l .()studiedasimpliﬁedproblem,wherethecorrespondence\\nbetween zand xisgiven.Speciﬁcally,thetrainingdataiscomputer-rendered\\nimageryofchairs.Thelatentvariables zareparametersgiventotherendering\\nenginedescribingthechoiceofwhichchairmodeltouse,thepositionofthechair,\\nandotherconﬁgurationdetailsthataﬀecttherenderingoftheimage.Usingthis\\nsyntheticallygenerateddata,aconvolutionalnetworkisabletolearntomap z\\ndescriptionsofthecontentofanimageto xapproximationsofrenderedimages.\\nThissuggeststhatcontemporarydiﬀerentiablegeneratornetworkshavesuﬃcient\\nmodelcapacitytobegoodgenerativemodels,andthatcontemporaryoptimization\\nalgorithmshavetheabilitytoﬁtthem.Thediﬃcultyliesindetermininghowto\\ntraingeneratornetworkswhenthevalueof zforeach xisnotﬁxedandknown\\naheadofeachtime.\\nThefollowingsectionsdescribeseveralapproachestotrainingdiﬀerentiable\\ngeneratornetsgivenonlytrainingsamplesof. x\\n20.10.3VariationalAutoencoders\\nThevariationalautoencoderorVAE(,; ,)isa Kingma2013Rezende e t a l .2014\\ndirectedmodelthatuseslearnedapproximate inferenceandcanbetrainedpurely\\nwithgradient-basedmethods.\\nTogenerateasamplefromthemodel,theVAEﬁrstdrawsasample zfrom\\nthecodedistribution p m o de l( z).Thesampleisthenrunthroughadiﬀerentiable\\ngeneratornetwork g( z).Finally, xissampledfromadistribution p m o de l( x; g( z)) =\\np m o de l( x z|).\\xa0However,duringtraining,theapproximate inferencenetwork(or\\nencoder) q( z x|)isusedtoobtain zand p m o de l( x z|)isthenviewedasadecoder\\nnetwork.\\nThekeyinsightbehindvariationalautoencodersisthattheymaybetrained\\nbymaximizingthevariationallowerboundassociatedwithdatapoint: L() q x\\nL() = q E z z x ∼ q (| )log p m o de l()+(( )) z x , H qz| x (20.76)\\n= E z z x ∼ q (| )log p m o de l( ) x z|− D K L(( ) qz| x|| p m o de l())z(20.77)\\n≤log p m o de l() x . (20.78)\\n6 9 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4281b0da-8987-4e26-a8bf-3d0ce8cff437', embedding=None, metadata={'page_label': '712', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nInequation,werecognizetheﬁrsttermasthejointlog-likelihoodofthevisible 20.76\\nandhiddenvariablesundertheapproximateposterioroverthelatentvariables\\n(justlikewithEM,exceptthatweuseanapproximateratherthantheexact\\nposterior).Werecognizealsoasecondterm,theentropyoftheapproximate\\nposterior.\\xa0When qischosentobeaGaussiandistribution,withnoiseaddedto\\napredictedmeanvalue,maximizingthisentropytermencouragesincreasingthe\\nstandarddeviationofthisnoise.Moregenerally,thisentropytermencouragesthe\\nvariationalposteriortoplacehighprobabilitymassonmany zvaluesthatcould\\nhavegenerated x,ratherthancollapsingtoasinglepointestimateofthemost\\nlikelyvalue.Inequation,werecognizetheﬁrsttermasthereconstruction 20.77\\nlog-likelihoodfoundinotherautoencoders.Thesecondtermtriestomakethe\\napproximateposteriordistribution q(z| x) andthemodelprior p m o de l( z) approach\\neachother.\\nTraditionalapproachestovariationalinferenceandlearninginfer qviaanopti-\\nmizationalgorithm,typicallyiteratedﬁxedpointequations(section).These19.4\\napproachesareslowandoftenrequiretheabilitytocompute E z∼ qlog p m o de l( z x ,)\\ninclosedform.Themainideabehindthevariationalautoencoderistotraina\\nparametricencoder(alsosometimescalledaninferencenetworkorrecognition\\nmodel)thatproducestheparametersof q.Solongas zisacontinuousvariable,we\\ncanthenback-propagate throughsamplesof zdrawnfrom q( z x|) = q( z; f( x; θ))\\ninordertoobtainagradientwithrespectto θ.Learningthenconsistssolelyof\\nmaximizing Lwithrespecttotheparametersoftheencoderanddecoder.Allof\\ntheexpectationsinmaybeapproximatedbyMonteCarlosampling. L\\nThevariationalautoencoderapproachiselegant,theoreticallypleasing,and\\nsimpletoimplement.Italsoobtainsexcellentresultsandisamongthestateofthe\\nartapproachestogenerativemodeling.Itsmaindrawbackisthatsamplesfrom\\nvariationalautoencoderstrainedonimagestendtobesomewhatblurry.Thecauses\\nofthisphenomenon arenotyetknown.Onepossibilityisthattheblurrinessis\\nanintrinsiceﬀectofmaximumlikelihood,whichminimizes D K L( p da t a\\ue06b p m o de l).As\\nillustratedinﬁgure,thismeansthatthemodelwillassignhighprobabilityto 3.6\\npointsthatoccurinthetrainingset,butmayalsoassignhighprobabilitytoother\\npoints.Theseotherpointsmayincludeblurryimages.Partofthereasonthatthe\\nmodelwouldchoosetoputprobabilitymassonblurryimagesratherthansome\\notherpartofthespaceisthatthevariationalautoencodersusedinpracticeusually\\nhaveaGaussiandistributionfor p m o de l( x; g( z)).\\xa0Maximizing alowerboundon\\nthelikelihoodofsuchadistributionissimilartotrainingatraditionalautoencoder\\nwithmeansquarederror,inthesensethatithasatendencytoignorefeatures\\noftheinputthatoccupyfewpixelsorthatcauseonlyasmallchangeinthe\\nbrightnessofthepixelsthattheyoccupy.ThisissueisnotspeciﬁctoVAEsand\\n6 9 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d8bda21a-5bba-48cb-b5a3-ed8ff074ac32', embedding=None, metadata={'page_label': '713', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nissharedwithgenerativemodelsthatoptimizealog-likelihood,orequivalently,\\nD K L( p da t a\\ue06b p m o de l),asarguedby ()andby().Another Theis e t a l .2015 Huszar2015\\ntroublingissuewithcontemporary VAEmodelsisthattheytendtouseonlyasmall\\nsubsetofthedimensionsof z,asiftheencoderwasnotabletotransformenough\\nofthelocaldirectionsininputspacetoaspacewherethemarginaldistribution\\nmatchesthefactorizedprior.\\nTheVAEframeworkisverystraightforwardtoextendtoawiderangeofmodel\\narchitectures.ThisisakeyadvantageoverBoltzmannmachines,whichrequire\\nextremelycarefulmodeldesigntomaintaintractability.VAEsworkverywellwith\\nadiversefamilyofdiﬀerentiable operators.OneparticularlysophisticatedVAE\\nisthedeeprecurrentattentionwriterorDRAWmodel( ,). Gregor e t a l .2015\\nDRAWusesarecurrentencoderandrecurrentdecodercombinedwithanattention\\nmechanism.ThegenerationprocessfortheDRAWmodelconsistsofsequentially\\nvisitingdiﬀerentsmallimagepatchesanddrawingthevaluesofthepixelsatthose\\npoints.VAEscanalsobeextendedtogeneratesequencesbydeﬁningvariational\\nRNNs( ,)byusingarecurrentencoderanddecoderwithin Chung e t a l .2015b\\ntheVAEframework.GeneratingasamplefromatraditionalRNNinvolvesonly\\nnon-determinis ticoperationsattheoutputspace.VariationalRNNsalsohave\\nrandomvariabilityatthepotentiallymoreabstractlevelcapturedbytheVAE\\nlatentvariables.\\nTheVAEframeworkhasbeenextendedtomaximizenotjustthetraditional\\nvariationallowerbound,butinsteadtheimportanceweightedautoencoder\\n(,)objective: Burda e t a l .2015\\nL k() = x , q Ez( 1 ) , . . . , z( ) k∼ | q ( z x )\\ue022\\nlog1\\nkk\\ue058\\ni = 1p m o de l( x z ,( ) i)\\nq( z( ) i| x)\\ue023\\n.(20.79)\\nThisnewobjectiveisequivalenttothetraditionallowerbound Lwhen k=1.\\nHowever,itmayalsobeinterpretedasforminganestimateofthetruelog p m o de l( x)\\nusingimportancesamplingof zfromproposaldistribution q( z x|).Theimportance\\nweightedautoencoderobjectiveisalsoalowerboundonlog p m o de l( x) andbecomes\\ntighterasincreases. k\\nVariationalautoencodershavesomeinterestingconnectionstotheMP-DBM\\nandotherapproachesthatinvolveback-propagationthroughtheapproximate\\ninferencegraph(Goodfellow2013bStoyanov2011Brakel2013 e t a l .,; e t a l .,; e t a l .,).\\nThesepreviousapproachesrequiredaninferenceproceduresuchasmeanﬁeldﬁxed\\npointequationstoprovidethecomputational graph.Thevariationalautoencoder\\nisdeﬁnedforarbitrarycomputational graphs,whichmakesitapplicabletoawider\\nrangeofprobabilisticmodelfamiliesbecausethereisnoneedtorestrictthechoice\\n6 9 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fac30f65-2f95-40da-a4a3-58086ac4b41b', embedding=None, metadata={'page_label': '714', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nofmodelstothosewithtractablemeanﬁeldﬁxedpointequations.Thevariational\\nautoencoderalsohastheadvantagethatitincreasesaboundonthelog-likelihood\\nofthemodel,whilethecriteriafortheMP-DBMandrelatedmodelsaremore\\nheuristicandhavelittleprobabilisticinterpretation beyondmakingtheresultsof\\napproximateinferenceaccurate.Onedisadvantageofthevariationalautoencoder\\nisthatitlearnsaninferencenetworkforonlyoneproblem,inferring zgiven x.\\nTheoldermethodsareabletoperformapproximateinferenceoveranysubsetof\\nvariablesgivenanyothersubsetofvariables,becausethemeanﬁeldﬁxedpoint\\nequationsspecifyhowtoshareparametersbetweenthecomputational graphsfor\\nallofthesediﬀerentproblems.\\nOneverynicepropertyofthevariationalautoencoderisthatsimultaneously\\ntrainingaparametricencoderincombinationwiththegeneratornetworkforcesthe\\nmodeltolearnapredictablecoordinatesystemthattheencodercancapture.This\\nmakesitanexcellentmanifoldlearningalgorithm.Seeﬁgureforexamplesof 20.6\\nlow-dimensionalmanifoldslearnedbythevariationalautoencoder.Inoneofthe\\ncasesdemonstratedintheﬁgure,thealgorithmdiscoveredtwoindependentfactors\\nofvariationpresentinimagesoffaces:angleofrotationandemotionalexpression.\\n20.10.4GenerativeAdversarialNetworks\\nGenerativeadversarialnetworksorGANs( ,)areanother Goodfellow e t a l .2014c\\ngenerativemodelingapproachbasedondiﬀerentiablegeneratornetworks.\\nGenerativeadversarialnetworksarebasedonagametheoreticscenarioin\\nwhichthegeneratornetworkmustcompeteagainstanadversary.Thegenerator\\nnetworkdirectlyproducessamples x= g( z; θ( ) g).Itsadversary,thediscriminator\\nnetwork,attemptstodistinguishbetweensamplesdrawnfromthetrainingdata\\nandsamplesdrawnfromthegenerator.Thediscriminatoremitsaprobabilityvalue\\ngivenby d( x; θ( ) d),indicatingtheprobabilitythat xisarealtrainingexample\\nratherthanafakesampledrawnfromthemodel.\\nThesimplestwaytoformulatelearningingenerativeadversarialnetworksis\\nasazero-sumgame,inwhichafunction v( θ( ) g, θ( ) d)determinesthepayoﬀofthe\\ndiscriminator.Thegeneratorreceives− v( θ( ) g, θ( ) d)asitsownpayoﬀ.During\\nlearning,eachplayerattemptstomaximizeitsownpayoﬀ,sothatatconvergence\\ng∗= argmin\\ngmax\\ndv g , d . () (20.80)\\nThedefaultchoiceforis v\\nv( θ( ) g, θ( ) d) = E x∼ pdatalog()+ d x E x∼ pmodellog(1 ()) − d x .(20.81)\\n6 9 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f17f892f-ae3a-421a-ab19-c05776d2a80e', embedding=None, metadata={'page_label': '715', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nFigure20.6:Examplesoftwo-dimensionalcoordinatesystemsforhigh-dimensionalmani-\\nfolds,learnedbyavariationalautoencoder(KingmaandWelling2014a,).Twodimensions\\nmaybeplotteddirectlyonthepageforvisualization,sowecangainanunderstandingof\\nhowthemodelworksbytrainingamodelwitha2-Dlatentcode,evenifwebelievethe\\nintrinsicdimensionalityofthedatamanifoldismuchhigher.Theimagesshownarenot\\nexamplesfromthetrainingsetbutimages xactuallygeneratedbythemodel p( x z|),\\nsimplybychangingthe2-D“code” z(eachimagecorrespondstoadiﬀerentchoiceof“code”\\nzona2-Duniformgrid). ( L e f t )Thetwo-dimensionalmapoftheFreyfacesmanifold.\\nOnedimensionthathasbeendiscovered(horizontal)mostlycorrespondstoarotationof\\ntheface,whiletheother(vertical)correspondstotheemotionalexpression.The ( R i g h t )\\ntwo-dimensionalmapoftheMNISTmanifold.\\nThisdrivesthediscriminatortoattempttolearntocorrectlyclassifysamplesasreal\\norfake.Simultaneous ly,thegeneratorattemptstofooltheclassiﬁerintobelieving\\nitssamplesarereal.Atconvergence,thegenerator’ssamplesareindistinguishable\\nfromrealdata,andthediscriminatoroutputs1\\n2everywhere.Thediscriminator\\nmaythenbediscarded.\\nThemainmotivationforthedesignofGANsisthatthelearningprocess\\nrequiresneitherapproximateinferencenorapproximation ofapartitionfunction\\ngradient.Inthecasewhere max d v( g , d)isconvexin θ( ) g(suchasthecasewhere\\noptimization isperformeddirectlyinthespaceofprobabilitydensityfunctions)\\ntheprocedureisguaranteedtoconvergeandisasymptoticallyconsistent.\\nUnfortunately,learninginGANscanbediﬃcultinpracticewhen gand d\\narerepresentedbyneuralnetworksandmax d v( g , d)isnotconvex.Goodfellow\\n7 0 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e52efbc3-b3c0-4b17-8f4c-93f825ca4f21', embedding=None, metadata={'page_label': '716', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\n()identiﬁednon-convergenceasanissuethatmaycauseGANstounderﬁt. 2014\\nIngeneral,simultaneousgradientdescentontwoplayers’costsisnotguaranteed\\ntoreachanequilibrium.Considerforexamplethevaluefunction v( a , b)= a b,\\nwhereoneplayercontrols aandincurscost a b,whiletheotherplayercontrols b\\nandreceivesacost− a b.Ifwemodeleachplayerasmakinginﬁnitesimallysmall\\ngradientsteps,eachplayerreducingtheirowncostattheexpenseoftheother\\nplayer,then aand bgointoastable,circularorbit,ratherthanarrivingatthe\\nequilibriumpointattheorigin.Notethattheequilibriaforaminimaxgameare\\nnotlocalminimaof v.Instead,theyarepointsthataresimultaneouslyminima\\nforbothplayers’costs.Thismeansthattheyaresaddlepointsof vthatarelocal\\nminimawithrespecttotheﬁrstplayer’sparametersandlocalmaximawithrespect\\ntothesecondplayer’sparameters.Itispossibleforthetwoplayerstotaketurns\\nincreasingthendecreasing vforever,ratherthanlandingexactlyonthesaddle\\npointwhereneitherplayeriscapableofreducingitscost.Itisnotknowntowhat\\nextentthisnon-convergenceproblemaﬀectsGANs.\\nGoodfellow2014()identiﬁedanalternativeformulationofthepayoﬀs,inwhich\\nthegameisnolongerzero-sum,thathasthesameexpectedgradientasmaximum\\nlikelihoodlearningwheneverthediscriminatorisoptimal.Becausemaximum\\nlikelihoodtrainingconverges,thisreformulationoftheGANgameshouldalso\\nconverge,givenenoughsamples.Unfortunately,thisalternativeformulationdoes\\nnotseemtoimproveconvergenceinpractice,possiblyduetosuboptimalityofthe\\ndiscriminator,orpossiblyduetohighvariancearoundtheexpectedgradient.\\nInrealisticexperiments,thebest-performingformulationoftheGANgame\\nisadiﬀerentformulationthatisneitherzero-sumnorequivalenttomaximum\\nlikelihood,introducedby ()withaheuristicmotivation.In Goodfellow e t a l .2014c\\nthisbest-performingformulation,thegeneratoraimstoincreasethelogprobability\\nthatthediscriminatormakesamistake,ratherthanaimingtodecreasethelog\\nprobabilitythatthediscriminatormakesthecorrectprediction.Thisreformulation\\nismotivatedsolelybytheobservationthatitcausesthederivativeofthegenerator’s\\ncostfunctionwithrespecttothediscriminator’slogitstoremainlargeeveninthe\\nsituationwherethediscriminatorconﬁdentlyrejectsallgeneratorsamples.\\nStabilization ofGANlearningremainsanopenproblem.\\xa0Fortunately,GAN\\nlearningperformswellwhenthemodelarchitectureandhyperparametersarecare-\\nfullyselected. ()craftedadeepconvolutionalGAN(DCGAN) Radford e t a l .2015\\nthatperformsverywellforimagesynthesistasks,andshowedthatitslatentrepre-\\nsentationspacecapturesimportantfactorsofvariation,asshowninﬁgure.15.9\\nSeeﬁgureforexamplesofimagesgeneratedbyaDCGANgenerator. 20.7\\nTheGANlearningproblemcanalsobesimpliﬁedbybreakingthegeneration\\n7 0 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f263f25e-dc1d-4db1-9b53-71fed82837c8', embedding=None, metadata={'page_label': '717', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nFigure20.7:ImagesgeneratedbyGANstrainedontheLSUNdataset. ( L e f t )Images\\nofbedroomsgeneratedbyaDCGANmodel,reproducedwithpermissionfromRadford\\ne t a l .().ImagesofchurchesgeneratedbyaLAPGANmodel,reproducedwith 2015 ( R i g h t )\\npermissionfrom (). Denton e t a l .2015\\nprocessintomanylevelsofdetail.ItispossibletotrainconditionalGANs(Mirza\\nandOsindero2014,)thatlearntosamplefromadistribution p( x y|)rather\\nthansimplysamplingfromamarginaldistribution p( x). () Denton e t a l .2015\\nshowedthataseriesofconditionalGANscanbetrainedtoﬁrstgenerateavery\\nlow-resolutionversionofanimage,thenincrementally adddetailstotheimage.\\nThistechniqueiscalledtheLAPGANmodel,duetotheuseofaLaplacianpyramid\\ntogeneratetheimagescontainingvaryinglevelsofdetail.LAPGANgenerators\\nareabletofoolnotonlydiscriminatornetworksbutalsohumanobservers,with\\nexperimentalsubjectsidentifyingupto40%oftheoutputsofthenetworkas\\nbeingrealdata.SeeﬁgureforexamplesofimagesgeneratedbyaLAPGAN 20.7\\ngenerator.\\nOneunusualcapabilityoftheGANtrainingprocedureisthatitcanﬁtproba-\\nbilitydistributionsthatassignzeroprobabilitytothetrainingpoints.Ratherthan\\nmaximizingthelogprobabilityofspeciﬁcpoints,thegeneratornetlearnstotrace\\noutamanifoldwhosepointsresembletrainingpointsinsomeway.Somewhatpara-\\ndoxically,thismeansthatthemodelmayassignalog-likelihoodofnegativeinﬁnity\\ntothetestset,whilestillrepresentingamanifoldthatahumanobserverjudges\\ntocapturetheessenceofthegenerationtask.Thisisnotclearlyanadvantageor\\nadisadvantage,andonemayalsoguaranteethatthegeneratornetworkassigns\\nnon-zeroprobabilitytoallpointssimplybymakingthelastlayerofthegenerator\\nnetworkaddGaussiannoisetoallofthegeneratedvalues.\\xa0Generatornetworks\\nthataddGaussiannoiseinthismannersamplefromthesamedistributionthatone\\nobtainsbyusingthegeneratornetworktoparametrizethemeanofaconditional\\n7 0 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='30d6a74b-40f8-4ec2-9847-865f08372351', embedding=None, metadata={'page_label': '718', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nGaussiandistribution.\\nDropoutseemstobeimportantinthediscriminatornetwork.\\xa0Inparticular,\\nunits\\xa0shouldbe\\xa0stochasticallydropped\\xa0whilecomputingthe\\xa0gradientfor\\xa0the\\ngeneratornetworktofollow.Followingthegradientofthedeterministicversionof\\nthediscriminatorwithitsweightsdividedbytwodoesnotseemtobeaseﬀective.\\nLikewise,neverusingdropoutseemstoyieldpoorresults.\\nWhiletheGANframeworkisdesignedfordiﬀerentiablegeneratornetworks,\\nsimilarprinciplescanbeusedtotrainotherkindsofmodels.Forexample,self-\\nsupervisedboostingcanbeusedtotrainanRBMgeneratortofoolalogistic\\nregressiondiscriminator(Welling2002 e t a l .,).\\n20.10.5GenerativeMomentMatchingNetworks\\nGenerativemomentmatchingnetworks(,; , Li e t a l .2015Dziugaite e t a l .\\n2015)areanotherformofgenerativemodelbasedondiﬀerentiablegenerator\\nnetworks.UnlikeVAEsandGANs,theydonotneedtopairthegeneratornetwork\\nwithanyothernetwork—neither aninferencenetworkasusedwithVAEsnora\\ndiscriminatornetworkasusedwithGANs.\\nThesenetworksaretrainedwithatechniquecalledmomentmatching.The\\nbasicideabehindmomentmatchingistotrainthegeneratorinsuchawaythat\\nmanyofthestatisticsofsamplesgeneratedbythemodelareassimilaraspossible\\ntothoseofthestatisticsoftheexamplesinthetrainingset.Inthiscontext,a\\nmomentisanexpectationofdiﬀerentpowersofarandomvariable.Forexample,\\ntheﬁrstmomentisthemean,thesecondmomentisthemeanofthesquared\\nvalues,andsoon.Inmultipledimensions,eachelementoftherandomvectormay\\nberaisedtodiﬀerentpowers,sothatamomentmaybeanyquantityoftheform\\nE xΠ i xn i\\ni (20.82)\\nwhere n= [ n 1 , n 2 , . . . , n d]\\ue03eisavectorofnon-negativeintegers.\\nUponﬁrstexamination,thisapproachseemstobecomputationally infeasible.\\nForexample,ifwewanttomatchallthemomentsoftheform x i x j,thenweneed\\ntominimizethediﬀerencebetweenanumberofvaluesthatisquadraticinthe\\ndimensionof x.Moreover,evenmatchingalloftheﬁrstandsecondmoments\\nwouldonlybesuﬃcienttoﬁtamultivariateGaussiandistribution,whichcaptures\\nonlylinearrelationshipsbetweenvalues.Ourambitionsforneuralnetworksareto\\ncapturecomplexnonlinearrelationships,whichwouldrequirefarmoremoments.\\nGANsavoidthisproblemofexhaustivelyenumeratingallmomentsbyusinga\\n7 0 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b05c4af-2e55-435f-8236-ccbdc9e24068', embedding=None, metadata={'page_label': '719', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\ndynamicallyupdateddiscriminatorthatautomatically focusesitsattentionon\\nwhicheverstatisticthegeneratornetworkismatchingtheleasteﬀectively.\\nInstead,generativemomentmatchingnetworkscanbetrainedbyminimizing\\nacostfunctioncalledmaximummeandiscrepancy(SchölkopfandSmola,\\n2002Gretton2012 ; e t a l .,)orMMD.Thiscostfunctionmeasurestheerrorin\\ntheﬁrstmomentsinaninﬁnite-dimens ionalspace,usinganimplicitmapping\\ntofeaturespacedeﬁnedbyakernelfunctioninordertomakecomputations on\\ninﬁnite-dimens ionalvectorstractable.TheMMDcostiszeroifandonlyifthetwo\\ndistributionsbeingcomparedareequal.\\nVisually,thesamplesfromgenerativemomentmatchingnetworksaresomewhat\\ndisappointing.Fortunately,theycanbeimprovedbycombiningthegenerator\\nnetworkwithanautoencoder.First,anautoencoderistrainedtoreconstructthe\\ntrainingset.Next,theencoderoftheautoencoderisusedtotransformtheentire\\ntrainingsetintocodespace.Thegeneratornetworkisthentrainedtogenerate\\ncodesamples,whichmaybemappedtovisuallypleasingsamplesviathedecoder.\\nUnlikeGANs,thecostfunctionisdeﬁnedonlywithrespecttoabatchof\\nexamplesfromboththetrainingsetandthegeneratornetwork.Itisnotpossible\\ntomakeatrainingupdateasafunctionofonlyonetrainingexampleoronly\\nonesamplefromthegeneratornetwork.Thisisbecausethemomentsmustbe\\ncomputedasanempiricalaverageacrossmanysamples.Whenthebatchsizeistoo\\nsmall,MMDcanunderestimatethetrueamountofvariationinthedistributions\\nbeingsampled.Noﬁnitebatchsizeissuﬃcientlylargetoeliminatethisproblem\\nentirely,butlargerbatchesreducetheamountofunderestimation.Whenthebatch\\nsizeistoolarge,thetrainingprocedurebecomesinfeasiblyslow,becausemany\\nexamplesmustbeprocessedinordertocomputeasinglesmallgradientstep.\\nAswithGANs,itispossibletotrainageneratornetusingMMDevenifthat\\ngeneratornetassignszeroprobabilitytothetrainingpoints.\\n20.10.6ConvolutionalGenerativeNetworks\\nWhengeneratingimages,itisoftenusefultouseageneratornetworkthatincludes\\naconvolutionalstructure(seeforexampleGoodfellow2014cDosovitskiy e t a l .()or\\ne t a l .()).Todoso,\\xa0weusethe“transpose”oftheconvolutionoperator, 2015\\ndescribedinsection.Thisapproachoftenyieldsmorerealisticimagesanddoes 9.5\\nsousingfewerparametersthanusingfullyconnectedlayerswithoutparameter\\nsharing.\\nConvolutionalnetworksforrecognitiontaskshaveinformationﬂowfromthe\\nimagetosomesummarizationlayeratthetopofthenetwork,oftenaclasslabel.\\n7 0 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='57739fb4-32c9-49a1-9c7e-f9134a2e8bd5', embedding=None, metadata={'page_label': '720', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nAsthisimageﬂowsupwardthroughthenetwork,informationisdiscardedasthe\\nrepresentationoftheimagebecomesmoreinvarianttonuisancetransformations.\\nInageneratornetwork,\\xa0theoppositeistrue.Richdetailsmustbeaddedas\\ntherepresentationoftheimagetobegeneratedpropagatesthroughthenetwork,\\nculminatingintheﬁnalrepresentationoftheimage,whichisofcoursetheimage\\nitself,inallofitsdetailedglory,withobjectpositionsandposesandtexturesand\\nlighting.\\xa0Theprimarymechanismfordiscardinginformationinaconvolutional\\nrecognitionnetworkisthepoolinglayer.Thegeneratornetworkseemstoneedto\\naddinformation. Wecannotputtheinverseofapoolinglayerintothegenerator\\nnetworkbecausemostpoolingfunctionsarenotinvertible.Asimpleroperationis\\ntomerelyincreasethespatialsizeoftherepresentation.Anapproachthatseems\\ntoperformacceptablyistousean“un-pooling”asintroducedbyDosovitskiy e t a l .\\n().Thislayercorrespondstotheinverseofthemax-poolingoperationunder 2015\\ncertainsimplifyingconditions.\\xa0Firs t,thestrideofthemax-poolingoperationis\\nconstrainedtobeequaltothewidthofthepoolingregion.Second,themaximum\\ninputwithineachpoolingregionisassumedtobetheinputintheupper-left\\ncorner.Finally,allnon-maximal inputswithineachpoolingregionareassumedto\\nbezero.Theseareverystrongandunrealisticassumptions,buttheydoallowthe\\nmax-poolingoperatortobeinverted.Theinverseun-poolingoperationallocates\\natensorofzeros,thencopieseachvaluefromspatialcoordinate ioftheinput\\ntospatialcoordinate i k×oftheoutput.Theintegervalue kdeﬁnesthesize\\nofthepoolingregion.Eventhoughtheassumptionsmotivatingthedeﬁnitionof\\ntheun-poolingoperatorareunrealistic,thesubsequentlayersareabletolearnto\\ncompensateforitsunusualoutput,sothesamplesgeneratedbythemodelasa\\nwholearevisuallypleasing.\\n20.10.7Auto-RegressiveNetworks\\nAuto-regressivenetworksaredirectedprobabilisticmodelswithnolatentrandom\\nvariables.Theconditionalprobabilitydistributionsinthesemodelsarerepresented\\nbyneuralnetworks(sometimesextremelysimpleneuralnetworkssuchaslogistic\\nregression).Thegraphstructureofthesemodelsisthecompletegraph.They\\ndecomposeajointprobabilityovertheobservedvariablesusingthechainruleof\\nprobabilitytoobtainaproductofconditionalsoftheform P( x d| x d− 1 , . . . , x 1).\\nSuchmodelshavebeencalledfully-visibleBayesnetworks(FVBNs)andused\\nsuccessfully\\xa0inmany\\xa0forms,\\xa0ﬁrstwith\\xa0logistic regression\\xa0foreachconditional\\ndistribution(Frey1998,)andthenwithneuralnetworkswithhiddenunits(Bengio\\nandBengio2000bLarochelleandMurray2011 ,; ,).Insomeformsofauto-\\nregressivenetworks,suchasNADE( ,),described LarochelleandMurray2011\\n7 0 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='47e53c62-4fc5-4bd0-b409-248fc2c70482', embedding=None, metadata={'page_label': '721', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\ninsection below,wecanintroduceaformofparametersharingthat 20.10.10\\nbringsbothastatisticaladvantage(feweruniqueparameters)andacomputational\\nadvantage(lesscomputation). Thisisonemoreinstanceoftherecurringdeep\\nlearningmotifof r e u s e o f f e a t u r e s.\\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )\\nP x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4\\nFigure20.8:A\\xa0fullyvisiblebelief\\xa0networkpredictsthe i-thvariable\\xa0fromthe i−1\\npreviousones. ( T o p ) ( Bottom ) ThedirectedgraphicalmodelforanFVBN. Corresponding\\ncomputationalgraph,inthecaseofthelogisticFVBN,whereeachpredictionismadeby\\nalinearpredictor.\\n20.10.8LinearAuto-RegressiveNetworks\\nThesimplestformofauto-regressiv enetworkhasnohiddenunitsandnosharing\\nofparametersorfeatures.Each P( x i| x i− 1 , . . . , x 1)isparametrized asalinear\\nmodel(linearregressionforreal-valueddata,logisticregressionforbinarydata,\\nsoftmaxregressionfordiscretedata).ThismodelwasintroducedbyFrey1998()\\nandhas O( d2)parameterswhenthereare dvariablestomodel.Itisillustratedin\\nﬁgure.20.8\\nIfthevariablesarecontinuous,alinearauto-regressive modelismerelyanother\\nwaytoformulateamultivariateGaussiandistribution,capturinglinearpairwise\\ninteractionsbetweentheobservedvariables.\\nLinearauto-regressiv enetworksareessentiallythegeneralization oflinear\\nclassiﬁcationmethodstogenerativemodeling.Theythereforehavethesame\\n7 0 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='914c5a2c-acde-40d3-9f55-75a3e9f0de1a', embedding=None, metadata={'page_label': '722', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nadvantagesanddisadvantagesaslinearclassiﬁers.Likelinearclassiﬁers,theymay\\nbetrainedwithconvexlossfunctions,andsometimesadmitclosedformsolutions\\n(asintheGaussiancase).Likelinearclassiﬁers,themodelitselfdoesnotoﬀer\\nawayofincreasingitscapacity,socapacitymustberaisedusingtechniqueslike\\nbasisexpansionsoftheinputorthekerneltrick.\\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )\\nP x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )\\nFigure20.9:Aneuralauto-regressivenetworkpredictsthe i-thvariable x ifromthe i−1\\npreviousones,butisparametrizedsothatfeatures(groupsofhiddenunitsdenoted h i)\\nthatarefunctionsof x 1 , . . . , x icanbereusedinpredictingallofthesubsequentvariables\\nx i + 1 , x i + 2 , . . . , x d.\\n20.10.9NeuralAuto-RegressiveNetworks\\nNeuralauto-regressiv enetworks( ,,)havethesame BengioandBengio2000ab\\nleft-to-rightgraphicalmodelaslogisticauto-regressiv enetworks(ﬁgure)but20.8\\nemployadiﬀerentparametrization oftheconditionaldistributionswithinthat\\ngraphicalmodelstructure.Thenewparametrization ismorepowerfulinthesense\\nthatitscapacitycanbeincreasedasmuchasneeded,allowingapproximation of\\nanyjointdistribution.Thenewparametrization canalsoimprovegeneralization\\nbyintroducingaparametersharingandfeaturesharingprinciplecommontodeep\\nlearningingeneral.Themodelsweremotivatedbytheobjectiveofavoidingthe\\ncurseofdimensionalityarisingoutoftraditionaltabulargraphicalmodels,sharing\\nthesamestructureasﬁgure.Intabulardiscreteprobabilisticmodels,each 20.8\\nconditionaldistributionisrepresentedbyatableofprobabilities, withoneentry\\nandoneparameterforeachpossibleconﬁgurationofthevariablesinvolved.By\\nusinganeuralnetworkinstead,twoadvantagesareobtained:\\n7 0 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09ecd24a-b52c-4fe5-b254-cfdc8beb9900', embedding=None, metadata={'page_label': '723', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\n1.Theparametrization ofeach P( x i| x i− 1 , . . . , x 1)byaneuralnetworkwith\\n( i−1)× kinputsand koutputs(ifthevariablesarediscreteandtake k\\nvalues,encodedone-hot)allowsonetoestimatetheconditionalprobability\\nwithoutrequiringanexponentialnumberofparameters(andexamples),yet\\nstillisabletocapturehigh-orderdependenciesbetweentherandomvariables.\\n2.Insteadofhavingadiﬀerentneuralnetworkforthepredictionofeach x i,\\na connectivityillustratedinﬁgureallowsonetomergeall l e f t - t o - r i g h t 20.9\\ntheneuralnetworksintoone.Equivalently,itmeansthatthehiddenlayer\\nfeaturescomputedforpredicting x icanbereusedforpredicting x i k +( k >0).\\nThehiddenunitsarethusorganizedin g r o u p sthathavetheparticularity\\nthatalltheunitsinthe i-thgrouponlydependontheinputvalues x 1 , . . . , x i.\\nTheparametersusedtocomputethesehiddenunitsarejointlyoptimized\\nto\\xa0improvethe\\xa0prediction ofall\\xa0thevariables\\xa0inthe\\xa0sequence.This\\xa0is\\naninstanceofthe r e u s e p r i nc i p l ethatrecursthroughoutdeeplearningin\\nscenariosrangingfromrecurrentandconvolutionalnetworkarchitectures to\\nmulti-taskandtransferlearning.\\nEach P( x i| x i− 1 , . . . , x 1)canrepresentaconditionaldistributionbyhaving\\noutputsoftheneuralnetworkpredict p a r a m e t e r softheconditionaldistribution\\nof x i,asdiscussedinsection.Althoughtheoriginalneuralauto-regressive 6.2.1.1\\nnetworkswereinitiallyevaluatedinthecontextofpurelydiscretemultivariate\\ndata(withasigmoidoutputforaBernoullivariableorsoftmaxoutputfora\\nmultinoullivariable)itisnaturaltoextendsuchmodelstocontinuousvariablesor\\njointdistributionsinvolvingbothdiscreteandcontinuousvariables.\\n20.10.10NADE\\nTheneuralautoregressivedensityestimator(NADE)isaverysuccessful\\nrecentformofneuralauto-regressive network(LarochelleandMurray2011,).The\\nconnectivityisthesameasfortheoriginalneuralauto-regressive networkofBengio\\nandBengio2000b()butNADEintroducesanadditionalparametersharingscheme,\\nasillustratedinﬁgure.Theparametersofthehiddenunitsofdiﬀerentgroups 20.10\\njareshared.\\nTheweights W\\ue030\\nj , k, ifromthe i-thinput x itothe k-thelementofthe j-thgroup\\nofhiddenunit h( ) j\\nk()aresharedamongthegroups: j i≥\\nW\\ue030\\nj , k, i= W k, i . (20.83)\\nTheremainingweights,where,arezero. j < i\\n7 0 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='42ba96ee-e33c-4a0b-9b26-8c455ea90d4e', embedding=None, metadata={'page_label': '724', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nx 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 )\\nP x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )\\nW : 1 , W : 1 , W : 1 ,\\nW : 2 , W : 2 , W : 3 ,\\nFigure20.10:Anillustrationoftheneuralautoregressivedensityestimator(NADE).The\\nhiddenunitsareorganizedingroups h( ) jsothatonlytheinputs x 1 , . . . , x iparticipate\\nincomputing h( ) iandpredicting P( x j| x j− 1 , . . . , x 1),for j > i.NADEisdiﬀerentiated\\nfromearlierneuralauto-regressivenetworksbytheuseofaparticularweightsharing\\npattern: W\\ue030\\nj , k , i= W k , iisshared(indicatedintheﬁgurebytheuseofthesamelinepattern\\nforeveryinstanceofareplicatedweight)foralltheweightsgoingoutfrom x itothe k-th\\nunitofanygroup.Recallthatthevector j i≥ ( W 1 , i , W 2 , i , . . . , W n , i)isdenoted W : , i.\\nLarochelleandMurray2011()chosethissharingschemesothatforward\\npropagationinaNADEmodellooselyresemblesthecomputations performedin\\nmeanﬁeldinferencetoﬁllinmissinginputsinanRBM.Thismeanﬁeldinference\\ncorrespondstorunningarecurrentnetworkwithsharedweightsandtheﬁrststep\\nofthatinferenceisthesameasinNADE.TheonlydiﬀerenceisthatwithNADE,\\ntheoutputweightsconnectingthehiddenunitstotheoutputareparametrized\\nindependentlyfromtheweightsconnectingtheinputunitstothehiddenunits.In\\ntheRBM,thehidden-to-output weightsarethetransposeoftheinput-to-hidden\\nweights.TheNADEarchitecturecanbeextendedtomimicnotjustonetimestep\\nofthemeanﬁeldrecurrentinferencebuttomimic ksteps.Thisapproachiscalled\\nNADE-(,). kRaiko e t a l .2014\\nAsmentionedpreviously,auto-regressiv enetworksmaybeextendtoprocess\\ncontinuous-valueddata.Aparticularlypowerfulandgenericwayofparametrizing\\nacontinuousdensityisasaGaussianmixture(introducedinsection)with3.9.6\\nmixtureweights α i(thecoeﬃcientorpriorprobabilityforcomponent i),per-\\ncomponentconditionalmean µ iandper-componentconditionalvariance σ2\\ni.\\xa0A\\nmodelcalledRNADE(,)usesthisparametrization toextendNADE Uria e t a l .2013\\ntorealvalues.Aswithothermixturedensitynetworks,theparametersofthis\\n7 0 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5bd8ead5-4686-4fe7-a85e-05fa13855ce5', embedding=None, metadata={'page_label': '725', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\ndistributionareoutputsofthenetwork,withthemixtureweightprobabilities\\nproducedbyasoftmaxunit,andthevariancesparametrized sothattheyare\\npositive.\\xa0Stochasticgradientdescentcanbenumericallyill-behavedduetothe\\ninteractionsbetweentheconditionalmeans µ iandtheconditionalvariances σ2\\ni.\\nToreducethisdiﬃculty,()useapseudo-gradientthatreplacesthe Uria e t a l .2013\\ngradientonthemean,intheback-propagationphase.\\nAnotherveryinterestingextensionoftheneuralauto-regressiv earchitectures\\ngetsridoftheneedtochooseanarbitraryorderfortheobservedvariables(Murray\\nandLarochelle2014,).Inauto-regressive networks,theideaistotrainthenetwork\\ntobeabletocopewithanyorderbyrandomlysamplingordersandprovidingthe\\ninformationtohiddenunitsspecifyingwhichoftheinputsareobserved(onthe\\nrightsideoftheconditioningbar)andwhicharetobepredictedandarethus\\nconsideredmissing(ontheleftsideoftheconditioningbar).Thisisnicebecause\\nitallowsonetouseatrainedauto-regressiv enetworkto p e r f o r m a ny i nfe r e nc e\\np r o b l e m(i.e.predictorsamplefromtheprobabilitydistributionoveranysubset\\nofvariablesgivenanysubset)extremelyeﬃciently.Finally,sincemanyordersof\\nvariablesarepossible( n!for nvariables)andeachorder oofvariablesyieldsa\\ndiﬀerent,wecanformanensembleofmodelsformanyvaluesof: p o(x|) o\\np e nse m bl e() =x1\\nkk\\ue058\\ni = 1p o(x|( ) i) . (20.84)\\nThisensemblemodelusuallygeneralizesbetterandassignshigherprobabilityto\\nthetestsetthandoesanindividualmodeldeﬁnedbyasingleordering.\\nInthesamepaper,theauthorsproposedeepversionsofthearchitecture, but\\nunfortunately thatimmediatelymakescomputationasexpensiveasintheoriginal\\nneuralauto-regressiv eneuralnetwork( ,).Theﬁrstlayer BengioandBengio2000b\\nandtheoutputlayercanstillbecomputedin O( n h)multiply-addoperations,\\nasintheregularNADE,where histhenumberofhiddenunits(thesizeofthe\\ngroups h i,inﬁguresand),whereasitis 20.1020.9 O( n2h)inBengioandBengio\\n().However,fortheotherhiddenlayers,thecomputationis 2000b O( n2h2)ifevery\\n“previous”groupatlayer lparticipatesinpredictingthe“next”groupatlayer l+1,\\nassuming ngroupsof hhiddenunitsateachlayer.Makingthe i-thgroupatlayer\\nl+1onlydependonthe i-thgroup,asinMurrayandLarochelle2014()atlayer l\\nreducesitto O n h(2),whichisstilltimesworsethantheregularNADE. h\\n7 1 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f435e302-d07c-4b97-a729-0f3d4d322d89', embedding=None, metadata={'page_label': '726', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20. DEEPGENERATIVEMODELS\\n20.11 DrawingSamplesfromAutoencoders\\nInchapter ,wesawthatmanykindsofautoencoderslearnthedatadistribution.14\\nTherearecloseconnectionsbetweenscorematching,denoisingautoencoders,and\\ncontractive autoencoders. These connections demonstrate that some kinds of\\nautoencoderslearnthedatadistributioninsomeway. Wehavenotyetseenhow\\ntodrawsamplesfromsuchmodels.\\nSomekindsof autoencoders, suchasthe variational autoencoder,explicitly\\nrepresentaprobabilitydistributionandadmitstraightforwardancestralsampling.\\nMostotherkindsofautoencodersrequireMCMCsampling.\\nContractiveautoencodersaredesignedtorecoveranestimateofthetangent\\nplaneofthedatamanifold. Thismeansthatrepeatedencodinganddecodingwith\\ninjectednoisewillinducearandomwalkalongthesurfaceofthemanifold(Rifai\\net al. et al. , ;2012 Mesnil , ). Thismanifolddiﬀusiontechniqueisakindof2012\\nMarkovchain.\\nThereisalsoamoregeneralMarkovchainthatcansamplefromanydenoising\\nautoencoder.\\n20.11.1 Markov Chain Associated with any Denoising Autoen-\\ncoder\\nTheabovediscussion left openthe question of what noiseto inject and where,\\nin order to obtain a Markov chain that would generate from the distribution\\nestimated bythe autoencoder. ( ) showed how to construct Bengio et al.2013c\\nsuch a Markov chainfor generalizeddenoisingautoencoders . Generalized\\ndenoisingautoencodersarespeciﬁedbyadenoisingdistributionforsamplingan\\nestimateofthecleaninputgiventhecorruptedinput.\\nEachstepoftheMarkovchainthatgeneratesfromtheestimateddistribution\\nconsistsofthefollowingsub-steps,illustratedinﬁgure : 20.11\\n1.Startingfromthepreviousstate x,injectcorruptionnoise,sampling ˜ xfrom\\nC(˜ x x|).\\n2. Encode ˜ xinto h= ( f˜ x).\\n3. Decode toobtaintheparameters of h ω h= ( g) p g p( = x | ω ( )) = h (x|˜ x).\\n4. Samplethenextstate from x p g p( = x| ω ( )) = h (x|˜ x).\\n711', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='386e6dd2-df52-49d8-a1cb-51b6b4b97d35', embedding=None, metadata={'page_label': '727', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nxx˜ x˜ xh h\\nωω\\nˆ x ˆ xC ( ˜ x x| ) p ( ) x| ωfg\\nFigure20.11:EachstepoftheMarkovchainassociatedwithatraineddenoisingautoen-\\ncoder,thatgeneratesthesamplesfromtheprobabilisticmodelimplicitlytrainedbythe\\ndenoisinglog-likelihoodcriterion.Eachstepconsistsin(a)injectingnoiseviacorruption\\nprocess Cinstate x,yielding˜ x,(b)encodingitwithfunction f,yielding h= f(˜ x),\\n(c)decodingtheresultwithfunction g,yieldingparameters ωforthereconstruction\\ndistribution,and(d)given ω,samplinganewstatefromthereconstructiondistribution\\np(x | ω= g( f(˜ x))).Inthetypicalsquaredreconstructionerrorcase, g( h)=ˆ x,which\\nestimates E[ x|˜ x],corruptionconsistsinaddingGaussiannoiseandsamplingfrom\\np(x| ω)consistsinaddingGaussiannoise,asecondtime,tothereconstructionˆ x.The\\nlatternoiselevelshouldcorrespondtothemeansquarederrorofreconstructions,whereas\\ntheinjectednoiseisahyperparameterthatcontrolsthemixingspeedaswellasthe\\nextenttowhichtheestimatorsmoothstheempiricaldistribution(,).Inthe Vincent2011\\nexampleillustratedhere,onlythe Cand pconditionalsarestochasticsteps( fand gare\\ndeterministiccomputations),althoughnoisecanalsobeinjectedinsidetheautoencoder,\\nasingenerativestochasticnetworks( ,). Bengio e t a l .2014\\n7 1 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d7e8292-e0e7-4467-8902-68daff722cc5', embedding=None, metadata={'page_label': '728', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nBengio2014 e t a l .()showedthatiftheautoencoder p(x |˜x)formsaconsistent\\nestimatorofthecorrespondingtrueconditionaldistribution,thenthestationary\\ndistributionoftheaboveMarkovchainformsaconsistentestimator(albeitan\\nimplicitone)ofthedatageneratingdistributionof.x\\n20.11.2ClampingandConditionalSampling\\nSimilarlytoBoltzmannmachines,denoisingautoencodersandtheirgeneralizations\\n(suchasGSNs,describedbelow)canbeusedtosamplefromaconditionaldistri-\\nbution p(x f|x o),simplybyclampingthe o b s e r v e dunits x fandonlyresampling\\nthe f r e eunits x ogivenx fandthesampledlatentvariables(ifany).Forexample,\\nMP-DBMscanbeinterpretedasaformofdenoisingautoencoder,andareable\\ntosamplemissinginputs.GSNslatergeneralizedsomeoftheideaspresentin\\nMP-DBMstoperformthesameoperation( ,). () Bengio e t a l .2014Alain e t a l .2015\\nidentiﬁedamissingconditionfromProposition1of (),whichis Bengio e t a l .2014\\nthatthetransitionoperator(deﬁnedbythestochasticmappinggoingfromone\\nstateofthechaintothenext)shouldsatisfyapropertycalleddetailedbalance,\\nwhichspeciﬁesthataMarkovChainatequilibriumwillremaininequilibrium\\nwhetherthetransitionoperatorisruninforwardorreverse.\\nAnexperimentinclampinghalfofthepixels(therightpartoftheimage)and\\nrunningtheMarkovchainontheotherhalfisshowninﬁgure.20.12\\n7 1 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23a8b4f7-b4c8-42e9-affb-9deab3ccdb3f', embedding=None, metadata={'page_label': '729', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nFigure20.12:IllustrationofclampingtherighthalfoftheimageandrunningtheMarkov\\nChainbyresamplingonlythelefthalfateachstep.\\xa0ThesesamplescomefromaGSN\\ntrainedtoreconstructMNISTdigitsateachtimestepusingthewalkbackprocedure.\\n20.11.3Walk-BackTrainingProcedure\\nThewalk-backtrainingprocedurewasproposedby ()asaway Bengio e t a l .2013c\\ntoacceleratetheconvergenceofgenerativetrainingofdenoisingautoencoders.\\nInsteadofperformingaone-stepencode-decodereconstruction,thisprocedure\\nconsistsinalternativemultiplestochasticencode-decodesteps(asinthegenerative\\nMarkovchain)initializedatatrainingexample(justlikewiththecontrastive\\ndivergencealgorithm,describedinsection)andpenalizingthelastprobabilistic 18.2\\nreconstructions(orallofthereconstructionsalongtheway).\\nTrainingwith kstepsisequivalent(inthesenseofachievingthesamestationary\\ndistribution)astrainingwithonestep,butpracticallyhastheadvantagethat\\nspuriousmodesfurtherfromthedatacanberemovedmoreeﬃciently.\\n20.12GenerativeStochasticNetworks\\nGenerativestochasticnetworksorGSNs( ,)aregeneraliza- Bengio e t a l .2014\\ntionsofdenoisingautoencodersthatincludelatentvariables hinthegenerative\\n7 1 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='608c7c7c-18b1-448e-9a41-3692c8c6bcb3', embedding=None, metadata={'page_label': '730', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20. DEEPGENERATIVEMODELS\\nMarkovchain,inadditiontothevisiblevariables(usuallydenoted ). x\\nA GSN is parametrized by two conditional probability distributions which\\nspecifyonestepoftheMarkovchain:\\n1. p(x( ) k|h( ) k)tellshowtogeneratethenextvisiblevariablegiventhecurrent\\nlatentstate. Sucha“reconstructiondistribution” isalsofoundindenoising\\nautoencoders,RBMs,DBNsandDBMs.\\n2. p(h( ) k|h( 1) k−,x( 1) k−)tellshowtoupdatethelatentstatevariable,given\\nthepreviouslatentstateandvisiblevariable.\\nDenoisingautoencodersandGSNsdiﬀer fromclassicalprobabilisticmodels\\n(directedorundirected)inthattheyparametrizethegenerativeprocessitselfrather\\nthanthemathematicalspeciﬁcationofthejointdistributionofvisibleandlatent\\nvariables. Instead, thelatter is deﬁned , , asthe stationary implicitly if it exists\\ndistributionofthegenerativeMarkovchain. Theconditionsforexistenceofthe\\nstationarydistributionaremildandarethesameconditionsrequiredbystandard\\nMCMCmethods(seesection ). Theseconditionsarenecessarytoguarantee 17.3\\nthatthechainmixes,buttheycanbeviolatedbysomechoicesofthetransition\\ndistributions(forexample,iftheyweredeterministic).\\nOnecouldimaginediﬀerenttrainingcriteriaforGSNs. Theoneproposedand\\nevaluatedby ( )issimplyreconstructionlog-probabilityonthe Bengio et al.2014\\nvisibleunits,justlikefordenoisingautoencoders. Thisisachievedbyclamping\\nx(0)= xtotheobservedexampleandmaximizingtheprobabilityofgenerating x\\natsomesubsequenttimesteps,i.e.,maximizing log p(x( ) k= x|h( ) k),where h( ) k\\nissampledfromthechain,given x(0)= x.\\xa0Inordertoestimatethegradientof\\nlog p(x( ) k= x|h( ) k)withrespecttotheotherpiecesofthemodel, Bengio et al.\\n( )usethereparametrizationtrick,introducedinsection .2014 20.9\\nThewalk-backtraining protocol(describedinsection )wasused( 20.11.3 Ben-\\ngio 2014 et al., )toimprovetrainingconvergenceofGSNs.\\n20.12.1 Discriminant GSNs\\nTheoriginalformulationofGSNs( , )wasmeantforunsupervisedBengio et al.2014\\nlearningandimplicitlymodeling p(x)forobserveddata x,butitispossibleto\\nmodifytheframeworktooptimize . p( )y| x\\nForexample, ZhouandTroyanskaya 2014 ( )generalizeGSNsinthisway,by\\nonlyback-propagatingthereconstructionlog-probabilityovertheoutputvariables,\\nkeepingtheinputvariablesﬁxed. Theyappliedthissuccessfullytomodelsequences\\n715', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f5f8ec9c-0939-4019-954c-4bfa87aa211c', embedding=None, metadata={'page_label': '731', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\n(proteinsecondarystructure)andintroduceda(one-dimensional) convolutional\\nstructureinthetransitionoperatoroftheMarkovchain.Itisimportantto\\nrememberthat,foreachstepoftheMarkovchain,onegeneratesanewsequence\\nforeachlayer,andthatsequenceistheinputforcomputingotherlayervalues(say\\ntheonebelowandtheoneabove)atthenexttimestep.\\nHencetheMarkovchainisreallyovertheoutputvariable(andassociatedhigher-\\nlevelhiddenlayers),andtheinputsequenceonlyservestoconditionthatchain,\\nwithback-propagationallowingtolearnhowtheinputsequencecanconditionthe\\noutputdistributionimplicitlyrepresentedbytheMarkovchain.Itisthereforea\\ncaseofusingtheGSNinthecontextofstructuredoutputs.\\nZöhrerandPernkopf2014()introducedahybridmodelthatcombinesasuper-\\nvisedobjective(asintheabovework)andanunsupervisedobjective(asinthe\\noriginalGSNwork),bysimplyadding(withadiﬀerentweight)thesupervisedand\\nunsupervisedcostsi.e.,thereconstructionlog-probabilities ofyandxrespectively.\\nSuchahybridcriterionhadpreviouslybeenintroducedforRBMsbyLarochelle\\nandBengio2008().Theyshowimprovedclassiﬁcationperformanceusingthis\\nscheme.\\n20.13OtherGenerationSchemes\\nThemethodswehavedescribedsofaruseeitherMCMCsampling,ancestral\\nsampling,orsomemixtureofthetwotogeneratesamples.\\xa0Whilethesearethe\\nmostpopularapproachestogenerativemodeling,theyarebynomeanstheonly\\napproaches.\\nSohl-Dickstein2015 e t a l .()developedadiﬀusioninversiontrainingscheme\\nforlearningagenerativemodel,basedonnon-equilibrium thermodynamics.The\\napproachisbasedontheideathattheprobabilitydistributionswewishtosample\\nfromhavestructure.Thisstructurecangraduallybedestroyedbyadiﬀusion\\nprocessthatincrementally changestheprobabilitydistributiontohave\\xa0more\\nentropy.Toformagenerativemodel,wecanruntheprocessinreverse,bytraining\\namodelthatgraduallyrestoresthestructuretoanunstructureddistribution.\\nByiterativelyapplyingaprocessthatbringsadistributionclosertothetarget\\none,wecangraduallyapproachthattargetdistribution.Thisapproachresembles\\nMCMCmethodsinthesensethatitinvolvesmanyiterationstoproduceasample.\\nHowever,themodelisdeﬁnedtobetheprobabilitydistributionproducedby\\ntheﬁnalstepofthechain.\\xa0Inthissense,thereisnoapproximation inducedby\\ntheiterativeprocedure.Theapproachintroducedby () Sohl-Dickstein e t a l .2015\\nisalsoveryclosetothegenerativeinterpretation ofthedenoisingautoencoder\\n7 1 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c253d730-bb0f-4cb0-9d29-38d5b0f4bc64', embedding=None, metadata={'page_label': '732', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\n(section).Aswiththedenoisingautoencoder,diﬀusioninversiontrainsa 20.11.1\\ntransitionoperatorthatattemptstoprobabilisticallyundotheeﬀectofadding\\nsomenoise.Thediﬀerenceisthatdiﬀusioninversionrequresundoingonlyonestep\\nofthediﬀusionprocess,ratherthantravelingallthewaybacktoacleandatapoint.\\nThisaddressesthefollowingdilemmapresentwiththeordinaryreconstruction\\nlog-likelihoodobjectiveofdenoisingautoencoders:withsmalllevelsofnoisethe\\nlearneronlyseesconﬁgurations nearthedatapoints,whilewithlargelevelsof\\nnoiseitisaskedtodoanalmostimpossiblejob(becausethedenoisingdistribution\\nishighlycomplexandmulti-modal). Withthediﬀusioninversionobjective,the\\nlearnercanlearntheshapeofthedensityaroundthedatapointsmoreprecisely\\naswellasremovespuriousmodesthatcouldshowupfarfromthedatapoints.\\nAnotherapproachtosamplegenerationistheapproximateBayesiancom-\\nputation(ABC)framework(,).Inthisapproach,samplesare Rubin e t a l .1984\\nrejectedormodiﬁedinordertomakethemomentsofselectedfunctionsofthe\\nsamplesmatchthoseofthedesireddistribution.Whilethisideausesthemoments\\nofthesampleslikeinmomentmatching,itisdiﬀerentfrommomentmatching\\nbecauseitmodiﬁesthesamplesthemselves,ratherthantrainingthemodelto\\nautomatically emitsampleswiththecorrectmoments. () BachmanandPrecup2015\\nshowedhowtouseideasfromABCinthecontextofdeeplearning,byusingABC\\ntoshapetheMCMCtrajectoriesofGSNs.\\nWeexpectthatmanyotherpossibleapproachestogenerativemodelingawait\\ndiscovery.\\n20.14EvaluatingGenerativeModels\\nResearchersstudyinggenerativemodelsoftenneedtocompareonegenerative\\nmodeltoanother,usuallyinordertodemonstratethatanewlyinventedgenerative\\nmodelisbetteratcapturingsomedistributionthanthepre-existingmodels.\\nThiscanbeadiﬃcultandsubtletask.Inmanycases,wecannotactually\\nevaluatethelogprobabilityofthedataunderthemodel,butonlyanapproximation.\\nInthesecases,itisimportanttothinkandcommunicateclearlyaboutexactlywhat\\nisbeingmeasured.Forexample,supposewecanevaluateastochasticestimateof\\nthelog-likelihoodformodelA,andadeterministiclowerboundonthelog-likelihood\\nformodelB.IfmodelAgetsahigherscorethanmodelB,whichisbetter?Ifwe\\ncareaboutdeterminingwhichmodelhasabetterinternalrepresentationofthe\\ndistribution,weactuallycannottell,unlesswehavesomewayofdetermininghow\\nloosetheboundformodelBis.However,ifwecareabouthowwellwecanuse\\nthemodelinpractice,forexampletoperformanomalydetection,thenitisfairto\\n7 1 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5c6a04f0-f082-4bff-b274-87a51af11a20', embedding=None, metadata={'page_label': '733', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nsaythatamodelispreferablebasedonacriterionspeciﬁctothepracticaltaskof\\ninterest,e.g.,basedonrankingtestexamplesandrankingcriteriasuchasprecision\\nandrecall.\\nAnothersubtletyofevaluatinggenerativemodelsisthattheevaluationmetrics\\nareoftenhardresearchproblemsinandofthemselves.Itcanbeverydiﬃcult\\ntoestablishthatmodelsarebeingcomparedfairly.Forexample,supposeweuse\\nAIStoestimate log Zinordertocompute log ˜ p( x)−log Zforanewmodelwe\\nhavejustinvented.Acomputationally economicalimplementation ofAISmayfail\\ntoﬁndseveralmodesofthemodeldistributionandunderestimate Z,whichwill\\nresultinusoverestimatinglog p( x).Itcanthusbediﬃculttotellwhetherahigh\\nlikelihoodestimateisduetoagoodmodelorabadAISimplementation.\\nOtherﬁeldsofmachinelearningusuallyallowforsomevariationinthepre-\\nprocessingofthedata.Forexample,whencomparingtheaccuracyofobject\\nrecognitionalgorithms,itisusuallyacceptabletopreprocesstheinputimages\\nslightlydiﬀerentlyforeachalgorithmbasedonwhatkindofinputrequirements\\nithas.Generativemodelingisdiﬀerentbecausechangesinpreprocessing,even\\nverysmallandsubtleones,arecompletelyunacceptable. Anychangetotheinput\\ndatachangesthedistributiontobecapturedandfundamentallyaltersthetask.\\nForexample,multiplyingtheinputby0.1willartiﬁciallyincreaselikelihoodbya\\nfactorof10.\\nIssueswithpreprocessingcommonlyarisewhenbenchmarkinggenerativemodels\\nontheMNISTdataset,oneofthemorepopulargenerativemodelingbenchmarks.\\nMNISTconsistsofgrayscaleimages.SomemodelstreatMNISTimagesaspoints\\ninarealvectorspace,whileotherstreatthemasbinary.Yetotherstreatthe\\ngrayscalevaluesasprobabilities forabinarysamples.Itisessentialtocompare\\nreal-valuedmodelsonlytootherreal-valuedmodelsandbinary-valuedmodelsonly\\ntootherbinary-valuedmodels.\\xa0Otherwisethelikelihoodsmeasuredarenotonthe\\nsamespace.Forbinary-valuedmodels,thelog-likelihoodcanbeatmostzero,while\\nforreal-valuedmodelsitcanbearbitrarilyhigh,sinceitisthemeasurementofa\\ndensity.Amongbinarymodels,itisimportanttocomparemodelsusingexactly\\nthesamekindofbinarization. Forexample,wemightbinarizeagraypixelto0or1\\nbythresholdingat0.5,orbydrawingarandomsamplewhoseprobabilityofbeing\\n1isgivenbythegraypixelintensity.Ifweusetherandombinarization, wemight\\nbinarizethewholedatasetonce,orwemightdrawadiﬀerentrandomexamplefor\\neachstepoftrainingandthendrawmultiplesamplesforevaluation.Eachofthese\\nthreeschemesyieldswildlydiﬀerentlikelihoodnumbers,andwhencomparing\\ndiﬀerentmodelsitisimportantthatbothmodelsusethesamebinarizationscheme\\nfortrainingandforevaluation.Infact,researcherswhoapplyasinglerandom\\n7 1 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='05f67f41-b072-4d8e-a532-a788757139cd', embedding=None, metadata={'page_label': '734', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nbinarizationstepshareaﬁlecontainingtheresultsoftherandombinarization, so\\nthatthereisnodiﬀerenceinresultsbasedondiﬀerentoutcomesofthebinarization\\nstep.\\nBecausebeingabletogeneraterealisticsamplesfromthedatadistribution\\nisoneofthegoalsofagenerativemodel,practitioners oftenevaluategenerative\\nmodelsbyvisuallyinspectingthesamples.Inthebestcase,thisisdonenotbythe\\nresearchersthemselves,butbyexperimentalsubjectswhodonotknowthesource\\nofthesamples(Denton2015 e t a l .,).Unfortunately,itispossibleforaverypoor\\nprobabilisticmodeltoproduceverygoodsamples.Acommonpracticetoverifyif\\nthemodelonlycopiessomeofthetrainingexamplesisillustratedinﬁgure.16.1\\nTheideaistoshowforsomeofthegeneratedsamplestheirnearestneighborin\\nthetrainingset,accordingtoEuclideandistanceinthespaceof x.\\xa0Thistestis\\nintendedtodetectthecasewherethemodeloverﬁtsthetrainingsetandjust\\nreproducestraininginstances.Itisevenpossibletosimultaneouslyunderﬁtand\\noverﬁtyetstillproducesamplesthatindividuallylookgood.Imagineagenerative\\nmodeltrainedonimagesofdogsandcatsthatsimplylearnstoreproducethe\\ntrainingimagesofdogs.Suchamodelhasclearlyoverﬁt,becauseitdoesnot\\nproducesimagesthatwerenotinthetrainingset,butithasalsounderﬁt,because\\nitassignsnoprobabilitytothetrainingimagesofcats.Yetahumanobserver\\nwouldjudgeeachindividualimageofadogtobehighquality.Inthissimple\\nexample,itwouldbeeasyforahumanobserverwhocaninspectmanysamplesto\\ndeterminethatthecatsareabsent.Inmorerealisticsettings,agenerativemodel\\ntrainedondatawithtensofthousandsofmodesmayignoreasmallnumberof\\nmodes,andahumanobserverwouldnoteasilybeabletoinspectorremember\\nenoughimagestodetectthemissingvariation.\\nSince\\xa0thevisual\\xa0quality\\xa0ofsamples\\xa0is\\xa0not\\xa0areliable\\xa0guide,\\xa0we\\xa0often also\\nevaluatethelog-likelihoodthatthemodelassignstothetestdata,whenthisis\\ncomputationally feasible.Unfortunately,insomecasesthelikelihoodseemsnot\\ntomeasureanyattributeofthemodelthatwereallycareabout.Forexample,\\nreal-valuedmodelsofMNISTcanobtainarbitrarilyhighlikelihoodbyassigning\\narbitrarilylowvariancetobackgroundpixelsthatneverchange.Modelsand\\nalgorithmsthatdetecttheseconstantfeaturescanreapunlimitedrewards,even\\nthoughthisisnotaveryusefulthingtodo.Thepotentialtoachieveacost\\napproaching\\xa0negativeinﬁnityis\\xa0present\\xa0foranykind\\xa0of\\xa0maximum\\xa0likelihood\\nproblemwithrealvalues,butitisespeciallyproblematicforgenerativemodelsof\\nMNISTbecausesomanyoftheoutputvaluesaretrivialtopredict.Thisstrongly\\nsuggestsaneedfordevelopingotherwaysofevaluatinggenerativemodels.\\nTheis2015 e t a l .()reviewmanyoftheissuesinvolvedinevaluatinggenerative\\n7 1 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='06c8bbcb-28d9-4b01-95da-aca995987425', embedding=None, metadata={'page_label': '735', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='CHAPTER20.DEEPGENERATIVEMODELS\\nmodels,includingmanyoftheideasdescribedabove.Theyhighlightthefact\\nthattherearemanydiﬀerentusesofgenerativemodelsandthatthechoiceof\\nmetricmustmatchtheintendeduseofthemodel.Forexample,somegenerative\\nmodelsarebetteratassigninghighprobabilitytomostrealisticpointswhileother\\ngenerativemodelsarebetteratrarelyassigninghighprobabilitytounrealistic\\npoints.Thesediﬀerencescanresultfromwhetheragenerativemodelisdesigned\\ntominimize D K L( p da t a\\ue06b p m o de l)or D K L( p m o de l\\ue06b p da t a),asillustratedinﬁgure.3.6\\nUnfortunately,evenwhenwerestricttheuseofeachmetrictothetaskitismost\\nsuitedfor,allofthemetricscurrentlyinusecontinuetohaveseriousweaknesses.\\nOneofthemostimportantresearchtopicsingenerativemodelingisthereforenot\\njusthowtoimprovegenerativemodels,butinfact,designingnewtechniquesto\\nmeasureourprogress.\\n20.15Conclusion\\nTraininggenerativemodelswithhiddenunitsisapowerfulwaytomakemodels\\nunderstandtheworldrepresentedinthegiventrainingdata.Bylearningamodel\\np m o de l( x)andarepresentation p m o de l( h x|),agenerativemodelcanprovide\\nanswerstomanyinferenceproblemsabouttherelationshipsbetweeninputvariables\\nin xandcanprovidemanydiﬀerentwaysofrepresenting xbytakingexpectations\\nof hatdiﬀerentlayersofthehierarchy.\\xa0Generativemodelsholdthepromiseto\\nprovideAIsystemswithaframeworkforallofthemanydiﬀerentintuitiveconcepts\\ntheyneedtounderstand,andtheabilitytoreasonabouttheseconceptsinthe\\nfaceofuncertainty.Wehopethatourreaderswillﬁndnewwaystomakethese\\napproachesmorepowerfulandcontinuethejourneytounderstandingtheprinciples\\nthatunderlielearningandintelligence.\\n7 2 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='32a33c0b-0de7-4b40-b4a8-51c5d0799c76', embedding=None, metadata={'page_label': '736', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Bibliograp hy\\nAbadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis,\\nA.,Dean,J.,Devin,M.,Ghemawat,S.,Goodfellow,I.,Harp,A.,Irving,G.,Isard,M.,\\nJia,Y.,Jozefowicz,R.,Kaiser,L.,Kudlur,M.,Levenberg,J.,Mané,D.,Monga,R.,\\nMoore,S.,Murray,D.,Olah,C.,Schuster,M.,Shlens,J.,Steiner,B.,Sutskever,I.,\\nTalwar,K.,Tucker,P.,Vanhoucke,V.,Vasudevan,V.,Viégas,F.,Vinyals,O.,Warden,\\nP.,Wattenberg,M.,Wicke,M.,Yu,Y.,andZheng,X.(2015).TensorFlow:Large-scale\\nmachinelearningonheterogeneoussystems.Softwareavailablefromtensorﬂow.org.,25\\n214446,\\nAckley,D.H.,Hinton,G.E.,andSejnowski,T.J.(1985).Alearningalgorithmfor\\nBoltzmannmachines.CognitiveScience,,147–169. , 9 570654\\nAlain,G.andBengio,Y.(2013).\\xa0Whatregularizedauto-encoderslearnfromthedata\\ngeneratingdistribution.In .,,, ICLR’2013,arXiv:1211.4246507513514521\\nAlain,G.,Bengio,Y.,Yao,L.,ÉricThibodeau-Laufer,Yosinski,J.,andVincent,P.(2015).\\nGSNs:Generativestochasticnetworks.arXiv:1503.05571.,510713\\nAnderson,E.(1935).TheIrisesoftheGaspéPeninsula.BulletinoftheAmericanIris\\nSociety,,2–5. 5 921\\nBa,J.,Mnih,V.,andKavukcuoglu,K.(2014).Multipleobjectrecognitionwithvisual\\nattention. . arXiv:1412.7755691\\nBachman,P.andPrecup,D.(2015).Variationalgenerativestochasticnetworkswith\\ncollaborativeshaping.InProceedingsofthe32ndInternationalConferenceonMachine\\nLearning,ICML2015,Lille,France,6-11July2015,pages1964–1972. 717\\nBacon,P.-L.,Bengio,E.,Pineau,J.,andPrecup,D.(2015).Conditionalcomputationin\\nneuralnetworksusingadecision-theoreticapproach.In2ndMultidisciplinaryConference\\nonReinforcementLearningandDecisionMaking(RLDM2015).450\\nBagnell,J.A.andBradley,D.M.(2009).Diﬀerentiablesparsecoding.InD.Koller,\\nD.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformation\\nProcessingSystems21(NIPS’08),pages113–120.498\\n721', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ffae6882-81e0-4e3d-88af-3570d617c83b', embedding=None, metadata={'page_label': '737', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nBahdanau,D.,Cho,K.,andBengio,Y.(2015).Neuralmachinetranslationbyjointly\\nlearningtoalignandtranslate.In .,,,,, ICLR’2015,arXiv:1409.047325101397418420\\n465475476,,\\nBahl,L.R.,Brown,P.,deSouza,P.V.,andMercer,R.L.(1987).\\xa0Speechrecognition\\nwithcontinuous-parameterhiddenMarkovmodels.Computer,SpeechandLanguage, 2,\\n219–234.458\\nBaldi,P.andHornik,K.(1989).Neuralnetworksandprincipalcomponentanalysis:\\nLearningfromexampleswithoutlocalminima.NeuralNetworks,,53–58. 2286\\nBaldi,P.,Brunak,S.,Frasconi,P.,Soda,G.,andPollastri,G.(1999).Exploitingthe\\npastandthefutureinproteinsecondarystructureprediction. , Bioinformatics 1 5(11),\\n937–946.395\\nBaldi,\\xa0P.,\\xa0Sadowski,\\xa0P.,\\xa0andWhiteson,\\xa0D.(2014).Searchingforexoticparticlesin\\nhigh-energyphysicswithdeeplearning.Naturecommunications,. 526\\nBallard,D.H.,Hinton,G.E.,andSejnowski,T.J.(1983).Parallelvisioncomputation.\\nNature.452\\nBarlow,H.B.(1989).Unsupervisedlearning.NeuralComputation,,295–311. 1 147\\nBarron,A.E.(1993).Universalapproximationboundsforsuperpositionsofasigmoidal\\nfunction.IEEETrans.onInformationTheory,,930–945. 3 9 199\\nBartholomew,D.J.(1987).Latentvariablemodelsandfactoranalysis.OxfordUniversity\\nPress.490\\nBasilevsky,A.(1994).StatisticalFactorAnalysisandRelatedMethods:Theoryand\\nApplications.Wiley.490\\nBastien,F.,Lamblin,P.,\\xa0Pascanu,R.,Bergstra,J.,\\xa0Goodfellow,I.J.,Bergeron,A.,\\nBouchard,N.,andBengio,Y.(2012).Theano:newfeaturesandspeedimprovements.\\nDeepLearningandUnsupervisedFeatureLearningNIPS2012Workshop.,,,2582214\\n222446,\\nBasu,S.andChristensen,J.(2013).\\xa0Teachingclassiﬁcationboundariestohumans.\\xa0In\\nAAAI’2013.329\\nBaxter,J.(1995).Learninginternalrepresentations.InProceedingsofthe8thInternational\\nConferenceonComputationalLearningTheory(COLT’95),pages311–320,SantaCruz,\\nCalifornia.ACMPress.245\\nBayer,J.andOsendorfer,C.(2014).Learningstochasticrecurrentnetworks.ArXiv\\ne-prints.265\\nBecker,S.andHinton,G.(1992).Aself-organizingneuralnetworkthatdiscoverssurfaces\\ninrandom-dotstereograms.Nature,,161–163. 3 5 5 541\\n7 2 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d371dd91-63f7-4845-b098-a2a90f2f172b', embedding=None, metadata={'page_label': '738', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nBehnke,S.(2001).Learningiterativeimagereconstructionintheneuralabstraction\\npyramid.Int.J.ComputationalIntelligenceandApplications,(4),427–438. 1 515\\nBeiu,V.,Quintana,J.M.,andAvedillo,M.J.(2003).VLSIimplementationsofthreshold\\nlogic-acomprehensivesurvey.NeuralNetworks,IEEETransactionson, 1 4(5),1217–\\n1243.451\\nBelkin,\\xa0M.and\\xa0Niyogi,\\xa0P.(2002).Laplacianeigenmapsandspectraltechniquesfor\\nembeddingandclustering.\\xa0InT.Dietterich,S.Becker,andZ.Ghahramani,editors,\\nAdvancesinNeuralInformationProcessingSystems14(NIPS’01),Cambridge,MA.\\nMITPress.244\\nBelkin,M.andNiyogi,P.(2003).Laplacianeigenmapsfordimensionalityreductionand\\ndatarepresentation.NeuralComputation,(6),1373–1396. , 1 5 164518\\nBengio,E.,Bacon,P.-L.,Pineau,J.,andPrecup,D.(2015a).Conditionalcomputationin\\nneuralnetworksforfastermodels.arXiv:1511.06297.450\\nBengio,\\xa0S.\\xa0andBengio,\\xa0Y.\\xa0(2000a).Taking\\xa0onthecurseofdimensionalityinjoint\\ndistributionsusingneuralnetworks.IEEETransactionsonNeuralNetworks,special\\nissueonDataMiningandKnowledgeDiscovery,(3),550–557. 1 1 707\\nBengio,S.,Vinyals,O.,Jaitly,N.,andShazeer,N.(2015b).Scheduledsamplingfor\\nsequencepredictionwithrecurrentneuralnetworks.Technicalreport,arXiv:1506.03099.\\n384\\nBengio,Y.(1991).ArtiﬁcialNeuralNetworksandtheirApplicationtoSequenceRecognition.\\nPh.D.thesis,McGillUniversity,(ComputerScience),Montreal,Canada.407\\nBengio,Y.(2000).Gradient-basedoptimizationofhyperparameters.NeuralComputation,\\n1 2(8),1889–1900. 435\\nBengio,Y.(2002).Newdistributedprobabilisticlanguagemodels.TechnicalReport1215,\\nDept.IRO,UniversitédeMontréal.467\\nBengio,Y.(2009).LearningdeeparchitecturesforAI.NowPublishers.,201622\\nBengio,Y.(2013).Deeplearningofrepresentations:lookingforward.InStatistical\\nLanguageandSpeechProcessing,volume7978ofLectureNotesinComputerScience,\\npages1–37.Springer,alsoinarXivathttp://arxiv.org/abs/1305.0445.448\\nBengio,Y.(2015).Earlyinferenceinenergy-basedmodelsapproximatesback-propagation.\\nTechnicalReportarXiv:1510.02777,UniversitedeMontreal.656\\nBengio,Y.andBengio,S.(2000b).Modelinghigh-dimensionaldiscretedatawithmulti-\\nlayerneuralnetworks.In,pages400–406.MITPress.,,, NIPS12 705707708710\\nBengio,Y.andDelalleau,O.(2009).Justifyingandgeneralizingcontrastivedivergence.\\nNeuralComputation,(6),1601–1621. , 2 1 513611\\n7 2 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2f976f27-4c51-46eb-bbdd-69f325410bb0', embedding=None, metadata={'page_label': '739', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nBengio,Y.andGrandvalet,Y.(2004).Nounbiasedestimatorofthevarianceofk-fold\\ncross-validation.InS.Thrun,L.Saul,andB.Schölkopf,editors,AdvancesinNeural\\nInformationProcessingSystems16(NIPS’03),Cambridge,MA.MITPress,Cambridge.\\n122\\nBengio,Y.andLeCun,Y.(2007).ScalinglearningalgorithmstowardsAI.InLargeScale\\nKernelMachines.19\\nBengio,Y.andMonperrus,M.(2005).Non-localmanifoldtangentlearning.InL.Saul,\\nY.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems\\n17(NIPS’04),pages129–136.MITPress.,160519\\nBengio,Y.andSénécal,J.-S.(2003).Quicktrainingofprobabilisticneuralnetsby\\nimportancesampling.InProceedingsofAISTATS2003.470\\nBengio,Y.andSénécal,J.-S.(2008).Adaptiveimportancesamplingtoacceleratetraining\\nofaneuralprobabilisticlanguagemodel.IEEETrans.NeuralNetworks, 1 9(4),713–722.\\n470\\nBengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1991).Phoneticallymotivated\\nacousticparametersforcontinuousspeechrecognitionusingartiﬁcialneuralnetworks.\\nInProceedingsofEuroSpeech’91.,27459\\nBengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1992).Neuralnetwork-Gaussian\\nmixturehybridforspeechrecognitionordensityestimation.In,pages175–182. NIPS4\\nMorganKaufmann.459\\nBengio,Y.,Frasconi,P.,andSimard,P.(1993).Theproblemoflearninglong-term\\ndependenciesinrecurrentnetworks.InIEEEInternationalConferenceonNeural\\nNetworks,pages1183–1195, SanFrancisco.IEEEPress.(invitedpaper).403\\nBengio,Y.,Simard,P.,andFrasconi,P.(1994).Learninglong-termdependencieswith\\ngradientdescentisdiﬃcult.IEEETr.NeuralNets.,,, 18401403411\\nBengio,Y.,Latendresse,S.,andDugas,C.(1999).Gradient-basedlearningofhyper-\\nparameters.LearningConference,Snowbird.435\\nBengio,Y.,Ducharme,R.,andVincent,P.(2001).Aneuralprobabilisticlanguagemodel.\\nInT.K.Leen,T.G.Dietterich,andV.Tresp,editors, ,pages932–938.MIT NIPS’2000\\nPress.,,,,,, 18447464466472477482\\nBengio,Y.,Ducharme,R.,Vincent,P.,andJauvin,C.(2003).Aneuralprobabilistic\\nlanguagemodel.,,1137–1155. , JMLR 3 466472\\nBengio,Y.,LeRoux,N.,Vincent,P.,Delalleau,O.,andMarcotte,P.(2006a).Convex\\nneuralnetworks.In ,pages123–130. NIPS’2005 258\\nBengio,Y.,Delalleau,O.,andLeRoux,N.(2006b).Thecurseofhighlyvariablefunctions\\nforlocalkernelmachines.In . NIPS’2005158\\n7 2 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7e81473a-322b-4647-b3a3-cf904f18bb9a', embedding=None, metadata={'page_label': '740', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nBengio,Y.,Larochelle,H.,andVincent,P.(2006c).Non-localmanifoldParzenwindows.\\nIn .MITPress., NIPS’2005 160520\\nBengio,Y.,Lamblin,P.,Popovici,D.,andLarochelle,H.(2007).Greedylayer-wise\\ntrainingofdeepnetworks.In .,,,,,, NIPS’20061419201323324528530\\nBengio,Y.,Louradour,J.,Collobert,R.,andWeston,J.(2009).Curriculumlearning.In\\nICML’09.328\\nBengio,Y.,Mesnil,G.,Dauphin,Y.,andRifai,S.(2013a).Bettermixingviadeep\\nrepresentations.In . ICML’2013604\\nBengio,Y.,Léonard,N.,andCourville,A.(2013b).Estimatingorpropagatinggradients\\nthroughstochasticneuronsforconditionalcomputation.\\xa0arXiv:1308.3432.,,448450\\n689691,\\nBengio,Y.,Yao,L.,Alain,G.,andVincent,P.(2013c).Generalizeddenoisingauto-\\nencodersasgenerativemodels.In .,, NIPS’2013507711714\\nBengio,Y.,Courville,A.,andVincent,P.(2013d).Representationlearning:Areviewand\\nnewperspectives.IEEETrans.PatternAnalysisandMachineIntelligence(PAMI),\\n3 5(8),1798–1828. 555\\nBengio,Y.,Thibodeau-Laufer,E.,Alain,G.,andYosinski,J.(2014).\\xa0Deepgenerative\\nstochasticnetworkstrainablebybackprop.In .,,,, ICML’2014711712713714715\\nBennett,C.(1976).EﬃcientestimationoffreeenergydiﬀerencesfromMonteCarlodata.\\nJournalofComputationalPhysics,(2),245–268. 2 2 628\\nBennett,J.andLanning,S.(2007).TheNetﬂixprize.479\\nBerger,A.L.,DellaPietra,V.J.,andDellaPietra,S.A.(1996).Amaximumentropy\\napproachtonaturallanguageprocessing. ,,39–71. ComputationalLinguistics 2 2473\\nBerglund,M.andRaiko,T.(2013).Stochasticgradientestimatevarianceincontrastive\\ndivergenceandpersistentcontrastivedivergence., . CoRR a b s/ 1 3 1 2 .6 0 0 2614\\nBergstra,\\xa0J.(2011).IncorporatingComplexCellsintoNeuralNetworksfor\\xa0Pattern\\nClassiﬁcation.Ph.D.thesis,UniversitédeMontréal.255\\nBergstra,J.andBengio,Y.(2009).Slow,decorrelatedfeaturesforpretrainingcomplex\\ncell-likenetworks.In . NIPS’2009494\\nBergstra,J.andBengio,Y.(2012).Randomsearchforhyper-parameteroptimization.J.\\nMachineLearningRes.,,281–305. ,, 1 3 433434435\\nBergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pascanu,R.,Desjardins,G.,Turian,\\nJ.,Warde-Farley,D.,andBengio,Y.(2010).Theano:aCPUandGPUmathexpression\\ncompiler.InProc.SciPy.,,,, 2582214222446\\n7 2 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d033db5a-ec9f-4e08-add1-444f35d4e2b2', embedding=None, metadata={'page_label': '741', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nBergstra,J.,Bardenet,R.,Bengio,Y.,andKégl,B.(2011).Algorithmsforhyper-parameter\\noptimization.In . NIPS’2011436\\nBerkes,P.andWiskott,L.(2005).Slowfeatureanalysisyieldsarichrepertoireofcomplex\\ncellproperties. ,(6),579–602. JournalofVision 5 495\\nBertsekas,D.P.andTsitsiklis,J.(1996).Neuro-DynamicProgramming.AthenaScientiﬁc.\\n106\\nBesag,J.(1975).Statisticalanalysisofnon-latticedata. , TheStatistician 2 4(3),179–195.\\n615\\nBishop,C.M.(1994).Mixturedensitynetworks.189\\nBishop,C.M.(1995a).Regularizationandcomplexitycontrolinfeed-forwardnetworks.\\nInProceedingsInternationalConferenceonArtiﬁcialNeuralNetworksICANN’95,\\nvolume1,page141–148. ,242250\\nBishop,C.M.(1995b).TrainingwithnoiseisequivalenttoTikhonovregularization.\\nNeuralComputation,(1),108–116. 7 242\\nBishop,C.M.(2006).PatternRecognitionandMachineLearning.Springer.,98146\\nBlum,A.L.andRivest,R.L.(1992).Traininga3-nodeneuralnetworkisNP-complete.\\n293\\nBlumer,A.,Ehrenfeucht,A.,Haussler,D.,andWarmuth,M.K.(1989).Learnabilityand\\ntheVapnik–Chervonenkisdimension. ,(4),929––865. JournaloftheACM 3 6 114\\nBonnet,G.(1964).Transformationsdessignauxaléatoiresàtraverslessystèmesnon\\nlinéairessansmémoire.AnnalesdesTélécommunications,(9–10),203–220. 1 9 689\\nBordes,\\xa0A.,\\xa0Weston,\\xa0J.,\\xa0Collobert,\\xa0R.,\\xa0andBengio,\\xa0Y.(2011).Learningstructured\\nembeddingsofknowledgebases.In . AAAI2011484\\nBordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2012).Jointlearningofwordsand\\nmeaningrepresentationsforopen-textsemanticparsing.AISTATS’2012.,,401484485\\nBordes,A.,Glorot,X.,Weston,J.,andBengio,Y.(2013a).Asemanticmatchingenergy\\nfunctionforlearningwithmulti-relationaldata.MachineLearning:SpecialIssueon\\nLearningSemantics.483\\nBordes,A.,Usunier,\\xa0N.,Garcia-Duran,A.,Weston,J.,andYakhnenko,O.(2013b).\\nTranslatingembeddingsformodelingmulti-relationaldata.InC.Burges,L.Bottou,\\nM.Welling,Z.Ghahramani,andK.Weinberger,editors,AdvancesinNeuralInformation\\nProcessingSystems26,pages2787–2795. CurranAssociates,Inc.484\\nBornschein,J.andBengio,Y.(2015).Reweightedwake-sleep.InICLR’2015,\\narXiv:1406.2751.693\\n7 2 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5e029936-e19e-4b3a-b716-da30ebea6f96', embedding=None, metadata={'page_label': '742', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nBornschein,J.,Shabanian,S.,Fischer,A.,andBengio,Y.(2015).Trainingbidirectional\\nHelmholtzmachines.Technicalreport,arXiv:1506.03877.693\\nBoser,B.E.,Guyon,I.M.,andVapnik,V.N.(1992).Atrainingalgorithmforopti-\\nmalmarginclassiﬁers.InCOLT’92:Proceedingsoftheﬁfthannualworkshopon\\nComputationallearningtheory,pages144–152,NewYork,NY,USA.ACM.,18141\\nBottou,L.(1998).Onlinealgorithmsandstochasticapproximations.InD.Saad,editor,\\nOnlineLearninginNeuralNetworks.CambridgeUniversityPress,Cambridge,UK.296\\nBottou,\\xa0L.(2011).Frommachinelearning\\xa0tomachinereasoning.Technicalreport,\\narXiv.1102.1808.401\\nBottou,L.(2015).Multilayerneuralnetworks.DeepLearningSummerSchool.440\\nBottou,L.andBousquet,O.(2008).Thetradeoﬀsoflargescalelearning.In . NIPS’2008\\n282295,\\nBoulanger-Lewandowski,N.,Bengio,Y.,andVincent,P.(2012).Modelingtemporal\\ndependenciesinhigh-dimensionalsequences:Applicationtopolyphonicmusicgeneration\\nandtranscription.In ., ICML’12685686\\nBoureau,Y.,Ponce,J.,andLeCun,Y.(2010).Atheoreticalanalysisoffeaturepoolingin\\nvisionalgorithms.InProc.InternationalConferenceonMachinelearning(ICML’10).\\n345\\nBoureau,Y.,LeRoux,N.,Bach,F.,Ponce,J.,andLeCun,Y.(2011).\\xa0Askthelocals:\\nmulti-waylocalpoolingforimagerecognition.InProc.InternationalConferenceon\\nComputerVision(ICCV’11).IEEE.345\\nBourlard,H.andKamp,Y.(1988).Auto-associationbymultilayerperceptronsand\\nsingularvaluedecomposition.BiologicalCybernetics,,291–294. 5 9 502\\nBourlard,H.andWellekens,C.(1989).Speechpatterndiscriminationandmulti-layered\\nperceptrons.ComputerSpeechandLanguage,,1–19. 3459\\nBoyd,S.andVandenberghe,L.(2004). .CambridgeUniversity ConvexOptimization\\nPress,NewYork,NY,USA.93\\nBrady,M.L.,Raghavan,R.,andSlawny,J.(1989).Back-propagationfailstoseparate\\nwhereperceptronssucceed.IEEETransactionsonCircuitsandSystems, 3 6,665–674.\\n284\\nBrakel,P.,Stroobandt,D.,andSchrauwen,B.(2013).Trainingenergy-basedmodelsfor\\ntime-seriesimputation.JournalofMachineLearningResearch, 1 4,2771–2797. ,674\\n698\\nBrand,M.(2003).Chartingamanifold.In ,pages961–968.MITPress., NIPS’2002 164\\n518\\n7 2 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd78ef89-3ad7-42f5-80bf-7bef8024f029', embedding=None, metadata={'page_label': '743', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nBreiman,L.(1994).Baggingpredictors.MachineLearning,(2),123–140. 2 4 256\\nBreiman,L.,Friedman,J.H.,Olshen,R.A.,andStone,C.J.(1984).Classiﬁcationand\\nRegressionTrees.WadsworthInternationalGroup,Belmont,CA.146\\nBridle,J.S.(1990).Alphanets:arecurrent‘neural’networkarchitecturewithahidden\\nMarkovmodelinterpretation.SpeechCommunication,(1),83–92. 9 186\\nBriggman,K.,Denk,W.,Seung,S.,Helmstaedter,M.N.,andTuraga,S.C.(2009).\\nMaximinaﬃnitylearningofimagesegmentation.In ,pages1865–1873. NIPS’2009 360\\nBrown,P.F.,Cocke,J.,Pietra,S.A.D.,Pietra,V.J.D.,Jelinek,F.,Laﬀerty,J.D.,\\nMercer,R.L.,andRoossin,P.S.(1990).Astatisticalapproachtomachinetranslation.\\nComputationallinguistics,(2),79–85. 1 6 21\\nBrown,P.F.,Pietra,V.J.D.,DeSouza,P.V.,Lai,J.C.,andMercer,R.L.(1992).Class-\\nbased-grammodelsofnaturallanguage. , n ComputationalLinguistics 1 8,467–479.\\n463\\nBryson,A.andHo,Y.(1969).\\xa0Appliedoptimalcontrol:\\xa0optimization,estimation,and\\ncontrol.BlaisdellPub.Co.225\\nBryson,Jr.,A.E.andDenham,W.F.(1961).Asteepest-ascentmethodforsolving\\noptimumprogrammingproblems.TechnicalReportBR-1303,RaytheonCompany,\\nMissleandSpaceDivision.225\\nBuciluˇa,\\xa0C.,Caruana,R.,\\xa0and\\xa0Niculescu-Mizil,\\xa0A.\\xa0(2006).Model\\xa0compression.In\\nProceedingsofthe12thACMSIGKDDinternationalconferenceonKnowledgediscovery\\nanddatamining,pages535–541.ACM.448\\nBurda,Y.,Grosse,R.,andSalakhutdinov,R.(2015).Importanceweightedautoencoders.\\narXivpreprintarXiv:1509.00519.698\\nCai,M.,Shi,Y.,andLiu,J.(2013).Deepmaxoutneuralnetworksforspeechrecognition.\\nInAutomaticSpeechRecognitionandUnderstanding(ASRU),2013IEEEWorkshop\\non,pages291–296.IEEE.194\\nCarreira-Perpiñan,M.A.andHinton,G.E.(2005).Oncontrastivedivergencelearning.\\nInR.G.CowellandZ.Ghahramani,editors,ProceedingsoftheTenthInternational\\nWorkshoponArtiﬁcialIntelligenceandStatistics(AISTATS’05),pages33–40.Society\\nforArtiﬁcialIntelligenceandStatistics.611\\nCaruana,R.(1993).Multitaskconnectionistlearning.InProc.1993ConnectionistModels\\nSummerSchool,pages372–379.244\\nCauchy,A.(1847).Méthodegénéralepourlarésolutiondesystèmesd’équationssimul-\\ntanées.InCompterendudesséancesdel’académiedessciences,pages536–538. ,83\\n225\\n7 2 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e987c977-c19e-4073-97f1-5d6d2e89981a', embedding=None, metadata={'page_label': '744', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nCayton,L.(2005).Algorithmsformanifoldlearning.TechnicalReportCS2008-0923,\\nUCSD.164\\nChandola,V.,Banerjee,A.,andKumar,V.(2009).Anomalydetection:Asurvey.ACM\\ncomputingsurveys(CSUR),(3),15. 4 1102\\nChapelle,O.,Weston,J.,andSchölkopf,B.(2003).Clusterkernelsforsemi-supervised\\nlearning.InS.Becker,S.Thrun,andK.Obermayer,editors,AdvancesinNeural\\nInformationProcessingSystems15(NIPS’02),pages585–592,Cambridge,MA.MIT\\nPress.244\\nChapelle,O.,Schölkopf,B.,andZien,A.,editors(2006).Semi-SupervisedLearning.MIT\\nPress,Cambridge,MA.,244541\\nChellapilla,K.,Puri,S.,andSimard,P.(2006).HighPerformanceConvolutionalNeural\\nNetworks\\xa0forDocumentProcessing.In\\xa0GuyLorette,\\xa0editor,\\xa0Tenth\\xa0International\\nWorkshoponFrontiersinHandwritingRecognition,LaBaule(France).Universitéde\\nRennes1,Suvisoft.http://www.suvisoft.com.,,2427445\\nChen,B.,Ting,J.-A.,Marlin,B.M.,anddeFreitas,N.(2010).Deeplearningofinvariant\\nspatio-temporalfeaturesfromvideo.NIPS*2010DeepLearningandUnsupervised\\nFeatureLearningWorkshop.360\\nChen,S.F.andGoodman,J.T.(1999).Anempiricalstudyofsmoothingtechniquesfor\\nlanguagemodeling.Computer,SpeechandLanguage,(4),359–393. ,, 1 3 462463473\\nChen,T.,Du,Z.,Sun,N.,Wang,J.,Wu,C.,Chen,Y.,andTemam,O.(2014a).DianNao:\\nAsmall-footprinthigh-throughputacceleratorforubiquitousmachine-learning.InPro-\\nceedingsofthe19thinternationalconferenceonArchitecturalsupportforprogramming\\nlanguagesandoperatingsystems,pages269–284.ACM.451\\nChen,T.,Li,M.,Li,Y.,Lin,M.,Wang,N.,Wang,M.,Xiao,T.,Xu,B.,Zhang,C.,\\nandZhang,Z.(2015).MXNet:\\xa0A ﬂexibleandeﬃcientmachinelearninglibraryfor\\nheterogeneousdistributedsystems.arXivpreprintarXiv:1512.01274.25\\nChen,Y.,Luo,T.,Liu,S.,Zhang,S.,He,L.,Wang,J.,Li,L.,Chen,T.,Xu,Z.,Sun,N.,\\netal. Microarchitecture (2014b).DaDianNao:Amachine-learningsupercomputer.In\\n(MICRO),201447thAnnualIEEE/ACMInternationalSymposiumon,pages609–622.\\nIEEE.451\\nChilimbi,T.,Suzue,Y.,Apacible,J.,andKalyanaraman,K.(2014).ProjectAdam:\\nBuildinganeﬃcientandscalabledeeplearningtrainingsystem.In11thUSENIX\\nSymposiumonOperatingSystemsDesignandImplementation(OSDI’14).447\\nCho,K.,Raiko,T.,andIlin,A.(2010).Paralleltemperingiseﬃcientforlearningrestricted\\nBoltzmannmachines.In ., IJCNN’2010603614\\n7 2 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dd52433b-9a86-4c72-b9da-bfac3a5577f9', embedding=None, metadata={'page_label': '745', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nCho,K.,Raiko,T.,andIlin,A.(2011).Enhancedgradientandadaptivelearningratefor\\ntrainingrestrictedBoltzmannmachines.In ,pages105–112. ICML’2011 674\\nCho,K.,vanMerriënboer,B.,Gulcehre,C.,Bougares,F.,Schwenk,H.,andBengio,Y.\\n(2014a).LearningphraserepresentationsusingRNNencoder-decoderforstatistical\\nmachinetranslation.\\xa0InProceedingsoftheEmpiricialMethodsinNaturalLanguage\\nProcessing(EMNLP2014).,,397474475\\nCho,K.,VanMerriënboer,B.,Bahdanau,D.,andBengio,Y.(2014b).Ontheprop-\\nertiesofneuralmachinetranslation:Encoder-decoderapproaches. , ArXive-prints\\na b s/ 1 4 0 9 .1 2 5 9.412\\nChoromanska,A.,Henaﬀ,M.,Mathieu,M.,Arous,G.B.,andLeCun,Y.(2014).\\xa0The\\nlosssurfaceofmultilayernetworks.,285286\\nChorowski,J.,Bahdanau,D.,Cho,K.,andBengio,Y.(2014).\\xa0End-to-endcontinuous\\nspeechrecognitionusingattention-basedrecurrentNN:Firstresults.arXiv:1412.1602.\\n461\\nChristianson,B.(1992).AutomaticHessiansbyreverseaccumulation.IMAJournalof\\nNumericalAnalysis,(2),135–150. 1 2 224\\nChrupala,G.,Kadar,A.,andAlishahi,A.(2015).Learninglanguagethroughpictures.\\narXiv1506.03694. 412\\nChung,J.,Gulcehre,C.,Cho,K.,andBengio,Y.(2014).Empiricalevaluationofgated\\nrecurrentneuralnetworksonsequencemodeling.NIPS’2014DeepLearningworkshop,\\narXiv1412.3555. ,412460\\nChung,J.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2015a).Gatedfeedbackrecurrent\\nneuralnetworks.In . ICML’15412\\nChung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.,andBengio,Y.(2015b).A\\nrecurrentlatentvariablemodelforsequentialdata.In . NIPS’2015698\\nCiresan,D.,Meier,U.,Masci,J.,andSchmidhuber,J.(2012).Multi-columndeepneural\\nnetworkfortraﬃcsignclassiﬁcation.NeuralNetworks,,333–338. , 3 2 23201\\nCiresan,D.C.,Meier,U.,Gambardella,L.M.,andSchmidhuber,J.(2010).\\xa0Deepbig\\nsimpleneuralnetsforhandwrittendigitrecognition.NeuralComputation, 2 2,1–14.\\n2427446,,\\nCoates,A.andNg,A.Y.(2011).Theimportanceofencodingversustrainingwithsparse\\ncodingandvectorquantization.In .,, ICML’201127256498\\nCoates,\\xa0A.,Lee,\\xa0H.,andNg,A.Y.\\xa0(2011).Ananalysisofsingle-layernetworksin\\nunsupervisedfeaturelearning.InProceedingsoftheThirteenthInternationalConference\\nonArtiﬁcialIntelligenceandStatistics(AISTATS2011).,,363364455\\n7 3 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee3c6821-cb66-40eb-bdea-28e0c12c8969', embedding=None, metadata={'page_label': '746', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nCoates,A.,Huval,B.,Wang,\\xa0T.,Wu,\\xa0D.,Catanzaro,\\xa0B.,and\\xa0Andrew,N.\\xa0(2013).\\nDeeplearningwithCOTSHPCsystems.InS.DasguptaandD.McAllester,editors,\\nProceedingsofthe30thInternationalConferenceonMachineLearning(ICML-13),\\nvolume28(3),pages1337–1345. JMLRWorkshopandConferenceProceedings.,,2427\\n364447,\\nCohen,N.,Sharir,O.,andShashua,A.(2015).Ontheexpressivepowerofdeeplearning:\\nAtensoranalysis.arXiv:1509.05009.554\\nCollobert,R.(2004).LargeScaleMachineLearning.Ph.D.thesis,UniversitédeParisVI,\\nLIP6.197\\nCollobert,R.(2011).Deeplearningforeﬃcientdiscriminativeparsing.InAISTATS’2011.\\n101477,\\nCollobert,R.andWeston,J.(2008a).Auniﬁedarchitecturefornaturallanguageprocessing:\\nDeepneuralnetworkswithmultitasklearning.In ., ICML’2008471477\\nCollobert,\\xa0R.\\xa0and\\xa0Weston,J.\\xa0(2008b).A\\xa0uniﬁed\\xa0architecture\\xa0fornatural\\xa0language\\nprocessing:Deepneuralnetworkswithmultitasklearning.In . ICML’2008535\\nCollobert,R.,Bengio,S.,andBengio,Y.(2001).\\xa0AparallelmixtureofSVMsforvery\\nlargescaleproblems.TechnicalReportIDIAP-RR-01-12,IDIAP.450\\nCollobert,R.,Bengio,S.,andBengio,Y.(2002).ParallelmixtureofSVMsforverylarge\\nscaleproblems.NeuralComputation,(5),1105–1114. 1 4 450\\nCollobert,R.,Weston,J.,Bottou,L.,Karlen,M.,Kavukcuoglu,K.,andKuksa,P.(2011a).\\nNaturallanguageprocessing(almost)fromscratch.TheJournalofMachineLearning\\nResearch,,2493–2537. ,,, 1 2 328477535536\\nCollobert,R.,Kavukcuoglu,K.,andFarabet,C.(2011b).Torch7:AMatlab-likeenviron-\\nmentformachinelearning.InBigLearn,NIPSWorkshop.,,25214446\\nComon,P.(1994).Independentcomponentanalysis-anewconcept?SignalProcessing,\\n3 6,287–314.491\\nCortes,C.andVapnik,V.(1995).Supportvectornetworks.MachineLearning, 2 0,\\n273–297. ,18141\\nCouprie,C.,Farabet,C.,Najman,L.,andLeCun,Y.(2013).Indoorsemanticsegmentation\\nusingdepthinformation.InInternationalConferenceonLearningRepresentations\\n(ICLR2013).,23201\\nCourbariaux,M.,Bengio,Y.,andDavid,J.-P.(2015).Lowprecisionarithmeticfordeep\\nlearning.InArxiv:1412.7024,ICLR’2015Workshop.452\\nCourville,A.,Bergstra,J.,andBengio,Y.(2011).Unsupervisedmodelsofimagesby\\nspike-and-slabRBMs.In ., ICML’11561681\\n7 3 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8fd41707-7e0d-49f5-bea4-243eca0b8b50', embedding=None, metadata={'page_label': '747', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nCourville,A.,Desjardins,G.,Bergstra,J.,andBengio,Y.(2014).Thespike-and-slab\\nRBMandextensionstodiscreteandsparsedatadistributions.PatternAnalysisand\\nMachineIntelligence,IEEETransactionson,(9),1874–1887. 3 6 682\\nCover,T.M.andThomas,J.A.(2006).ElementsofInformationTheory,2ndEdition.\\nWiley-Interscience.73\\nCox,D.andPinto,N.(2011).Beyondsimplefeatures:Alarge-scalefeaturesearch\\napproachtounconstrainedfacerecognition.InAutomaticFace&GestureRecognition\\nandWorkshops(FG2011),2011IEEEInternationalConferenceon,pages8–15.IEEE.\\n363\\nCramér,H.(1946).Mathematicalmethodsofstatistics.PrincetonUniversityPress.,135\\n295\\nCrick,F.H.C.andMitchison,G.(1983).Thefunctionofdreamsleep.Nature, 3 0 4,\\n111–114.609\\nCybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction.Mathematics\\nofControl,Signals,andSystems,,303–314. 2 198\\nDahl,G.E.,Ranzato,M.,Mohamed,A.,andHinton,G.E.(2010).Phonerecognition\\nwiththemean-covariancerestrictedBoltzmannmachine.In . NIPS’201023\\nDahl,G.E.,Yu,D.,Deng,L.,andAcero,A.(2012).Context-dependentpre-traineddeep\\nneuralnetworksforlargevocabularyspeechrecognition.IEEETransactionsonAudio,\\nSpeech,andLanguageProcessing,(1),33–42. 2 0 459\\nDahl,G.E.,Sainath,T.N.,andHinton,G.E.(2013).Improvingdeepneuralnetworks\\nforLVCSRusingrectiﬁedlinearunitsanddropout.In . ICASSP’2013460\\nDahl,G.E.,Jaitly,N.,andSalakhutdinov,R.(2014).Multi-taskneuralnetworksfor\\nQSARpredictions.arXiv:1406.1231.26\\nDauphin,\\xa0Y.andBengio,\\xa0Y.(2013).Stochasticratiomatchingof\\xa0RBMsforsparse\\nhigh-dimensionalinputs.In.NIPSFoundation. NIPS26 619\\nDauphin,Y.,Glorot,X.,andBengio,Y.(2011).Large-scalelearningofembeddingswith\\nreconstructionsampling.In . ICML’2011471\\nDauphin,Y.,Pascanu,R.,Gulcehre,C.,Cho,K.,Ganguli,S.,andBengio,Y.(2014).\\nIdentifyingandattackingthesaddlepointprobleminhigh-dimensionalnon-convex\\noptimization.In .,, NIPS’2014285286288\\nDavis,A.,Rubinstein,M.,Wadhwa,N.,Mysore,G.,Durand,F.,andFreeman,W.T.\\n(2014).Thevisualmicrophone:Passiverecoveryofsoundfromvideo.ACMTransactions\\nonGraphics(Proc.SIGGRAPH),(4),79:1–79:10. 3 3 452\\n7 3 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='757dd589-27ec-4b25-bdc9-be04bbe1bd87', embedding=None, metadata={'page_label': '748', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nDayan,P.(1990).Reinforcementcomparison.InConnectionistModels:Proceedingsof\\nthe1990ConnectionistSummerSchool,SanMateo,CA.691\\nDayan,P.andHinton,G.E.(1996).VarietiesofHelmholtzmachine.NeuralNetworks,\\n9(8),1385–1403. 693\\nDayan,P.,Hinton,G.E.,Neal,R.M.,andZemel,R.S.(1995).TheHelmholtzmachine.\\nNeuralcomputation,(5),889–904. 7 693\\nDean,J.,Corrado,G.,Monga,R.,Chen,K.,Devin,M.,Le,Q.,Mao,M.,Ranzato,M.,\\nSenior,A.,Tucker,P.,Yang,K.,andNg,A.Y.(2012).Largescaledistributeddeep\\nnetworks.In ., NIPS’201225447\\nDean,T.andKanazawa,K.(1989).Amodelforreasoningaboutpersistenceandcausation.\\nComputationalIntelligence,(3),142–150. 5 662\\nDeerwester,S.,Dumais,S.T.,Furnas,G.W.,Landauer,T.K.,andHarshman,R.(1990).\\nIndexingbylatentsemanticanalysis.JournaloftheAmericanSocietyforInformation\\nScience,(6),391–407. , 4 1 477482\\nDelalleau,O.andBengio,Y.(2011).Shallowvs.deepsum-productnetworks.In.NIPS\\n19554,\\nDeng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei,L.(2009).ImageNet:\\xa0A\\nLarge-ScaleHierarchicalImageDatabase.In . CVPR0921\\nDeng,J.,Berg,A.C.,Li,K.,andFei-Fei,L.(2010a).Whatdoesclassifyingmorethan\\n10,000imagecategoriestellus?InProceedingsofthe11thEuropeanConferenceon\\nComputerVision:PartV,ECCV’10,pages71–84,Berlin,Heidelberg.Springer-Verlag.\\n21\\nDeng,L.andYu,D.(2014).Deeplearning–methodsandapplications.Foundationsand\\nTrendsinSignalProcessing.460\\nDeng,L.,Seltzer,M.,Yu,D.,Acero,A.,Mohamed,A.,andHinton,G.(2010b).Binary\\ncodingofspeechspectrogramsusingadeepauto-encoder.InInterspeech2010,Makuhari,\\nChiba,Japan.23\\nDenil,M.,Bazzani,L.,Larochelle,H.,anddeFreitas,N.(2012).Learningwheretoattend\\nwithdeeparchitecturesforimagetracking.NeuralComputation, 2 4(8),2151–2184. 367\\nDenton,E.,Chintala,S.,Szlam,A.,andFergus,R.(2015).Deepgenerativeimagemodels\\nusingaLaplacianpyramidofadversarialnetworks.., NIPS702719\\nDesjardins,G.andBengio,Y.(2008).EmpiricalevaluationofconvolutionalRBMsfor\\nvision.\\xa0TechnicalReport1327,Départementd’InformatiqueetdeRechercheOpéra-\\ntionnelle,UniversitédeMontréal.683\\n7 3 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='892d2494-83c9-4294-847c-d596415d2cb5', embedding=None, metadata={'page_label': '749', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nDesjardins,\\xa0G.,\\xa0Courville,\\xa0A.C.,\\xa0Bengio,\\xa0Y.,\\xa0Vincen t,\\xa0P.,\\xa0andDelalleau,\\xa0O.(2010).\\nTemperedMarkovchainMonteCarlofortrainingofrestrictedBoltzmannmachines.In\\nInternationalConferenceonArtiﬁcialIntelligenceandStatistics,pages145–152. ,603\\n614\\nDesjardins,G.,Courville,A.,andBengio,Y.(2011).Ontrackingthepartitionfunction.\\nIn . NIPS’2011629\\nDesjardins,G.,Simonyan,K.,Pascanu,R.,(2015).\\xa0Naturalneuralnetworks.\\xa0In etal.\\nAdvancesinNeuralInformationProcessingSystems,pages2062–2070. 320\\nDevlin,J.,Zbib,R.,Huang,Z.,Lamar,T.,Schwartz,R.,andMakhoul,J.(2014).Fast\\nandrobustneuralnetworkjointmodelsforstatisticalmachinetranslation.\\xa0InProc.\\nACL’2014.473\\nDevroye,L.(2013).Non-UniformRandomVariateGeneration.SpringerLink:Bücher.\\nSpringerNewYork.694\\nDiCarlo,J.J.(2013).Mechanismsunderlyingvisualobjectrecognition:Humansvs.\\nneuronsvs.machines.NIPSTutorial.,26366\\nDinh,L.,Krueger,D.,andBengio,Y.(2014).NICE:Non-linearindependentcomponents\\nestimation.arXiv:1410.8516.493\\nDonahue,J.,Hendricks,L.A.,Guadarrama,S.,Rohrbach,M.,Venugopalan,S.,Saenko,\\nK.,andDarrell,T.(2014).Long-termrecurrentconvolutionalnetworksforvisual\\nrecognitionanddescription.arXiv:1411.4389.102\\nDonoho,D.L.andGrimes,C.(2003).Hessianeigenmaps:newlocallylinearembedding\\ntechniquesforhigh-dimensional\\xa0data.TechnicalReport2003-08,\\xa0Dept.Statistics,\\nStanfordUniversity.,164519\\nDosovitskiy,A.,Springenberg,J.T.,andBrox,T.(2015).Learningtogeneratechairswith\\nconvolutionalneuralnetworks.InProceedingsoftheIEEEConferenceonComputer\\nVisionandPatternRecognition,pages1538–1546. ,,696704705\\nDoya,K.(1993).Bifurcationsofrecurrentneuralnetworksingradientdescentlearning.\\nIEEETransactionsonNeuralNetworks,,75–80., 1401403\\nDreyfus,\\xa0S.\\xa0E.(1962).The\\xa0numerical\\xa0solutionofvariational\\xa0problems.Journalof\\nMathematicalAnalysisandApplications,,30–45. 5 ( 1 )225\\nDreyfus,S.E.(1973).Thecomputationalsolutionofoptimalcontrolproblemswithtime\\nlag.IEEETransactionsonAutomaticControl,,383–385. 1 8 ( 4 ) 225\\nDrucker,H.andLeCun,Y.(1992).Improvinggeneralisationperformanceusingdouble\\nback-propagation.IEEETransactionsonNeuralNetworks,(6),991–997. 3 271\\n7 3 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='032d0357-b4ed-494f-bba4-972d1309fba7', embedding=None, metadata={'page_label': '750', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nDuchi,J.,Hazan,E.,andSinger,Y.(2011).\\xa0Adaptivesubgradientmethodsforonline\\nlearningandstochasticoptimization.JournalofMachineLearningResearch.307\\nDudik,M.,Langford,J.,andLi,L.(2011).Doublyrobustpolicyevaluationandlearning.\\nInProceedingsofthe28thInternationalConferenceonMachinelearning,ICML’11.\\n482\\nDugas,C.,Bengio,Y.,Bélisle,F.,andNadeau,C.(2001).\\xa0Incorporatingsecond-order\\nfunctionalknowledgeforbetteroptionpricing.InT.Leen,T.Dietterich,andV.Tresp,\\neditors,\\xa0AdvancesinNeural\\xa0InformationProcessingSystems\\xa013(NIPS’00),\\xa0pages\\n472–478.MITPress.,68197\\nDziugaite,G.K.,Roy,D.M.,andGhahramani,Z.(2015).Traininggenerativeneuralnet-\\nworksviamaximummeandiscrepancyoptimization.arXivpreprintarXiv:1505.03906.\\n703\\nElHihi,S.andBengio,Y.(1996).Hierarchicalrecurrentneuralnetworksforlong-term\\ndependencies.In .,, NIPS’1995398407408\\nElkahky,A.M.,Song,Y.,andHe,X.(2015).Amulti-viewdeeplearningapproachfor\\ncrossdomainusermodelinginrecommendationsystems.\\xa0InProceedingsofthe24th\\nInternationalConferenceonWorldWideWeb,pages278–288.480\\nElman,J.L.(1993).Learninganddevelopmentinneuralnetworks:Theimportanceof\\nstartingsmall.Cognition,,781–799. 4 8 328\\nErhan,D.,Manzagol,P.-A.,Bengio,Y.,Bengio,S.,andVincent,P.(2009).Thediﬃculty\\noftrainingdeeparchitecturesandtheeﬀectofunsupervisedpre-training.InProceedings\\nofAISTATS’2009.201\\nErhan,D.,Bengio,Y.,Courville,A.,Manzagol,P.,Vincent,P.,andBengio,S.(2010).\\nWhydoesunsupervisedpre-traininghelpdeeplearning?J.MachineLearningRes.\\n529533534,,\\nFahlman,S.E.,Hinton,G.E.,andSejnowski,T.J.(1983).Massivelyparallelarchitectures\\nfor\\xa0AI:NETL,thistle,\\xa0andBoltzmann\\xa0machines.In\\xa0Proceedings\\xa0ofthe\\xa0National\\nConferenceonArtiﬁcialIntelligenceAAAI-83.,570654\\nFang,H.,Gupta,S.,Iandola,F.,Srivastava,R.,Deng,L.,Dollár,P.,Gao,J.,He,X.,\\nMitchell,M.,Platt,J.C.,Zitnick,C.L.,andZweig,G.(2015).Fromcaptionstovisual\\nconceptsandback.arXiv:1411.4952.102\\nFarabet,C.,LeCun,Y.,Kavukcuoglu,K.,Culurciello,E.,Martini,B.,Akselrod,P.,and\\nTalay,S.(2011).Large-scaleFPGA-basedconvolutionalnetworks.InR.Bekkerman,\\nM.Bilenko,andJ.Langford,\\xa0editors,ScalingupMachineLearning:Paralleland\\nDistributedApproaches.CambridgeUniversityPress.523\\n7 3 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='991b3461-4a49-4bd7-8c40-3ac98b6d03a4', embedding=None, metadata={'page_label': '751', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nFarabet,C.,Couprie,C.,Najman,L.,andLeCun,Y.(2013).Learninghierarchicalfeatures\\nforscenelabeling.IEEETransactionsonPatternAnalysisandMachineIntelligence,\\n3 5(8),1915–1929. ,,23201360\\nFei-Fei,L.,Fergus,R.,andPerona,P.(2006).One-shotlearningofobjectcategories.\\nIEEETransactionsonPatternAnalysisandMachineIntelligence, 2 8(4),594–611.538\\nFinn,C.,Tan,X.Y.,Duan,Y.,Darrell,T.,Levine,S.,andAbbeel,P.(2015).Learning\\nvisualfeaturespacesforroboticmanipulationwithdeepspatialautoencoders.arXiv\\npreprintarXiv:1509.06113.25\\nFisher,R.A.(1936).Theuseofmultiplemeasurementsintaxonomicproblems.Annals\\nofEugenics,,179–188. , 7 21105\\nFöldiák,P.(1989).Adaptivenetworkforoptimallinearfeatureextraction.InInternational\\nJointConferenceonNeuralNetworks(IJCNN),volume1,pages401–405,Washington\\n1989.IEEE,NewYork.494\\nFranzius,M.,Sprekeler,H.,andWiskott,L.(2007).Slownessandsparsenessleadtoplace,\\nhead-direction,andspatial-viewcells.495\\nFranzius,M.,Wilbert,N.,andWiskott,L.(2008).Invariantobjectrecognitionwithslow\\nfeatureanalysis.InArtiﬁcialNeuralNetworks-ICANN2008,pages961–970.Springer.\\n496\\nFrasconi,P.,Gori,M.,andSperduti,A.(1997).Ontheeﬃcientclassiﬁcationofdata\\nstructuresbyneuralnetworks.InProc.Int.JointConf.onArtiﬁcialIntelligence.401\\nFrasconi,\\xa0P.,\\xa0Gori,\\xa0M.,\\xa0andSperduti,\\xa0A.(1998).Ageneralframeworkforadaptive\\nprocessingofdatastructures.IEEETransactionsonNeuralNetworks, 9(5),768–786.\\n401\\nFreund,Y.andSchapire,R.E.(1996a).Experimentswithanewboostingalgorithm.In\\nMachineLearning:ProceedingsofThirteenthInternationalConference,pages148–156,\\nUSA.ACM.258\\nFreund,Y.andSchapire,R.E.(1996b).Gametheory,on-linepredictionandboosting.In\\nProceedingsoftheNinthAnnualConferenceonComputationalLearningTheory,pages\\n325–332.258\\nFrey,B.J.(1998).Graphicalmodelsformachinelearninganddigitalcommunication.\\nMITPress.,705706\\nFrey,B.J.,Hinton,G.E.,andDayan,P.(1996).Doesthewake-sleepalgorithmlearngood\\ndensityestimators?InD.Touretzky,M.Mozer,andM.Hasselmo,editors,Advances\\ninNeuralInformationProcessingSystems8(NIPS’95),pages661–670.MITPress,\\nCambridge,MA.651\\n7 3 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='77149675-950a-4885-9422-fcbe8c5dd11e', embedding=None, metadata={'page_label': '752', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nFrobenius,G.(1908).Übermatrizenauspositivenelementen,s.B.Preuss.Akad.Wiss.\\nBerlin,Germany.597\\nFukushima,K.(1975).Cognitron:Aself-organizingmultilayeredneuralnetwork.Biological\\nCybernetics,,121–136. ,, 2 0 16226528\\nFukushima,\\xa0K.(1980).Neocognitron:Aself-organizingneuralnetworkmodelfora\\nmechanismofpatternrecognitionunaﬀectedbyshiftinposition.BiologicalCybernetics,\\n3 6,193–202. ,,,, 162427226367\\nGal,Y.andGhahramani,Z.(2015).BayesianconvolutionalneuralnetworkswithBernoulli\\napproximatevariationalinference.arXivpreprintarXiv:1506.02158.264\\nGallinari,P.,LeCun,Y.,Thiria,S.,andFogelman-Soulie,F.(1987).Memoiresassociatives\\ndistribuees.InProceedingsofCOGNITIVA87,Paris,LaVillette.515\\nGarcia-Duran,A.,Bordes,A.,Usunier,N.,andGrandvalet,Y.(2015).Combiningtwo\\nandthree-wayembeddingsmodelsforlinkpredictioninknowledgebases.arXivpreprint\\narXiv:1506.00999.484\\nGarofolo,J.S.,Lamel,L.F.,Fisher,W.M.,Fiscus,J.G.,andPallett,D.S.(1993).\\nDarpatimitacoustic-phoneticcontinousspeechcorpuscd-rom.nistspeechdisc1-1.1.\\nNASASTI/ReconTechnicalReportN,,27403. 9 3459\\nGarson,J.(1900).Themetricsystemofidentiﬁcationofcriminals,asusedinGreat\\nBritainandIreland.TheJournaloftheAnthropologicalInstituteofGreatBritainand\\nIreland,(2),177–227.21\\nGers,F.A.,Schmidhuber,J.,andCummins,F.(2000).\\xa0Learningtoforget:Continual\\npredictionwithLSTM.Neuralcomputation,(10),2451–2471. , 1 2 410412\\nGhahramani,Z.andHinton,G.E.(1996).TheEMalgorithmformixturesoffactor\\nanalyzers.TechnicalReportCRG-TR-96-1,Dpt.ofComp.Sci.,Univ.ofToronto.489\\nGillick,D.,Brunk,C.,Vinyals,O.,andSubramanya,A.(2015).Multilinguallanguage\\nprocessingfrombytes.arXivpreprintarXiv:1512.00103.477\\nGirshick,R.,Donahue,J.,Darrell,T.,andMalik,J.(2015).Region-basedconvolutional\\nnetworksforaccurateobjectdetectionandsegmentation.426\\nGiudice,M.D.,Manera,V.,andKeysers,C.(2009).Programmedtolearn?Theontogeny\\nofmirrorneurons.,(2),350––363. Dev.Sci. 1 2 656\\nGlorot,X.andBengio,Y.(2010).Understandingthediﬃcultyoftrainingdeepfeedforward\\nneuralnetworks.InAISTATS’2010.303\\nGlorot,X.,Bordes,A.,andBengio,Y.(2011a).Deepsparserectiﬁerneuralnetworks.In\\nAISTATS’2011.,,,, 16174197226227\\n7 3 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3a7f5e32-345b-4b0b-ae63-36b0e321a044', embedding=None, metadata={'page_label': '753', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nGlorot,\\xa0X.,Bordes,\\xa0A.,andBengio,\\xa0Y.(2011b).Domainadaptationforlarge-scale\\nsentimentclassiﬁcation:Adeeplearningapproach.In ., ICML’2011507537\\nGoldberger,J.,Roweis,S.,Hinton,G.E.,andSalakhutdinov,R.(2005).Neighbourhood\\ncomponentsanalysis.InL.Saul,Y.Weiss,andL.Bottou,editors,AdvancesinNeural\\nInformationProcessingSystems17(NIPS’04).MITPress.115\\nGong,S.,McKenna,S.,andPsarrou,A.(2000).DynamicVision:FromImagestoFace\\nRecognition.ImperialCollegePress.,165519\\nGoodfellow,I.,Le,Q.,Saxe,A.,\\xa0andNg,A.(2009).Measuringinvariancesindeep\\nnetworks.In ,pages646–654. NIPS’2009 255\\nGoodfellow,I.,Koenig,N.,Muja,M.,Pantofaru,C.,Sorokin,A.,andTakayama,L.(2010).\\nHelpmehelpyou:Interfacesforpersonalrobots.InProc.ofHumanRobotInteraction\\n(HRI),Osaka,Japan.ACMPress,ACMPress.100\\nGoodfellow,I.J.(2010).Technicalreport:Multidimensional,downsampledconvolution\\nforautoencoders.Technicalreport,UniversitédeMontréal.357\\nGoodfellow,I.J.(2014).Ondistinguishabilitycriteriaforestimatinggenerativemodels.\\nInInternationalConferenceonLearningRepresentations,WorkshopsTrack.,,622700\\n701\\nGoodfellow,I.J.,Courville,A.,andBengio,Y.(2011).Spike-and-slabsparsecoding\\nforunsupervisedfeaturediscovery.InNIPSWorkshoponChallengesinLearning\\nHierarchicalModels.,532538\\nGoodfellow,I.J.,Warde-Farley,D.,Mirza,M.,Courville,A.,andBengio,Y.(2013a).\\nMaxoutnetworks.InS.DasguptaandD.McAllester,editors,,pages1319– ICML’13\\n1327.,,,, 193264344365455\\nGoodfellow,I.J.,Mirza,M.,Courville,A.,andBengio,Y.(2013b).Multi-predictiondeep\\nBoltzmannmachines.In.NIPSFoundation.,,,,,,, NIPS26 100617671672673674675\\n698\\nGoodfellow,I.J.,Warde-Farley,D.,Lamblin,P.,Dumoulin,V.,Mirza,M.,Pascanu,R.,\\nBergstra,J.,Bastien,F.,andBengio,Y.(2013c).Pylearn2:amachinelearningresearch\\nlibrary.arXivpreprintarXiv:1308.4214.,25446\\nGoodfellow,I.J.,Courville,A.,andBengio,Y.(2013d).Scalingupspike-and-slabmodels\\nforunsupervisedfeaturelearning.IEEETransactionsonPatternAnalysisandMachine\\nIntelligence,(8),1902–1914. ,,,, 3 5 497498499650683\\nGoodfellow,I.J.,Mirza,M.,Xiao,D.,Courville,A.,andBengio,Y.(2014a).Anempirical\\ninvestigationofcatastrophicforgetingingradient-basedneuralnetworks.In . ICLR’2014\\n194\\n7 3 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cff40a40-d9ed-4ced-b248-5df20b8c4c4f', embedding=None, metadata={'page_label': '754', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nGoodfellow,I.J.,Shlens,J.,andSzegedy,C.(2014b).Explainingandharnessingadver-\\nsarialexamples., .,,,, CoRR a b s/ 1 4 1 2 .6 5 7 2268269271555556\\nGoodfellow,I.J.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,\\nCourville,A.,andBengio,Y.(2014c).Generativeadversarialnetworks.In . NIPS’2014\\n544689699701704 ,,,,\\nGoodfellow,I.J.,Bulatov,Y.,Ibarz,J.,Arnoud,S.,andShet,V.(2014d).Multi-digit\\nnumberrecognitionfromStreetViewimageryusingdeepconvolutionalneuralnetworks.\\nInInternationalConferenceonLearningRepresentations.,,,,,, 25101201202203391\\n422449,\\nGoodfellow,I.J.,Vinyals,O.,andSaxe,A.M.(2015).Qualitativelycharacterizingneural\\nnetworkoptimizationproblems.InInternationalConferenceonLearningRepresenta-\\ntions.,,, 285286287291\\nGoodman,J.(2001).Classes\\xa0forfast\\xa0maximumentropytraining.InInternational\\nConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),Utah.467\\nGori,M.andTesi,A.(1992).Ontheproblemoflocalminimainbackpropagation.IEEE\\nTransactionsonPatternAnalysisandMachineIntelligence, P A M I - 1 4(1),76–86.284\\nGosset,W.S.(1908).Theprobableerrorofamean. , Biometrika 6(1),1–25.Originally\\npublishedunderthepseudonym“Student”.21\\nGouws,S.,Bengio,Y.,andCorrado,G.(2014).BilBOWA:Fastbilingualdistributed\\nrepresentationswithoutwordalignments.Technicalreport,arXiv:1410.2455.,476539\\nGraf,H.P.andJackel,L.D.(1989).Analogelectronicneuralnetworkcircuits.Circuits\\nandDevicesMagazine,IEEE,(4),44–49. 5 451\\nGraves,A.(2011).Practicalvariationalinferenceforneuralnetworks.In . NIPS’2011242\\nGraves,A.(2012).SupervisedSequenceLabellingwithRecurrentNeuralNetworks.Studies\\ninComputationalIntelligence.Springer.,,, 374395411460\\nGraves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks.Technicalreport,\\narXiv:1308.0850.,,, 190410415420\\nGraves,A.andJaitly,N.(2014).Towardsend-to-endspeechrecognitionwithrecurrent\\nneuralnetworks.In . ICML’2014410\\nGraves,A.andSchmidhuber,J.(2005).Framewisephonemeclassiﬁcationwithbidirec-\\ntionalLSTMandotherneuralnetworkarchitectures.NeuralNetworks, 1 8(5),602–610.\\n395\\nGraves,A.andSchmidhuber,J.(2009).Oﬄinehandwritingrecognitionwithmultidi-\\nmensionalrecurrentneuralnetworks.InD.Koller,D.Schuurmans,Y.Bengio,and\\nL.Bottou,editors, ,pages545–552. NIPS’2008 395\\n7 3 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76696be4-f174-419a-8a30-fa0ad04c102b', embedding=None, metadata={'page_label': '755', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nGraves,A.,Fernández,S.,Gomez,F.,andSchmidhuber,J.(2006).Connectionisttemporal\\nclassiﬁcation:Labellingunsegmentedsequencedatawithrecurrentneuralnetworks.In\\nICML’2006,pages369–376,Pittsburgh,USA.460\\nGraves,A.,Liwicki,M.,Bunke,H.,Schmidhuber,J.,andFernández,S.(2008).Uncon-\\nstrainedon-linehandwritingrecognitionwithrecurrentneuralnetworks.InJ.Platt,\\nD.Koller,Y.Singer,andS.Roweis,editors, ,pages577–584. NIPS’2007 395\\nGraves,A.,Liwicki,M.,Fernández,S.,Bertolami,R.,Bunke,H.,andSchmidhuber,J.\\n(2009).Anovelconnectionistsystemforunconstrainedhandwritingrecognition.Pattern\\nAnalysisandMachineIntelligence,IEEETransactionson,(5),855–868. 3 1 410\\nGraves,A.,Mohamed,A.,andHinton,G.(2013).Speechrecognitionwithdeeprecurrent\\nneuralnetworks.In ,pages6645–6649. ,,,, ICASSP’2013 395398410411460\\nGraves,A.,Wayne,G.,andDanihelka,I.(2014a).NeuralTuringmachines.\\narXiv:1410.5401.25\\nGraves,A.,Wayne,G.,andDanihelka,I.(2014b).NeuralTuringmachines.arXivpreprint\\narXiv:1410.5401.418\\nGrefenstette,E.,Hermann,K.M.,Suleyman,M.,andBlunsom,P.(2015).Learningto\\ntransducewithunboundedmemory.In . NIPS’2015418\\nGreﬀ,K.,Srivastava,R.K.,Koutník,J.,Steunebrink,B.R.,andSchmidhuber,J.(2015).\\nLSTM:asearchspaceodyssey.arXivpreprintarXiv:1503.04069.412\\nGregor,K.andLeCun,Y.(2010a).Emergenceofcomplex-likecellsinatemporalproduct\\nnetworkwithlocalreceptiveﬁelds.Technicalreport,arXiv:1006.0448.352\\nGregor,K.andLeCun,Y.(2010b).Learningfastapproximationsofsparsecoding.In\\nL.BottouandM.Littman,editors,ProceedingsoftheTwenty-seventhInternational\\nConferenceonMachineLearning(ICML-10).ACM.652\\nGregor,\\xa0K.,Danihelka,\\xa0I.,Mnih,A.,\\xa0Blundell,C.,and\\xa0Wierstra,\\xa0D.\\xa0(2014).Deep\\nautoregressivenetworks.InInternationalConferenceonMachineLearning(ICML’2014).\\n693\\nGregor,K.,Danihelka,I.,Graves,A.,andWierstra,D.(2015).DRAW:Arecurrentneural\\nnetworkforimagegeneration.arXivpreprintarXiv:1502.04623.698\\nGretton,A.,Borgwardt,K.M.,Rasch,M.J.,Schölkopf,B.,andSmola,A.(2012).A\\nkerneltwo-sampletest.TheJournalofMachineLearningResearch, 1 3(1),723–773.\\n704\\nGülçehre,Ç.andBengio,Y.(2013).Knowledgematters:Importanceofpriorinformation\\nforoptimization.InInternationalConferenceonLearningRepresentations(ICLR’2013).\\n25\\n7 4 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='638b1e45-f133-4664-a367-39c9baf1ccb2', embedding=None, metadata={'page_label': '756', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nGuo,H.andGelfand,S.B.(1992).Classiﬁcationtreeswithneuralnetworkfeature\\nextraction.NeuralNetworks,IEEETransactionson,(6),923–933. 3 450\\nGupta,S.,Agrawal,A.,Gopalakrishnan,K.,andNarayanan,P.(2015).Deeplearning\\nwithlimitednumericalprecision., . CoRR a b s/ 1 5 0 2 .0 2 5 5 1452\\nGutmann,M.andHyvarinen,A.(2010).Noise-contrastiveestimation:\\xa0Anewestima-\\ntionprincipleforunnormalizedstatisticalmodels.\\xa0InProceedingsofTheThirteenth\\nInternationalConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10).620\\nHadsell,\\xa0R.,\\xa0Sermanet,\\xa0P.,\\xa0Ben,\\xa0J.,Erkan,A.,\\xa0Han,\\xa0J.,\\xa0Muller, U.,\\xa0andLeCun,\\xa0Y.\\n(2007).Onlinelearningforoﬀroadrobots:Spatiallabelpropagationtolearnlong-range\\ntraversability.InProceedingsofRobotics:ScienceandSystems,Atlanta,GA,USA.453\\nHajnal,A.,Maass,W.,Pudlak,P.,Szegedy,M.,andTuran,G.(1993).Thresholdcircuits\\nofboundeddepth. ,,129–154. J.Comput.System.Sci. 4 6 199\\nHåstad,J.(1986).Almostoptimallowerboundsforsmalldepthcircuits.InProceedings\\nofthe18thannualACMSymposiumonTheoryofComputing,pages6–20,Berkeley,\\nCalifornia.ACMPress.199\\nHåstad,J.andGoldmann,M.(1991).Onthepowerofsmall-depththresholdcircuits.\\nComputationalComplexity,,113–129. 1 199\\nHastie,T.,Tibshirani,R.,andFriedman,J.(2001).Theelementsofstatisticallearning:\\ndatamining,inferenceandprediction.\\xa0SpringerSeriesinStatistics.SpringerVerlag.\\n146\\nHe,K.,Zhang,X.,Ren,S.,andSun,J.(2015).Delvingdeepintorectiﬁers:Surpassing\\nhuman-levelperformanceonImageNetclassiﬁcation.arXivpreprintarXiv:1502.01852.\\n28193,\\nHebb,D.O.(1949).TheOrganizationofBehavior.Wiley,NewYork.,,1417656\\nHenaﬀ,M.,Jarrett,K.,Kavukcuoglu,K.,andLeCun,Y.(2011).Unsupervisedlearning\\nofsparsefeaturesforscalableaudioclassiﬁcation.In . ISMIR’11523\\nHenderson,J.(2003).Inducinghistoryrepresentationsforbroadcoveragestatistical\\nparsing.InHLT-NAACL,pages103–110.477\\nHenderson,J.(2004).Discriminativetrainingofaneuralnetworkstatisticalparser.In\\nProceedingsofthe42ndAnnualMeetingonAssociationforComputationalLinguistics,\\npage95.477\\nHenniges,M.,Puertas,G.,Bornschein,J.,Eggert,J.,andLücke,J.(2010).Binarysparse\\ncoding.InLatentVariableAnalysisandSignalSeparation,pages450–457.Springer.\\n640\\n7 4 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8dcc5825-5d78-46f5-962e-a6464195609e', embedding=None, metadata={'page_label': '757', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nHerault,J.andAns,B.(1984).Circuitsneuronauxàsynapsesmodiﬁables:Décodagede\\nmessagescompositesparapprentissagenonsupervisé.ComptesRendusdel’Académie\\ndesSciences, ,525––528. 2 9 9 ( I I I - 1 3 ) 491\\nHinton,G.(2012).Neuralnetworksformachinelearning.Coursera,videolectures.307\\nHinton,G.,Deng,L.,Dahl,G.E.,Mohamed,A.,Jaitly,N.,Senior,A.,Vanhoucke,V.,\\nNguyen,P.,Sainath,T.,andKingsbury,B.(2012a).Deepneuralnetworksforacoustic\\nmodelinginspeechrecognition.IEEESignalProcessingMagazine, 2 9(6),82–97.,23\\n460\\nHinton,G.,Vinyals,O.,andDean,J.(2015).Distillingtheknowledgeinaneuralnetwork.\\narXivpreprintarXiv:1503.02531.448\\nHinton,G.E.(1989).Connectionistlearningprocedures.ArtiﬁcialIntelligence, 4 0,\\n185–234.494\\nHinton,G.E.(1990).Mappingpart-wholehierarchiesintoconnectionistnetworks.Artiﬁcial\\nIntelligence,(1),47–75. 4 6 418\\nHinton,G.E.(1999).Productsofexperts.In . ICANN’1999571\\nHinton,G.E.(2000).Trainingproductsofexpertsbyminimizingcontrastivedivergence.\\nTechnicalReportGCNUTR2000-004,GatsbyUnit,UniversityCollegeLondon.,610\\n676\\nHinton,G.E.(2006).Torecognizeshapes,ﬁrstlearntogenerateimages.TechnicalReport\\nUTMLTR2006-003,UniversityofToronto.,528595\\nHinton,G.E.(2007a).Howtodobackpropagationinabrain.Invitedtalkatthe\\nNIPS’2007DeepLearningWorkshop.656\\nHinton,G.E.(2007b).\\xa0Learningmultiplelayersofrepresentation.\\xa0Trendsincognitive\\nsciences,(10),428–434. 1 1 660\\nHinton,\\xa0G.E.(2010).ApracticalguidetotrainingrestrictedBoltzmannmachines.\\nTechnicalReportUTMLTR2010-003,DepartmentofComputerScience,Universityof\\nToronto.610\\nHinton,G.E.andGhahramani,Z.(1997).Generativemodelsfordiscoveringsparse\\ndistributedrepresentations.PhilosophicalTransactionsoftheRoyalSocietyofLondon.\\n147\\nHinton,G.E.andMcClelland, J.L.(1988).Learningrepresentationsbyrecirculation.In\\nNIPS’1987,pages358–366.502\\nHinton,G.E.andRoweis,S.(2003).Stochasticneighborembedding.In . NIPS’2002519\\n7 4 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dee2512c-d382-4b48-8ef2-6a274248df48', embedding=None, metadata={'page_label': '758', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nHinton,G.E.andSalakhutdinov,R.(2006).Reducingthedimensionalityofdatawith\\nneuralnetworks.Science,(5786),504–507. ,,,, 3 1 3 509524528529534\\nHinton,G.E.andSejnowski,T.J.(1986).LearningandrelearninginBoltzmannmachines.\\nInD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributedProcessing,\\nvolume1,chapter7,pages282–317.MITPress,Cambridge.,570654\\nHinton,G.E.andSejnowski,T.J.(1999).Unsupervisedlearning:foundationsofneural\\ncomputation.MITpress.541\\nHinton,G.E.andShallice,T.(1991).Lesioninganattractornetwork:investigationsof\\nacquireddyslexia.Psychologicalreview,(1),74. 9 813\\nHinton,G.E.andZemel,R.S.(1994).Autoencoders,minimumdescriptionlength,and\\nHelmholtzfreeenergy.In . NIPS’1993502\\nHinton,G.E.,Sejnowski,T.J.,andAckley,D.H.(1984).Boltzmannmachines:Constraint\\nsatisfactionnetworksthatlearn.TechnicalReportTR-CMU-CS-84-119,Carnegie-Mellon\\nUniversity,Dept.ofComputerScience.,570654\\nHinton,G.E.,McClelland, J.,andRumelhart,D.(1986).\\xa0Distributedrepresentations.\\nInD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributedProcessing:\\nExplorationsintheMicrostructureofCognition,volume1,pages77–109.MITPress,\\nCambridge.,,17225526\\nHinton,G.E.,Revow,M.,andDayan,P.(1995a).Recognizinghandwrittendigitsusing\\nmixturesoflinearmodels.InG.Tesauro,D.Touretzky,andT.Leen,editors,Advances\\ninNeuralInformationProcessingSystems7(NIPS’94),pages1015–1022. MITPress,\\nCambridge,MA.489\\nHinton,G.E.,Dayan,P.,Frey,B.J.,andNeal,R.M.(1995b).Thewake-sleepalgorithm\\nforunsupervisedneuralnetworks.Science,,1558–1161. , 2 6 8 504651\\nHinton,G.E.,Dayan,P.,andRevow,M.(1997).Modellingthemanifoldsofimagesof\\nhandwrittendigits.IEEETransactionsonNeuralNetworks,,65–74. 8499\\nHinton,G.E.,Welling,M.,Teh,Y.W.,andOsindero,S.(2001).AnewviewofICA.In\\nProceedingsof3rdInternationalConferenceonIndependentComponentAnalysisand\\nBlindSignalSeparation(ICA’01),pages746–751,SanDiego,CA.491\\nHinton,G.E.,Osindero,S.,andTeh,Y.(2006).Afastlearningalgorithmfordeepbelief\\nnets.NeuralComputation,,1527–1554. ,,,,,,, 1 8 141927143528529660661\\nHinton,G.E.,\\xa0Deng,L.,Yu,\\xa0D.,Dahl,G.E.,Mohamed,\\xa0A.,Jaitly,\\xa0N.,Senior,A.,\\nVanhoucke,V.,Nguyen,P.,Sainath,T.N.,andKingsbury,B.(2012b).Deepneural\\nnetworksforacousticmodelinginspeechrecognition:Thesharedviewsoffourresearch\\ngroups.IEEESignalProcess.Mag.,(6),82–97. 2 9 101\\n7 4 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2c9e3791-997b-40e8-b81d-d79a4cef52f7', embedding=None, metadata={'page_label': '759', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nHinton,G.E.,Srivastava,N.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2012c).\\nImprovingneuralnetworksbypreventingco-adaptationoffeaturedetectors.Technical\\nreport,arXiv:1207.0580.,,238263267\\nHinton,G.E.,Vinyals,O.,andDean,J.(2014).Darkknowledge.\\xa0Invitedtalkatthe\\nBayLearnBayAreaMachineLearningSymposium.448\\nHochreiter,S.(1991).UntersuchungenzudynamischenneuronalenNetzen.Diploma\\nthesis,T.U.München.,,18401403\\nHochreiter,S.andSchmidhuber,J.(1995).\\xa0Simplifyingneuralnetsbydiscoveringﬂat\\nminima.InAdvancesinNeuralInformationProcessingSystems7,pages529–536.MIT\\nPress.243\\nHochreiter,S.andSchmidhuber,J.(1997).Longshort-termmemory.NeuralComputation,\\n9(8),1735–1780. ,,18410411\\nHochreiter,S.,Bengio,Y.,andFrasconi,P.(2001).Gradientﬂowinrecurrentnets:the\\ndiﬃcultyoflearninglong-termdependencies.InJ.KolenandS.Kremer,editors,Field\\nGuidetoDynamicalRecurrentNetworks.IEEEPress.411\\nHoli,J.L.andHwang,J.-N.(1993).Finiteprecisionerroranalysisofneuralnetwork\\nhardwareimplementations.Computers,IEEETransactionson,(3),281–290. 4 2 451\\nHolt,J.L.andBaker,T.E.(1991).\\xa0Backpropagationsimulationsusinglimitedpreci-\\nsioncalculations.InNeuralNetworks,1991.,IJCNN-91-SeattleInternationalJoint\\nConferenceon,volume2,pages121–126.IEEE.451\\nHornik,K.,Stinchcombe,M.,andWhite,H.(1989).Multilayerfeedforwardnetworksare\\nuniversalapproximators.NeuralNetworks,,359–366. 2 198\\nHornik,K.,Stinchcombe,M.,andWhite,H.(1990).Universalapproximationofan\\nunknownmappinganditsderivativesusingmultilayerfeedforwardnetworks.Neural\\nnetworks,(5),551–560. 3 198\\nHsu,F.-H.(2002).BehindDeepBlue:BuildingtheComputerThatDefeatedtheWorld\\nChessChampion.PrincetonUniversityPress,Princeton,NJ,USA.2\\nHuang,F.andOgata,Y.(2002).Generalizedpseudo-likelihoodestimatesforMarkov\\nrandomﬁeldsonlattice.AnnalsoftheInstituteofStatisticalMathematics, 5 4(1),1–18.\\n616\\nHuang,P.-S.,He,X.,Gao,J.,Deng,L.,Acero,A.,andHeck,L.(2013).Learningdeep\\nstructuredsemanticmodelsforwebsearchusingclickthroughdata.InProceedingsof\\nthe22ndACMinternationalconferenceonConferenceoninformation&knowledge\\nmanagement,pages2333–2338. ACM.480\\nHubel,D.andWiesel,T.(1968).Receptiveﬁeldsandfunctionalarchitectureofmonkey\\nstriatecortex.JournalofPhysiology(London),,215–243. 1 9 5 364\\n7 4 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='54e998a6-5f4a-4a4c-b774-178e86cf4377', embedding=None, metadata={'page_label': '760', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nHubel,D.H.andWiesel,T.N.(1959).Receptiveﬁeldsofsingleneuronsinthecat’s\\nstriatecortex.JournalofPhysiology,,574–591. 1 4 8 364\\nHubel,D.H.andWiesel,\\xa0T.N.(1962).Receptiveﬁelds,\\xa0binocularinteraction,and\\nfunctionalarchitectureinthecat’svisualcortex.JournalofPhysiology(London), 1 6 0,\\n106–154.364\\nHuszar,F.(2015).How(not)totrainyourgenerativemodel:schedulesampling,likelihood,\\nadversary? . arXiv:1511.05101698\\nHutter,F.,Hoos,H.,andLeyton-Brown,K.(2011).\\xa0Sequentialmodel-basedoptimization\\nforgeneralalgorithmconﬁguration.In.ExtendedversionasUBCTechreport LION-5\\nTR-2010-10.436\\nHyotyniemi,H.(1996).Turingmachinesarerecurrentneuralnetworks.InSTeP’96,pages\\n13–24.379\\nHyvärinen,A.(1999).\\xa0Surveyonindependentcomponentanalysis.NeuralComputing\\nSurveys,,94–128. 2 491\\nHyvärinen,A.(2005).Estimationofnon-normalizedstatisticalmodelsusingscorematching.\\nJournalofMachineLearningResearch,,695–709. , 6 513617\\nHyvärinen,A.(2007a).Connectionsbetweenscorematching,contrastivedivergence,\\nandpseudolikelihoodforcontinuous-valuedvariables.IEEETransactionsonNeural\\nNetworks,,1529–1531. 1 8 618\\nHyvärinen,A.(2007b).Someextensionsofscorematching.ComputationalStatisticsand\\nDataAnalysis,,2499–2512. 5 1 618\\nHyvärinen,A.andHoyer,P.O.(1999).Emergenceoftopographyandcomplexcell\\npropertiesfromnaturalimagesusingextensionsofica.In,pages827–833. NIPS 493\\nHyvärinen,\\xa0A.andPajunen,\\xa0P.(1999).Nonlinearindependentcomponentanalysis:\\nExistenceanduniquenessresults.NeuralNetworks,(3),429–439. 1 2 493\\nHyvärinen,A.,Karhunen,J.,andOja,E.(2001a).IndependentComponentAnalysis.\\nWiley-Interscience.491\\nHyvärinen,A.,Hoyer,P.O.,andInki,M.O.(2001b).Topographicindependentcomponent\\nanalysis.NeuralComputation,(7),1527–1558. 1 3 493\\nHyvärinen,A.,Hurri,J.,andHoyer,P.O.(2009).NaturalImageStatistics:Aprobabilistic\\napproachtoearlycomputationalvision.Springer-Verlag.370\\nIba,Y.(2001).ExtendedensembleMonteCarlo.InternationalJournalofModernPhysics,\\nC 1 2,623–656.603\\n7 4 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f8b1735f-6695-4d70-b2e6-2ac32a81a6ed', embedding=None, metadata={'page_label': '761', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nInayoshi,\\xa0H.\\xa0and\\xa0Kurita,\\xa0T.\\xa0(2005).Improved\\xa0generalizationbyadding\\xa0both\\xa0auto-\\nassociationandhidden-layernoisetoneural-network-based-classiﬁers.IEEEWorkshop\\nonMachineLearningforSignalProcessing,pages141—-146.515\\nIoﬀe,S.andSzegedy,C.(2015).Batchnormalization:Acceleratingdeepnetworktraining\\nbyreducinginternalcovariateshift.,,100317320\\nJacobs,R.A.(1988).\\xa0Increasedratesofconvergencethroughlearningrateadaptation.\\nNeuralnetworks,(4),295–307. 1 307\\nJacobs,R.A.,Jordan,M.I.,Nowlan,S.J.,andHinton,G.E.(1991).Adaptivemixtures\\noflocalexperts.NeuralComputation,,79–87., 3189450\\nJaeger,H.(2003).Adaptivenonlinearsystemidentiﬁcationwithechostatenetworks.In\\nAdvancesinNeuralInformationProcessingSystems15.404\\nJaeger,H.(2007a).Discoveringmultiscaledynamicalfeatureswithhierarchicalechostate\\nnetworks.Technicalreport,JacobsUniversity.398\\nJaeger,H.(2007b).Echostatenetwork.Scholarpedia,(9),2330. 2 404\\nJaeger,H.(2012).Longshort-termmemoryinechostatenetworks:Detailsofasimulation\\nstudy.Technicalreport,Technicalreport,JacobsUniversityBremen.405\\nJaeger,H.andHaas,H.(2004).Harnessingnonlinearity:Predictingchaoticsystemsand\\nsavingenergyinwirelesscommunication.Science,(5667),78–80., 3 0 4 27404\\nJaeger,H.,Lukosevicius,M.,Popovici,D.,andSiewert,U.(2007).Optimizationand\\napplicationsofechostatenetworkswithleaky-integratorneurons.NeuralNetworks,\\n2 0(3),335–352.407\\nJain,V.,Murray,J.F.,Roth,F.,Turaga,S.,Zhigulin,V.,Briggman,K.L.,Helmstaedter,\\nM.N.,Denk,W.,andSeung,H.S.(2007).Supervisedlearningofimagerestoration\\nwithconvolutionalnetworks.InComputer\\xa0Vision,2007.ICCV2007.IEEE11th\\nInternationalConferenceon,pages1–8.IEEE.359\\nJaitly,N.andHinton,G.(2011).Learningabetterrepresentationofspeechsoundwaves\\nusingrestrictedBoltzmannmachines.InAcoustics,\\xa0SpeechandSignalProcessing\\n(ICASSP),2011IEEEInternationalConferenceon,pages5884–5887. IEEE.458\\nJaitly,N.andHinton,G.E.(2013).Vocaltractlengthperturbation(VTLP)improves\\nspeechrecognition.In . ICML’2013241\\nJarrett,K.,Kavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2009).Whatisthebest\\nmulti-stagearchitectureforobjectrecognition?In.,,,,,, ICCV’09162427174193226\\n363364523,,\\nJarzynski,C.(1997).Nonequilibriumequalityforfreeenergydiﬀerences.Phys.Rev.Lett.,\\n7 8,2690–2693. ,625628\\n7 4 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f30250fe-8c52-4488-9abb-0f5d9670a637', embedding=None, metadata={'page_label': '762', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nJaynes,E.T.(2003).ProbabilityTheory:TheLogicofScience.CambridgeUniversity\\nPress.53\\nJean,S.,Cho,K.,Memisevic,R.,andBengio,Y.(2014).Onusingverylargetarget\\nvocabularyforneuralmachinetranslation.arXiv:1412.2007.,474475\\nJelinek,F.andMercer,R.L.(1980).InterpolatedestimationofMarkovsourceparameters\\nfromsparsedata.InE.S.GelsemaandL.N.Kanal,editors,PatternRecognitionin\\nPractice.North-Holland,Amsterdam.,462473\\nJia,Y.(2013).Caﬀe:Anopensourceconvolutionalarchitectureforfastfeatureembedding.\\nhttp://caffe.berkeleyvision.org/.,25214\\nJia,Y.,Huang,C.,andDarrell,T.(2012).Beyondspatialpyramids:Receptiveﬁeld\\nlearningforpooledimagefeatures.InComputerVisionandPatternRecognition\\n(CVPR),2012IEEEConferenceon,pages3370–3377. IEEE.345\\nJim,K.-C.,Giles,C.L.,andHorne,B.G.(1996).Ananalysisofnoiseinrecurrentneural\\nnetworks:\\xa0convergenceandgeneralization.IEEETransactionsonNeuralNetworks,\\n7(6),1424–1438. 242\\nJordan,M.I.(1998).LearninginGraphicalModels.Kluwer,Dordrecht,Netherlands.18\\nJoulin,A.andMikolov,T.(2015).Inferringalgorithmicpatternswithstack-augmented\\nrecurrentnets.arXivpreprintarXiv:1503.01007.418\\nJozefowicz,R.,Zaremba,W.,andSutskever,I.(2015).Anempiricalevaluationofrecurrent\\nnetworkarchitectures.In ., ICML’2015306412\\nJudd,J.S.(1989).NeuralNetworkDesignandtheComplexityofLearning.MITpress.\\n293\\nJutten,\\xa0C.andHerault,\\xa0J.(1991).Blindseparationofsources,\\xa0partI:anadaptive\\nalgorithmbasedonneuromimeticarchitecture.SignalProcessing,,1–10. 2 4491\\nKahou,S.E.,Pal,C.,Bouthillier,X.,Froumenty,P.,Gülçehre,c.,Memisevic,R.,Vincent,\\nP.,Courville,A.,Bengio,Y.,Ferrari,R.C.,Mirza,M.,Jean,S.,Carrier,P.L.,Dauphin,\\nY.,Boulanger-Lewandowski,N.,Aggarwal,A.,Zumer,\\xa0J.,Lamblin,P.,Raymond,\\nJ.-P.,Desjardins,G.,Pascanu,R.,Warde-Farley,D.,Torabi,A.,Sharma,A.,Bengio,\\nE.,Côté,M.,Konda,K.R.,andWu,Z.(2013).Combiningmodalityspeciﬁcdeep\\nneuralnetworksforemotionrecognitioninvideo.InProceedingsofthe15thACMon\\nInternationalConferenceonMultimodalInteraction.201\\nKalchbrenner,N.andBlunsom,P.(2013).Recurrentcontinuoustranslationmodels.In\\nEMNLP’2013.,474475\\nKalchbrenner,N.,Danihelka,I.,andGraves,A.(2015).\\xa0Gridlongshort-termmemory.\\narXivpreprintarXiv:1507.01526.395\\n7 4 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='846fa3c2-c7e7-449f-8f96-c5a1349d5416', embedding=None, metadata={'page_label': '763', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nKamyshanska,H.andMemisevic,R.(2015).Thepotentialenergyofanautoencoder.\\nIEEETransactionsonPatternAnalysisandMachineIntelligence.515\\nKarpathy,A.andLi,F.-F.(2015).Deepvisual-semanticalignmentsforgeneratingimage\\ndescriptions.In .arXiv:1412.2306. CVPR’2015 102\\nKarpathy,A.,Toderici,G.,Shetty,S.,Leung,T.,Sukthankar,R.,andFei-Fei,L.(2014).\\nLarge-scalevideoclassiﬁcationwithconvolutionalneuralnetworks.In.CVPR21\\nKarush,W.(1939).MinimaofFunctionsofSeveralVariableswithInequalitiesasSide\\nConstraints.Master’sthesis,Dept.ofMathematics,Univ.ofChicago.95\\nKatz,S.M.(1987).Estimationofprobabilitiesfromsparsedataforthelanguagemodel\\ncomponentofaspeechrecognizer. IEEETransactionsonAcoustics,Speech,andSignal\\nProcessing, (3),400–401. , A S S P-3 5 462473\\nKavukcuoglu,K.,Ranzato,M.,andLeCun,Y.(2008).\\xa0Fastinferenceinsparsecoding\\nalgorithmswithapplicationstoobjectrecognition.Technicalreport,Computationaland\\nBiologicalLearningLab,CourantInstitute,NYU.TechReportCBLL-TR-2008-12-01.\\n523\\nKavukcuoglu,K.,Ranzato,M.-A.,Fergus,R.,andLeCun,Y.(2009).Learninginvariant\\nfeaturesthroughtopographicﬁltermaps.In . CVPR’2009523\\nKavukcuoglu,K.,Sermanet,P.,Boureau,Y.-L.,Gregor,K.,Mathieu,M.,andLeCun,Y.\\n(2010).Learningconvolutionalfeaturehierarchiesforvisualrecognition.In . NIPS’2010\\n364523,\\nKelley,H.J.(1960).Gradienttheoryofoptimalﬂightpaths. , ARSJournal 3 0(10),\\n947–954.225\\nKhan,F.,Zhu,X.,andMutlu,B.(2011).Howdohumansteach:Oncurriculumlearning\\nandteachingdimension.InAdvancesinNeuralInformationProcessingSystems24\\n(NIPS’11),pages1449–1457. 328\\nKim,S.K.,McAfee,L.C.,McMahon, P.L.,andOlukotun,K.(2009).Ahighlyscalable\\nrestrictedBoltzmannmachineFPGAimplementation.InFieldProgrammableLogic\\nandApplications,2009.FPL2009.InternationalConferenceon,pages367–372.IEEE.\\n451\\nKindermann,R.(1980).MarkovRandomFieldsandTheirApplications(Contemporary\\nMathematics;V.1).AmericanMathematicalSociety.566\\nKingma,D.andBa,J.(2014).Adam:Amethodforstochasticoptimization.arXiv\\npreprintarXiv:1412.6980.308\\nKingma,D.andLeCun,Y.(2010).Regularizedestimationofimagestatisticsbyscore\\nmatching.In ., NIPS’2010513620\\n7 4 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='77e7c78b-5c6e-4c98-926b-3ae478d8510a', embedding=None, metadata={'page_label': '764', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nKingma,D.,Rezende,D.,Mohamed,S.,andWelling,M.(2014).Semi-supervisedlearning\\nwithdeepgenerativemodels.In . NIPS’2014426\\nKingma,D.P.(2013).Fastgradient-basedinferencewithcontinuouslatentvariable\\nmodelsinauxiliaryform.Technicalreport,arxiv:1306.0733.,,652689696\\nKingma,D.P.andWelling,M.(2014a).Auto-encodingvariationalbayes.InProceedings\\noftheInternationalConferenceonLearningRepresentations(ICLR).,689700\\nKingma,\\xa0D.P.andWelling,\\xa0M.(2014b).Eﬃcientgradient-basedinferencethrough\\ntransformationsbetweenbayesnetsandneuralnets.Technicalreport,arxiv:1402.0480.\\n689\\nKirkpatrick,S.,Jr.,C.D.G.,,andVecchi,M.P.(1983).Optimizationbysimulated\\nannealing.Science,,671–680. 2 2 0 327\\nKiros,R.,Salakhutdinov,R.,andZemel,R.(2014a).Multimodalneurallanguagemodels.\\nIn . ICML’2014102\\nKiros,R.,Salakhutdinov,R.,andZemel,R.(2014b).Unifyingvisual-semanticembeddings\\nwithmultimodalneurallanguagemodels. ., arXiv:1411.2539[cs.LG]102410\\nKlementiev,A.,Titov,I.,andBhattarai,B.(2012).Inducingcrosslingualdistributed\\nrepresentationsofwords.InProceedingsofCOLING2012.,476539\\nKnowles-Barley,S.,Jones,T.R.,Morgan,J.,Lee,D.,Kasthuri,N.,Lichtman,J.W.,and\\nPﬁster,H.(2014).Deeplearningfortheconnectome.GPUTechnologyConference.26\\nKoller,D.andFriedman,\\xa0N.(2009).ProbabilisticGraphicalModels:Principlesand\\nTechniques.MITPress.,,583595645\\nKonig,Y.,Bourlard,H.,andMorgan,N.(1996).REMAP:Recursiveestimationand\\nmaximizationofaposterioriprobabilities–applicationtotransition-basedconnectionist\\nspeechrecognition.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,Advancesin\\nNeuralInformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.459\\nKoren,Y.(2009).TheBellKorsolutiontotheNetﬂixgrandprize.,258480\\nKotzias,D.,Denil,M.,deFreitas,N.,andSmyth,P.(2015).Fromgrouptoindividual\\nlabelsusingdeepfeatures.In . ACMSIGKDD106\\nKoutnik,J.,Greﬀ,K.,Gomez,F.,andSchmidhuber,J.(2014).AclockworkRNN.In\\nICML’2014.408\\nKočiský,T.,Hermann,K.M.,andBlunsom,P.(2014).LearningBilingualWordRepre-\\nsentationsbyMarginalizingAlignments.InProceedingsofACL.476\\nKrause,O.,Fischer,A.,Glasmachers,T.,andIgel,C.(2013).Approximationproperties\\nofDBNswithbinaryhiddenunitsandreal-valuedvisibleunits.In . ICML’2013553\\n7 4 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ee1e42ac-e390-4e8b-a7d5-64c79969974e', embedding=None, metadata={'page_label': '765', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nKrizhevsky,A.(2010).ConvolutionaldeepbeliefnetworksonCIFAR-10.Technicalreport,\\nUniversityofToronto.UnpublishedManuscript:http://www.cs.utoronto.ca/kriz/conv-\\ncifar10-aug2010.pdf.446\\nKrizhevsky,A.andHinton,G.(2009).Learningmultiplelayersoffeaturesfromtiny\\nimages.Technicalreport,UniversityofToronto.,21561\\nKrizhevsky,A.andHinton,G.E.(2011).Usingverydeepautoencodersforcontent-based\\nimageretrieval.In.ESANN525\\nKrizhevsky,A.,Sutskever,I.,andHinton,G.(2012).ImageNetclassiﬁcationwithdeep\\nconvolutionalneuralnetworks.In .,,,,,,, NIPS’2012232427100201371454458\\nKrueger,K.A.andDayan,P.(2009).Flexibleshaping:howlearninginsmallstepshelps.\\nCognition,,380–394. 1 1 0 328\\nKuhn,H.W.andTucker,A.W.(1951).Nonlinearprogramming.InProceedingsofthe\\nSecondBerkeleySymposiumonMathematicalStatisticsandProbability,pages481–492,\\nBerkeley,Calif.UniversityofCaliforniaPress.95\\nKumar,A.,Irsoy,O.,Su,J.,Bradbury,J.,English,R.,Pierce,B.,Ondruska,P.,Iyyer,\\nM.,Gulrajani,I.,andSocher,R.(2015).Askmeanything:Dynamicmemorynetworks\\nfornaturallanguageprocessing. ., arXiv:1506.07285418485\\nKumar,M.P.,Packer,B.,andKoller,D.(2010).Self-pacedlearningforlatentvariable\\nmodels.In . NIPS’2010328\\nLang,K.J.andHinton,G.E.(1988).Thedevelopmentofthetime-delayneuralnetwork\\narchitectureforspeechrecognition.TechnicalReportCMU-CS-88-152,Carnegie-Mellon\\nUniversity.,,367374407\\nLang,K.J.,Waibel,A.H.,andHinton,G.E.(1990).Atime-delayneuralnetwork\\narchitectureforisolatedwordrecognition.Neuralnetworks,(1),23–43. 3 374\\nLangford,J.andZhang,T.(2008).Theepoch-greedyalgorithmforcontextualmulti-armed\\nbandits.In ,pages1096––1103. NIPS’2008 480\\nLappalainen,H.,Giannakopoulos,X.,Honkela,A.,andKarhunen,J.(2000).Nonlinear\\nindependentcomponentanalysisusingensemblelearning:Experimentsanddiscussion.\\nInProc.ICA.Citeseer.493\\nLarochelle,\\xa0H.\\xa0and\\xa0Bengi o,\\xa0Y.(2008).Classiﬁcation\\xa0usingdiscriminative\\xa0restricted\\nBoltzmannmachines.In .,,,, ICML’2008244255530686716\\nLarochelle,H.andHinton,G.E.(2010).Learningtocombinefovealglimpseswitha\\nthird-orderBoltzmannmachine.InAdvancesinNeuralInformationProcessingSystems\\n23,pages1243–1251. 367\\n7 5 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8056c1f5-e25a-4b89-8327-5d00da292a65', embedding=None, metadata={'page_label': '766', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nLarochelle,H.andMurray,I.(2011).TheNeuralAutoregressiveDistributionEstimator.\\nInAISTATS’2011.,,705708709\\nLarochelle,H.,Erhan,D.,andBengio,Y.(2008).\\xa0Zero-datalearningofnewtasks.In\\nAAAIConferenceonArtiﬁcialIntelligence.539\\nLarochelle,H.,Bengio,Y.,Louradour,J.,andLamblin,P.(2009).Exploringstrategiesfor\\ntrainingdeepneuralnetworks.JournalofMachineLearningResearch,,1–40. 1 0535\\nLasserre,J.A.,Bishop,C.M.,andMinka,T.P.(2006).Principledhybridsofgenerativeand\\ndiscriminativemodels.InProceedingsoftheComputerVisionandPatternRecognition\\nConference(CVPR’06),pages87–94,Washington,DC,USA.IEEEComputerSociety.\\n244253,\\nLe,Q.,Ngiam,J.,Chen,Z.,haoChia,D.J.,Koh,P.W.,andNg,A.(2010).Tiled\\nconvolutionalneuralnetworks.InJ.Laﬀerty,C.K.I.Williams,J.Shawe-Taylor,\\nR.Zemel,andA.Culotta,editors,AdvancesinNeuralInformationProcessingSystems\\n23(NIPS’10),pages1279–1287. 352\\nLe,Q.,Ngiam,J.,Coates,A.,Lahiri,A.,Prochnow,B.,andNg,A.(2011).Onoptimization\\nmethodsfordeeplearning.InProc.ICML’2011.ACM.316\\nLe,Q.,Ranzato,M.,Monga,R.,Devin,M.,Corrado,G.,Chen,K.,Dean,J.,andNg,\\nA.(2012).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning.In\\nICML’2012.,2427\\nLeRoux,N.andBengio,Y.(2008).RepresentationalpowerofrestrictedBoltzmann\\nmachinesanddeepbeliefnetworks.NeuralComputation,(6),1631–1649. , 2 0 553655\\nLeRoux,N.andBengio,Y.(2010).Deepbeliefnetworksarecompactuniversalapproxi-\\nmators.NeuralComputation,(8),2192–2207. 2 2 553\\nLeCun,Y.(1985).Uneprocédured’apprentissagepourRéseauàseuilassymétrique.In\\nCognitiva85:AlaFrontièredel’IntelligenceArtiﬁcielle,desSciencesdelaConnaissance\\netdesNeurosciences,pages599–604,Paris1985.CESTA,Paris.225\\nLeCun,Y.(1986).Learningprocessesinanasymmetricthresholdnetwork.InF.Fogelman-\\nSoulié,E.Bienenstock,andG.Weisbuch,editors,DisorderedSystemsandBiological\\nOrganization,pages233–240.Springer-Verlag,LesHouches,France.352\\nLeCun,Y.(1987).Modèlesconnexionistesdel’apprentissage.Ph.D.thesis,Universitéde\\nParisVI.,,18502515\\nLeCun,\\xa0Y.(1989).Generalizationandnetworkdesignstrategies.TechnicalReport\\nCRG-TR-89-4,UniversityofToronto.,330352\\n7 5 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62ae8482-a04f-487a-b402-1d71ac5c1242', embedding=None, metadata={'page_label': '767', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nLeCun,Y.,Jackel,L.D.,Boser,B.,Denker,J.S.,Graf,H.P.,Guyon,I.,Henderson,D.,\\nHoward,R.E.,andHubbard,W.(1989).Handwrittendigitrecognition:Applications\\nofneuralnetworkchipsandautomaticlearning.IEEECommunicationsMagazine,\\n2 7(11),41–46.368\\nLeCun,Y.,Bottou,L.,Orr,G.B.,andMüller,K.-R.(1998a).Eﬃcientbackprop.In\\nNeuralNetworks,TricksoftheTrade,LectureNotesinComputerScienceLNCS1524.\\nSpringerVerlag.,310429\\nLeCun,Y.,Bottou,L.,Bengio,Y.,andHaﬀner,P.(1998b).Gradientbasedlearning\\nappliedtodocumentrecognition.Proc.IEEE.,,,,,, 16182127371458460\\nLeCun,\\xa0Y.,\\xa0Kavukcuoglu,\\xa0K.,\\xa0andFarabet,\\xa0C.(2010).Convolutionalnetworksand\\napplicationsinvision.\\xa0InCircuitsandSystems(ISCAS),Proceedingsof2010IEEE\\nInternationalSymposiumon,pages253–256.IEEE.371\\nL’Ecuyer,P.(1994).Eﬃciencyimprovementandvariancereduction.InProceedingsof\\nthe1994WinterSimulationConference,pages122––132.690\\nLee,C.-Y.,Xie,S.,Gallagher,P.,Zhang,Z.,andTu,Z.(2014).Deeply-supervisednets.\\narXivpreprintarXiv:1409.5185.326\\nLee,H.,Battle,A.,Raina,R.,andNg,A.(2007).Eﬃcientsparsecodingalgorithms.\\nInB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeuralInformation\\nProcessingSystems19(NIPS’06),pages801–808.MITPress.637\\nLee,H.,Ekanadham,C.,andNg,A.(2008).Sparsedeepbeliefnetmodelforvisualarea\\nV2.In.NIPS’07255\\nLee,H.,Grosse,R.,Ranganath,R.,andNg,A.Y.(2009).Convolutionaldeepbelief\\nnetworksforscalableunsupervisedlearningofhierarchicalrepresentations.InL.Bottou\\nandM.Littman,editors,ProceedingsoftheTwenty-sixthInternationalConferenceon\\nMachineLearning(ICML’09).ACM,Montreal,Canada.,,363683684\\nLee,Y.J.andGrauman,K.(2011).Learningtheeasythingsﬁrst:self-pacedvisual\\ncategorydiscovery.In . CVPR’2011328\\nLeibniz,G.W.(1676).Memoirusingthechainrule.(CitedinTMME7:2&3p321-332,\\n2010).225\\nLenat,D.B.andGuha,R.V.(1989).Buildinglargeknowledge-basedsystems;representa-\\ntionandinferenceintheCycproject.Addison-WesleyLongmanPublishingCo.,Inc.\\n2\\nLeshno,M.,Lin,V.Y.,Pinkus,A.,andSchocken,S.(1993).Multilayerfeedforward\\nnetworkswithanonpolynomialactivationfunctioncanapproximateanyfunction.\\nNeuralNetworks,,861––867. , 6 198199\\n7 5 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0cf31bde-492a-4bf8-aa5d-d51ff150b60f', embedding=None, metadata={'page_label': '768', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nLevenberg,K.(1944).Amethodforthesolutionofcertainnon-linearproblemsinleast\\nsquares.QuarterlyJournalofAppliedMathematics,(2),164–168. I I 312\\nL’Hôpital,G.F.A.(1696).Analysedesinﬁnimentpetits,pourl’intelligencedeslignes\\ncourbes.Paris:L’ImprimerieRoyale.225\\nLi,Y.,Swersky,K.,andZemel,R.S.(2015).Generativemomentmatchingnetworks.\\nCoRR, . a b s/ 1 5 0 2 .0 2 7 6 1703\\nLin,T.,Horne,B.G.,Tino,P.,andGiles,C.L.(1996).Learninglong-termdependencies\\nisnotasdiﬃcultwithNARXrecurrentneuralnetworks.IEEETransactionsonNeural\\nNetworks,(6),1329–1338. 7 407\\nLin,Y.,Liu,Z.,Sun,M.,Liu,Y.,andZhu,X.(2015).Learningentityandrelation\\nembeddingsforknowledgegraphcompletion.InProc.AAAI’15.484\\nLinde,N.(1992).Themachinethatchangedtheworld,episode3.Documentaryminiseries.\\n2\\nLindsey,C.andLindblad,T.(1994).Reviewofhardwareneuralnetworks:auser’s\\nperspective.InProc.ThirdWorkshoponNeuralNetworks:FromBiologytoHigh\\nEnergyPhysics,pages195––202,Isolad’Elba,Italy.451\\nLinnainmaa,\\xa0S.\\xa0(1976).Taylorexpansionofthe\\xa0accumulated\\xa0roundingerror.BIT\\nNumericalMathematics,(2),146–160. 1 6 225\\nLISA(2008).Deeplearningtutorials:RestrictedBoltzmannmachines.Technicalreport,\\nLISALab,UniversitédeMontréal.589\\nLong,P.M.andServedio,R.A.(2010).RestrictedBoltzmannmachinesarehardto\\napproximatelyevaluateorsimulate.InProceedingsofthe27thInternationalConference\\nonMachineLearning(ICML’10).658\\nLotter,W.,Kreiman,G.,andCox,D.(2015).Unsupervisedlearningofvisualstructure\\nusingpredictivegenerativenetworks.arXivpreprintarXiv:1511.06380.,544545\\nLovelace,A.(1842).NotesuponL.F.Menabrea’s“SketchoftheAnalyticalEngine\\ninventedbyCharlesBabbage”.1\\nLu,L.,Zhang,X.,Cho,K.,andRenals,S.(2015).Astudyoftherecurrentneuralnetwork\\nencoder-decoderforlargevocabularyspeechrecognition.InProc.Interspeech.461\\nLu,T.,Pál,D.,andPál,M.(2010).Contextualmulti-armedbandits.InInternational\\nConferenceonArtiﬁcialIntelligenceandStatistics,pages485–492.480\\nLuenberger,D.G.(1984).LinearandNonlinearProgramming.AddisonWesley.316\\nLukoševičius,M.andJaeger,H.(2009).Reservoircomputingapproachestorecurrent\\nneuralnetworktraining.ComputerScienceReview,(3),127–149. 3 404\\n7 5 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0d679a8b-1b29-4b7c-9b02-485e0d8a1d3a', embedding=None, metadata={'page_label': '769', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nLuo,H.,Shen,R.,Niu,C.,andUllrich,C.(2011).Learningclass-relevantfeaturesand\\nclass-irrelevantfeaturesviaahybridthird-orderRBM.InInternationalConferenceon\\nArtiﬁcialIntelligenceandStatistics,pages470–478.686\\nLuo,H.,Carrier,P.L.,Courville,A.,andBengio,Y.(2013).Texturemodelingwith\\nconvolutionalspike-and-slabRBMsanddeepextensions.InAISTATS’2013.102\\nLyu,S.(2009).Interpretationandgeneralizationofscorematching.InProceedingsofthe\\nTwenty-ﬁfthConferenceinUncertaintyinArtiﬁcialIntelligence(UAI’09).618\\nMa,J.,Sheridan,R.P.,Liaw,A.,Dahl,G.E.,andSvetnik,V.(2015).Deepneuralnets\\nasamethodforquantitativestructure–activityrelationships.J.Chemicalinformation\\nandmodeling.530\\nMaas,A.L.,Hannun,A.Y.,andNg,A.Y.(2013).Rectiﬁernonlinearitiesimproveneural\\nnetworkacousticmodels.InICMLWorkshoponDeepLearningforAudio,Speech,and\\nLanguageProcessing.193\\nMaass,W.(1992).Boundsforthecomputationalpowerandlearningcomplexityofanalog\\nneuralnets(extendedabstract).InProc.ofthe25thACMSymp.TheoryofComputing,\\npages335–344.199\\nMaass,W.,Schnitger,G.,andSontag,E.D.(1994).Acomparisonofthecomputational\\npowerofsigmoidandBooleanthresholdcircuits.TheoreticalAdvancesinNeural\\nComputationandLearning,pages127–151.199\\nMaass,W.,Natschlaeger,T.,andMarkram,H.(2002).Real-timecomputingwithout\\nstablestates:Anewframeworkforneuralcomputationbasedonperturbations.Neural\\nComputation,(11),2531–2560. 1 4 404\\nMacKay,D.(2003).\\xa0InformationTheory,InferenceandLearningAlgorithms.Cambridge\\nUniversityPress.73\\nMaclaurin,D.,Duvenaud,D.,andAdams,R.P.(2015).Gradient-basedhyperparameter\\noptimizationthroughreversiblelearning.arXivpreprintarXiv:1502.03492.435\\nMao,J.,Xu,W.,Yang,Y.,Wang,J.,Huang,Z.,andYuille,A.L.(2015).Deepcaptioning\\nwithmultimodalrecurrentneuralnetworks.In .arXiv:1410.1090. ICLR’2015 102\\nMarcotte,P.andSavard,G.(1992).Novelapproachestothediscriminationproblem.\\nZeitschriftfürOperationsResearch(Theory),,517–545. 3 6 276\\nMarlin,B.anddeFreitas,N.(2011).Asymptoticeﬃciencyofdeterministicestimatorsfor\\ndiscreteenergy-basedmodels:Ratiomatchingandpseudolikelihood.In ., UAI’2011617\\n619\\n7 5 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9e12939-2aad-4adb-9a6e-625b18f36a1c', embedding=None, metadata={'page_label': '770', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nMarlin,B.,Swersky,K.,Chen,B.,anddeFreitas,N.(2010).Inductiveprinciplesfor\\nrestrictedBoltzmannmachinelearning.InProceedingsofTheThirteenthInternational\\nConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’10),volume9,pages\\n509–516. ,,613618619\\nMarquardt,D.W.(1963).Analgorithmforleast-squaresestimationofnon-linearparam-\\neters.JournaloftheSocietyofIndustrialandAppliedMathematics, 1 1(2),431–441.\\n312\\nMarr,D.andPoggio,T.(1976).Cooperativecomputationofstereodisparity.Science,\\n1 9 4.367\\nMartens,\\xa0J.(2010).DeeplearningviaHessian-freeoptimization.InL.Bottouand\\nM.Littman,editors,ProceedingsoftheTwenty-seventhInternationalConferenceon\\nMachineLearning(ICML-10),pages735–742.ACM.304\\nMartens,J.andMedabalimi,V.(2014).Ontheexpressiveeﬃciencyofsumproduct\\nnetworks. . arXiv:1411.7717554\\nMartens,J.andSutskever,I.(2011).LearningrecurrentneuralnetworkswithHessian-free\\noptimization.InProc.ICML’2011.ACM.413\\nMase,S.(1995).Consistencyofthemaximumpseudo-likelihoodestimatorofcontinuous\\nstatespaceGibbsianprocesses.TheAnnalsofAppliedProbability, 5(3),pp.603–612.\\n616\\nMcClelland, J.,Rumelhart,D.,andHinton,G.(1995).Theappealofparalleldistributed\\nprocessing.InComputation&intelligence,pages305–341.AmericanAssociationfor\\nArtiﬁcialIntelligence.17\\nMcCulloch,W.S.andPitts,W.(1943).Alogicalcalculusofideasimmanentinnervous\\nactivity.BulletinofMathematicalBiophysics,,115–133. , 5 1415\\nMead,C.andIsmail,M.(2012).AnalogVLSIimplementationofneuralsystems,volume80.\\nSpringerScience&BusinessMedia.451\\nMelchior,J.,Fischer,A.,andWiskott,L.(2013).HowtocenterbinarydeepBoltzmann\\nmachines.arXivpreprintarXiv:1311.1354.674\\nMemisevic,R.andHinton,G.E.(2007).Unsupervisedlearningofimagetransformations.\\nInProceedingsoftheComputerVisionandPatternRecognitionConference(CVPR’07).\\n686\\nMemisevic,R.andHinton,G.E.(2010).Learningtorepresentspatialtransformations\\nwithfactoredhigher-orderBoltzmannmachines.NeuralComputation, 2 2(6),1473–1492.\\n686\\n7 5 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='502adeb4-8839-4a23-9c0c-1bf4071abee5', embedding=None, metadata={'page_label': '771', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nMesnil,G.,Dauphin,Y.,Glorot,X.,Rifai,S.,Bengio,Y.,Goodfellow,I.,Lavoie,E.,\\nMuller,X.,Desjardins,G.,Warde-Farley,D.,Vincent,P.,Courville,A.,andBergstra,\\nJ.(2011).Unsupervisedandtransferlearningchallenge:adeeplearningapproach.In\\nJMLRW&CP:Proc.UnsupervisedandTransferLearning,volume7.,,201532538\\nMesnil,G.,Rifai,S.,Dauphin,Y.,Bengio,Y.,andVincent,P.(2012).Surﬁngonthe\\nmanifold.LearningWorkshop,Snowbird.711\\nMiikkulainen,R.andDyer,M.G.(1991).Naturallanguageprocessingwithmodular\\nPDPnetworksanddistributedlexicon.CognitiveScience,,343–399. 1 5 477\\nMikolov,T.(2012).StatisticalLanguageModelsbasedonNeuralNetworks.Ph.D.thesis,\\nBrnoUniversityofTechnology.414\\nMikolov,T.,Deoras,A.,Kombrink,S.,Burget,L.,andCernocky,J.(2011a).Empirical\\nevaluationandcombinationofadvancedlanguagemodelingtechniques.InProc.12than-\\nnualconferenceoftheinternationalspeechcommunicationassociation(INTERSPEECH\\n2011).472\\nMikolov,T.,Deoras,A.,Povey,D.,Burget,L.,andCernocky,J.(2011b).Strategiesfor\\ntraininglargescaleneuralnetworklanguagemodels.InProc.ASRU’2011.,328472\\nMikolov,T.,Chen,K.,Corrado,G.,andDean,J.(2013a).Eﬃcientestimationofwordrep-\\nresentationsinvectorspace.InInternationalConferenceonLearningRepresentations:\\nWorkshopsTrack.536\\nMikolov,T.,Le,Q.V.,andSutskever,I.(2013b).Exploitingsimilaritiesamonglanguages\\nformachinetranslation.Technicalreport,arXiv:1309.4168.539\\nMinka,T.(2005).Divergencemeasuresandmessagepassing.MicrosoftResearchCambridge\\nUKTechRepMSRTR2005173,(TR-2005-173). 7 2 625\\nMinsky,M.L.andPapert,S.A.(1969).Perceptrons.MITPress,Cambridge.15\\nMirza,M.andOsindero,S.(2014).Conditionalgenerativeadversarialnets.arXivpreprint\\narXiv:1411.1784.702\\nMishkin,D.and\\xa0Matas,J.(2015).Allyouneedisagoodinit.arXivpreprint\\narXiv:1511.06422.305\\nMisra,J.andSaha,I.(2010).\\xa0Artiﬁcialneuralnetworksinhardware:Asurveyoftwo\\ndecadesofprogress.Neurocomputing,(1),239–255. 7 4 451\\nMitchell,T.M.(1997).MachineLearning.McGraw-Hill,NewYork.99\\nMiyato,T.,Maeda,S.,Koyama,M.,Nakae,K.,andIshii,S.(2015).Distributional\\nsmoothingwithvirtualadversarialtraining.In.Preprint:arXiv:1507.00677. ICLR 269\\n7 5 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5031fecd-f718-4762-933b-3c4eb385a9ef', embedding=None, metadata={'page_label': '772', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nMnih,A.andGregor,\\xa0K.(2014).Neuralvariationalinferenceandlearninginbelief\\nnetworks.In .,, ICML’2014691692693\\nMnih,A.andHinton,G.E.(2007).Threenewgraphicalmodelsforstatisticallanguage\\nmodelling.InZ.Ghahramani,editor,ProceedingsoftheTwenty-fourthInternational\\nConferenceonMachineLearning(ICML’07),pages641–648.ACM.465\\nMnih,A.andHinton,G.E.(2009).Ascalablehierarchicaldistributedlanguagemodel.\\nInD.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeural\\nInformationProcessingSystems21(NIPS’08),pages1081–1088. 467\\nMnih,A.andKavukcuoglu,K.(2013).Learningwordembeddingseﬃcientlywithnoise-\\ncontrastiveestimation.InC.Burges,L.Bottou,M.Welling,Z.Ghahramani,and\\nK.Weinberger,editors,AdvancesinNeuralInformationProcessingSystems26,pages\\n2265–2273. CurranAssociates,Inc.,472622\\nMnih,\\xa0A.andTeh,\\xa0Y.\\xa0W.(2012).Afastandsimple\\xa0algorithmfortrainingneural\\nprobabilisticlanguagemodels.In ,pages1751–1758. ICML’2012 472\\nMnih,V.andHinton,G.(2010).Learningtodetectroadsinhigh-resolutionaerialimages.\\nInProceedingsofthe11thEuropeanConferenceonComputerVision(ECCV).102\\nMnih,V.,Larochelle,H.,\\xa0andHinton,G.(2011).ConditionalrestrictedBoltzmann\\nmachinesforstructureoutputprediction.InProc.Conf.onUncertaintyinArtiﬁcial\\nIntelligence(UAI).685\\nMnih,V.,Kavukcuoglo,K.,Silver,D.,Graves,A.,Antonoglou,I.,andWierstra,D.(2013).\\nPlayingAtariwithdeepreinforcementlearning.Technicalreport,arXiv:1312.5602.106\\nMnih,V.,Heess,N.,Graves,A.,andKavukcuoglu,K.(2014).Recurrentmodelsofvisual\\nattention.InZ.Ghahramani,M.Welling,C.Cortes,N.Lawrence,andK.Weinberger,\\neditors, ,pages2204–2212. NIPS’2014 691\\nMnih,V.,Kavukcuoglo,K.,Silver,D.,Rusu,A.A.,Veness,J.,Bellemare,M.G.,Graves,\\nA.,Riedmiller,M.,Fidgeland,A.K.,Ostrovski,G.,Petersen,S.,Beattie,C.,Sadik,A.,\\nAntonoglou,I.,King,H.,Kumaran,D.,Wierstra,D.,Legg,S.,andHassabis,D.(2015).\\nHuman-levelcontrolthroughdeepreinforcementlearning.Nature,,529–533. 5 1 8 25\\nMobahi,H.andFisher,\\xa0III,J.W.(2015).Atheoreticalanalysisofoptimizationby\\nGaussiancontinuation.In . AAAI’2015327\\nMobahi,H.,Collobert,R.,andWeston,J.(2009).Deeplearningfromtemporalcoherence\\ninvideo.InL.BottouandM.Littman,editors,Proceedingsofthe26thInternational\\nConferenceonMachineLearning,pages737–744,Montreal.Omnipress.494\\nMohamed,A.,Dahl,G.,andHinton,G.(2009).Deepbeliefnetworksforphonerecognition.\\n459\\n7 5 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='74a7839a-0c45-4713-9407-2aab35f4ba89', embedding=None, metadata={'page_label': '773', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nMohamed,A.,Sainath,T.N.,Dahl,G.,Ramabhadran,B.,Hinton,G.E.,andPicheny,\\nM.A.(2011).Deepbeliefnetworksusingdiscriminativefeaturesforphonerecognition.In\\nAcoustics,SpeechandSignalProcessing(ICASSP),2011IEEEInternationalConference\\non,pages5060–5063. IEEE.459\\nMohamed,A.,Dahl,G.,andHinton,G.(2012a).\\xa0Acousticmodelingusingdeepbelief\\nnetworks.IEEETrans.onAudio,SpeechandLanguageProcessing, 2 0(1),14–22.459\\nMohamed,A.,Hinton,G.,andPenn,G.(2012b).Understandinghowdeepbeliefnetworks\\nperformacousticmodelling.InAcoustics,SpeechandSignalProcessing(ICASSP),\\n2012IEEEInternationalConferenceon,pages4273–4276. IEEE.459\\nMoller,M.F.(1993).Ascaledconjugategradientalgorithmforfastsupervisedlearning.\\nNeuralNetworks,,525–533. 6 316\\nMontavon,G.andMuller,K.-R.(2012).\\xa0DeepBoltzmannmachinesandthecentering\\ntrick.InG.Montavon,G.Orr,andK.-R.Müller,editors,NeuralNetworks:Tricksof\\ntheTrade,volume7700ofLectureNotesinComputerScience,pages621–637.Preprint:\\nhttp://arxiv.org/abs/1203.3783.673\\nMontúfar,G.(2014).Universalapproximationdepthanderrorsofnarrowbeliefnetworks\\nwithdiscreteunits.NeuralComputation,. 2 6553\\nMontúfar,G.andAy,N.(2011).Reﬁnementsofuniversalapproximationresultsfor\\ndeepbeliefnetworksandrestrictedBoltzmannmachines.NeuralComputation, 2 3(5),\\n1306–1319. 553\\nMontufar,G.F.,Pascanu,R.,Cho,K.,andBengio,Y.(2014).Onthenumberoflinear\\nregionsofdeepneuralnetworks.In .,, NIPS’201419199200\\nMor-Yosef,S.,Samueloﬀ,A.,Modan,B.,Navot,D.,andSchenker,J.G.(1990).Ranking\\ntheriskfactorsforcesarean:logisticregressionanalysisofanationwidestudy.Obstet\\nGynecol,(6),944–7. 7 5 3\\nMorin,F.andBengio,Y.(2005).Hierarchicalprobabilisticneuralnetworklanguage\\nmodel.InAISTATS’2005.,467469\\nMozer,M.C.(1992).Theinductionofmultiscaletemporalstructure.InJ.M.S.Hanson\\nandR.Lippmann,\\xa0editors,\\xa0AdvancesinNeural\\xa0InformationProcessingSystems4\\n(NIPS’91),pages275–282,SanMateo,CA.MorganKaufmann.,407408\\nMurphy,K.\\xa0P.(2012).MachineLearning:a\\xa0Probabilistic\\xa0Perspective.MIT\\xa0Press,\\nCambridge,MA,USA.,,6298146\\nMurray,B.U.I.andLarochelle,H.(2014).Adeepandtractabledensityestimator.In\\nICML’2014.,190710\\nNair,V.andHinton,G.(2010).RectiﬁedlinearunitsimproverestrictedBoltzmann\\nmachines.In .,, ICML’201016174197\\n7 5 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c59e88fb-0344-4aa7-82c5-eb33144634b1', embedding=None, metadata={'page_label': '774', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nNair,V.andHinton,G.E.(2009).3dobjectrecognitionwithdeepbeliefnets.InY.Bengio,\\nD.Schuurmans,J.D.Laﬀerty,C.K.I.Williams,andA.Culotta,editors,Advancesin\\nNeuralInformationProcessingSystems22,pages1339–1347. CurranAssociates,Inc.\\n686\\nNarayanan,H.andMitter,S.(2010).Samplecomplexityoftestingthemanifoldhypothesis.\\nIn . NIPS’2010164\\nNaumann,U.(2008).OptimalJacobianaccumulationisNP-complete.Mathematical\\nProgramming,(2),427–441. 1 1 2 222\\nNavigli,R.andVelardi,P.(2005).\\xa0Structuralsemanticinterconnections:aknowledge-\\nbasedapproachtowordsensedisambiguation.IEEETrans.PatternAnalysisand\\nMachineIntelligence,(7),1075––1086. 2 7 485\\nNeal,R.andHinton,G.(1999).AviewoftheEMalgorithmthatjustiﬁesincremental,\\nsparse,andothervariants.InM.I.Jordan,editor,LearninginGraphicalModels.MIT\\nPress,Cambridge,MA.634\\nNeal,R.M.(1990).Learningstochasticfeedforwardnetworks.Technicalreport.692\\nNeal,R.M.(1993).ProbabilisticinferenceusingMarkovchainMonte-Carlomethods.\\nTechnicalReportCRG-TR-93-1,Dept.ofComputerScience,UniversityofToronto.680\\nNeal,R.M.(1994).Samplingfrommultimodaldistributionsusingtemperedtransitions.\\nTechnicalReport9421,Dept.ofStatistics,UniversityofToronto.603\\nNeal,R.M.(1996).Bayesian LearningforNeuralNetworks.LectureNotesinStatistics.\\nSpringer.265\\nNeal,R.M.(2001).Annealedimportancesampling. , StatisticsandComputing 1 1(2),\\n125–139. ,,625627628\\nNeal,R.M.(2005).Estimatingratiosofnormalizingconstantsusinglinkedimportance\\nsampling.629\\nNesterov,Y.(1983).Amethodofsolvingaconvexprogrammingproblemwithconvergence\\nrate O /k ( 12). ,,372–376. SovietMathematicsDoklady 2 7 300\\nNesterov,Y.(2004).Introductorylecturesonconvexoptimization:abasiccourse.Applied\\noptimization.KluwerAcademicPubl.,Boston,Dordrecht,London.300\\nNetzer,Y.,Wang,T.,Coates,A.,Bissacco,A.,Wu,B.,andNg,A.Y.(2011).Reading\\ndigits\\xa0in\\xa0naturalimages\\xa0withunsupervised\\xa0feature\\xa0learning.Deep\\xa0Learning\\xa0and\\nUnsupervisedFeatureLearningWorkshop,NIPS.21\\nNey,H.andKneser,R.(1993).Improvedclusteringtechniquesforclass-basedstatistical\\nlanguagemodelling.InEuropeanConferenceonSpeechCommunicationandTechnology\\n(Eurospeech),pages973–976,Berlin.463\\n7 5 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0148b65c-6044-409e-b1ff-5a94b65a929d', embedding=None, metadata={'page_label': '775', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nNg,A.(2015). Adviceforapplyingmachinelearning.\\nhttps://see.stanford.edu/materials/aimlcs229/ML-advice.pdf.421\\nNiesler,T.R.,Whittaker,E.W.D.,andWoodland,P.C.(1998).Comparisonofpart-of-\\nspeechandautomaticallyderivedcategory-basedlanguagemodelsforspeechrecognition.\\nInInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),\\npages177–180.463\\nNing,F.,Delhomme,D.,LeCun,Y.,Piano,F.,Bottou,L.,andBarbano,P.E.(2005).\\nTowardautomaticphenotypingofdevelopingembryosfromvideos.ImageProcessing,\\nIEEETransactionson,(9),1360–1371. 1 4 360\\nNocedal,J.andWright,S.(2006).NumericalOptimization.Springer.,9296\\nNorouzi,M.andFleet,D.J.(2011).Minimallosshashingforcompactbinarycodes.In\\nICML’2011.525\\nNowlan,S.J.(1990).Competingexperts:Anexperimentalinvestigationofassociative\\nmixturemodels.TechnicalReportCRG-TR-90-5,UniversityofToronto.450\\nNowlan,S.J.andHinton,G.E.(1992).Simplifyingneuralnetworksbysoftweight-sharing.\\nNeuralComputation,(4),473–493. 4 139\\nOlshausen,B.andField,D.J.(2005).HowclosearewetounderstandingV1?Neural\\nComputation,,1665–1699. 1 7 16\\nOlshausen,B.A.andField,D.J.(1996).Emergenceofsimple-cellreceptiveﬁeldproperties\\nbylearningasparsecodefornaturalimages.\\xa0Nature, 3 8 1,607–609. ,,, 147255370496\\nOlshausen,B.A.,Anderson,C.H.,andVanEssen,D.C.(1993).Aneurobiological\\nmodelofvisualattentionandinvariantpatternrecognitionbasedondynamicrouting\\nofinformation.J.Neurosci.,(11),4700–4719. 1 3 450\\nOpper,M.andArchambeau,C.(2009).ThevariationalGaussianapproximationrevisited.\\nNeuralcomputation,(3),786–792. 2 1 689\\nOquab,M.,Bottou,L.,Laptev,I.,andSivic,J.(2014).Learningandtransferringmid-level\\nimagerepresentationsusingconvolutionalneuralnetworks.InComputerVisionand\\nPatternRecognition(CVPR),2014IEEEConferenceon,pages1717–1724. IEEE.536\\nOsindero,S.andHinton,G.E.(2008).Modelingimagepatcheswithadirectedhierarchy\\nofMarkovrandomﬁelds.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,\\nAdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1121–1128,\\nCambridge,MA.MITPress.632\\nOvidandMartin,C.(2004). .W.W.Norton. Metamorphoses 1\\n7 6 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b792222c-f2d9-4cc3-831b-5aa15fcb225e', embedding=None, metadata={'page_label': '776', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nPaccanaro,A.andHinton,G.E.(2000).Extractingdistributedrepresentationsofconcepts\\nandrelationsfrompositiveandnegativepropositions.InInternationalJointConference\\nonNeuralNetworks(IJCNN),Como,Italy.IEEE,NewYork.484\\nPaine,T.L.,Khorrami,P.,Han,W.,andHuang,T.S.(2014).Ananalysisofunsupervised\\npre-traininginlightofrecentadvances.arXivpreprintarXiv:1412.6597.532\\nPalatucci,M.,Pomerleau,D.,Hinton,G.E.,andMitchell,T.M.(2009).Zero-shot\\nlearningwithsemanticoutputcodes.InY.Bengio,D.Schuurmans,J.D.Laﬀerty,\\nC.K.I.Williams,andA.Culotta,editors,AdvancesinNeuralInformationProcessing\\nSystems22,pages1410–1418. CurranAssociates,Inc.539\\nParker,D.B.(1985).Learning-logic.TechnicalReportTR-47,CenterforComp.Research\\ninEconomicsandManagementSci.,MIT.225\\nPascanu,R.,Mikolov,T.,andBengio,Y.(2013).Onthediﬃcultyoftrainingrecurrent\\nneuralnetworks.In .,,,,, ICML’2013289402403408414416\\nPascanu,R.,Gülçehre,Ç.,Cho,K.,andBengio,Y.(2014a).Howtoconstructdeep\\nrecurrentneuralnetworks.In .,,,,, ICLR’201419265398399410460\\nPascanu,R.,Montufar,G.,andBengio,Y.(2014b).Onthenumberofinferenceregions\\nofdeepfeedforwardnetworkswithpiece-wiselinearactivations.In . ICLR’2014550\\nPati,Y.,Rezaiifar,R.,andKrishnaprasad,P.(1993).Orthogonalmatchingpursuit:\\nRecursivefunctionapproximationwithapplicationstowaveletdecomposition.InPro-\\nceedingsofthe27thAnnualAsilomarConferenceonSignals,Systems,andComputers,\\npages40–44.255\\nPearl,J.(1985).Bayesiannetworks:Amodelofself-activatedmemoryforevidential\\nreasoning.In\\xa0Proceedingsofthe7thConferenceofthe\\xa0CognitiveScience\\xa0Society,\\nUniversityofCalifornia,Irvine,pages329–334.563\\nPearl,J.(1988).\\xa0ProbabilisticReasoninginIntelligentSystems:\\xa0NetworksofPlausible\\nInference.MorganKaufmann.54\\nPerron,O.(1907).Zurtheoriedermatrices.MathematischeAnnalen, 6 4(2),248–263.597\\nPetersen,K.B.andPedersen,M.S.(2006).Thematrixcookbook.Version20051003.31\\nPeterson,G.B.(2004).Adayofgreatillumination:B.F.Skinner’sdiscoveryofshaping.\\nJournaloftheExperimentalAnalysisofBehavior,(3),317–328. 8 2 328\\nPham,D.-T.,Garat,P.,andJutten,C.(1992).Separationofamixtureofindependent\\nsourcesthroughamaximumlikelihoodapproach.In ,pages771–774. EUSIPCO 491\\n7 6 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='59ace915-b2dc-4a1a-83a3-4161dbdc8a10', embedding=None, metadata={'page_label': '777', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nPham,P.-H.,Jelaca,D.,Farabet,C.,Martini,B.,LeCun,Y.,andCulurciello,E.(2012).\\nNeuFlow:dataﬂowvisionprocessingsystem-on-a-chip.InCircuitsandSystems(MWS-\\nCAS),2012IEEE55thInternationalMidwestSymposiumon,pages1044–1047. IEEE.\\n451\\nPinheiro,P.H.O.andCollobert,R.(2014).Recurrentconvolutionalneuralnetworksfor\\nscenelabeling.In . ICML’2014359\\nPinheiro,P.H.O.andCollobert,R.(2015).Fromimage-leveltopixel-levellabelingwith\\nconvolutionalnetworks.InConferenceonComputerVisionandPatternRecognition\\n(CVPR).359\\nPinto,N.,Cox,D.D.,andDiCarlo,J.J.(2008).Whyisreal-worldvisualobjectrecognition\\nhard?PLoSComputBiol,. 4456\\nPinto,N.,Stone,Z.,Zickler,T.,andCox,D.(2011).Scalingupbiologically-inspired\\ncomputervision:Acasestudyinunconstrainedfacerecognition onfacebook.In\\nComputerVisionandPatternRecognitionWorkshops(CVPRW),2011IEEEComputer\\nSocietyConferenceon,pages35–42.IEEE.363\\nPollack,J.B.(1990).Recursivedistributedrepresentations.ArtiﬁcialIntelligence, 4 6(1),\\n77–105.401\\nPolyak,B.andJuditsky,A.(1992).Accelerationofstochasticapproximationbyaveraging.\\nSIAMJ.ControlandOptimization,,838–855. 3 0 ( 4 ) 322\\nPolyak,B.T.(1964).Somemethodsofspeedinguptheconvergenceofiterationmethods.\\nUSSRComputationalMathematicsandMathematicalPhysics,(5),1–17. 4 296\\nPoole,B.,Sohl-Dickstein,J.,andGanguli,S.(2014).\\xa0Analyzingnoiseinautoencoders\\nanddeepnetworks., . CoRR a b s/ 1 4 0 6 .1 8 3 1241\\nPoon,H.andDomingos,P.(2011).Sum-productnetworks:Anewdeeparchitecture.In\\nProceedingsoftheTwenty-seventhConferenceinUncertaintyinArtiﬁcialIntelligence\\n(UAI),Barcelona,Spain.554\\nPresley,R.K.andHaggard,R.L.(1994).Aﬁxedpointimplementationofthebackpropa-\\ngationlearningalgorithm.InSoutheastcon’94.CreativeTechnologyTransfer-AGlobal\\nAﬀair.,Proceedingsofthe1994IEEE,pages136–138.IEEE.451\\nPrice,R.(1958).AusefultheoremfornonlineardeviceshavingGaussianinputs.IEEE\\nTransactionsonInformationTheory,(2),69–72. 4 689\\nQuiroga,R.Q.,Reddy,L.,Kreiman,G.,Koch,C.,andFried,I.(2005).Invariantvisual\\nrepresentationbysingleneuronsinthehumanbrain.Nature, 4 3 5(7045),1102–1107.\\n366\\n7 6 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='421696ba-9cf0-42a5-bada-495c8b203c43', embedding=None, metadata={'page_label': '778', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nRadford,A.,Metz,L.,andChintala,S.(2015).Unsupervisedrepresentationlearningwith\\ndeepconvolutionalgenerativeadversarialnetworks.arXivpreprintarXiv:1511.06434.\\n552701702,,\\nRaiko,T.,Yao,L.,Cho,K.,andBengio,\\xa0Y.(2014).Iterativeneuralautoregressive\\ndistributionestimator(NADE-k).Technicalreport,arXiv:1406.1485.,676709\\nRaina,R.,Madhavan,A.,andNg,A.Y.(2009).Large-scaledeepunsupervisedlearning\\nusinggraphicsprocessors.\\xa0InL.BottouandM.Littman,editors,Proceedingsofthe\\nTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages873–880,\\nNewYork,NY,USA.ACM.,27446\\nRamsey,F.P.(1926).Truthandprobability.InR.B.Braithwaite,editor,TheFoundations\\nofMathematicsandotherLogicalEssays,chapter7,pages156–198.McMasterUniversity\\nArchivefortheHistoryofEconomicThought.56\\nRanzato,M.andHinton,G.H.(2010).Modelingpixelmeansandcovariancesusing\\nfactorizedthird-orderBoltzmannmachines.In ,pages2551–2558. CVPR’2010 680\\nRanzato,M.,Poultney,C.,Chopra,S.,andLeCun,Y.(2007a).Eﬃcientlearningofsparse\\nrepresentationswithanenergy-basedmodel.In .,,,, NIPS’20061419507528530\\nRanzato,M.,Huang,F.,Boureau,Y.,andLeCun,Y.(2007b).Unsupervisedlearningof\\ninvariantfeaturehierarchieswithapplicationstoobjectrecognition.InProceedingsof\\ntheComputerVisionandPatternRecognitionConference(CVPR’07).IEEEPress.364\\nRanzato,M.,Boureau,Y.,andLeCun,Y.(2008).Sparsefeaturelearningfordeepbelief\\nnetworks.In . NIPS’2007507\\nRanzato,M.,Krizhevsky,A.,andHinton,G.E.(2010a).Factored3-wayrestricted\\nBoltzmannmachinesformodelingnaturalimages.InProceedingsofAISTATS2010.\\n678679,\\nRanzato,M.,Mnih,V.,andHinton,G.(2010b).Generatingmorerealisticimagesusing\\ngatedMRFs.In . NIPS’2010680\\nRao,C.(1945).Informationandtheaccuracyattainableintheestimationofstatistical\\nparameters.BulletinoftheCalcuttaMathematicalSociety,,81–89., 3 7135295\\nRasmus,A.,Valpola,H.,Honkala,M.,Berglund,M.,andRaiko,T.(2015).Semi-supervised\\nlearningwithladdernetwork.arXivpreprintarXiv:1507.02672.,426530\\nRecht,B.,Re,C.,Wright,S.,andNiu,F.(2011).Hogwild:Alock-freeapproachto\\nparallelizingstochasticgradientdescent.In . NIPS’2011447\\nReichert,D.P.,Seriès,P.,andStorkey,A.J.(2011).Neuronaladaptationforsampling-\\nbasedprobabilisticinferenceinperceptualbistability.InAdvancesinNeuralInformation\\nProcessingSystems,pages2357–2365. 666\\n7 6 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cebef893-9028-4a77-aec8-0e1341321d94', embedding=None, metadata={'page_label': '779', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nRezende,D.J.,Mohamed,S.,andWierstra,D.(2014).Stochasticbackpropagation\\nand\\xa0approximateinferencein\\xa0deepgenerative\\xa0models.In\\xa0 .Preprint: ICML’2014\\narXiv:1401.4082.,,652689696\\nRifai,\\xa0S.,\\xa0Vincent,\\xa0P.,\\xa0Muller,X.,\\xa0Glorot,\\xa0X.,andBengio,Y.(2011a).Contractive\\nauto-encoders:Explicitinvarianceduringfeatureextraction.In .,, ICML’2011521522\\n523\\nRifai,S.,Mesnil,G.,Vincent,P.,Muller,X.,Bengio,Y.,Dauphin,Y.,andGlorot,X.\\n(2011b).Higherordercontractiveauto-encoder.In ., ECMLPKDD521522\\nRifai,S.,Dauphin,Y.,Vincent,P.,Bengio,Y.,andMuller,X.(2011c).\\xa0Themanifold\\ntangentclassiﬁer.In .,, NIPS’2011271272523\\nRifai,S.,Bengio,Y.,Dauphin,Y.,andVincent,P.(2012).Agenerativeprocessfor\\nsamplingcontractiveauto-encoders.In . ICML’2012711\\nRingach,D.andShapley,R.(2004).Reversecorrelationinneurophysiology.Cognitive\\nScience,(2),147–166. 2 8 368\\nRoberts,S.andEverson,R.(2001).Independentcomponentanalysis:principlesand\\npractice.CambridgeUniversityPress.493\\nRobinson,A.J.andFallside,F.(1991).Arecurrenterrorpropagationnetworkspeech\\nrecognitionsystem.ComputerSpeechandLanguage,(3),259–274. , 5 27459\\nRockafellar,R.T.(1997).Convexanalysis.princetonlandmarksinmathematics.93\\nRomero,A.,Ballas,N.,EbrahimiKahou,S.,Chassang,A.,Gatta,C.,andBengio,Y.\\n(2015).Fitnets:Hintsforthindeepnets.In . ICLR’2015,arXiv:1412.6550325\\nRosen,J.B.(1960).Thegradientprojectionmethodfornonlinearprogramming.parti.\\nlinearconstraints.JournaloftheSocietyforIndustrialandAppliedMathematics, 8(1),\\npp.181–217.93\\nRosenblatt,F.(1958).Theperceptron:Aprobabilisticmodelforinformationstorageand\\norganizationinthebrain.PsychologicalReview,,386–408. ,, 6 5 141527\\nRosenblatt,F.(1962).PrinciplesofNeurodynamics.Spartan,NewYork.,1527\\nRoweis,S.andSaul,L.K.(2000).Nonlineardimensionalityreductionbylocallylinear\\nembedding.Science,(5500)., 2 9 0164518\\nRoweis,S.,Saul,L.,andHinton,G.(2002).Globalcoordinationoflocallinearmodels.In\\nT.Dietterich,S.Becker,andZ.Ghahramani,editors,AdvancesinNeuralInformation\\nProcessingSystems14(NIPS’01),Cambridge,MA.MITPress.489\\nRubin,D.B.(1984).Bayesianlyjustiﬁableandrelevantfrequencycalculationsfor etal.\\ntheappliedstatistician. ,(4),1151–1172. TheAnnalsofStatistics 1 2 717\\n7 6 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4d3098fe-e4b9-4499-a091-ef6757c10027', embedding=None, metadata={'page_label': '780', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nRumelhart,\\xa0D.,\\xa0Hinton,\\xa0G.,\\xa0andWilliams,\\xa0R. (1986a).Learningrepresentationsby\\nback-propagatingerrors.Nature,,533–536. ,,,,,,, 3 2 3 141823204225373476482\\nRumelhart,D.E.,Hinton,G.E.,andWilliams,R.J.(1986b).Learninginternalrepresen-\\ntationsbyerrorpropagation.InD.E.RumelhartandJ.L.McClelland, editors,Parallel\\nDistributedProcessing,volume1,chapter8,pages318–362.MITPress,Cambridge.,21\\n27225,\\nRumelhart,D.E.,McClelland, J.L.,andthePDPResearchGroup(1986c).Parallel\\nDistributedProcessing:ExplorationsintheMicrostructureofCognition.MITPress,\\nCambridge.17\\nRussakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,\\nA.,Khosla,A.,Bernstein,M.,Berg,A.C.,andFei-Fei,L.(2014a).ImageNetLarge\\nScaleVisualRecognitionChallenge.21\\nRussakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,\\nA.,Khosla,A.,Bernstein,M.,(2014b).Imagenetlargescalevisualrecognition etal.\\nchallenge.arXivpreprintarXiv:1409.0575.28\\nRussel,S.J.andNorvig,P.(2003).ArtiﬁcialIntelligence:aModernApproach.Prentice\\nHall.86\\nRust,N.,Schwartz,O.,Movshon,J.A.,andSimoncelli,E.(2005).Spatiotemporal\\nelementsofmacaqueV1receptiveﬁelds.Neuron,(6),945–956. 4 6 367\\nSainath,T.,Mohamed,A.,Kingsbury,B.,andRamabhadran,B.(2013).Deepconvolu-\\ntionalneuralnetworksforLVCSR.In . ICASSP2013460\\nSalakhutdinov,R.(2010).LearninginMarkovrandomﬁeldsusingtemperedtransitions.In\\nY.Bengio,D.Schuurmans,C.Williams,J.Laﬀerty,andA.Culotta,editors,Advances\\ninNeuralInformationProcessingSystems22(NIPS’09).603\\nSalakhutdinov,R.andHinton,G.(2009a).DeepBoltzmannmachines.InProceedingsof\\ntheInternationalConferenceonArtiﬁcialIntelligenceandStatistics,volume5,pages\\n448–455. ,,,,,, 2427529663666671672\\nSalakhutdinov,R.andHinton,G.(2009b).Semantichashing.InInternationalJournalof\\nApproximateReasoning.525\\nSalakhutdinov,\\xa0R.\\xa0andHinton,\\xa0G.\\xa0E.(2007a).Learning\\xa0a\\xa0nonlinearembedding\\xa0by\\npreservingclassneighbourhoodstructure.InProceedingsoftheEleventhInternational\\nConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS’07),SanJuan,Porto\\nRico.Omnipress.527\\nSalakhutdinov,R.andHinton,G.E.(2007b).Semantichashing.In . SIGIR’2007525\\n7 6 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cb516518-74d4-4c63-94db-8bd71cede258', embedding=None, metadata={'page_label': '781', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nSalakhutdinov,R.andHinton,G.E.(2008).Usingdeepbeliefnetstolearncovariance\\nkernelsforGaussianprocesses.InJ.Platt,D.Koller,Y.Singer,andS.Roweis,editors,\\nAdvancesinNeuralInformationProcessingSystems20(NIPS’07),pages1249–1256,\\nCambridge,MA.MITPress.244\\nSalakhutdinov,R.andLarochelle,H.(2010).EﬃcientlearningofdeepBoltzmannmachines.\\nInProceedingsoftheThirteenthInternationalConferenceonArtiﬁcialIntelligenceand\\nStatistics(AISTATS2010),JMLRW&CP,volume9,pages693–700.652\\nSalakhutdinov,R.andMnih,A.(2008).Probabilisticmatrixfactorization.In . NIPS’2008\\n480\\nSalakhutdinov,R.andMurray,I.(2008).Onthequantitativeanalysisofdeepbelief\\nnetworks.InW.W.Cohen,A.McCallum, andS.T.Roweis,editors,Proceedingsof\\ntheTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),volume25,\\npages872–879.ACM.,628662\\nSalakhutdinov,R.,Mnih,A.,andHinton,G.(2007).RestrictedBoltzmannmachinesfor\\ncollaborativeﬁltering.In.ICML480\\nSanger,\\xa0T.D.\\xa0(1994).Neuralnetworklearningcontrolofrobotmanipulatorsusing\\ngraduallyincreasingtaskdiﬃculty.IEEETransactionsonRoboticsandAutomation,\\n1 0(3).328\\nSaul,L.K.andJordan,M.I.(1996).Exploitingtractablesubstructuresinintractable\\nnetworks.InD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeural\\nInformationProcessingSystems8(NIPS’95).MITPress,Cambridge,MA.638\\nSaul,L.K.,Jaakkola,T.,andJordan,M.I.(1996).Meanﬁeldtheoryforsigmoidbelief\\nnetworks.JournalofArtiﬁcialIntelligenceResearch,,61–76., 427693\\nSavich,A.W.,Moussa,M.,andAreibi,S.(2007).Theimpactofarithmeticrepresentation\\nonimplementingmlp-bponfpgas:Astudy.NeuralNetworks,IEEETransactionson,\\n1 8(1),240–252.451\\nSaxe,A.M.,Koh,P.W.,Chen,Z.,Bhand,M.,Suresh,B.,andNg,A.(2011).Onrandom\\nweightsandunsupervisedfeaturelearning.InProc.ICML’2011.ACM.363\\nSaxe,A.M.,McClelland, J.L.,andGanguli,S.(2013).Exactsolutionstothenonlinear\\ndynamicsoflearningindeeplinearneuralnetworks.In.,, ICLR285286303\\nSchaul,T.,Antonoglou,I.,andSilver,D.(2014).Unittestsforstochasticoptimization.\\nInInternationalConferenceonLearningRepresentations.309\\nSchmidhuber,J.(1992).Learningcomplex,extendedsequencesusingtheprincipleof\\nhistorycompression.NeuralComputation,(2),234–242. 4 398\\nSchmidhuber,J.(1996).Sequentialneuraltextcompression.IEEETransactionsonNeural\\nNetworks,(1),142–146. 7 477\\n7 6 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e4a4d346-a85e-4554-9096-01678a8824c3', embedding=None, metadata={'page_label': '782', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nSchmidhuber,J.(2012).Self-delimitingneuralnetworks.arXivpreprintarXiv:1210.0118.\\n390\\nSchölkopf,B.andSmola,A.J.(2002).Learningwithkernels:Supportvectormachines,\\nregularization,optimization,andbeyond.MITpress.704\\nSchölkopf,B.,Smola,A.,andMüller,K.-R.(1998).Nonlinearcomponentanalysisasa\\nkerneleigenvalueproblem.NeuralComputation,,1299–1319. , 1 0 164518\\nSchölkopf,B.,Burges,C.J.C.,andSmola,A.J.(1999).AdvancesinKernelMethods—\\nSupportVectorLearning.MITPress,Cambridge,MA.,18142\\nSchölkopf,B.,Janzing,D.,Peters,J.,Sgouritsa,E.,Zhang,K.,andMooij,J.(2012).On\\ncausalandanticausallearning.In ,pages1255–1262. ICML’2012 545\\nSchuster,M.(1999).Onsupervisedlearningfromsequentialdatawithapplicationsfor\\nspeechrecognition.190\\nSchuster,M.andPaliwal,K.(1997).Bidirectionalrecurrentneuralnetworks.IEEE\\nTransactionsonSignalProcessing,(11),2673–2681. 4 5 395\\nSchwenk,H.(2007).Continuousspacelanguagemodels.Computerspeechandlanguage,\\n2 1,492–518.466\\nSchwenk,H.(2010).Continuousspacelanguagemodelsforstatisticalmachinetranslation.\\nThePragueBulletinofMathematicalLinguistics,,137–146. 9 3 473\\nSchwenk,H.(2014).CleanedsubsetofWMT’14dataset.21\\nSchwenk,H.andBengio,Y.(1998).Trainingmethodsforadaptiveboostingofneuralnet-\\nworks.InM.Jordan,M.Kearns,andS.Solla,editors,AdvancesinNeuralInformation\\nProcessingSystems10(NIPS’97),pages647–653.MITPress.258\\nSchwenk,\\xa0H.andGauvain,\\xa0J.-L.(2002).Connectionistlanguagemodeling\\xa0forlarge\\nvocabularycontinuousspeechrecognition.InInternationalConferenceonAcoustics,\\nSpeechandSignalProcessing(ICASSP),pages765–768,Orlando,Florida.466\\nSchwenk,H.,Costa-jussà,M.R.,andFonollosa,J.A.R.(2006).Continuousspace\\nlanguagemodelsfortheIWSLT2006task.InInternationalWorkshoponSpoken\\nLanguageTranslation,pages166–173.473\\nSeide,F.,Li,G.,andYu,D.(2011).Conversationalspeechtranscriptionusingcontext-\\ndependentdeepneuralnetworks.InInterspeech2011,pages437–440.23\\nSejnowski,T.(1987).Higher-orderBoltzmannmachines.InAIPConferenceProceedings\\n151onNeuralNetworksforComputing,pages398–403.AmericanInstituteofPhysics\\nInc.686\\n7 6 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='939e403e-132e-4310-a80c-e899a3ee40ed', embedding=None, metadata={'page_label': '783', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nSeries,P.,Reichert,D.P.,andStorkey,A.J.(2010).HallucinationsinCharlesBonnet\\nsyndromeinducedbyhomeostasis:adeepBoltzmannmachinemodel.InAdvancesin\\nNeuralInformationProcessingSystems,pages2020–2028. 666\\nSermanet,P.,Chintala,S.,andLeCun,Y.(2012).Convolutionalneuralnetworksapplied\\ntohousenumbersdigitclassiﬁcation., . CoRR a b s/ 1 2 0 4 .3 9 6 8457\\nSermanet,P.,Kavukcuoglu,K.,Chintala,S.,andLeCun,Y.(2013).Pedestriandetection\\nwithunsupervisedmulti-stagefeaturelearning.InProc.InternationalConferenceon\\nComputerVisionandPatternRecognition(CVPR’13).IEEE.,23201\\nShilov,G.(1977).LinearAlgebra.DoverBooksonMathematicsSeries.DoverPublications.\\n31\\nSiegelmann,H.(1995).ComputationbeyondtheTuringlimit.Science, 2 6 8(5210),\\n545–548.379\\nSiegelmann,H.andSontag,E.(1991).Turingcomputabilitywithneuralnets.Applied\\nMathematicsLetters,(6),77–80. 4 379\\nSiegelmann,H.T.andSontag,E.D.(1995).Onthecomputationalpowerofneuralnets.\\nJournalofComputerandSystemsSciences,(1),132–150. , 5 0 379403\\nSietsma,J.andDow,R.(1991).Creatingartiﬁcialneuralnetworksthatgeneralize.Neural\\nNetworks,(1),67–79. 4 241\\nSimard,D.,Steinkraus,P.Y.,andPlatt,J.C.(2003).Bestpracticesforconvolutional\\nneuralnetworks.In . ICDAR’2003371\\nSimard,P.andGraf,H.P.(1994).Backpropagationwithoutmultiplication.InAdvances\\ninNeuralInformationProcessingSystems,pages232–239.451\\nSimard,P.,Victorri,B.,LeCun,Y.,andDenker,J.(1992).Tangentprop-Aformalism\\nforspecifyingselectedinvariancesinanadaptivenetwork.In .,,, NIPS’1991270271272\\n356\\nSimard,P.Y.,LeCun,Y.,andDenker,J.(1993).Eﬃcientpatternrecognitionusinga\\nnewtransformationdistance.In.NIPS’92270\\nSimard,P.Y.,LeCun,Y.A.,Denker,J.S.,andVictorri,B.(1998).Transformation\\ninvarianceinpatternrecognition—tangentdistanceandtangentpropagation.Lecture\\nNotesinComputerScience,. 1 5 2 4270\\nSimons,D.J.andLevin,D.T.(1998).Failuretodetectchangestopeopleduringa\\nreal-worldinteraction.PsychonomicBulletin&Review,(4),644–649. 5 543\\nSimonyan,K.andZisserman,A.(2015).Verydeepconvolutionalnetworksforlarge-scale\\nimagerecognition.In.ICLR323\\n7 6 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d71948c9-19c0-446c-8ed0-5943118566a2', embedding=None, metadata={'page_label': '784', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nSjöberg,J.andLjung,L.(1995).Overtraining,regularizationandsearchingforaminimum,\\nwithapplicationtoneuralnetworks.InternationalJournalofControl, 6 2(6),1391–1407.\\n250\\nSkinner,B.F.(1958).Reinforcementtoday.AmericanPsychologist,,94–99. 1 3328\\nSmolensky,P.(1986).Informationprocessingindynamicalsystems:Foundationsof\\nharmonytheory.InD.E.RumelhartandJ.L.McClelland, editors,ParallelDistributed\\nProcessing,volume1,chapter6,pages194–281.MITPress,Cambridge.,,571587656\\nSnoek,J.,Larochelle,H.,andAdams,R.P.(2012).PracticalBayesianoptimizationof\\nmachinelearningalgorithms.In . NIPS’2012436\\nSocher,R.,Huang,E.H.,Pennington,J.,Ng,A.Y.,andManning,C.D.(2011a).Dynamic\\npoolingandunfoldingrecursiveautoencodersforparaphrasedetection.In . NIPS’2011\\n401\\nSocher,R.,Manning,C.,andNg,A.Y.(2011b).Parsingnaturalscenesandnaturallan-\\nguagewithrecursiveneuralnetworks.InProceedingsoftheTwenty-EighthInternational\\nConferenceonMachineLearning(ICML’2011).401\\nSocher,\\xa0R.,\\xa0Pennington,\\xa0J.,\\xa0Huang,\\xa0E.H.,\\xa0Ng,\\xa0A. Y.,andManning,\\xa0C.D.(2011c).\\nSemi-supervisedrecursiveautoencoders\\xa0forpredictingsentimentdistributions.In\\nEMNLP’2011.401\\nSocher,R.,Perelygin,A.,Wu,J.Y.,Chuang,J.,Manning,C.D.,Ng,A.Y.,andPotts,\\nC.(2013a).Recursivedeepmodelsforsemanticcompositionalityoverasentiment\\ntreebank.In . EMNLP’2013401\\nSocher,R.,Ganjoo,M.,Manning,C.D.,andNg,A.Y.(2013b).Zero-shotlearningthrough\\ncross-modaltransfer.In27thAnnualConferenceonNeuralInformationProcessing\\nSystems(NIPS2013).539\\nSohl-Dickstein,J.,Weiss,E.A.,Maheswaranathan,N.,andGanguli,S.(2015).Deep\\nunsupervisedlearningusingnonequilibriumthermodynamics.716\\nSohn,K.,Zhou,G.,andLee,H.(2013).Learningandselectingfeaturesjointlywith\\npoint-wisegatedBoltzmannmachines.In . ICML’2013687\\nSolomonoﬀ,R.J.(1989).Asystemforincrementallearningbasedonalgorithmicproba-\\nbility.328\\nSontag,E.D.(1998).VCdimensionofneuralnetworks.NATOASISeriesFComputer\\nandSystemsSciences,,69–96., 1 6 8547551\\nSontag,E.D.andSussman,H.J.(1989).Backpropagationcangiverisetospuriouslocal\\nminimaevenfornetworkswithouthiddenlayers. ,,91–106. ComplexSystems 3 284\\n7 6 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3115bcd0-4f8a-4264-a614-c56a849d6808', embedding=None, metadata={'page_label': '785', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nSparkes,B.(1996).TheRedandtheBlack:StudiesinGreekPottery.Routledge.1\\nSpitkovsky,V.I.,Alshawi,H.,andJurafsky,D.(2010).Frombabystepstoleapfrog:how\\n“lessismore”inunsuperviseddependencyparsing.InHLT’10.328\\nSquire,W.andTrapp,G.(1998).\\xa0Usingcomplexvariablestoestimatederivativesofreal\\nfunctions.SIAMRev.,(1),110––112. 4 0 439\\nSrebro,N.andShraibman,A.(2005).Rank,trace-normandmax-norm.InProceedingsof\\nthe18thAnnualConferenceonLearningTheory,pages545–560.Springer-Verlag.238\\nSrivastava,N.(2013).ImprovingNeuralNetworksWithDropout.Master’sthesis,U.\\nToronto.535\\nSrivastava,N.andSalakhutdinov,R.(2012).MultimodallearningwithdeepBoltzmann\\nmachines.In . NIPS’2012541\\nSrivastava,N.,Salakhutdinov,R.R.,andHinton,G.E.(2013).Modelingdocumentswith\\ndeepBoltzmannmachines.arXivpreprintarXiv:1309.6865.663\\nSrivastava,N.,Hinton,G.,Krizhevsky,A.,Sutskever,I.,andSalakhutdinov,R.(2014).\\nDropout:Asimplewaytopreventneuralnetworksfromoverﬁtting.JournalofMachine\\nLearningResearch,,1929–1958. ,,, 1 5 258265267672\\nSrivastava,R.K.,Greﬀ,K.,andSchmidhuber,J.(2015).Highwaynetworks.\\narXiv:1505.00387.326\\nSteinkrau,D.,Simard,P.Y.,andBuck,I.(2005).UsingGPUsformachinelearning\\nalgorithms.201312thInternationalConferenceonDocumentAnalysisandRecognition,\\n0,1115–1119. 445\\nStoyanov,V.,Ropson,A.,andEisner,J.(2011).Empiricalriskminimizationofgraphical\\nmodelparametersgivenapproximateinference,decoding,andmodelstructure.In\\nProceedingsofthe14thInternationalConferenceonArtiﬁcialIntelligenceandStatistics\\n(AISTATS) JMLRWorkshopandConferenceProceedings ,volume15of ,pages725–733,\\nFortLauderdale.Supplementarymaterial(4pages)alsoavailable.,674698\\nSukhbaatar,S.,Szlam,A.,Weston,J.,andFergus,R.(2015).Weaklysupervisedmemory\\nnetworks.arXivpreprintarXiv:1503.08895.418\\nSupancic,J.andRamanan,D.(2013).Self-pacedlearningforlong-termtracking.In\\nCVPR’2013.328\\nSussillo,D.(2014).Randomwalks:Trainingverydeepnonlinearfeed-forwardnetworks\\nwithsmartinitialization., .,,, CoRR a b s/ 1 4 1 2 .6 5 5 8290303305403\\nSutskever,I.(2012).TrainingRecurrentNeuralNetworks.Ph.D.thesis,Departmentof\\ncomputerscience,UniversityofToronto.,406413\\n7 7 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d7108e0-acc5-488a-b4bc-73aa197ed48f', embedding=None, metadata={'page_label': '786', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nSutskever,I.andHinton,G.E.(2008).Deepnarrowsigmoidbeliefnetworksareuniversal\\napproximators.NeuralComputation,(11),2629–2636. 2 0 693\\nSutskever,I.andTieleman,T.(2010).OntheConvergencePropertiesofContrastive\\nDivergence.InY.W.TehandM.Titterington,editors,Proc.oftheInternational\\nConferenceonArtiﬁcialIntelligenceandStatistics(AISTATS),volume9,pages789–795.\\n612\\nSutskever,I.,Hinton,G.,andTaylor,G.(2009).Therecurrenttemporalrestricted\\nBoltzmannmachine.In . NIPS’2008685\\nSutskever,I.,Martens,J.,andHinton,G.E.(2011).Generatingtextwithrecurrent\\nneuralnetworks.In ,pages1017–1024. ICML’2011 477\\nSutskever,\\xa0I.,Martens,J.,Dahl,\\xa0G.,andHinton,G.(2013).Ontheimportanceof\\ninitializationandmomentumindeeplearning.In.,, ICML300406413\\nSutskever,I.,Vinyals,O.,andLe,Q.V.(2014).Sequencetosequencelearningwith\\nneuralnetworks.In .,,,,,, NIPS’2014,arXiv:1409.321525101397410411474475\\nSutton,R.andBarto,A.(1998).ReinforcementLearning:AnIntroduction.MITPress.\\n106\\nSutton,R.S.,Mcallester,D.,Singh,S.,andMansour,Y.(2000).Policygradientmethods\\nforreinforcementlearningwithfunctionapproximation.In ,pages1057– NIPS’1999\\n–1063.MITPress.691\\nSwersky,K.,Ranzato,M.,Buchman,D.,Marlin,B.,anddeFreitas,N.(2011).On\\nautoencodersandscorematchingforenergybasedmodels.In .ACM. ICML’2011513\\nSwersky,K.,Snoek,J.,andAdams,R.P.(2014).Freeze-thawBayesianoptimization.\\narXivpreprintarXiv:1406.3896.436\\nSzegedy,C.,Liu,W.,Jia,Y.,Sermanet,P.,Reed,S.,Anguelov,D.,Erhan,D.,Vanhoucke,\\nV.,andRabinovich,A.(2014a).Goingdeeperwithconvolutions.Technicalreport,\\narXiv:1409.4842.,,,,,, 2427201258269326347\\nSzegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.J.,and\\nFergus,R.(2014b).Intriguingpropertiesofneuralnetworks.,ICLR a b s/ 1 3 1 2 .6 1 9 9.\\n268271,\\nSzegedy,C.,Vanhoucke,V.,Ioﬀe,S.,Shlens,J.,andWojna,Z.(2015).Rethinkingthe\\nInceptionArchitectureforComputerVision. ., ArXive-prints243322\\nTaigman,Y.,Yang,M.,Ranzato,M.,andWolf,L.(2014).DeepFace:Closingthegapto\\nhuman-levelperformanceinfaceveriﬁcation.In . CVPR’2014100\\nTandy,D.W.(1997).WorksandDays:ATranslationandCommentaryfortheSocial\\nSciences.UniversityofCaliforniaPress.1\\n7 7 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d61a191a-c7f5-408e-bec3-18deb2e23a8d', embedding=None, metadata={'page_label': '787', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nTang,Y.andEliasmith,C.(2010).Deepnetworksforrobustvisualrecognition.In\\nProceedingsofthe27thInternationalConferenceonMachineLearning,June21-24,\\n2010,Haifa,Israel.241\\nTang,Y.,Salakhutdinov,R.,andHinton,G.(2012).Deepmixturesoffactoranalysers.\\narXivpreprintarXiv:1206.4635.489\\nTaylor,G.andHinton,G.(2009).FactoredconditionalrestrictedBoltzmannmachines\\nformodelingmotionstyle.InL.BottouandM.Littman,\\xa0editors,Proceedingsof\\ntheTwenty-sixthInternationalConferenceonMachineLearning(ICML’09),pages\\n1025–1032, Montreal,Quebec,Canada.ACM.685\\nTaylor,G.,Hinton,G.E.,andRoweis,S.(2007).Modelinghumanmotionusingbinary\\nlatentvariables.InB.Schölkopf,J.Platt,andT.Hoﬀman,editors,AdvancesinNeural\\nInformationProcessingSystems19(NIPS’06),pages1345–1352. MITPress,Cambridge,\\nMA.685\\nTeh,Y.,Welling,M.,Osindero,S.,andHinton,G.E.(2003).Energy-basedmodels\\nforsparseovercompleterepresentations.JournalofMachineLearningResearch, 4,\\n1235–1260. 491\\nTenenbaum,J.,deSilva,V.,andLangford,J.C.(2000).Aglobalgeometricframework\\nfornonlineardimensionalityreduction.Science,(5500),2319–2323. ,, 2 9 0 164518533\\nTheis,L.,vandenOord,A.,andBethge,M.(2015).Anoteontheevaluationofgenerative\\nmodels.arXiv:1511.01844.,698719\\nThompson,J.,Jain,A.,LeCun,Y.,andBregler,C.(2014).Jointtrainingofaconvolutional\\nnetworkandagraphicalmodelforhumanposeestimation.In . NIPS’2014360\\nThrun,S.(1995).Learningtoplaythegameofchess.In . NIPS’1994271\\nTibshirani,R.J.(1995).Regressionshrinkageandselectionviathelasso.Journalofthe\\nRoyalStatisticalSocietyB,,267–288. 5 8 236\\nTieleman,T.(2008).TrainingrestrictedBoltzmannmachinesusingapproximationsto\\nthelikelihoodgradient.InW.W.Cohen,A.McCallum, andS.T.Roweis,editors,Pro-\\nceedingsoftheTwenty-ﬁfthInternationalConferenceonMachineLearning(ICML’08),\\npages1064–1071. ACM.612\\nTieleman,T.andHinton,G.(2009).Usingfastweightstoimprovepersistentcontrastive\\ndivergence.InL.BottouandM.Littman,editors,ProceedingsoftheTwenty-sixth\\nInternationalConferenceonMachineLearning(ICML’09),pages1033–1040. ACM.\\n614\\nTipping,M.E.andBishop,C.M.(1999).Probabilisticprincipalcomponentsanalysis.\\nJournaloftheRoyalStatisticalSocietyB,(3),611–622. 6 1 491\\n7 7 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72ecbcd5-ee5a-4980-a6f1-094472be9a34', embedding=None, metadata={'page_label': '788', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nTorralba,A.,Fergus,R.,andWeiss,Y.(2008).Smallcodesandlargedatabasesfor\\nrecognition.InProceedingsoftheComputerVisionandPatternRecognitionConference\\n(CVPR’08),pages1–8.525\\nTouretzky,D.S.andMinton,G.E.(1985).Symbolsamongtheneurons:Detailsof\\naconnectionistinferencearchitecture.\\xa0InProceedingsofthe9thInternationalJoint\\nConferenceonArtiﬁcialIntelligence-Volume1,IJCAI’85,pages238–243,SanFrancisco,\\nCA,USA.MorganKaufmannPublishersInc.17\\nTu,K.andHonavar,V.(2011).\\xa0Ontheutilityofcurriculainunsupervisedlearningof\\nprobabilisticgrammars.In . IJCAI’2011328\\nTuraga,S.C.,Murray,J.F.,Jain,V.,Roth,F.,Helmstaedter,M.,Briggman,K.,Denk,\\nW.,andSeung,H.S.(2010).Convolutionalnetworkscanlearntogenerateaﬃnity\\ngraphsforimagesegmentation.NeuralComputation,(2),511–538. 2 2 360\\nTurian,J.,Ratinov,L.,andBengio,Y.(2010).Wordrepresentations:Asimpleand\\ngeneralmethodforsemi-supervisedlearning.InProc.ACL’2010,pages384–394.535\\nTöscher,A.,Jahrer,M.,andBell,R.M.(2009).\\xa0TheBigChaossolutiontotheNetﬂix\\ngrandprize.480\\nUria,B.,Murray,I.,andLarochelle,H.(2013).Rnade:Thereal-valuedneuralautoregres-\\nsivedensity-estimator.In ., NIPS’2013709710\\nvandenOörd,A.,Dieleman,S.,andSchrauwen,B.(2013).Deepcontent-basedmusic\\nrecommendation.In . NIPS’2013480\\nvanderMaaten,L.andHinton,G.E.(2008).Visualizingdatausingt-SNE.J.Machine\\nLearningRes.,., 9477519\\nVanhoucke,V.,Senior,A.,andMao,M.Z.(2011).Improvingthespeedofneuralnetworks\\nonCPUs.InProc.DeepLearningandUnsupervisedFeatureLearningNIPSWorkshop.\\n444452,\\nVapnik,V.N.(1982).EstimationofDependencesBasedonEmpiricalData.Springer-\\nVerlag,Berlin.114\\nVapnik,V.N.(1995).TheNatureofStatisticalLearningTheory.Springer,NewYork.\\n114\\nVapnik,V.N.andChervonenkis,A.Y.(1971).Ontheuniformconvergenceofrelative\\nfrequenciesofeventstotheirprobabilities.TheoryofProbabilityandItsApplications,\\n1 6,264–280.114\\nVincent,P.(2011).\\xa0Aconnectionbetweenscorematchinganddenoisingautoencoders.\\nNeuralComputation,(7).,, 2 3513515712\\n7 7 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='81ec4e86-8fdd-45e3-a8ee-a1c850376d49', embedding=None, metadata={'page_label': '789', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nVincent,P.andBengio,Y.(2003).ManifoldParzenwindows.In .MITPress. NIPS’2002\\n520\\nVincent,P.,Larochelle,H.,Bengio,Y.,andManzagol,P.-A.(2008).Extractingand\\ncomposingrobustfeatureswithdenoisingautoencoders.In ., ICML2008241515\\nVincent,P.,Larochelle,H.,Lajoie,I.,Bengio,Y.,andManzagol,P.-A.(2010).Stacked\\ndenoisingautoencoders:Learningusefulrepresentationsinadeepnetworkwithalocal\\ndenoisingcriterion.J.MachineLearningRes.,. 1 1515\\nVincent,P.,deBrébisson,A.,andBouthillier,X.(2015).Eﬃcientexactgradientupdate\\nfortrainingdeepnetworkswithverylargesparsetargets.InC.Cortes,N.D.Lawrence,\\nD.D.Lee,M.Sugiyama,andR.Garnett,editors,AdvancesinNeuralInformation\\nProcessingSystems28,pages1108–1116. CurranAssociates,Inc.466\\nVinyals,\\xa0O.,\\xa0Kaiser,\\xa0L.,\\xa0Koo,\\xa0T.,\\xa0Petrov,\\xa0S.,\\xa0Sutskever,\\xa0I.,\\xa0andHinton,\\xa0G.(2014a).\\nGrammarasaforeignlanguage.Technicalreport,arXiv:1412.7449.410\\nVinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2014b).Showandtell:aneuralimage\\ncaptiongenerator.arXiv1411.4555. 410\\nVinyals,O.,Fortunato,M.,andJaitly,N.(2015a).Pointernetworks.arXivpreprint\\narXiv:1506.03134.418\\nVinyals,O.,Toshev,A.,Bengio,S.,andErhan,D.(2015b).Showandtell:aneuralimage\\ncaptiongenerator.In .arXiv:1411.4555. CVPR’2015 102\\nViola,P.andJones,M.(2001).Robustreal-timeobjectdetection.InInternational\\nJournalofComputerVision.449\\nVisin,F.,Kastner,K.,Cho,K.,Matteucci,M.,Courville,A.,andBengio,Y.(2015).\\nReNet:Arecurrentneuralnetworkbasedalternativetoconvolutionalnetworks.arXiv\\npreprintarXiv:1505.00393.395\\nVonMelchner,L.,Pallas,S.L.,andSur,M.(2000).Visualbehaviourmediatedbyretinal\\nprojectionsdirectedtotheauditorypathway.Nature,(6780),871–876. 4 0 4 16\\nWager,S.,Wang,S.,andLiang,P.(2013).Dropouttrainingasadaptiveregularization.\\nInAdvancesinNeuralInformationProcessingSystems26,pages351–359.265\\nWaibel,A.,Hanazawa,T.,Hinton,G.E.,Shikano,K.,andLang,K.(1989).Phoneme\\nrecognitionusingtime-delayneuralnetworks.IEEETransactionsonAcoustics,Speech,\\nandSignalProcessing,,328–339. ,, 3 7 374453459\\nWan,L.,Zeiler,M.,Zhang,S.,LeCun,Y.,andFergus,R.(2013).Regularizationofneural\\nnetworksusingdropconnect.In . ICML’2013266\\nWang,S.andManning,C.(2013).Fastdropouttraining.In . ICML’2013266\\n7 7 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7b9543c9-513e-4d22-b352-d2fcde7a2936', embedding=None, metadata={'page_label': '790', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nWang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014a).Knowledgegraphandtextjointly\\nembedding.InProc.EMNLP’2014.484\\nWang,Z.,Zhang,J.,Feng,J.,andChen,Z.(2014b).\\xa0Knowledgegraphembeddingby\\ntranslatingonhyperplanes.InProc.AAAI’2014.484\\nWarde-Farley,D.,Goodfellow,I.J.,Courville,A.,andBengio,Y.(2014).Anempirical\\nanalysisofdropoutinpiecewiselinearnetworks.In .,, ICLR’2014262266267\\nWawrzynek,J.,Asanovic,K.,Kingsbury,B.,Johnson,D.,Beck,J.,andMorgan,N.\\n(1996).Spert-II:Avectormicroprocessorsystem. ,(3),79–86. Computer 2 9 451\\nWeaver,L.andTao,N.(2001).Theoptimalrewardbaselineforgradient-basedreinforce-\\nmentlearning.InProc.UAI’2001,pages538–545.691\\nWeinberger,K.Q.andSaul,L.K.(2004).Unsupervisedlearningofimagemanifoldsby\\nsemideﬁniteprogramming.In ,pages988–995. , CVPR’2004 164519\\nWeiss,\\xa0Y.,\\xa0Torralba,\\xa0A.,\\xa0andFergus,\\xa0R.(2008).Spectral\\xa0hashing.In,pagesNIPS\\n1753–1760. 525\\nWelling,M.,Zemel,R.S.,andHinton,G.E.(2002).Selfsupervisedboosting.InAdvances\\ninNeuralInformationProcessingSystems,pages665–672.703\\nWelling,\\xa0M.,Hinton,G.E.,\\xa0andOsindero,\\xa0S.(2003a).Learningsparsetopographic\\nrepresentationswithproductsofStudent-tdistributions.In . NIPS’2002680\\nWelling,M.,Zemel,R.,andHinton,G.E.(2003b).Self-supervisedboosting.InS.Becker,\\nS.Thrun,andK.Obermayer,editors,AdvancesinNeuralInformationProcessing\\nSystems15(NIPS’02),pages665–672.MITPress.622\\nWelling,M.,Rosen-Zvi,M.,andHinton,G.E.(2005).Exponentialfamilyharmoniums\\nwithanapplicationtoinformationretrieval.InL.Saul,Y.Weiss,andL.Bottou,\\neditors,AdvancesinNeuralInformationProcessingSystems17(NIPS’04),volume17,\\nCambridge,MA.MITPress.676\\nWerbos,\\xa0P.J.(1981).Applicationsofadvancesinnonlinearsensitivityanalysis.In\\nProceedingsofthe10thIFIPConference,31.8-4.9,NYC,pages762–770.225\\nWeston,J.,Bengio,S.,andUsunier,N.(2010).Largescaleimageannotation:learningto\\nrankwithjointword-imageembeddings.MachineLearning,(1),21–35. 8 1 401\\nWeston,\\xa0J.,\\xa0Chopra,\\xa0S.,\\xa0andBordes,\\xa0A.(2014).Memorynetworks.arXivpreprint\\narXiv:1410.3916.,418485\\nWidrow,B.andHoﬀ,M.E.(1960).Adaptiveswitchingcircuits.In1960IREWESCON\\nConventionRecord,volume4,pages96–104.IRE,NewYork.,,,15212427\\n7 7 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='03eee61a-b993-443f-a94d-6910e9c84539', embedding=None, metadata={'page_label': '791', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nWikipedia(2015).Listofanimalsbynumberofneurons—Wikipedia,thefreeencyclopedia.\\n[Online;accessed4-March-2015].,2427\\nWilliams,C.K.I.andAgakov,F.V.(2002).\\xa0ProductsofGaussiansandProbabilistic\\nMinorComponentAnalysis.NeuralComputation,,1169–1182. 1 4 ( 5 ) 682\\nWilliams,C.K.I.andRasmussen,C.E.(1996).\\xa0Gaussianprocessesforregression.\\xa0In\\nD.Touretzky,M.Mozer,andM.Hasselmo,editors,AdvancesinNeuralInformation\\nProcessingSystems8(NIPS’95),pages514–520.MITPress,Cambridge,MA.142\\nWilliams,R.J.(1992).Simplestatisticalgradient-followingalgorithmsconnectionist\\nreinforcementlearning.MachineLearning,,229–256. , 8 688689\\nWilliams,R.J.andZipser,D.(1989).Alearningalgorithmforcontinuallyrunningfully\\nrecurrentneuralnetworks.NeuralComputation,,270–280. 1 223\\nWilson,D.R.andMartinez,T.R.(2003).Thegeneralineﬃciencyofbatchtrainingfor\\ngradientdescentlearning.NeuralNetworks,(10),1429–1451. 1 6 279\\nWilson,J.R.(1984).Variancereductiontechniquesfordigitalsimulation.American\\nJournalofMathematicalandManagementSciences,(3),277––312. 4 690\\nWiskott,L.andSejnowski,T.J.(2002).Slowfeatureanalysis:Unsupervisedlearningof\\ninvariances.NeuralComputation,(4),715–770. 1 4 494\\nWolpert,D.andMacReady,W.(1997).Nofreelunchtheoremsforoptimization.IEEE\\nTransactionsonEvolutionaryComputation,,67–82. 1293\\nWolpert,D.H.(1996).Thelackofaprioridistinctionbetweenlearningalgorithms.Neural\\nComputation,(7),1341–1390. 8 116\\nWu,R.,Yan,S.,Shan,Y.,Dang,Q.,andSun,G.(2015).Deepimage:Scalingupimage\\nrecognition.arXiv:1501.02876.447\\nWu,Z.(1997).Globalcontinuationfordistancegeometryproblems.SIAMJournalof\\nOptimization,,814–836. 7 327\\nXiong,H.Y.,Barash,Y.,andFrey,B.J.(2011).Bayesianpredictionoftissue-regulated\\nsplicingusingRNAsequenceandcellularcontext. , Bioinformatics 2 7(18),2554–2562.\\n265\\nXu,K.,Ba,J.L.,Kiros,R.,Cho,K.,Courville,A.,Salakhutdinov,R.,Zemel,R.S.,and\\nBengio,Y.(2015).Show,attendandtell:Neuralimagecaptiongenerationwithvisual\\nattention.In .,, ICML’2015,arXiv:1502.03044102410691\\nYildiz,I.B.,Jaeger,H.,andKiebel,S.J.(2012).Re-visitingtheechostateproperty.\\nNeuralnetworks,,1–9. 3 5405\\n7 7 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d41f1d04-acdc-4500-bb82-db6f30a42386', embedding=None, metadata={'page_label': '792', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='BIBLIOGRAPHY\\nYosinski,J.,Clune,J.,Bengio,Y.,andLipson,H.(2014).Howtransferablearefeatures\\nindeepneuralnetworks?In ., NIPS’2014325536\\nYounes,L.(1998).OntheconvergenceofMarkovianstochasticalgorithmswithrapidly\\ndecreasingergodicityrates.InStochasticsandStochasticsModels,pages177–228.612\\nYu,D.,Wang,S.,and\\xa0Deng,\\xa0L.\\xa0(2010).Sequential\\xa0labeling\\xa0using\\xa0deep-structured\\nconditionalrandomﬁelds.IEEEJournalofSelectedTopicsinSignalProcessing.323\\nZaremba,W.andSutskever,I.(2014).Learningtoexecute.arXiv1410.4615. 329\\nZaremba,W.andSutskever,I.(2015).ReinforcementlearningneuralTuringmachines.\\narXiv:1505.00521.419\\nZaslavsky,T.(1975).FacingUptoArrangements:Face-CountFormulasforPartitions\\nofSpacebyHyperplanes.Numberno.154inMemoirsoftheAmericanMathematical\\nSociety.AmericanMathematicalSociety.550\\nZeiler,M.D.andFergus,R.(2014).Visualizingandunderstandingconvolutionalnetworks.\\nIn . ECCV’146\\nZeiler,M.D.,Ranzato,M.,Monga,R.,Mao,M.,Yang,K.,Le,Q.,Nguyen,P.,Senior,\\nA.,Vanhoucke,V.,Dean,J.,andHinton,G.E.(2013).\\xa0Onrectiﬁedlinearunitsfor\\nspeechprocessing.In . ICASSP2013460\\nZhou,B.,Khosla,A.,Lapedriza,A.,Oliva,A.,andTorralba,A.(2015).\\xa0Objectdetectors\\nemergeindeepsceneCNNs.ICLR’2015,arXiv:1412.6856.551\\nZhou,J.andTroyanskaya,O.G.(2014).Deepsupervisedandconvolutionalgenerative\\nstochasticnetworkforproteinsecondarystructureprediction.In . ICML’2014715\\nZhou,Y.andChellappa,R.(1988).Computationofopticalﬂowusinganeuralnetwork.\\nInNeuralNetworks,1988.,IEEEInternationalConferenceon,pages71–78.IEEE.339\\nZöhrer,M.andPernkopf,F.(2014).Generalstochasticnetworksforclassiﬁcation.In\\nNIPS’2014.716\\n7 7 7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e3133b99-8662-4ea0-94cd-b0db9cf7bb7f', embedding=None, metadata={'page_label': '793', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='I n d e x\\n0-1loss,, 1 0 2274\\nAbsolutevaluerectiﬁcation,191\\nAccuracy,420\\nActivationfunction,169\\nActiveconstraint,94\\nAdaGrad,305\\nADALINE, s e eadaptivelinearelement\\nAdam,,307422\\nAdaptivelinearelement,,,152326\\nAdversarialexample,265\\nAdversarialtraining,,,266268526\\nAﬃne,109\\nAIS, s e eannealedimportancesampling\\nAlmosteverywhere,70\\nAlmostsureconvergence,128\\nAncestralsampling,,576591\\nANN, s e eArtiﬁcialneuralnetwork\\nAnnealedimportancesampling,\\xa0,,621662\\n711\\nApproximateBayesiancomputation,710\\nApproximateinference,579\\nArtiﬁcialintelligence,1\\nArtiﬁcialneuralnetwork,\\xa0 s e eNeuralnet-\\nwork\\nASR, s e eautomaticspeechrecognition\\nAsymptoticallyunbiased,123\\nAudio,,,101357455\\nAutoencoder,,,4353 4 9 8\\nAutomaticspeechrecognition,455\\nBack-propagation,201\\nBack-propagationthroughtime, 3 8 1\\nBackprop, s e eback-propagationBagofwords,467\\nBagging,252\\nBatchnormalization,,264422\\nBayeserror, 1 1 6\\nBayes’rule,69\\nBayesianhyperparameteroptimization,433\\nBayesian\\xa0network,\\xa0 s e edirected\\xa0graphical\\nmodel\\nBayesianprobability,54\\nBayesianstatistics, 1 3 4\\nBeliefnetwork, s e edirectedgraphicalmodel\\nBernoullidistribution,61\\nBFGS,314\\nBias,,123227\\nBiasparameter,109\\nBiasedimportancesampling,589\\nBigram,458\\nBinaryrelation,478\\nBlockGibbssampling,595\\nBoltzmanndistribution,566\\nBoltzmannmachine,,566648\\nBPTT, s e eback-propagationthroughtime\\nBroadcasting,33\\nBurn-in,593\\nCAE, s e econtractiveautoencoder\\nCalculusofvariations,178\\nCategoricaldistribution, s e emultinoullidis-\\ntribution\\nCD, s e econtrastivedivergence\\nCenteringtrick(DBM),667\\nCentrallimittheorem,63\\nChainrule(calculus),203\\nChainruleofprobability,58\\n778', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d53c05cb-da8f-4f2c-bd23-86b3ce0a6a86', embedding=None, metadata={'page_label': '794', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INDEX\\nChess,2\\nChord,575\\nChordalgraph,575\\nClass-basedlanguagemodels,460\\nClassicaldynamicalsystem,372\\nClassiﬁcation,99\\nCliquepotential, s e efactor(graphicalmodel)\\nCNN, s e econvolutionalneuralnetwork\\nCollaborativeFiltering,474\\nCollider, s e eexplainingaway\\nColorimages,357\\nComplexcell,362\\nComputationalgraph,202\\nComputervision,449\\nConceptdrift,533\\nConditionnumber,277\\nConditionalcomputation, s e edynamicstruc-\\nture\\nConditionalindependence,,xiii59\\nConditionalprobability,58\\nConditionalRBM,679\\nConnectionism,,17440\\nConnectionisttemporalclassiﬁcation,457\\nConsistency,,128509\\nConstrainedoptimization,,92235\\nContent-basedaddressing,416\\nContent-basedrecommendersystems,475\\nContext-speciﬁcindependence,569\\nContextualbandits,476\\nContinuationmethods,324\\nContractiveautoencoder,516\\nContrast,451\\nContrastivedivergence,,,289606666\\nConvexoptimization,140\\nConvolution,,327677\\nConvolutionalnetwork,16\\nConvolutionalneuralnetwork,,250 3 2 7,,422\\n456\\nCoordinatedescent,,319665\\nCorrelation,60\\nCostfunction, s e eobjectivefunction\\nCovariance,,xiii60\\nCovariancematrix,61\\nCoverage,421Criticaltemperature,599\\nCross-correlation,329\\nCross-entropy,, 7 4131\\nCross-validation,121\\nCTC, s e econnectionisttemporalclassiﬁca-\\ntion\\nCurriculumlearning,326\\nCurseofdimensionality,153\\nCyc,2\\nD-separation,568\\nDAE, s e edenoisingautoencoder\\nDatageneratingdistribution,, 1 1 0130\\nDatageneratingprocess,110\\nDataparallelism,444\\nDataset,103\\nDatasetaugmentation,,268454\\nDBM, s e edeepBoltzmannmachine\\nDCGAN,,,547548695\\nDecisiontree,, 1 4 4544\\nDecoder,4\\nDeepbeliefnetwork,,,,,, 26525626651654\\n678686,\\nDeepBlue,2\\nDeepBoltzmannmachine,,,,, 2326525626\\n647651657666678 ,,,,\\nDeepfeedforwardnetwork,,166422\\nDeeplearning,,25\\nDenoisingautoencoder,,506683\\nDenoisingscorematching,615\\nDensityestimation,102\\nDerivative,,xiii82\\nDesignmatrix, 1 0 5\\nDetectorlayer,336\\nDeterminant,xii\\nDiagonalmatrix,40\\nDiﬀerentialentropy,,73641\\nDiracdeltafunction,64\\nDirectedgraphicalmodel,,,, 76503559685\\nDirectionalderivative,84\\nDiscriminativeﬁne-tuning, s e esupervised\\nﬁne-tuning\\nDiscriminativeRBM,680\\nDistributedrepresentation,,,17149542\\nDomainadaptation,532\\n7 7 9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a92b854a-2bb7-41ec-b908-baa46fe40981', embedding=None, metadata={'page_label': '795', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INDEX\\nDotproduct,,33139\\nDoublebackprop,268\\nDoublyblockcirculantmatrix,330\\nDreamsleep,,605647\\nDropConnect,263\\nDropout,,,,,, 2 5 5422427428666683\\nDynamicstructure,445\\nE-step,629\\nEarlystopping,,,,, 244246270271422\\nEBM, s e eenergy-basedmodel\\nEchostatenetwork,,,2326401\\nEﬀectivecapacity,113\\nEigendecomposition,41\\nEigenvalue,41\\nEigenvector,41\\nELBO, s e eevidencelowerbound\\nElement-wiseproduct, s e eHadamardprod-\\nuct, s e eHadamardproduct\\nEM, s e eexpectationmaximization\\nEmbedding,512\\nEmpiricaldistribution,65\\nEmpiricalrisk,274\\nEmpiricalriskminimization,274\\nEncoder,4\\nEnergyfunction,565\\nEnergy-basedmodel,,,, 565591648657\\nEnsemblemethods,252\\nEpoch,244\\nEqualityconstraint,93\\nEquivariance,335\\nErrorfunction, s e eobjectivefunction\\nESN, s e eechostatenetwork\\nEuclideannorm,38\\nEuler-Lagrangeequation,641\\nEvidencelowerbound,,628655\\nExample,98\\nExpectation,59\\nExpectationmaximization,629\\nExpectedvalue, s e eexpectation\\nExplainingaway,,,570626639\\nExploitation,477\\nExploration,477\\nExponentialdistribution, 6 4F-score,420\\nFactor(graphicalmodel),563\\nFactoranalysis,486\\nFactorgraph,575\\nFactorsofvariation,4\\nFeature,98\\nFeatureselection,234\\nFeedforwardneuralnetwork,166\\nFine-tuning,321\\nFinitediﬀerences,436\\nForgetgate,304\\nForwardpropagation,201\\nFouriertransform,,357359\\nFovea,363\\nFPCD,610\\nFreeenergy,, 5 6 7674\\nFreebase,479\\nFrequentistprobability,54\\nFrequentiststatistics, 1 3 4\\nFrobeniusnorm,45\\nFully-visibleBayesnetwork,699\\nFunctionalderivatives,640\\nFVBN, s e efully-visibleBayesnetwork\\nGaborfunction,365\\nGANs, s e egenerativeadversarialnetworks\\nGatedrecurrentunit,422\\nGaussiandistribution, s e enormaldistribu-\\ntion\\nGaussiankernel,140\\nGaussianmixture,,66187\\nGCN, s e eglobalcontrastnormalization\\nGeneOntology,479\\nGeneralization,109\\nGeneralizedLagrangefunction, s e egeneral-\\nizedLagrangian\\nGeneralizedLagrangian,93\\nGenerativeadversarialnetworks,,683693\\nGenerativemomentmatchingnetworks,696\\nGeneratornetwork,687\\nGibbsdistribution,564\\nGibbssampling,,577595\\nGlobalcontrastnormalization,451\\nGPU, s e egraphicsprocessingunit\\nGradient,83\\n7 8 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1d793fa5-cc66-4e79-8022-91c67e1eb58e', embedding=None, metadata={'page_label': '796', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INDEX\\nGradientclipping,,287411\\nGradientdescent,,8284\\nGraph,xii\\nGraphicalmodel, s e estructuredprobabilis-\\nticmodel\\nGraphicsprocessingunit,441\\nGreedyalgorithm,321\\nGreedylayer-wiseunsupervisedpretraining,\\n524\\nGreedysupervisedpretraining,321\\nGridsearch,429\\nHadamardproduct,,xii33\\nHard,tanh195\\nHarmonium, s e erestrictedBoltzmannma-\\nchine\\nHarmonytheory,567\\nHelmholtz freeenergy, s e eevidencelower\\nbound\\nHessian,221\\nHessianmatrix,,xiii86\\nHeteroscedastic,186\\nHiddenlayer,,6166\\nHillclimbing,85\\nHyperparameteroptimization,429\\nHyperparameters,,119427\\nHypothesisspace,,111117\\ni.i.d.assumptions,,,110121265\\nIdentitymatrix,35\\nILSVRC, s e eImageNetLargeScaleVisual\\nRecognitionChallenge\\nImageNetLargeScaleVisualRecognition\\nChallenge,22\\nImmorality,573\\nImportancesampling,,,588620691\\nImportanceweightedautoencoder,691\\nIndependence,,xiii59\\nIndependentandidenticallydistributed, s e e\\ni.i.d.assumptions\\nIndependentcomponentanalysis,487\\nIndependentsubspaceanalysis,489\\nInequalityconstraint,93\\nInference,,,,,,,, 558579626628630633643\\n646Informationretrieval,520\\nInitialization,298\\nIntegral,xiii\\nInvariance,339\\nIsotropic,64\\nJacobianmatrix,,,xiii7185\\nJointprobability,56\\nk-means,,361542\\nk-nearestneighbors,, 1 4 1544\\nKarush-Kuhn-Tuckerconditions,,94235\\nKarush–Kuhn–Tucker,93\\nKernel(convolution),,328329\\nKernelmachine,544\\nKerneltrick,139\\nKKT, s e eKarush–Kuhn–Tucker\\nKKTconditions, s e eKarush-Kuhn-Tucker\\nconditions\\nKLdivergence, s e eKullback-Leiblerdiver-\\ngence\\nKnowledgebase,,2479\\nKrylovmethods,222\\nKullback-Leiblerdivergence,,xiii 7 3\\nLabelsmoothing,241\\nLagrangemultipliers,,93641\\nLagrangian, s e egeneralizedLagrangian\\nLAPGAN,695\\nLaplacedistribution,, 6 4492\\nLatentvariable,66\\nLayer(neuralnetwork),166\\nLCN, s e elocalcontrastnormalization\\nLeakyReLU,191\\nLeakyunits,404\\nLearningrate,84\\nLinesearch,,,848592\\nLinearcombination,36\\nLineardependence,37\\nLinearfactormodels,485\\nLinearregression,,, 1 0 6109138\\nLinkprediction,480\\nLipschitzconstant,91\\nLipschitzcontinuous,91\\nLiquidstatemachine,401\\n7 8 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80f54bed-b15d-4e02-beef-8dc3aff440b9', embedding=None, metadata={'page_label': '797', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INDEX\\nLocalconditionalprobabilitydistribution,\\n560\\nLocalcontrastnormalization,452\\nLogisticregression,,,3 1 3 8139\\nLogisticsigmoid,,766\\nLongshort-termmemory,,,,1824304 4 0 7,\\n422\\nLoop,575\\nLoopybeliefpropagation,581\\nLossfunction, s e eobjectivefunction\\nLpnorm,38\\nLSTM, s e elongshort-termmemory\\nM-step,629\\nMachinelearning,2\\nMachinetranslation,100\\nMaindiagonal,32\\nManifold,159\\nManifoldhypothesis,160\\nManifoldlearning,160\\nManifoldtangentclassiﬁer,268\\nMAPapproximation,,137501\\nMarginalprobability,57\\nMarkovchain,591\\nMarkovchainMonteCarlo,591\\nMarkovnetwork, s e eundirectedmodel\\nMarkovrandomﬁeld, s e eundirectedmodel\\nMatrix,,,xixii31\\nMatrixinverse,35\\nMatrixproduct,33\\nMaxnorm,39\\nMaxpooling,336\\nMaximumlikelihood, 1 3 0\\nMaxout,,191422\\nMCMC, s e eMarkovchainMonteCarlo\\nMeanﬁeld,,,633634666\\nMeansquarederror,107\\nMeasuretheory,70\\nMeasurezero,70\\nMemorynetwork,,413415\\nMethodof\\xa0steepestdescent,\\xa0 s e egradient\\ndescent\\nMinibatch,277\\nMissinginputs,99\\nMixing(Markovchain),597Mixturedensitynetworks,187\\nMixturedistribution,65\\nMixturemodel,,187506\\nMixtureofexperts,,446544\\nMLP, s e emultilayerperception\\nMNIST,,,2021666\\nModelaveraging,252\\nModelcompression,444\\nModelidentiﬁability,282\\nModelparallelism,444\\nMomentmatching,696\\nMoore-Penrosepseudoinverse,,44237\\nMoralizedgraph,573\\nMP-DBM, s e emulti-predictionDBM\\nMRF(Markov\\xa0RandomField), s e eundi-\\nrectedmodel\\nMSE, s e emeansquarederror\\nMulti-modallearning,535\\nMulti-predictionDBM,668\\nMulti-tasklearning,,242533\\nMultilayerperception,5\\nMultilayerperceptron,26\\nMultinomialdistribution,61\\nMultinoullidistribution,61\\nn-gram, 4 5 8\\nNADE,702\\nNaiveBayes,3\\nNat,72\\nNaturalimage,555\\nNaturallanguageprocessing,457\\nNearestneighborregression, 1 1 4\\nNegativedeﬁnite,88\\nNegativephase,,,466602604\\nNeocognitron,,,,162326364\\nNesterovmomentum,298\\nNetﬂixGrandPrize,,255475\\nNeurallanguagemodel,,460472\\nNeuralnetwork,13\\nNeuralTuringmachine,415\\nNeuroscience,15\\nNewton’smethod,,88309\\nNLM, s e eneurallanguagemodel\\nNLP, s e enaturallanguageprocessing\\nNofreelunchtheorem,115\\n7 8 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dbf1ddab-542a-456b-ae78-ab9bd6fa59d2', embedding=None, metadata={'page_label': '798', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INDEX\\nNoise-contrastiveestimation,616\\nNon-parametricmodel, 1 1 3\\nNorm,,xiv38\\nNormaldistribution,,,6263124\\nNormalequations,,,, 1 0 8108111232\\nNormalizedinitialization,301\\nNumericaldiﬀerentiation, s e eﬁnitediﬀer-\\nences\\nObjectdetection,449\\nObjectrecognition,449\\nObjectivefunction,81\\nOMP-, k s e eorthogonalmatchingpursuit\\nOne-shotlearning,534\\nOperation,202\\nOptimization,,7981\\nOrthodoxstatistics, s e efrequentiststatistics\\nOrthogonalmatchingpursuit,,26 2 5 2\\nOrthogonalmatrix,41\\nOrthogonality,40\\nOutputlayer,166\\nParalleldistributedprocessing,17\\nParameterinitialization,,298403\\nParametersharing,,,,, 249332370372386\\nParametertying, s e eParametersharing\\nParametricmodel, 1 1 3\\nParametricReLU,191\\nPartialderivative,83\\nPartitionfunction,,,564601663\\nPCA, s e eprincipalcomponentsanalysis\\nPCD, s e estochasticmaximumlikelihood\\nPerceptron,,1526\\nPersistentcontrastivedivergence, s e estochas-\\nticmaximumlikelihood\\nPerturbationanalysis, s e ereparametrization\\ntrick\\nPointestimator,121\\nPolicy,476\\nPooling,,327677\\nPositivedeﬁnite,88\\nPositivephase,,,,, 466602604650662\\nPrecision,420\\nPrecision(ofanormaldistribution),,6264\\nPredictivesparsedecomposition,519Preprocessing,450\\nPretraining,,320524\\nPrimaryvisualcortex,362\\nPrincipalcomponentsanalysis,,,, 47145146\\n486626,\\nPriorprobabilitydistribution, 1 3 4\\nProbabilisticmaxpooling,677\\nProbabilisticPCA,,,486487627\\nProbabilitydensityfunction,57\\nProbabilitydistribution,55\\nProbabilitymassfunction,55\\nProbabilitymassfunctionestimation,102\\nProductofexperts,566\\nProductruleofprobability, s e echainrule\\nofprobability\\nPSD, s e epredictivesparsedecomposition\\nPseudolikelihood,611\\nQuadraturepair,366\\nQuasi-Newtonmethods,314\\nRadialbasisfunction,195\\nRandomsearch,431\\nRandomvariable,55\\nRatiomatching,614\\nRBF,195\\nRBM, s e erestrictedBoltzmannmachine\\nRecall,420\\nReceptiveﬁeld,334\\nRecommenderSystems,474\\nRectiﬁedlinearunit,,,, 170191422503\\nRecurrentnetwork,26\\nRecurrentneuralnetwork,375\\nRegression,99\\nRegularization,,,,, 1 1 9119176226427\\nRegularizer,118\\nREINFORCE,683\\nReinforcementlearning,,,, 24105476683\\nRelationaldatabase,479\\nRelations,478\\nReparametrizationtrick,682\\nRepresentationlearning,3\\nRepresentationalcapacity,113\\nRestrictedBoltzmannmachine,\\xa0,\\xa0,353456\\n475583626650651666670 ,,,,,,,\\n7 8 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4a03323-8dd1-40be-86cd-029950bcdd8a', embedding=None, metadata={'page_label': '799', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INDEX\\n672674677,,\\nRidgeregression, s e eweightdecay\\nRisk,273\\nRNN-RBM,679\\nSaddlepoints,283\\nSamplemean,124\\nScalar,,,xixii30\\nScorematching,,509613\\nSecondderivative,85\\nSecondderivativetest,88\\nSelf-information,72\\nSemantichashing,521\\nSemi-supervisedlearning,241\\nSeparableconvolution,359\\nSeparation(probabilisticmodeling),568\\nSet,xii\\nSGD, s e estochasticgradientdescent\\nShannonentropy,,xiii73\\nShortlist,462\\nSigmoid,,xiv s e elogisticsigmoid\\nSigmoidbeliefnetwork,26\\nSimplecell,362\\nSingularvalue, s e esingularvaluedecompo-\\nsition\\nSingularvaluedecomposition,,,43146475\\nSingularvector, s e esingularvaluedecom-\\nposition\\nSlowfeatureanalysis,489\\nSML, s e estochasticmaximumlikelihood\\nSoftmax,,,182415446\\nSoftplus,,,xiv67195\\nSpamdetection,3\\nSparsecoding,,,,, 319353492626686\\nSparseinitialization,,302403\\nSparserepresentation,,,,, 145224251501\\n552\\nSpearmint,433\\nSpectralradius,401\\nSpeechrecognition, s e e \\xa0automaticspeech\\nrecognition\\nSphering, s e ewhitening\\nSpikeand\\xa0slabrestricted\\xa0Boltzmannma-\\nchine,674\\nSPN, s e esum-productnetworkSquarematrix,37\\nssRBM, s e espikeandslabrestrictedBoltz-\\nmannmachine\\nStandarddeviation,60\\nStandarderror,126\\nStandarderrorofthemean,,126276\\nStatistic,121\\nStatisticallearningtheory,109\\nSteepestdescent, s e egradientdescent\\nStochasticback-propagation, s e ereparametriza-\\ntiontrick\\nStochasticgradientdescent,,,, 15149277\\n2 9 2,666\\nStochasticmaximumlikelihood,,608666\\nStochasticpooling,263\\nStructurelearning,578\\nStructuredoutput,,100679\\nStructuredprobabilisticmodel,,76554\\nSumruleofprobability,57\\nSum-productnetwork,549\\nSupervisedﬁne-tuning,,525656\\nSupervisedlearning, 1 0 4\\nSupportvectormachine,139\\nSurrogatelossfunction,274\\nSVD, s e esingularvaluedecomposition\\nSymmetricmatrix,,4042\\nTangentdistance,267\\nTangentplane,511\\nTangentprop,267\\nTDNN, s e etime-delayneuralnetwork\\nTeacherforcing,,379380\\nTempering,599\\nTemplatematching,140\\nTensor,,,xixii32\\nTestset,109\\nTikhonovregularization, s e eweightdecay\\nTiledconvolution,349\\nTime-delayneuralnetwork,,364371\\nToeplitzmatrix,330\\nTopographicICA,489\\nTraceoperator,45\\nTrainingerror,109\\nTranscription,100\\nTransferlearning,532\\n7 8 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='aea83b18-a5a0-4eb8-95e1-1bedf5f064ec', embedding=None, metadata={'page_label': '800', 'file_name': 'deeplearningbook.pdf', 'file_path': 'c:\\\\Users\\\\ashutosh\\\\OneDrive\\\\Desktop\\\\Langchain\\\\llama-index(RAG)\\\\data\\\\deeplearningbook.pdf', 'file_type': 'application/pdf', 'file_size': 16117178, 'creation_date': '2024-05-16', 'last_modified_date': '2024-05-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='INDEX\\nTranspose,,xii32\\nTriangleinequality,38\\nTriangulatedgraph, s e echordalgraph\\nTrigram,458\\nUnbiased,123\\nUndirectedgraphicalmodel,,76503\\nUndirectedmodel,562\\nUniformdistribution,56\\nUnigram,458\\nUnitnorm,40\\nUnitvector,40\\nUniversalapproximationtheorem,196\\nUniversalapproximator,549\\nUnnormalizedprobabilitydistribution,563\\nUnsupervisedlearning,, 1 0 4144\\nUnsupervisedpretraining,,456524\\nV-structure, s e eexplainingaway\\nV1,362\\nVAE, s e evariationalautoencoder\\nVapnik-Chervonenkisdimension,113\\nVariance,,,xiii60227\\nVariationalautoencoder,,683 6 9 0\\nVariationalderivatives, s e efunctionalderiva-\\ntives\\nVariationalfreeenergy, s e eevidencelower\\nbound\\nVCdimension, s e eVapnik-Chervonenkisdi-\\nmension\\nVector,,,xixii31\\nVirtualadversarialexamples,266\\nVisiblelayer,6\\nVolumetricdata,357\\nWake-sleep,,646655\\nWeightdecay,,,, 1 1 7176229428\\nWeightspacesymmetry,282\\nWeights,,15106\\nWhitening,452\\nWikibase,479\\nWikibase,479\\nWordembedding,460\\nWord-sensedisambiguation,480\\nWordNet,479Zero-datalearning, s e ezero-shotlearning\\nZero-shotlearning,534\\n7 8 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-gemini\n",
      "  Using cached llama_index_embeddings_gemini-0.1.6-py3-none-any.whl.metadata (660 bytes)\n",
      "Collecting google-generativeai<0.5.0,>=0.4.1 (from llama-index-embeddings-gemini)\n",
      "  Using cached google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-embeddings-gemini) (0.10.37)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (0.4.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (2.29.0)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (2.18.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (2.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (4.11.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (1.23.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.6.1)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.30.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.31.0)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.7.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.7.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.9.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (1.63.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (4.9)\n",
      "Requirement already satisfied: ply in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.11)\n",
      "Requirement already satisfied: anyio in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2024.4.28)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pydantic->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pydantic->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from tqdm->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2024.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (1.62.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (1.62.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-embeddings-gemini) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-embeddings-gemini) (1.1.1)\n",
      "Using cached llama_index_embeddings_gemini-0.1.6-py3-none-any.whl (2.9 kB)\n",
      "Using cached google_generativeai-0.4.1-py3-none-any.whl (137 kB)\n",
      "Installing collected packages: google-generativeai, llama-index-embeddings-gemini\n",
      "  Attempting uninstall: google-generativeai\n",
      "    Found existing installation: google-generativeai 0.4.1\n",
      "    Uninstalling google-generativeai-0.4.1:\n",
      "      Successfully uninstalled google-generativeai-0.4.1\n",
      "Successfully installed google-generativeai-0.4.1 llama-index-embeddings-gemini-0.1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 1.0.3 requires google-generativeai<0.6.0,>=0.5.2, but you have google-generativeai 0.4.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-embeddings-gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = GeminiEmbedding(\n",
    "    model_name=\"models/embedding-001\", api_key=gemini_api_key, title=\"this is a document\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 800/800 [00:02<00:00, 347.53it/s]\n",
      "Generating embeddings: 100%|██████████| 807/807 [06:13<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "index=VectorStoreIndex.from_documents(documents,embed_model=embed_model,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x271195f25d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-multi-modal-llms-geminiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading llama_index_multi_modal_llms_gemini-0.1.5-py3-none-any.whl.metadata (775 bytes)\n",
      "Requirement already satisfied: google-generativeai<0.5.0,>=0.4.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-multi-modal-llms-gemini) (0.4.1)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-multi-modal-llms-gemini) (0.10.37)\n",
      "Collecting llama-index-llms-gemini<0.2.0,>=0.1.7 (from llama-index-multi-modal-llms-gemini)\n",
      "  Downloading llama_index_llms_gemini-0.1.7-py3-none-any.whl.metadata (683 bytes)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-multi-modal-llms-gemini) (10.3.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (0.4.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (2.29.0)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (2.18.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (2.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (4.11.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (1.23.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.6.1)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.30.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.31.0)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.7.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.7.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.9.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-api-core->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (1.63.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (4.9)\n",
      "Requirement already satisfied: ply in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.11)\n",
      "Requirement already satisfied: anyio in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (4.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2024.4.28)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pydantic->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pydantic->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from tqdm->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2024.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (1.62.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (1.62.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.5.0,>=0.4.1->llama-index-multi-modal-llms-gemini) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from jinja2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\ashutosh\\onedrive\\desktop\\langchain\\myenv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-multi-modal-llms-gemini) (1.1.1)\n",
      "Downloading llama_index_multi_modal_llms_gemini-0.1.5-py3-none-any.whl (3.9 kB)\n",
      "Downloading llama_index_llms_gemini-0.1.7-py3-none-any.whl (4.8 kB)\n",
      "Installing collected packages: llama-index-llms-gemini, llama-index-multi-modal-llms-gemini\n",
      "Successfully installed llama-index-llms-gemini-0.1.7 llama-index-multi-modal-llms-gemini-0.1.5\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-multi-modal-llms-gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.gemini import GeminiMultiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "llm2 = Gemini(api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_eng=index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "retriever=VectorIndexRetriever(index=index,similarity_top_k=4,llm=llm2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ashutosh\\OneDrive\\Desktop\\Langchain\\myenv\\Lib\\site-packages\\llama_index\\core\\llms\\utils.py:41\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     40\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ashutosh\\OneDrive\\Desktop\\Langchain\\myenv\\Lib\\site-packages\\llama_index\\llms\\openai\\utils.py:407\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m query_eng\u001b[38;5;241m=\u001b[39m\u001b[43mRetrieverQueryEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ashutosh\\OneDrive\\Desktop\\Langchain\\myenv\\Lib\\site-packages\\llama_index\\core\\query_engine\\retriever_query_engine.py:49\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.__init__\u001b[1;34m(self, retriever, response_synthesizer, node_postprocessors, callback_manager)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     42\u001b[0m     retriever: BaseRetriever,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     callback_manager: Optional[CallbackManager] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     46\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retriever \u001b[38;5;241m=\u001b[39m retriever\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer \u001b[38;5;241m=\u001b[39m response_synthesizer \u001b[38;5;129;01mor\u001b[39;00m get_response_synthesizer(\n\u001b[1;32m---> 49\u001b[0m         llm\u001b[38;5;241m=\u001b[39m\u001b[43mllm_from_settings_or_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_service_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     50\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m callback_manager_from_settings_or_context(\n\u001b[0;32m     52\u001b[0m             Settings, retriever\u001b[38;5;241m.\u001b[39mget_service_context()\n\u001b[0;32m     53\u001b[0m         ),\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node_postprocessors \u001b[38;5;241m=\u001b[39m node_postprocessors \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m     57\u001b[0m     callback_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     58\u001b[0m         callback_manager \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39mcallback_manager\n\u001b[0;32m     59\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ashutosh\\OneDrive\\Desktop\\Langchain\\myenv\\Lib\\site-packages\\llama_index\\core\\settings.py:264\u001b[0m, in \u001b[0;36mllm_from_settings_or_context\u001b[1;34m(settings, context)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mllm\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ashutosh\\OneDrive\\Desktop\\Langchain\\myenv\\Lib\\site-packages\\llama_index\\core\\settings.py:39\u001b[0m, in \u001b[0;36m_Settings.llm\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[1;32mc:\\Users\\ashutosh\\OneDrive\\Desktop\\Langchain\\myenv\\Lib\\site-packages\\llama_index\\core\\llms\\utils.py:48\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-llms-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-llms-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         )\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     59\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "query_eng=RetrieverQueryEngine(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=query_eng.query(\"What is Vanishing Gradient Problem?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vanishing gradient problem refers to the fact that gradients through a computational graph that contains a path that consists of repeatedly multiplying by a matrix W are also scaled according to diag(λ)t. Vanishing gradients make it difficult to know which direction the parameters should move to improve the cost function.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: The vanishing gradient problem refers to the fact that\n",
      "gradients through a computational graph that contains a path that\n",
      "consists of repeatedly multiplying by a matrix W are also scaled\n",
      "according to diag(λ)t. Vanishing gradients make it difficult to know\n",
      "which direction the parameters should move to improve the cost\n",
      "function.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 44f42350-0abd-4926-8c00-eac4f5289ffa\n",
      "Similarity: 0.7160228826962091\n",
      "Text: CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS\n",
      "byrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal\n",
      "sequence.Repeatedapplicationofthesameparametersgivesrisetoespecially\n",
      "pronounceddiﬃculties. Forexample,supposethatacomputational\n",
      "graphcontainsapaththatconsists ofrepeatedlymultiplyingbyamatrixW.After\n",
      "tsteps,thisisequivalenttomul- tiplying...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: d2390119-93b3-4bad-bb56-89eb60dea29a\n",
      "Similarity: 0.6950327671658745\n",
      "Text: CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS\n",
      "withorthogonal ,therecurrencemaybesimpliﬁedfurtherto Q h( ) t= QΛtQh(\n",
      "0 ). (10.39) Theeigenvaluesareraisedtothepowerof\n",
      "tcausingeigenvalueswithmagnitude\n",
      "lessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto\n",
      "explode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector\n",
      "wil...\n",
      "The vanishing gradient problem refers to the fact that gradients through a computational graph that contains a path that consists of repeatedly multiplying by a matrix W are also scaled according to diag(λ)t. Vanishing gradients make it difficult to know which direction the parameters should move to improve the cost function.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "pprint_response(response,show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
